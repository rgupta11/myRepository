bteq
.LOGON tddb.vantage-ss10.tdap.demos.teradatacloud.io/alice

drop table test_ravi;

create table test_ravi
(
id integer NOT NULL GENERATED ALWAYS AS IDENTITY
           (START WITH 1 
            INCREMENT BY 1 
            MINVALUE 1 
            MAXVALUE 2147483647 
            NO CYCLE),
recordid integer,
data varchar (5000), 
record_time TIMESTAMP(6) DEFAULT CURRENT_TIMESTAMP
);
delete from test_ravi;

select count(*) from test_ravi;
select min(id), max(id) from test_ravi;
select id, record_time from test_ravi where id in (1000001, 2133200);

select id, record_time from test_ravi where id in (select min(id), max(id) from test_ravi);


https://teraworks.teradata.com/display/HACK/Updates
https://teraworks.teradata.com/display/HACK/Navishkaar+-+Teradata+India+Hackathon+2019
Wiki: https://teraworks.teradata.com/display/HACK/Real+time+streaming+using+custom+connector+with+Kafka
Git: https://github.td.teradata.com/teradata-managed-cloud/teradata-kafka-connector

There are few drawbacks with the listener:

-Listeners are not doing any active development on integration with Kafka.
-Listener packages its own Kafka and is not designed to work with customer deployment of Kafka.
-Listenerâ€™s integration with Kafka is with Kafka version 0.11 but current Kafka version is 2.0.1
-Listener performance data is not at its best (thats what I heard). 
-The solution will be deployed on public cloud and will be provided as a service



Use-Case achieved:
-Installed confluent Kafka
-Confluent start
-Create a Topic
-STart a producer and consumer 
-Start a custom consumer 
-Listens to the stream and puts in TD DB


Kafka -> consumer (COnsumer APIs)
Kafka connect sink
https://medium.com/@stephane.maarek/how-to-use-apache-kafka-to-transform-a-batch-pipeline-into-a-real-time-one-831b48a6ad85

Confluent consumer
https://www.confluent.io/blog/tutorial-getting-started-with-the-new-apache-kafka-0-9-consumer-client/

TD JDBC download link
http://downloads.teradata.com/download/connectivity/jdbc-driver


rgupta11 / ra..

Sample java code

https://developer.teradata.com/doc/connectivity/jdbc/reference/current/samplePrograms.html
https://developer.teradata.com/doc/connectivity/jdbc/reference/current/samp/T20301JD.java.txt
https://developer.teradata.com/doc/connectivity/jdbc/reference/current/samp/T20200JD.java.txt

Ser/Deser:
https://docs.confluent.io/current/schema-registry/docs/serializer-formatter.html#

SinkConnectors
SinkTask.

export CONFLUENT_HOME=/home/ravi/work/sw/confluent-5.1.0/	
export CONFLUENT_HOME=/home/ec2-user/ravi/confluent/confluent-5.1.0
export PATH=${CONFLUENT_HOME}/bin:${PATH}
$ confluent start

Using CONFLUENT_CURRENT: /tmp/confluent.PR2LiJJp
Starting zookeeper
zookeeper is [UP]
Starting kafka
kafka is [UP]
Starting schema-registry
schema-registry is [UP]
Starting kafka-rest
kafka-rest is [UP]
Starting connect
connect is [UP]
Starting ksql-server
ksql-server is [UP]
Starting control-center
control-center is [UP]

maven: 
https://docs.confluent.io/1.0.1/installation.html#installation-maven


https://www.youtube.com/watch?v=JiG4IbXPLmQ

kafka-topics.sh --zookeeper 52.36.75.45:2181 --create --replication-factor 1 --partitions 4 --topic testTopic4
kafka-topics --zookeeper localhost:2181 --create --replication-factor 1 --partitions 3 --topic avroTopic
kafka-topics --list

kafka-topics.sh --alter --zookeeper 52.36.75.45:2181 --topic testTopic --partitions 6
kafka-topics.sh  --zookeeper 52.36.75.45:2181 --describe


kafka-topics.sh --zookeeper 52.36.75.45:2181 --create --replication-factor 1 --partitions 4 --topic TopicWith4Partition
kafka-topics.sh --zookeeper 52.36.75.45:2181 --create --replication-factor 1 --partitions 4 --topic testTopic

String producer consumer 
kafka-console-producer --topic testTopic --broker-list localhost:9091
kafka-console-consumer --bootstrap-server localhost:9092 --topic testTopic --from-beginning

Avro producer-consumer
PRODUCERS

kafka-avro-console-producer --broker-list localhost:9091 --topic avroTopic --property parse.key=true --property key.schema='{"type" : "int", "name" : "id"}' --property value.schema='{"type":"record","name":"tdrecord","fields":[{"name":"f1","type":"string"}]}'
kafka-avro-console-producer --broker-list localhost:9091 --topic avroTopic --property value.schema='{"type":"record","name":"tdrecord","fields":[{"name":"f1","type":"string"}, {"name":"f2","type":"string"}]}'
kafka-avro-console-producer --broker-list localhost:9091 --topic avroTopic --property parse.key=true --property key.schema='{"type" : "int", "name" : "id"}' --property value.schema='{ "type" : "record", "name" : "td_schema", "namespace" : "com.teradata", "fields" : [ { "name" : "emp_id", "type" : "int", "doc" : "Id of the employee account" }, { "name" : "year", "type" : "int", "doc" : "year of joining" }, { "name" : "address", "type" : "string", "doc" : "Address" } ], "doc:" : "A basic schema for storing TD messages" }'

kafka-avro-console-producer --broker-list 54.213.148.161:9092 --topic TopicWith4Partition --property value.schema='{"type":"record","name":"tdrecord","fields":[{"name":"f1","type":"string"}]}'


CONSUMERS
kafka-avro-console-consumer --bootstrap-server 54.213.148.161:9092 --topic testTopic4 --from-beginning
kafka-avro-console-consumer --bootstrap-server localhost:9091 --topic avroTopic --from-beginning --property print.key=true

------------Debug-----------
bin/kafka-topics.sh  --zookeeper 52.36.75.45:2181 --list
bin/kafka-topics.sh  --zookeeper 52.36.75.45:2181 --topic TopicWith4Partition --describe
bin/kafka-consumer-groups.sh --bootstrap-server 54.213.148.161:9092 --list
bin/kafka-consumer-groups.sh --bootstrap-server 54.213.148.161:9092 --group KafkaPOCGroup4Partition --describe

{"f1": "value1"}
{"f1": "value1", "f2": "value2"}


KafkaPOC4-7013f68e-4439-4dce-acc6-9d2faea15b54

kafka-avro-console-producer --broker-list localhost:9091 --topic avroTopic --property parse.key=true --property key.schema='{"type" : "int", "name" : "id"}' --property value.schema='{ "type" : "record", "name" : "td_schema", "namespace" : "com.teradata", "fields" : [ { "name" : "emp_id", "type" : "int", "doc" : "Id of the employee account" }, { "name" : "year", "type" : "int", "doc" : "year of joining" }, { "name" : "address", "type" : "string", "doc" : "Address" } ], "doc:" : "A basic schema for storing TD messages" }'

kafka-avro-console-producer --broker-list localhost:9091 --topic avroTopic --property parse.key=true --property key.schema='{"type" : "int", "name" : "id"}' 
--property value.schema='{ "type" : "record", "name" : "td_schema", "namespace" : "com.teradata", "fields" : [ { "name" : "emp_id", "type" : "int", "doc" : "Id of the employee account" }, { "name" : "year", "type" : "int", "doc" : "year of joining" }, { "name" : "address", "type" : "string", "doc" : "Address" } ], "doc:" : "A basic schema for storing TD messages" }'
{"emp_id":13, "year":2012, "address": "Address 1"}


kafka-avro-console-producer --broker-list localhost:9091 --topic testTopic --property value.schema='{"type":"record","name":"myrecord","fields":[{"name":"f1","type":"string"}]}'
{"f1": "value1"}
kafka-avro-console-consumer --bootstrap-server localhost:9091 --topic testTopic


kafka-console-producer --topic testTopic --broker-list 54.213.148.161:9092
kafka-console-consumer --bootstrap-server 54.213.148.161:9092 --topic testTopic --from-beginning


Schema registry:
https://www.youtube.com/watch?v=1OdsRuKXWbM
https://www.youtube.com/watch?v=ig1TnNcULSg

streamsets
1)all into single table
2)shreding into separate columns

streamSets connector
Installed kafka confluent.. 
tested simple producer/consumer
Working on sinkConnector 

Consumption model
Q: Is there any agreement on streaming data vs static data files e.g. CSVs
Q: Sink: The TD database store data in structured form. so prior storing the data, we have to get define the table in TD DB.



Any notification sent to customer after prov
Deepti Mali 


java -jar kafka-connector-1.0-SNAPSHOT.jar 200 testTopic4


















----------------------------------------------------------------
package org.teradata.kafka;

import java.util.Arrays;
import java.util.Properties;

import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.common.serialization.StringDeserializer;

import java.sql.*;

/**
 * Hello world!
 *
 */
public class App {

	public static Connection con;
	public static String tableName = "alice.test_ravi";
	public static String url = "jdbc:teradata://tddb.vantage-ss7.tdap.demos.teradatacloud.io";
	public static String userName = "alice";
	public static String password = "alice";

	public static Connection getConnection() throws Exception {

		try {
			if (null == con || con.isClosed()) {
				Class.forName("com.teradata.jdbc.TeraDriver");
				con = DriverManager.getConnection(url, userName, password);
				System.out.println("Connection created");
			}
		} catch (Exception e) {
			System.out.println("Error creating Connection");
			e.printStackTrace();
			throw e;
		}
		return con;
	}

	public static void main(String[] args) throws Exception {

		System.out.println("Consumer starting...");
		Properties props = new Properties();
		props.put("bootstrap.servers", "localhost:9092");
		props.put("group.id", "KafkaPOC");
		props.put("key.deserializer", StringDeserializer.class.getName());
		props.put("value.deserializer", StringDeserializer.class.getName());
		KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
		consumer.subscribe(Arrays.asList("testTopic"));
		try {
			while (true) {
				ConsumerRecords<String, String> records = consumer.poll(100);
				for (ConsumerRecord<String, String> record : records) {
					System.out.println(record.offset() + ": " + record.value());
					addToDB(record);
					readFromDB(tableName);
				}

			}
		} finally {
			consumer.close();
		}
	}

	private static void readFromDB(String tableName2) {
		
		String sSelAll = " SELECT * FROM " + tableName;
		try {
			Statement stmt = getConnection().createStatement();
			ResultSet rset = stmt.executeQuery(sSelAll);
			System.out.println("****************************");
			while(rset.next()) {
                System.out.println("Record ID: " + rset.getInt(1));
                System.out.println("Data: " + rset.getString(2));
			}
			System.out.println("****************************");
		}catch (Exception e) {
			e.printStackTrace();
			System.out.println("Error in reading table " + tableName);
		}
		
		
	}

	private static void addToDB(ConsumerRecord<String, String> record) throws Exception {

		Connection connection = getConnection();
		Statement stmt = connection.createStatement();
		try {
			String insert = "INSERT INTO alice.test_ravi(recordId, data) VALUES(" + record.offset() + ",'" + record.value() + "')";
			System.out.println(insert);
			stmt.executeQuery(insert);
		} catch (Exception e) {
			e.printStackTrace();
			System.out.println("Error writing to db");
			throw e;
		} finally {
			stmt.close();
			//connection.close();
		}

	}

}

----------------------------------------------------------------
mvn archetype:generate -DgroupId=com.teradata.kafka -DartifactId=kafka-producer -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false

https://stackoverflow.com/questions/4955635/how-to-add-local-jar-files-to-a-maven-project
<dependency>
    <groupId>com.teradata</groupId>
    <artifactId>terajdbc</artifactId>
    <version>4.0</version>
    <scope>system</scope>
    <systemPath>${project.basedir}/lib/TeraJDBC__indep_indep.16.20.00.10/terajdbc4.jar</systemPath>
</dependency>
<dependency>
			<groupId>com.teradata</groupId>
			<artifactId>tdgssconfig</artifactId>
			<version>4.0</version>
			<scope>system</scope> <systemPath>${project.basedir}/lib/TeraJDBC__indep_indep.16.20.00.10/tdgssconfig.jar</systemPath>
		</dependency>


fat jar link
https://www.mkyong.com/maven/create-a-fat-jar-file-maven-shade-plugin/

mvn install:install-file \
   -Dfile= \
   -DgroupId=<group-id> \
   -DartifactId=<artifact-id> \
   -Dversion=<version> \
   -Dpackaging=<packaging> \
   -DgeneratePom=true

   
mvn install:install-file -Dfile=lib/TeraJDBC__indep_indep.16.20.00.10/terajdbc4.jar -DgroupId=com.teradata -DartifactId=terajdbc -Dversion=4.0 -Dpackaging=jar
mvn install:install-file -Dfile=lib/TeraJDBC__indep_indep.16.20.00.10/tdgssconfig.jar -DgroupId=com.teradata -DartifactId=tdgssconfig -Dversion=4.0 -Dpackaging=jar



LifeCycle of the output table. Whatt happens with the table, does it stay or deleted automatically



--property value.schema='{ "type" : "record", "name" : "td_schema_ticketmaster", "namespace" : "com.teradata.ticketmaster", "fields" : [ { "name" : "cust_id", "type" : "string", "doc" : "Id of the customer account" }, { "name" : "address", "type" : "string", "doc" : "Address of the customer" }, { "name" : "no_clicks", "type" : "int", "doc" : "Number of clicks" }, { "name" : "pages_visited", "type" : {"type": "array", "items": "string"}, "doc" : "Name of pages visited" } ], "doc:" : "A basic schema for storing Ticket Master messages" }'



java -jar target/kafka-connector-1.0-SNAPSHOT.jar


curl -O http://packages.confluent.io/archive/5.1/confluent-5.1.0-2.11.tar.gz
cd confluent
bin/confluent start 


sudo scp -i /home/ravi/work/codebase/learning/myRepository/key/rahulskeypair.pem target/kafka-producer-1.0-SNAPSHOT.jar  ec2-user@52.13.184.39:/home/ec2-user/ravi/producer/.
sudo scp -i /home/ravi/work/codebase/learning/myRepository/key/rahulskeypair.pem src/main/java/com/teradata/kafka/producer/schema1.json  ec2-user@52.13.184.39:/home/ec2-user/ravi/producer/.

sudo scp -i /home/ravi/work/codebase/learning/myRepository/key/rahulskeypair.pem target/kafka-connector-1.0-SNAPSHOT.jar   ec2-user@52.13.184.39:/home/ec2-user/ravi/connector/.



java -jar kafka-connector-1.0-SNAPSHOT.jar 
java -jar kafka-producer-1.0-SNAPSHOT.jar /home/ec2-user/ravi/producer/schema1.json 2000

AWS
kafka-cluster -  nohup bin/kafka-server-start.sh config/server.properties &
zookeeper - 
 
 
Comparison with snowFlake
As a service... 

----------------------------
1
   Count(*)
-----------
    1111400


  recordid                 record_time
-----------  --------------------------
     963353  2019-02-22 06:10:39.980000
     629783  2019-02-22 06:04:42.420000

	 
	 
2 6P - 3 C
   Count(*)
-----------
     999600


         id                 record_time
-----------  --------------------------
    6021600  2019-02-22 10:29:23.770000
    4688401  2019-02-22 10:26:21.080000
	 

3) 4p 4Consumer | 100000
	 
	 
-------------------------Server.properties------------------------------------
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# see kafka.server.KafkaConfig for additional details and defaults

############################# Server Basics #############################

# The id of the broker. This must be set to a unique integer for each broker.
broker.id=0

############################# Socket Server Settings #############################

# The address the socket server listens on. It will get the value returned from
# java.net.InetAddress.getCanonicalHostName() if not configured.
#   FORMAT:
#     listeners = listener_name://host_name:port
#   EXAMPLE:
#     listeners = PLAINTEXT://your.host.name:9092
#listeners=PLAINTEXT://:9092
#port = 9092
#advertised.host.name = 54.213.148.161
#advertised.port = 9092

# Hostname and port the broker will advertise to producers and consumers. If not set,
# it uses the value for "listeners" if configured.  Otherwise, it will use the value
# returned from java.net.InetAddress.getCanonicalHostName().
advertised.listeners=PLAINTEXT://54.213.148.161:9092

# Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details
listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL

# The number of threads that the server uses for receiving requests from the network and sending responses to the network
num.network.threads=3

# The number of threads that the server uses for processing requests, which may include disk I/O
num.io.threads=8

# The send buffer (SO_SNDBUF) used by the socket server
socket.send.buffer.bytes=102400

# The receive buffer (SO_RCVBUF) used by the socket server
socket.receive.buffer.bytes=102400

# The maximum size of a request that the socket server will accept (protection against OOM)
socket.request.max.bytes=104857600


############################# Log Basics #############################

# A comma separated list of directories under which to store log files
log.dirs=/tmp/kafka-logs

# The default number of log partitions per topic. More partitions allow greater
# parallelism for consumption, but this will also result in more files across
# the brokers.
num.partitions=1

# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
# This value is recommended to be increased for installations with data dirs located in RAID array.
num.recovery.threads.per.data.dir=1

############################# Internal Topic Settings  #############################
# The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state"
# For anything other than development testing, a value greater than 1 is recommended for to ensure availability such as 3.
offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1

############################# Log Flush Policy #############################

# Messages are immediately written to the filesystem but by default we only fsync() to sync
# the OS cache lazily. The following configurations control the flush of data to disk.
# There are a few important trade-offs here:
#    1. Durability: Unflushed data may be lost if you are not using replication.
#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.
#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to excessive seeks.
# The settings below allow one to configure the flush policy to flush data after a period of time or
# every N messages (or both). This can be done globally and overridden on a per-topic basis.

# The number of messages to accept before forcing a flush of data to disk
#log.flush.interval.messages=10000

# The maximum amount of time a message can sit in a log before we force a flush
#log.flush.interval.ms=1000

############################# Log Retention Policy #############################

# The following configurations control the disposal of log segments. The policy can
# be set to delete segments after a period of time, or after a given size has accumulated.
# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens
# from the end of the log.

# The minimum age of a log file to be eligible for deletion due to age
log.retention.hours=168

# A size-based retention policy for logs. Segments are pruned from the log unless the remaining
# segments drop below log.retention.bytes. Functions independently of log.retention.hours.
#log.retention.bytes=1073741824

# The maximum size of a log segment file. When this size is reached a new log segment will be created.
log.segment.bytes=1073741824

# The interval at which log segments are checked to see if they can be deleted according
# to the retention policies
log.retention.check.interval.ms=300000

############################# Zookeeper #############################

# Zookeeper connection string (see zookeeper docs for details).
# This is a comma separated host:port pairs, each corresponding to a zk
# server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".
# You can also append an optional chroot string to the urls to specify the
# root directory for all kafka znodes.
zookeeper.connect=10.12.0.119:2181

# Timeout in ms for connecting to zookeeper
zookeeper.connection.timeout.ms=6000


############################# Group Coordinator Settings #############################

# The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.
# The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms.
# The default value for this is 3 seconds.
# We override this to 0 here as it makes for a better out-of-the-box experience for development and testing.
# However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup.
group.initial.rebalance.delay.ms=0

-------------------------------------------------------------------------------	 