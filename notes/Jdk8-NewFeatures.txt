Revision + hadoop I/O

thread example + hashMap & concurrentHashMap impl
Garbage collection 
JVM memory model
Performance tuning
SSD II code
http://tutorials.jenkov.com/java-collections/streams.html

-HBase- WAL concept, Transaction, all questions 
-HDFS- editLog concept + questions
-Spark- Revision + questions
-Hadoop questions
-Kerberos implementation
-Protobuf + parquet implementation
-Spring security
-HashMap, concurrentHashMap, arrayBlockingQueue
-Carrier cup questions 
-

lightening spark (4/6) 
Advanced Spark internal (mohit) (5/6)
Advanced Hbase internal (mohit) (6/6)
Advanced Hive internal (mohit)(7/6)
Advanced Oozie internal (mohit)(8/6)
Hadoop real world solutions cook book(9-11/6)
Hadoop map reduce cook book (12-15/6)
Hadoop security() (16-17/6)

Core java2 (17-20) [Serialization + collections + Threading]
Java security(21-23)
JVM internals


28/6 - update resume & LinkedIn
Questions + Design patterns(24-30/6)


Algo books:
Algorithm design by jon kleinberg and eva tardos
Algorithms by Sanjoy dasgupta


YouTube: GoogleTalksArchive

JVM clustering concepts
(Terracotta video: https://www.youtube.com/watch?v=_aCPwRZBLAU) 
-codeless clustering
-heap level instrumentation at VM level
-byteCode injections
-configuration file to tell which thread, object, lock to run in cluster vs local only
-stateless - dump every thing in DB and no data in memory
-stateful - data in memory and is shared

-ProducerConsumer: http://examples.javacodegeeks.com/core-java/util/concurrent/locks-concurrent/condition/java-util-concurrent-locks-condition-example/
https://www.youtube.com/watch?v=FLcXf9pO27w



wall time 
Todos 
-custom Lock
-Fairness example
-Nested Monitor Lockout
-http://www.ibm.com/developerworks/library/j-jtp10264/ (shambhu)
-Java security
-https://0x0fff.com/spark-misconceptions/
-caching LRU etc

-------------------------------Misc---------------------------------------------------
Two terms for the same thing

"Map" is used by Java, C++
"Dictionary" is used by .Net, Python
"Associative array" is used by Javascript, PHP	

---------------------------------------------------------------------------------------
Java Questions
-****************************************************
ConcurrentHashMap
-ConcurrentHashMap  allows concurrent add  and  updates  that lock only certain parts of the internal data structure. Thus,
-If many keys return the same hashcode, performance will deteriorate because buckets are implemented as Lists with O(n) retrieval
-In Java 8, when the buckets become too big, they’re dynamically replaced with sorted trees, which have O(log(n)) retrieval
-Note that this is possible only when the keys are Comparable (for example, String or Number classes).
-JDK8 update:
	-forEach Performs a given action for each (key, value)
	-reduce Combines all (key, value) given a reduction function into a result
	-search Applies a function on each (key, value) until the function produces a non-null result
	-keySet that returns a view of the ConcurrentHashMap as a Set

-****************************************************-****************************************************
Bit operations:
---------------
Left shift: Multiplication
<< (signed left shift)
<<< (un-signed left shift)
Since each left shift has the effect of doubling the original value, programmers frequently use this 
fact as an efficient alternative to multiplying by 2
		byte a = 64; 
		i = a << 2; 
		// two left shift | 64 (0100 0000) twice results in i containing the value 256 (1 0000 0000)

Signed left shift uses multiplication... So this could also be calculated as 2 * (2^1) = 4. Another example [2 << 11] = 2 *(2^11) = 4096

Right shift:
>> (Signed right shift) : If the number is negative, then 1 is used as a filler and if the number is positive, then 0 is used as a filler
>>> (Unsigned right shift) In Java, the operator ‘>>>’ is unsigned right shift operator. It always fills 0 irrespective of the sign of the 
	number

Signed right shift uses division... So this could also be calculated as 4 / (2^1) = 2 Another example [4096 >> 11] = 4096 / (2^11) = 2


------------------------------------------------------------------------------------------------------------------------------------------------
-****************************************************-****************************************************
Unsafe
------

sun.misc.Unsafe class is intended to be only used by core Java classes which is why its authors made its 
only constructor private and only added an equally private singleton instance. The public getter for this 
instances performs a security check in order to avoid its public use:

public static Unsafe getUnsafe() {
  Class cc = sun.reflect.Reflection.getCallerClass(2);
  if (cc.getClassLoader() != null)
    throw new SecurityException("Unsafe");
  return theUnsafe;
}

This looked-up class is then checked for its ClassLoader where a null reference is used to 
represent the bootstrap class loader on a HotSpot virtual machine

You could force the VM to load your application classes using the bootstrap class loader by adding 
it to the -Xbootclasspath

-****************************************************-****************************************************
Create an Instance of a Class Without Calling a Constructor

Using the Unsafe, we can create an instance of ClassWithExpensiveConstructor (or any of its subclasses) 
without having to invoke the above constructor, simply by allocating an instance directly on the heap:


-****************************************************-****************************************************
unsafe - https://dzone.com/articles/understanding-sunmiscunsafe

Native Memory Allocation (using native (off-heap) memory in Java )

Did you ever want to allocate an array in Java that should have had more than Integer.MAX_VALUE entries? 
You can create such an array by allocating native memory

Native memory allocation is used by for example direct byte buffers that are offered in Java's NIO packages.
Other than heap memory, native memory is not part of the heap area and can be used non-exclusively 
for example for communicating with other processes. As a result, Java's heap space is in competition 
with the native space: the more memory you assign to the JVM, the less native memory is left.

By calling Unsafe#allocateMemory(long), the virtual machine allocates the requested amount of native 
memory for you. After that, it will be your responsibility to handle this memory correctly.

Be aware that directly allocated memory is always native memory and therefore not garbage collected. 
You therefore have to free memory explicitly as demonstrated in the above example by a call to Unsafe#freeMemory(long).

-****************************************************-****************************************************

-****************************************************-****************************************************

-****************************************************-****************************************************

-****************************************************-****************************************************

-****************************************************-****************************************************



JDK8 features:
***************
Functional in functional programming means "using functions as first class values," it often has
a secondary nuance of "no interaction during execution between components."


1) Methods and lambdas as first-class citizens:  being able to pass methods around at run-time, and 
	hence making them first-class citizens


		Passing the method reference
		-----------------------------


		Java 8 method reference :: syntax (meaning use this method as a value);
								--
		Java 8 when you write File::isHidden you create a method reference, which can similarly be passed around                        

		Lambdas -- anonymous functions
		----------------------------
		****Lambda expressions can only appear in places where they will be assigned to a variable whose type 
			is a functional interface. For example:
			Runnable r = () -> { System.out.println("hello"); };
			
		For example, you can now write (int x) -> x + 1 to mean "the function that, when called with argument x, returns the 
		value x + 1."
		?? -  	You might wonder why this is necessary because you could define a method add1 inside a
				class MyMathsUtils and then write MyMaths-Utils::add1! Yes, you could, but the new lambda
				syntax is more concise for cases where you don’t have a convenient method and class available.
				
		Predicate: a method is passed as predicate. Means something function-like that takes a value for an argument and returns 
		true or false
		
		Which above style to choose:
		----------------------------
		So you don’t even need to write a method definition that’s used only once; the code is crisper
		and clearer because you don’t need to search to find the code you’re passing. But if such a
		lambda exceeds a few lines in length (so that its behavior isn’t instantly clear), then you should
		instead  use  a  method  reference  to  a  method  with  a  descriptive  name  instead  of  using  an
		anonymous lambda. Code clarity should be your guide.
		
		Behaviour parameterization(passing block of code) is flexible than value parametrzn 
		as value parmeterization is rigid
		
		-Lambda syntax example:
		
				StateOwner stateOwner = new StateOwner();

				stateOwner.addStateListener(
					(oldState, newState) -> System.out.println("State changed")
				);
				The lambda expressions is this part:

				(oldState, newState) -> System.out.println("State changed")
				The lambda expression is matched against the parameter type of the addStateListener() method's parameter. 
				If the lambda expression matches the parameter type (in this case the StateChangeListener interface) , 
				then the lambda expression is turned into a function that implements the same interface as that parameter.

				Java lambda expressions can only be used where the type they are matched against is a single method interface. 
				In the example above, a lambda expression is used as parameter where the parameter type was the StateChangeListener 
				interface. This interface only has a single method. Thus, the lambda expression is matched successfully against 
				that interface.

		-Matching Lambdas to Interfaces
				
				A single method interface is also sometimes referred to as a functional interface. Matching a Java lambda 
				expression against a functional interface is divided into these steps:

				Does the interface have only one method?
				Does the parameters of the lambda expression match the parameters of the single method?
				Does the return type of the lambda expression match the return type of the single method?

				If the answer is yes to these three questions, then the given lambda expression is matched successfully 
				against the interface.
	
		(parameters) -> expression
		or (note the curly braces for statements)
		(parameters) -> { statements; }
		
		return is a control-flow statement. To make this lambda valid, curly braces are required as
		follows: (Integer i) -> {return "Alan" + i;}. 
		
		{} has statements and no {} states expressions
		Note: For {} i.e. multi statement use explicit return statement. 


		-Use case Examples of lambdas
				A boolean expression 			(List<String> list) -> list.isEmpty()
				Creating objects 				() -> new Apple(10)
				Consuming from an object 		(Apple a) -> {
														System.out.println(a.getWeight());
												}
				Select/extract from an object 	(String s) -> s.length()
				Combine two values 				(int a, int b) -> a * b
				Compare two objects 			(Apple a1, Apple a2) -> a1.getWeight().compareTo(a2.getWeight())



	-Because  of  the  idea  of  target  typing,  the  same  lambda  expression  can  be  associated  with
	 different functional interfaces if they have a compatible abstract method signature.
	 
	- Refactoring code:
		 -converting anonymous classes to lambda expressions
		 	Runnable r = () -> sysout("Hello");
		 	-catch: First,  the meanings  of this  and super  are  different for anonymous  classes  and lambda expressions.	
		 			Inside an anonymous class, this refers to the anonymous class itself, but
					inside  a  lambda  it  refers  to  the  enclosing  class
					
					Second,  anonymous  classes  are  allowed to shadow variables from the enclosing class. Lambda expressions can’t 
	
	-Compilation & byteCode info for Lambda
		-javap -c -v ClassName
	
		-anonymous classes have some undesirable characteristics that impact the performance of applications
			-The compiler generates a new class file for each anonymous class.
			-Each new anonymous class introduces a new subtype for a class or interface.
		
		-InvokeDynamic to the rescue	
			-The creation of an extra class has been replaced with an invokedynamic instruction
			-The typical use for this instruction is something like the following:
				def add(a, b) { a + b }
				Here the types of a and b aren’t known at compile time and can change from time to time. For
				this reason, when the JVM executes an invokedynamic for the first time, it consults a bootstrap
				method, implementing the language-dependent logic that determines the actual method to be
				called. The bootstrap method returns a linked call site. 
			
			-A lambda expression is translated into bytecode by putting its body into one of a static method
			 created at runtime. A stateless lambda, one that captures no state from its enclosing scope
			
			-More at http://cr.openjdk.java.net/~briangoetz/lambda/lambda-translation.html
				
			
			Sorting using lambda
			--------------------
			 List<Human> humans = Lists.newArrayList(
				  new Human("Sarah", 10), 
				  new Human("Jack", 12)
				);
				 
				humans.sort((h1, h2) -> h1.getName().compareTo(h2.getName()));
				
			Or
			
			List<Human> humans = Lists.newArrayList(
			  new Human("Sarah", 10), 
			  new Human("Jack", 12)
			);
			 
			Comparator<Human> comparator
			  = (h1, h2) -> h1.getName().compareTo(h2.getName());
			 
			humans.sort(comparator.reversed());

			OR
			Multiple conditions
				List<Human> humans = Lists.newArrayList(
				      new Human("Sarah", 12), 
				      new Human("Sarah", 10), 
				      new Human("Zack", 12)
				    );
				 
				    humans.sort(
				      Comparator.comparing(Human::getName).thenComparing(Human::getAge)
				    );	
			

2) Streams

	Using the Streams API, you don’tneed to think in terms of loops at all. The data processing happens internally inside the library.
	We call this idea internal iteration.


3) Default method

	The Java 8 solution is to break the last link an interface can now contain method signatures for
	which an implementing class doesn’t provide an implementation! So who implements them?
	The missing method bodies are given as part of the interface (hence default implementations)
	rather than in the implementing class.
	
	?? But  wait  a  second a single class can  implement  multiple  interfaces,  right?  So  if  you  have
		multiple default  implementations  in  several  interfaces,  does  that mean  you have  a  form  of
		multiple inheritance in Java? Yes, to some extent! We show in chapter 9 that there are some
		restrictions that prevent issues such as the infamous diamond inheritance problem in C++.


4) Static methods in Interface

5) Optional<T> class
	
	Can help you avoid NullPointer exceptions.  It’s  a  container  object  that  may  or  not  contain  a  value
	
	
6)	(structural) Pattern  matching.

7) Actually in JDK 7--try–catch-finally with try-with-resources
declare resources to be used in a try block with the assurance that the resources will be closed when after execution of that block. The resources declared must implement the AutoCloseable interface
Simply put, to be auto-closed, a resource must be both declared and initialized inside the try, as sho

try (Scanner scanner = new Scanner(new File("testRead.txt"));
    PrintWriter writer = new PrintWriter(new File("testWrite.txt"))) {
    while (scanner.hasNext()) {
    writer.print(scanner.nextLine());
    }
}catch (FileNotFoundException fnfe) {
    fnfe.printStackTrace();
}

Resources that were defined/acquired first will be closed last;

8)


9) Date issues:
---------------
Date:
		Doesn’t represent a date but a point in time with milliseconds precision.  years start from 1900, whereas the months start at index 0
		March 18, 2014
		Date date = new Date(114, 2, 18);
		toString method of the Date class could be quite misleading. It also includes the JVM’s default time zone

Calendar:
		months also start at index 0
		(at leastCalendar got rid of the 1900 offset for the year)

Finally, both Date and Calendar are mutable classes

DateFormat:  DateFormat also comes with its own set of problems. For example, it isn’t thread-safe

JDK 8 
java.time package includes
many new classes to help you: LocalDate, LocalTime, LocalDateTime, Instant, Duration, and Period
LocalDate: all instances are immutable. Has no info about timeZone


--------------------------------JDK 8 in details-----------------------------------------------

Passing code with behavior parameterization
-------------------------------------------

Behavior parameterization is a software development pattern

-------------------------------------------------------Core Java --------------------------------------------------

Anonymous classes
------------------
Anonymous classes are like the local classes (a class defined in a block) that you’re already
familiar with in Java. But anonymous classes don’t have a name. They allow you to declare and
instantiate  a  class  at  the  same  time.


Inner class Vs static inner classes
-----------------------------------
A nested class is a member of its enclosing class. Non-static nested classes (inner classes) have access to other 
members of the enclosing class, even if they are declared private. Static nested classes do not have access to other 
members of the enclosing class.
...

Note: A static nested class interacts with the instance members of its outer class (and other classes) just 
like any other top-level class. In effect, a static nested class is behaviorally a top-level class that has 
been nested in another top-level class for packaging convenience.
There is no need for LinkedList.Entry to be top-level class as it is only used by LinkedList 
(there are some other interfaces that also have static nested classes named Entry, such as Map.Entry - same concept). 
And since it does not need access to LinkedList's members, it makes sense for it to be static - it's a much cleaner approach.

Polymophism
-----------
abstraction Vs encapsulation
? Differences between Abstraction and Encapsulation
Abstraction and encapsulation are complementary concepts. On the one hand, abstraction focuses on the behavior of an object.
On the other hand, encapsulation focuses on the implementation of an object’s behavior.  Encapsulation is usually achieved by
hiding information about the internal state of an object and thus, can be seen as a strategy used in order to provide abstraction.


? What is functional interface ?

A functional interface is any interface that contains only one abstract method. (A functional interface 
may contain one or more default methods or static methods.) Because a functional interface contains only 
one abstract method, you can omit the name of that method when you implement it.

? What is the Difference between JDK and JRE ?

The Java Runtime Environment (JRE) is basically the Java Virtual Machine (JVM) where your Java programs are being executed.
It also includes browser plugins for applet execution. The Java Development Kit (JDK) is the full featured Software Development
Kit for Java, including the JRE, the compilers and tools (like JavaDoc, and Java Debugger), in order for a user to develop, compile
and execute Java applications.



? What is difference between fail-fast and fail-safe ?
The Iterator’s fail-safe property works with the clone of the underlying collection and thus, it is not affected by any modification
in the collection.  All the collection classes in java.util package are fail-fast, while the collection classes in java.util.concurrent
are fail-safe. Fail-fast iterators throw a ConcurrentModificationException, while fail-safe iterator never throws such
an exception.


? What is difference between Array and ArrayList ? When will you use Array over
ArrayList ?
The Array and ArrayList classes differ on the following features:
•  Arrays can contain primitive or objects, while an ArrayList can contain only objects.
•  Arrays have fixed size, while an ArrayList is dynamic.
•  An ArrayList provides more methods and features, such as addAll, removeAll, iterator, etc.
•  For a list of primitive data types, the collections use autoboxing to reduce the coding effort.  However, this approach makes
them slower when working on ?xed size primitive data types.


? What is difference between ArrayList and LinkedList ?
Both the ArrayList and LinkedList classes implement the List interface, but they differ on the following features:
•  An ArrayList is an index based data structure backed by an Array. It provides random access to its elements with a performance
equal to O(1). On the other hand, a LinkedList stores its data as list of elements and every element is linked to its previous and
next element. In this case, the search operation for an element has execution time equal to O(n).
•  The Insertion, addition and removal operations of an element are faster in a LinkedList compared to an ArrayList, because
there is no need of resizing an array or updating the index when an element is added in some arbitrary position inside the
collection.
•  A LinkedList consumes more memory than an ArrayList, because every node in a LinkedList stores two references, one for its
previous element and one for its next element.

? What do you know about the big-O notation and can you give some examples
with respect to different data structures ?
The Big-O notation simply describes how well an algorithm scales or performs in the worst case scenario as the number of ele-
ments in a data structure increases. The Big-O notation can also be used to describe other behavior such as memory consumption.
Since the collection classes are actually data structures, we usually use the Big-O notation to chose the best implementation to
use, based on time, memory and performance. Big-O notation can give a good indication about performance for large amounts
of data.


? What’s the difference between Enumeration and Iterator interfaces ?
Enumeration is twice as fast as compared to an Iterator and uses very less memory. However, the Iterator is much safer compared
to Enumeration, because other threads are not able to modify the collection object that is currently traversed by the iterator. Also,
Iterators allow the caller to remove elements from the underlying collection, something which is not possible with Enumerations.

? What is the difference between HashSet and TreeSet ?
The HashSet is Implemented using a hash table and thus, its elements are not ordered. The add, remove, and contains methods of
a HashSet have constant time complexity O(1). On the other hand, a TreeSet is implemented using a tree structure. The elements
in a TreeSet are sorted, and thus, the add, remove, and contains methods have time complexity of O(logn).

-class loader hierarchy
-WHY NEED CUSTOM CLASS LOADER:

-Serialization & DeSerialization
	Externalizable 
	-Issues with Java serialization(EffectiveJava):
		-A major cost of implementing  Serializable  is that it decreases the flexibility to 
		 change a class's implementation   once   it   has   been   released.
		
		-A second cost of implementing  Serializable  is that it increases the likelihood of bugs 
		 and  security  holes. 
		 	-deserialization  is  a  “hidden  constructor”  with  all  of  the  same  issues  as  other constructors.
		 	 Because there is no explicit constructor, it is easy to forget that you must ensure 
			 that deserialization guarantees all of the invariants established by real constructors and that it 
			 does  not  allow  an  attacker  to  gain  access  to  the  internals  of  the  object  under  construction.
		 	
		-A third  cost  of  implementing  Serializable   is  that  it  increases  the  testing  burden 
		 associated with releasing a new version of a class.
		
		-Classes designed for inheritance (Item 15) should rarely implement  Serializable , and 
		 interfaces should rarely extend it. 
			- it  may  be  impossible  to  write  a  serializable 
				subclass.  Specifically,  it  will  be  impossible  if  the  superclass  does  not  provide  an  accessible 
				parameterless  constructor.  Therefore  you  should  consider  providing  a  parameterless 
				constructor  on  nonserializable  classes  designed  for  inheritance. 
		
		-Inner  classes  (Item  18)  should  rarely,  if  ever,  implement  Serializable .
			-Serializable .   They  use compiler-generated  synthetic  fields  to  store  references  to  
			 enclosing  instances  and  to  store values  of  local  variables  from  enclosing  scopes.  
			 How  these  fields  correspond  to  the  class definition  is  unspecified,  as  are  the  
			 names  of  anonymous  and  local  classes.
			 
		- A static member class can, however, implement  Serializable.  
		
		-Do  not  accept  the  default  serialized  form  without  first  considering  whether  it  is appropriate.
			-Accepting the default serialized form should be a conscious decision on your 
				part  that  this  encoding  is  reasonable  from  the  standpoint  of  flexibility,  performance,  and 
				correctness.  Generally  speaking,  you  should  accept  the  default  serialized  form  only  if  it  is 
				largely  identical  to  the  encoding  that  you  would  choose  if  you  were  designing  a  custom 
				serialized form. 
			-The  default  serialized  form  of  an  object  is  a  reasonably  efficient  encoding  of  the  physical 
				representation  of  the  object  graph  rooted  at  the  object.  In  other words, it describes  the data 
				contained in the object and in every object that is reachable from this object. It also describes 
				the  topology  by  which  all  of  these  objects  are  interlinked.  The  ideal  serialized  form  of  an 
				object  contains  only  the  logical  data  represented  by  the  object.  It  is  independent  of  the 
				physical representation. 
			-The   default   serialized   form   is   likely   to   be   appropriate   if   an   object's   physical 
				representation is identical to its logical content.
				//Good candidate for default serialized form 
				public class Name implements Serializable { 
				    /** 
				     * Last name.  Must be non-null. 
				     * @serial 
				     */ 
				    private String lastName; 
				 
				    /** 
				     * First name.  Must be non-null. 
				     * @serial 
				     */ 
				    private String firstName; 
				    /** 
				     * Middle initial, or '\u0000' if name lacks middle initial. 
				     * @serial 
				     */ 
				    private char   middleInitial; 
				 
				    ... // Remainder omitted 
				}
			-The  presence  of  the  @serial   tag  tells  the  Javadoc  utility  to  place  this  documentation  on  a 
			 special page that documents serialized forms.	
				
			-Even  if  you  decide  that  the  default  serialized  form  is  appropriate,  you  often  must 
				provide a  readObject  method to ensure invariants and security. 	
			-In the case of  Name,  the readObject  method could ensure that  lastName  and  firstName  were non-null.	
			
			- If  an  instance  is  serialized  in  a  later  version  and  deserialized  in  an  earlier 
				version, the added fields will be ignored. Had the earlier version's  readObject  method failed 
				to invoke  defaultReadObject, the deserialization would fail with a  StreamCorruptedException.  		

? Why versioning of file is required in serilztn and deserialztn

When Java objects use serialization to save state in files, or as blobs in databases, the potential arises that the version of a 
class reading the data is different than the version that wrote the data.
A compatible change is a change that does not affect the contract between the class and its callers.	

SerialVersionUID is a must in serialization process. But it is optional for the developer to add it in java source file. 
If you are not going to add it in java source file, serialization runtime will generate a serialVersionUID and associate 
it with the class. The serialized object will contain this serialVersionUID along with other data.	
Even though serialVersionUID is a static field, it gets serialized along with the object. 
This is one exception to the general serialization rule that, “static fields are not serialized”

Javadocs says,

“the default serialVersionUID computation is highly sensitive to class details that may vary depending 
on compiler implementations, and can thus result in unexpected InvalidClassExceptions during deserialization”

Now you know why we should declare a serialVersionUID.

-Serialization does not preserve memory address in Java


-Immutable class
	To create immutable class in java, you have to do following steps.

	-Declare the class as final so it can’t be extended.
	-Make all fields private so that direct access is not allowed.
	-Don’t provide setter methods for variables
	-Make all mutable fields final so that it’s value can be assigned only once.
	-Initialize all the fields via a constructor performing deep copy.
	-Perform cloning of objects in the getter methods to return a copy rather than returning the actual object reference.

-marker interface

-memory leak
-How would you improve performance of a Java application
	-Manage Pools of reusable objects – thread, JDBC connection pool
	-Optimize I/O – use buffers
	-Avoid N/W trips
	-Memory mamangement
	-Vectors Vs ArrrayList, lazy initialization
	-Static - always make it a point to nullify the references as soon as you reach at a point in your code where 
	 the use of the static member is over.


	 ? What is structure of Java Heap ? What is Perm Gen space in Heap ?
The JVM has a heap that is the runtime data area from which memory for all class instances and arrays is allocated. It is created
at the JVM start-up. Heap memory for objects is reclaimed by an automatic memory management system which is known as a
garbage collector.  Heap memory consists of live and dead objects.  Live objects are accessible by the application and will not
be a subject of garbage collection. Dead objects are those which will never be accessible by the application, but have not been
collected by the garbage collector yet.  Such objects occupy the heap memory space until they are eventually collected by the
garbage collector.


50 
What is Singleton class?

XML
-xsd
-xpath

collection
-----------
-Every object has a default hash code. That hash code is derived from the object's memory address.
-Strings (s and t) have the same hash value because, for strings, the hash values are derived from their contents.


-Linked list & ArrayList: ordered data structure
-Linked lists and arrays let you specify in which order you want to arrange the elements.

HashTable
-HashTable: If you don't care about the ordering of the elements, then there are data structures 
		  that let you find elements much faster. No control over the order.
		  -Hashtable methods are synchronized.
		  -A hash table is an array of linked lists. Each list is called a bucket. 
		  -Hash table computes an integer, called the hash code, for each object
		  -Some researchers believe that it is a good idea to make the size of the hash table a prime number 
		   to prevent a clustering of keys.
		  -Load factor determines when a hash table is rehashed
		  -HashTable can be used to implement other DS like Set	

-Set: A set is a collection of elements without duplicates
	-HashSet:
	-LinkedHashSet(since 1.4) that keeps track of the order in which the elements are added to the set.
		-The iterator of a LinkedHashSet visits the elements in insertion order. That gives you an ordered collection with fast element lookup.
		- 
	-The TreeSet class is similar to the hash set, with one added improvement. A tree set is a sorted collection.	
		-You insert elements into the collection in any order. When you iterate through the collection, the values 
		 are automatically presented in sorted order
		-Every time an element is added to a tree, it is placed into its proper sorting position. 
		 Therefore, the iterator always visits the elements in sorted order
	
	
-ConcurrentModificationException


Spring 
-bean lifecycle
-dependency injection 
-autowiring
-With imp

J2EE
Diff b/w app server and webServer
*********************************************************
functional interface
----------------------
A functional interface is any interface that contains only one abstract method. (A functional interface 
may contain one or more default methods or static methods.) Because a functional interface contains only 
one abstract method, you can omit the name of that method when you implement it.

interface CheckPerson {
    boolean test(Person p);
}

The JDK defines several standard functional interfaces, which you can find in the package java.util.function
For example, you can use the Predicate<T> interface in place of CheckPerson. This interface contains the method boolean test(T t)
The interface Predicate<T> is an example of a generic interface.

public static void printPersonsWithPredicate(
    List<Person> roster, Predicate<Person> tester) {
    for (Person p : roster) {
        if (tester.test(p)) {
            p.printPerson();
        }
    }
}


---Same as above---- P is predicate generic interface

printPersonsWithPredicate(
    roster,
    p -> p.getGender() == Person.Sex.MALE
        && p.getAge() >= 18
        && p.getAge() <= 25
);
*********************************************************
Lambda Expressions:
------------------
https://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html
functional interface: The JDK defines several standard functional interfaces, which you can find in the 
package java.util.function


The syntax of anonymous classes is bulky considering that the CheckPerson interface contains only one method. 
In this case, you can use a lambda expression instead of an anonymous class
*********************************************************
Design

-Implementing  Serializable   is  a  serious commitment that should be made with care. 
 Extra caution is warranted if a class is designed for  inheritance .
 
-Take special care when making a class serializable if that class will be used in inheritance  
	-Classes designed for inheritance (Item 15) should rarely implement  Serializable , and 
	 interfaces should rarely extend it.
	-Therefore  you  should  consider  providing  a  parameterless constructor  on  nonserializable  
	 classes  designed  for  inheritance

-	 
	 
Design patterns
--------------------
-Singleton | http://www.javaworld.com/article/2073352/core-java/simply-singleton.html?page=2
	-java developers implement singletons, with code examples for multithreading, classloaders, and serialization using the Singleton pattern.
	-public class ClassicSingleton {
	   private static ClassicSingleton instance = null;
	   private ClassicSingleton() {
		  // Exists only to defeat instantiation.
	   }
	   public static ClassicSingleton getInstance() {
		  if(instance == null) {
			 instance = new ClassicSingleton();
		  }
		  return instance;
	   }
	}
	-If it not intended to be subclassed, mark it final
	-It's possible to have multiple singleton instances if classes loaded by different classloaders access a singleton. 
		- private static Class getClass(String classname) throws ClassNotFoundException {
				  ClassLoader classLoader = Thread.currentThread().getContextClassLoader();
				  if(classLoader == null)
					 classLoader = Singleton.class.getClassLoader();
			  return (classLoader.loadClass(classname));
		   }
		}
		The preceding method tries to associate the classloader with the current thread; if that classloader is null, 
		the method uses the same classloader that loaded a singleton base class. The preceding method can be used 
		instead of Class.forName().
	
	-If ClassicSingleton implements the java.io.Serializable interface, the class's instances can be serialized and 
	 deserialized. However, if you serialize a singleton object and subsequently deserialize that object more than 
	 once, you will have multiple singleton instances
		- If you serialize a singleton and then deserialize it twice, you will have two instances of your singleton, 
		  unless you implement the readResolve() method, like this:

			Example 12. A serializable singleton	

			import org.apache.log4j.Logger;
			public class Singleton implements Serializable {
			   public static Singleton INSTANCE = new Singleton();
			   protected Singleton() {
				  // Exists only to thwart instantiation.
			   }
				  private Object readResolve() {
						return INSTANCE;
				  }
			}
	 
	 
	-ThreadSafety: ClassicSingleton class is not thread-safe. If two threads—we'll call them Thread 1 and 
	 Thread 2—call ClassicSingleton.getInstance() at the same time, two ClassicSingleton instances can be 
	 created if Thread 1 is preempted just after it enters the if block and control is subsequently given to Thread 2. 
		-public static Singleton getInstance() {
		  if(singleton == null) {
			 synchronized(Singleton.class) {
			   if(singleton == null) {
				 singleton = new Singleton();
			   }
			}
		  }
		  return singleton;
		}
		Even double check does not guarantee thread safety
		so safest....
		
		public class Singleton {
		   public final static Singleton INSTANCE = new Singleton();
		   private Singleton() {
				 // Exists only to defeat instantiation.
			  }
		}
	-
	 

visitor pattern is a common pattern used to walk through a family of classes


-Design by Contract (DBC) 

Is as simple as possible
Is as clear as possible
Has no ambiguity
Is completely accurate
Allows the reader to completely ignore implementation details (unless there is a bug)
Pours understanding into the mind of the reader as quickly as possible, with little chance for misunderstanding

The fundamental idea of Design By Contract is to treat the services offered by a class or interface as a contract 
between the class (or interface) and its caller
Contract as being made of two parts:
	requirements upon the caller made by the class
	promises made by the class to the caller

Requirements must be stated in javadoc, and may be enforced by throwing checked or unchecked exceptions 
when the stated conditions are violated.
Promises are stated in javadoc.


-Program to an interface and not to an implementation.
-Favor object composition over inheritance.

UML:
class diagrams: 
	-member's visibility, 
		-where + means public, - means private, and # means protected. 
	-In UML, methods whose names are written in italics are abstract
	-Static methods are shown underlined.
	
	-Inheritance is represented using a solid line and a hollow triangular arrow.
	-An interface looks much like inheritance, except that the arrow has a dotted line tail.
		-name <<interface>> is shown enclosed within double angle brackets
	-Composition: how objects are contained in other objects. For example, a Company might include one Employee and one Person 
		-By solid line with no arrow head
		-The lines between classes show that there can be 0 to 1 instances of Person in Company and 0 to 1 
		 instances of Employee in Company.
		-object composition as a single line with either an * or 0, *
		-
	-Aggregation
		-Some writers use a hollow and a solid diamond arrowhead to indicate containment of aggregates
	
	-Composition Vs Aggregation
		-Simple rules:

		A "owns" B = Composition : B has no meaning or purpose in the system without A
		A "uses" B = Aggregation : B exists independently (conceptually) from A
		Example 1:

		A Company is an aggregation of People. A Company is a composition of Accounts. 
		When a Company ceases to do business its Accounts cease to exist but its People continue to exist.

		Example 2: (very simplified)

		A Text Editor owns a Buffer (composition). A Text Editor uses a File (aggregation). 
		When the Text Editor is closed, the Buffer is destroyed but the File itself is not destroyed.
		
	-UML notations for different kind of dependency between two classes enter image description here
	
		association: solidLine with arrow
		dependency: dottedLine with arrow
		inheritence: solidLine with hollow traingular arrow head
		interface/impl: dotted line hollow traingular arrow head
		Aggregation: solidLine with hollow diamond head
		Composition: solidLine with black filled diamond head
		
	
Structural
----------
-proxy: Proxy is used to control access to its implementation
-state: State allows you to change the implementation dynamically

	-Proxy is simply a special case of State. 
	-Both Proxy and State provide a surrogate class that you use in your code. 
	 The real class that does the work is hidden behind this surrogate class
	-Structurally, the difference between Proxy and State is simple: a Proxy 
	 has only one implementation, while State has more than one 
	-Of course, it isn’t necessary that Implementation have the same 
	 interface as Proxy; as long as Proxy is somehow “speaking for” the class 
	 that it is referring method calls to then the basic idea is satisfied. 
	
	-The common uses for Proxy as described in Design Patterns are: 
		1. 	Remote proxy. This proxies for an object in a different address space. 
			A remote proxy is created for you automatically by the RMI compiler rmic as it creates stubs and skeletons. 
		2.	Virtual proxy. This provides “lazy initialization” to create expensive objects on demand. 
		3.	Protection proxy. Used when you don’t want the client programmer to have full 
			access to the proxied object. 
		4.	Smart reference. To add additional actions when the proxied object is accessed. 
			For example, or to keep track of the number of references that are held for a 
			particular object, in order to implement the copy-on-write idiom and prevent 
			object aliasing. A simpler example is keeping track of the number of calls to a 
			particular method. 
	 

-dynamicProxies
	-Spring AOP uses it extensively, it internally creates a dynamic proxy for different AOP constructs
	-For any frameworks needing to support interface and annotation based features A real proxied 
	 instance need not even exist, a dynamic proxy can recreate the behavior expected of an interface, 
	 based on some meta-data provided through annotations

*********************************************************
Pipeline & Streaming
--------------------


*********************************************************
Collection
----------

*********************************************************
Threads
--------
-Thread
	Thread states
	Diff b/w sleep and wait

-The thread communication can take place via queues, pipes, unix sockets, TCP sockets etc. Whatever fits your system.
-Thread synchronization can be achieved using a 
	synchronized block of Java code. 
	locks or 
	atomic variables like java.util.concurrent.atomic.AtomicInteger.
-Local object reference
		Local references to objects are a bit different. The reference itself is not shared. The object referenced however, 
		is not stored in each threads's local stack. All objects are stored in the shared heap
		
		If an object created locally never escapes the method it was created in, it is thread safe. In fact you can 
		also pass it on to other methods and objects as long as none of these methods or objects make the passed 
		object available to other threads

-If a resource is created, used and disposed within the control of the same thread,
	and never escapes the control of this thread, the use of that resource is thread safe.

-Immutable objects: We can make sure that objects shared between threads are never updated by any of the 
 threads by making the shared objects immutable, and thereby thread safe.

-The Reference is not Thread Safe
	The ImmutableValue class is thread safe, but the use of it is not
		public class Calculator{
		  private ImmutableValue currentValue = null;

		  public ImmutableValue getValue(){
			return currentValue;
		  }

		  public void setValue(ImmutableValue newValue){
			this.currentValue = newValue;
		  }

		  public void add(int newValue){
			this.currentValue = this.currentValue.add(newValue);
		  }
		}
		
		To make the Calculator class thread safe you could have declared the getValue(), setValue(), 
		and add() methods synchronized. That would have done the trick.
		
		
-
	
-cached thread pool
			Basically imagine each thread from the pool doing this
			public void run() {
				while(true) {
					if(tasks available) {
					   Runnable task = taskqueue.dequeue();
					   task.run();
					} else {
					   // wait or whatever
					}
				}
			}

Threads:
? Volatile: Why would you need volatile keyword when member varaibles are shared across threads and will have same value.
			Ans: If your computer contains more than one CPU, each thread may run on a different CPU
			Java memory model and CPU architecture, main memory->cache->register. Value is flushed to main memory 
			from cache. It may be possible 1 thread update shared var value but its still in cache another thread 
			may read stale value from main memory(RAM). Volatile ensures any update is directly written to main memory.
			It ensures every read and write to volatile variable will be done to main memory than cache.

? Monitor Vs semaphore Vs mutex
		Mutex:
		Used to provide mutual exclusion i.e. ensures at most one process can do something (like execute a 
		section of code, or access a variable) at a time.
		A famous analogy is the bathroom key in a Starbucks; only one person can acquire it, 
		therefore only that one person may enter and use the bathroom. Everybody else who wants to 
		use the bathroom has to wait till the key is available again.

		Monitor:
		Provides mutual exclusion to an object i.e. at any point in time, at most one process may access any 
		of the object's members/ execute any of its methods. This is ideologically similar to a mutex for an 
		entire OOP instance*; no part of the instance can be touched by more than one process at a time.

		Semaphore:
		Is a counter which grants count number**of accesses to a resource at a time. So if a semaphore has 
		initial count = 5, the resource it protects may be accessed by at most 5 requestors*** at a time; 
		the 6th requestor onwards will have to wait for an empty 'slot'.
		Think of a patient's hospital room that has 5 seats; the nurse allows visitors to enter until all 
		5 seats are occupied. Any other visitors must wait outside until a seat becomes vacant, at which 
		point the nurse allows one to enter.

		Semaphores are typically used as a signaling mechanism between processes.

? Is synchronized block reentrant - yes
	Synchronized blocks in Java are reentrant. This means, that if a Java thread enters a synchronized block of code, 
	and thereby take the lock on the monitor object the block is synchronized on, the thread can enter other Java 
	code blocks synchronized on the same monitor object.
? Process Vs thread
? Serialization does not preserve memory address in Java

? What is Java Priority Queue ?
	The PriorityQueue is an unbounded queue, based on a priority heap and its elements are ordered in their natural order. 
	At the time of its creation, we can provide a Comparator that is responsible for ordering the elements of the PriorityQueue. 
	A PriorityQueue doesn’t allow null values, those objects that doesn’t provide natural ordering, or those objects that 
	don’t have any comparator associated with them. Finally, the Java PriorityQueue is not thread-safe and it requires 
	O(log(n)) time for its enqueing and dequeing operations.

? What is the difference between a synchronized method and a synchronized block ?
	In Java programming, each object has a lock.  A thread can acquire the lock for an object by using the synchronized keyword.
	The synchronized keyword can be applied in a method level (coarse grained lock) or block level of code (?ne grained lock).

? How do you ensure that N threads can access N resources without deadlock ?
	A very simple way to avoid deadlock while using N threads is to impose an ordering on the locks and force each thread to follow
	that ordering. Thus, if all threads lock and unlock the mutexes in the same order, no deadlocks can arise.


Locks:
	Locking default rules	
	- Always lock during updates to object fields. 
	- Always lock during access of possibly updated object fields. 
	- Never lock when invoking methods on other objects.

	-Many concurrent programs use final extensively, in part as helpful, automatically enforced documentation 
	 of design decisions that reduce the need for synchronization
	 
	- 




Java code to get Free memory:
-----------------------------
http://stackoverflow.com/questions/17374743/how-can-i-get-the-memory-that-my-java-program-uses-via-javas-runtime-api


Context Switching Overhead
--------------------------
When a CPU switches from executing one  to executing another, the CPU needs to save the local data, 
program pointer etc. of the current thread, and load the local data, program pointer etc. of the next thread 
to execute. This switch is called a "context switch"
https://en.wikipedia.org/wiki/Context_switch

Context switching overhead
-thread needs some resources from the computer
-thread needs some memory to keep its local stack.

Java object size:
computing an object's size is using the Instrumented class from Java's attach API which offers a dedicated 
method for this purpose called getObjectSize.

ORACLE 9i for Programmers & Database Administrators

missed signal - while loop instead of if
slipped signal - check twice or use 1 syncronized block
spurious wakeup: syncronized on member variable of String const literal, notify will wake other unwanted threads.. more read
deadlock:
starvation: fairness
Blocked Vs Waiting
Reentrant
Locks should be Reentrance. Thread should be given lock on object when already have lock on same object,
	in cases requesting lock again will block the thread and deadlock will arise.
A reentrance lockout: If a thread calls lock() twice without calling unlock() in between, the second call to lock() will block. 
					  A reentrance lockout has occurred.

synchronizers: locks, semaphores, blocking queue etc


Concurrency:

-Parallel Workers
-Assembly Line (event driven)
-Function parallelism:
	(JDK 7)ForkAndJoinPool which can help you implement something similar to functional parallelism.

-If a resource is created, used and disposed within the control of the same thread, and never 
	escapes the control of this thread, the use of that resource is thread safe.
	
synchronized
------------
The object taken in the parentheses by the synchronized construct is called a monitor object


Volatile:
--------
Why would you need volatile keyword when member varaibles are shared across threads and will have same value.
Ans: If your computer contains more than one CPU, each thread may run on a different CPU
Java memory model and CPU architecture, main memory->cache->register. Value is flushed to main memory 
from cache. It may be possible 1 thread update shared var value but its still in cache another thread 
may read stale value from main memory(RAM). Volatile ensures any update is directly written to main memory.
It ensures every read and write to volatile variable will be done to main memory than cache.

When a thread writes to a volatile variable, then not just the volatile variable itself is written to main memory. 
Also all other variables changed by the thread before writing to the volatile variable are also flushed to main memory. 
When a thread reads a volatile variable it will also read all other variables from main memory which were flushed 
to main memory together with the volatile variable.

Developers may use this extended visibility guarantee to optimize the visibility of variables between threads. 
Instead of declaring each and every variable volatile, only one or a few need be declared volatile

Volatile works without synchronized as long as only Thread 1 calls put() and only Thread 2 calls take().

Java volatile happens before guarantee:
the volatile keyword comes with a "happens before guarantee". The happens before guarantee guarantees 
that read and write instructions of volatile variables cannot be reordered. Instructions before and after 
can be reordered, but the volatile read/write instruction cannot be reordered with any instruction 
occurring before or after it.


Thread signaling - wait, notify(), notifyAll() 
----------------------------------------------
http://tutorials.jenkov.com/java-concurrency/thread-signaling.html

Missed signals - if
Spurious Wakeups - use while loop instead of if 

Multiple Threads Waiting for the Same Signals
The while loop is also a nice solution if you have multiple threads waiting, which are all awakened 
using notifyAll(), but only one of them should be allowed to continue. Only one thread at a time will 
be able to obtain the lock on the monitor object, meaning only one thread can exit the wait() call 
and clear the wasSignalled flag. Once this thread then exits the synchronized block in the doWait() 
method, the other threads can exit the wait() call and check the wasSignalled member variable inside 
the while loop. However, this flag was cleared by the first thread waking up, so the rest of the awakened 
threads go back to waiting, until the next signal arrives.


Don't call wait() on constant String's or global objects: Don't use global objects, string constants 
etc. for wait() / notify() mechanisms. Use an object that is unique to the construct using it 

The problem with calling wait() and notify() on the empty string, or any other constant string is, 
that the JVM/Compiler internally translates constant strings into the same object. That means, that 
even if you have two different MyWaitNotify instances, they both reference the same empty string instance. 
This also means that threads calling doWait() on the first MyWaitNotify instance risk being awakened by 
doNotify() calls on the second MyWaitNotify instance.


Remember, that even if the 4 threads call wait() and notify() on the same shared string instance, 
the signals from the doWait() and doNotify() calls are stored individually in the two MyWaitNotify 
instances. A doNotify() call on the MyWaitNotify 1 may wake threads waiting in MyWaitNotify 2, but 
the signal will only be stored in MyWaitNotify 1.

At first this may not seem like a big problem. After all, if doNotify() is called on the second 
MyWaitNotify instance all that can really happen is that Thread A and B are awakened by mistake. 
This awakened thread (A or B) will check its signal in the while loop, and go back to waiting because 
doNotify() was not called on the first MyWaitNotify instance, in which they are waiting. This situation 
is equal to a provoked spurious wakeup. Thread A or B awakens without having been signaled. But the code 
can handle this, so the threads go back to waiting.

The problem is, that since the doNotify() call only calls notify() and not notifyAll(), only one thread 
is awakened even if 4 threads are waiting on the same string instance (the empty string). So, if one of 
the threads A or B is awakened when really the signal was for C or D, the awakened thread (A or B) will 
check its signal, see that no signal was received, and go back to waiting. Neither C or D wakes up to check 
the signal they had actually received, so the signal is missed. This situation is equal to the missed signals 
problem described earlier. C and D were sent a signal but fail to respond to it.

If the doNotify() method had called notifyAll() instead of notify(), all waiting threads had been awakened 
and checked for signals in turn. Thread A and B would have gone back to waiting, but one of either C or D 
would have noticed the signal and left the doWait() method call. The other of C and D would go back to 
waiting, because the thread discovering the signal clears it on the way out of doWait().


You may be tempted then to always call notifyAll() instead notify(), but this is a bad idea performance wise.
There is no reason to wake up all threads waiting when only one of them can respond to the signal.

Slipped signal:
Slipped conditions means, that from the time a thread has checked a certain condition until it acts upon it, 
the condition has been changed by another thread so that it is errornous for the first thread to act

Reentrant
Synchronized blocks in Java are reentrant. This means, that if a Java thread enters a synchronized block of 
code, and thereby take the lock on the monitor object the block is synchronized on, the thread can enter 
other Java code blocks synchronized on the same monitor object


Thread local
------------
http://stackoverflow.com/questions/1202444/how-is-javas-threadlocal-implemented-under-the-hood
thinking in java + concurrency in practice

A second way to prevent tasks from colliding over shared resources is to eliminate the 
sharing of variables. Thread local storage is a mechanism that automatically creates 
different storage for the same variable, for each different thread that uses an object. Thus, if 
you have five threads using an object with a variable x, thread local storage generates five 
different pieces of storage for x. 

The ThreadLocal class internally maintains a table associating data (Object references) with 
Thread instances. 

Most applications of ThreadLocals construct one instance per thread.
ThreadLocal variables are normally declared as static

ThreadLocal can also be useful for constructing per-thread resource pools


class ServiceUsingThreadLocal {           // Fragments 
 static ThreadLocal output = new ThreadLocal(); 
 
 public void service() { 
  try { 
    final OutputStream s = new FileOutputStream("..."); 
    Runnable r = new Runnable() { 
      public void run() { 
        output.set(s); 
        try { doService(); } 
        catch (IOException e) { ... } 
        finally { 
          try { s.close(); } 
          catch (IOException ignore) {} 
        } 
      } 
    }; 
    new Thread(r).start(); 
  } 
  catch (IOException e) { ...} 
 } 
 
 void doService() throws IOException { 
  ((OutputStream)(output.get())).write(...); 
  // ... 
 } 
} 

Since values set on a ThreadLocal object only are visible to the thread who set the value, 
no thread can set an initial value on a ThreadLocal using set() which is visible to all threads.

Instead you can specify an initial value for a ThreadLocal object by subclassing ThreadLocal 
and overriding the initialValue() method. Here is how that looks:

private ThreadLocal myThreadLocal = new ThreadLocal<String>() {
    @Override protected String initialValue() {
        return "This is the initial value";
    }
};


Now all threads will see the same initial value when calling get() before having called set() .


-	ThreadLocal objects are usually stored as static fields. When you create a ThreadLocal 
	object, you are only able to access the contents of the object using the get( ) and set( ) 
	methods. 

-	increment( ) and get( ) are not synchronized, because ThreadLocal guarantees that no race condition can occur.  

-	Thread local storage is a mechanism that automatically creates 
	different storage for the same variable.
	
-	Think of ThreadLocal<T> as holding a Map<Thread,T> that stores thread specific values
-	InheritableThreadLocal: Passing threadLocal storage values to child processes.

Benifits:
-	Should not be used in pool threads to communicate values between tasks
-	A disadvantage but one must be really careful with cleaning up ThreadLocals properly since whatever data you 
	put in there stays there as long as the thread lives unless it is explicitly removed. This is especially 
	troublesome in an environment where the threads are reused using thread pools so some garbage data maybe 
	attached to thread unless it's cleaned properly.
-	ThreadLocal offers a number of benefits. It is often the easiest way to render a stateful class thread-safe, 
	or to encapsulate non-thread-safe classes so that they can safely be used in multithreaded environments. 
	Using ThreadLocal allows us to bypass the complexity of determining when to synchronize in order to achieve 
	thread-safety, and it improves scalability because it doesn't require any synchronization. In addition to 
	simplicity, using ThreadLocal to store a per-thread-singleton or per-thread context information has a valuable 
	documentation perk -- by using a ThreadLocal, it's clear that the object stored in the ThreadLocal is not 
	shared between threads, simplifying the task of determining whether a class is thread-safe or not.	

e.g.
-	Transaction management in J2EE for example is done with ThreadLocals
- 	Frequent use of a non-threadsafe utility object that has a (relatively) high cost of construction (such as SimpleDateFormat)

*********************************************************
Memoization or caching

JDK 8 new methods
Map: getOrDefault, putIfAbsent
collection: removeIf() -> remove all elements in a collection that match a predicate.
List: replaceAll() -> similar to the map method in a stream, but it mutates the elements of the List.
						In contrast, the map method produces new elements.
					List<Integer> numbers = ....
					numbers.replaceAll(x -> x * 2);
					....
					
					

*********************************************************
Java memory model ||  memory->cache->Register
-----------------
	JSR-133
	http://www.cs.umd.edu/~pugh/java/memoryModel/CommunityReview.pdf

	http://tutorials.jenkov.com/java-concurrency/java-memory-model.html
	The Java memory model specifies how the Java virtual machine works with the computer's memory (RAM). 
	The Java virtual machine is a model of a whole computer so this model naturally includes a 
	memory model - AKA the Java memory model.

	The Java memory model used internally in the JVM divides memory between thread stacks and the heap

	All local variables of primitive types ( boolean, byte, short, char, int, long, float, double) are 
	fully stored on the thread stack and are thus not visible to other threads. One thread may pass a 
	copy of a pritimive variable to another thread, but it cannot share the primitive local variable itself
		 -----------------
	CPU->|Register->cache|---->Memory
		 -------CPU-------
	On the hardware, both the thread stack and the heap are located in main memory
	Parts of the thread stacks and heap may sometimes be present in CPU caches and 
	in internal CPU registers


	When objects and variables can be stored in various different memory areas in the computer, 
	certain problems may occur. The two main problems are:
		-Visibility of thread updates (writes) to shared variables.
		-Race conditions when reading, checking and writing shared variables.

Stack and heap:
http://www.kdgregory.com/index.php?page=java.refobj

-	Local variables (& parameters) are still stored on the stack, but they hold a pointer to the object, 
	not the object itself, in heap

-GC 
	-Mark & Sweep
		-Phase 1, Mark: The GC starts from root and walks thru the graph marking all objects
		-Phase 2, Sweep: Anything which is not marked in1st phase is eligible for collection. If GC obj has finalize
				It is added to finalization queue and if not space is made available
		-Phase 3, Compact:Some GC have this step and objects are moved to coalesce free space left behind. 	 		
	
	***Root: local variable & parameter in stack, static class member variables, operands of the 
			currently executing expression
			
	
	So what are the "roots"? In a simple Java application, they're method arguments and local variables 
	(stored on the stack), the operands of the currently executing expression (also stored on the stack), 
	and static class member variables.
	
	
-	In programs that use their own classloaders, such as app-servers, the picture gets muddy: only classes loaded 
	by the system classloader (the loader used by the JVM when it starts) contain root references. Any classloaders 
	that the application creates are themselves subject to collection, once there are no more references to them. 
	This is what allows app-servers to hot-deploy: they create a separate classloader for each deployed application, 
	and let go of the classloader reference when the application is undeployed or redeployed.

-	You may be wondering what happens if you have a circular reference: object A contains a reference to 
	object B, which contains a reference back to A. The answer is that a mark-sweep collector isn't fooled: 
	if neither A nor B can be reached by a chain of strong references, then they're eligible for collection.

-	However, memory isn't the only resource that might need to be cleaned up. 
	Consider FileOutputStream: when you create an instance of this object, it allocates a file handle from 
	the operating system. If you let all references to the stream go out of scope before closing it, 
	what happens to that file handle? The answer is that the stream has a finalizer method: a method that's 
	called by the JVM just before the garbage collector reclaims the object. In the case of FileOutputStream, 
	the finalizer closes the stream, which releases the file handle back to the operating system — and also 
	flushes any buffers, ensuring that all data is properly written to disk.

-	While finalizers seem like an easy way to clean up after yourself, they do have some serious limitations. 
	First, you should never rely on them for anything important, since an object's finalizer may never be 
	called — the application might exit before the object is eligible for garbage collection. There are some 
	other, more subtle problems with finalizers.
	
-	object life-cycle, without reference objects
	created - initialized - inUse - Unreachable - finalized

- 	Three new stages in the object life cycle: softly-reachable, weakly-reachable, and phantom-reachable

															-SoftlyReachable - 
															|		|	 	   |
	created - initialized - StronglyReachable---------------|-------|----------|-> Finalize
															WeaklyReachable - 		|
																					|
																					|					
															-PhantomReachable <-----	
	
	You use Reference objects when you want to continue to hold on to a reference to that 
	object you want to reach that object but you also want to allow the garbage collector to 
	release that object.
	
	You accomplish this by using a Reference object as an intermediary (a proxy) between you 
	and the ordinary reference. In addition, there must be no ordinary references to the object 
	(ones that are not wrapped inside Reference objects). If the garbage collector discovers that 
	an object is reachable through an ordinary reference, it will not release that object. 

	-softly reachable
		The object is the referent of a SoftReference, and there are no strong references to it. 
		The garbage collector will attempt to preserve the object as long as possible, but will collect 
		it before throwing an OutOfMemoryError.
	-weakly reachable
		The object is the referent of a WeakReference, and there are no strong or soft references 
		to it. The garbage collector is free to collect the object at any time, with no attempt to 
		preserve it. In practice, the object will be collected during a major collection, but may 
		survive a minor collection.
	-phantom reachable
		The object is the referent of a PhantomReference, and it has already been selected for collection 
		and its finalizer (if any) has run. The term “reachable” is really a misnomer in this case, as 
		there's no way for you to access the actual object.
	
	-References and Referents
		A reference object is a layer of indirection between your program code and some other object, called 
		a referent. Each reference object is constructed around its referent, and the referent cannot be changed
		
		-Soft references are for implementing memory-sensitive caches. 
		-Weak references are for implementing "canonicalizing mappings" where instances of objects can be 
		 simultaneously used in multiple places in a program, to save storage
		 
		 canonicalized mappings: http://wiki.c2.com/?CanonicalizedMapping
			A "canonicalized" mapping is where you keep one instance of the object in question in memory and 
			all others look up that particular instance via pointers or somesuch mechanism. 
			This is where weaks references can help.
			
			The short answer is that WeakReference objects can be used to create pointers to objects in your 
			system while still allowing those objects to be reclaimed by the garbage-collector once they 
			pass out of scope.
			
			WeakHashMap works exactly like HashMap, except that the keys (not the values!) 
			are referred to using weak references. If a WeakHashMap key becomes garbage, its entry is removed 
			automatically.

	-WeakHashMap | https://community.oracle.com/blogs/enicholas/2006/05/04/understanding-weak-references
		
		One of the requirement was to use serialNo for a Widget, the widget class was not extensible(declared final)
		so we had to use a hashMap for that
		serialNumberMap.put(widget, widgetSerialNumber);
		
		To solve the "widget serial number" problem above, the easiest thing to do is use the built-in 
		WeakHashMap class. WeakHashMap works exactly like HashMap, except that the keys (not the values!) 
		are referred to using weak references. If a WeakHashMap key becomes garbage, its entry is removed 
		automatically.
		
		But we have to exactly when we do not need the widget as above is a strong reference and we need to remove 
		the entry from the Map. 		
		WeakReference<Widget> weakWidget = new WeakReference<Widget>(widget);
		
		And then elsewhere in the code you can useweakWidget.get() to get the actual Widgetobject. Of course the 
		weak reference isn't strong enough to prevent garbage collection, so you may find (if there are no strong 
		references to the widget) that weakWidget.get()suddenly starts returning null.
		
		This generally means that some sort of cleanup is required;WeakHashMap, for example, has to remove such 
		defunct entries to avoid holding onto an ever-increasing number of deadWeakReferences
		
		The weakHashMap keys are automatically wrapped in WeakReferences by the map. The trigger to allow cleanup 
		is that the key is no longer in use. 
		The Key class must have a hashCode( ) and an equals( ) since it is being used as a key

		example:
		public class WeakHashMapTest {
			public static void main(String[] args) {
				Map hashMap= new HashMap();
		        Map weakHashMap = new WeakHashMap();
		        String keyHashMap = new String("keyHashMap");
		        String keyWeakHashMap = new String("keyWeakHashMap");
		        hashMap.put(keyHashMap, "Ankita");
		        weakHashMap.put(keyWeakHashMap, "Atul");
		        System.gc();
		        System.out.println("Before: hash map value:"+hashMap.get("keyHashMap")+" and weak hash map value:"+weakHashMap.get("keyWeakHashMap"));
		        keyHashMap = null;
		        keyWeakHashMap = null;
		        System.gc();  
		        System.out.println("After: hash map value:"+hashMap.get("keyHashMap")+" and weak hash map value:"+weakHashMap.get("keyWeakHashMap"));
		    }
		}
		//output:
		Before: hash map value:Ankita and weak hash map value:Atul
		After: hash map value:Ankita and weak hash map value:null		
		
		

-	they're method arguments and local variables (stored on the stack), the operands of the currently executing expression 
	(also stored on the stack), and static class member variables
	
-	In programs that use their own classloaders, such as app-servers, the picture gets muddy: only classes 
	loaded by the system classloader (the loader used by the JVM when it starts) contain root references. 
	Any classloaders that the application creates are themselves subject to collection, once there are no 
	more references to them. This is what allows app-servers to hot-deploy: they create a separate classloader 
	for each deployed application, and let go of the classloader reference when the application is undeployed 
	or redeployed.

-	You may be wondering what happens if you have a circular reference: object A contains a reference to object B, 
	which contains a reference back to A. The answer is that a mark-sweep collector isn't fooled: if neither 
	A nor B can be reached by a chain of strong references, then they're eligible for collection

-	Garbage collector runs in its own thread, and doesn't care what your code is doing

Object Life Cycle (without Reference Objects)
----------------------------------------------
JDK 1.2 introduced the java.lang.ref package, and three new stages in the object life cycle: 

-softly reachable
	The object is the referent of a SoftReference, and there are no strong references to it. The garbage 
	collector will attempt to preserve the object as long as possible, but will collect it before 
	throwing an OutOfMemoryError.
	
	The JDK documentation says that soft references are appropriate for a memory-sensitive cache: each of the 
	cached objects is accessed through a SoftReference, and if the JVM decides that it needs space, then it 
	will clear some or all of the references and reclaim their referents
	
	To be useful in this role, however, the cached objects need to be pretty large — on the order of several 
	kilobytes each. Useful, perhaps, if you're implementing a fileserver that expects the same files to be 
	retrieved on a regular basis, or have large object graphs that need to be cached. But if your objects are 
	small, then you'll have to clear a lot of them to make a difference, and the reference objects will add 
	overhead to the whole process.
	
	Soft Reference as Circuit Breaker
	
	A better use of soft references is to provide a "Circuit Breaker" for memory allocation: put a soft 
	reference between your code and the memory it allocates, and you avoid the dreaded OutOfMemoryError.
	
	1: List<List<Object>> results = new LinkedList<List<Object>>();
			
	2: SoftReference<List<List<Object>>> ref
        = new SoftReference<List<List<Object>>>(new LinkedList<List<Object>>());
        
       List<List<Object>> results = ref.get();
		if (results == null)
			throw new TooManyResultsException(rowCount);
		else
        	results.add(row); 
	
	While those expensive operations happen, the only reference to the list is via the SoftReference. 
	If you run out of memory the reference will be cleared, and the list will become garbage. It means 
	that the method throws, but the effect of that throw can be confined. And perhaps the calling code can 
	recreate the query with a retrieval limit.
	
	Once the expensive operations complete, you can hold a strong reference to the list with relative impunity. 
	However, note that I use a LinkedList for my results rather than an ArrayList: I know that linked lists 
	grow in increments of a few dozen bytes, which is unlikely to trigger OutOfMemoryError. By comparison, 
	if an ArrayList needs to increase its capacity, it must create a new array to do so. In a large list, 
	this could mean a multi-megabyte allocation.
	
	Also note that I set the results variable to null after adding the new element; this is one of the 
	few cases where doing so is justified. Although the variable goes out of scope at the end of the loop, 
	the garbage collector might not know that (because there's no reason for the JVM to clear the variable's 
	slot in the call stack). So, if I didn't clear the variable, it would be an unintended strong reference 
	during the subsequent pass through the loop.
	
	??
	Finally, think carefully about non-obvious strong references. For example, you might want to add a 
	circuit breaker while constructing XML documents using the DOM. However, each node in a DOM holds 
	a reference to its parent, in effect holding a reference to every other node in the tree. And if 
	you use a recursive call to build that document, your stack might be full of references to individual nodes.
	
-weakly reachable
	The object is the referent of a WeakReference, and there are no strong or soft references to it. 
	The garbage collector is free to collect the object at any time, with no attempt to preserve it. 
	In practice, the object will be collected during a major collection, but may survive a minor collection.
	
	There are two main uses: 
	-	associating objects that have no inherent relationship
	-	reducing duplication via a canonicalizing map.
	
	??
	The Problem With ObjectOutputStream
	
-phantom reachable
	The object is the referent of a PhantomReference, and it has already been selected for collection and 
	its finalizer (if any) has run. The term “reachable” is really a misnomer in this case, as there's no way 
	for you to access the actual object, but it's the terminology that the API docs use 

	Finallizer issue:
	
	-if all objects eligible for collection have finalizers, then the collection will have no effect: those 
	objects remain in memory awaiting finalization
	
	-A finalizer might never be invoked
	-Finalizers can create another strong reference to an object
	
	-finalization happens on its own thread, independent of the garbage collector's thread
	
	
These states only apply to objects eligible for collection in other words, those with no strong references

Phantom references
------------------


JVM
1) Mark
2) Sweep
3) Compact (optional)

*********************************************************
Garbage collection
-------------------

*********************************************************
hadoop
------

http://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html

URLs
----
-Spark UI: http://localhost:4040 [ http://<driver-node>:4040 ]
- cluster  manager’s  web  UI  should  appear  at @ http://masternode:8080 
	and show all your workers.
-spark.yarn.historyServer.address : http://lnxcdh21.emeter.com:18088/history


yarn logs -applicationId <appid> --appOwner <userid>
resource manager UI -> nodes page -> particular node -> particular container

Oozie urls

oozie job -oozie http://lnxcdh21.emeter.com:11000/oozie/ -kill 0000029-160511162000176-oozie-oozi-W
oozie job -oozie http://lnxcdh21.emeter.com:11000/oozie/ -info 0000029-160511162000176-oozie-oozi-W
oozie job -oozie http://lnxcdh21.emeter.com:11000/oozie/ -log 0000029-160511162000176-oozie-oozi-W

JobTracker webUI
-http://<job_tracker_address>:50030/ 
-http://localhost:50030/     | if"running"in"pseudo/distributed"mode"

NameNode webUI
-http://<name_node_address>:50070/ 

HBase WebUI
http://master.foo.com:60010
http://lnxcdh01.emeter.com:60010

HBaseMaster:  ports: HBaseMaster @60000 | <ip>:60000
RegionServer: http://<region-server-address>:60030

*********************************************************
What"is"Common"Across"Hadoop/able"Problems?"
Nature of the data
		-Volume"
		-Velocity"
		-Variety"
Nature of the analysis
		-Batch"processing"
		-Parallel"execuBon"
		-Distributed"data"


InputFile->InputFormat->partition split1->RecordReader->Mapper->Combiner->Partitioner->Reducer->OutputFormat->OutputFile
						---------------		----------	------	-----		-----		----
The%Mapper%
	–?Each"Map"task"(typically)"operates"on"a"single"HDFS" block"
	–?Map"tasks"(usually)"run"on"the"node"where"the" block"is"stored"

	- Combiners
		-VERY IMPORTANT: The Combiner may run once, or more than once, on  the output from any given Mapper
		-Combiner and Reducer code are of identical structure i.e same kind of code 
			-Technically,"this"is"possible"if"the"operation"performed"is"commutative" and"associative"
			-Input"and"output"data"types"for"the"Combiner/Reducer"must"be"	identical"

				job.setMapperClass(WordMapper.class); 
				job.setReducerClass(SumReducer.class); 
				job.setCombinerClass(SumReducer.class); 


Shuffle and  Sort  is done by partitioners
	-Sorts"and"consolidates"intermediate"data"from"all" mappers"
	-Happens"after"all"Map"tasks"are"complete"and" before"Reduce"tasks"start"
	-Partitioner: The Partitioner determines which Reducer each intermediate key and its associated values goes to
	-The default Partitioner is the HashPartitioner 
		-Uses"the"Java"hashCode"method"
		-Guarantees"all"pairs"with"the"same"key"go"to"the"same"Reducer"
	

The Reducer
	–?Operates"on"shuffled/sorted"intermediate"data" (Map"task"output)"
	–?Produces"final"output"

Shuffle and sort
	After the Map phase is over, all intermediate values for a given intermediate key are grouped together

Each key and value list is passed to a Reducer
	-All"values"for"a"parAcular"intermediate"key"go"to"the"same"Reducer"
	-The"intermediate"keys/value"lists"are"passed"in"sorted"key"order
	
	
MRv2%daemons%
–?ResourceManager"–"one"per"cluster". Starts"ApplicaGonMasters,"allocates"resources"on"slave"nodes"
–?ApplicationMaster"–"one"per"job". Requests"resources,"manages"individual"Map"and"Reduce"tasks"
–?NodeManager"–"one"per"slave"node". Manages"resources"on"individual"slave"nodes"
–?JobHistory"–"one"per"cluster". Archives"jobs’"metrics"and"metadata"	

HDFS%daemons%
–?NameNode"–"holds"the"metadata"for"HDFS"". Typically"two"on"a"producGon"cluster:"one"acGve,"one"standby"
–?DataNode"–"holds"the"actual"HDFS"data. One"per"slave"node"

HDFS programtically
		Configuration conf = new Configuration(); 
		FileSystem fs = FileSystem.get(conf);
		Path p = new Path("/path/to/my/file");

-The conf object has read in the Hadoop con?guraHon ?les, and therefore  knows the address of the NameNode 
-A File in HDFS is represented by a Path object

-Directory listings
		Path p = new Path("/my/path"); 
		Configuration conf = new Configuration(); 
		FileSystem fs = FileSystem.get(conf); 
		
		//Directory listings
		FileStatus[] fileStats = fs.listStatus(p); 
		for (int i = 0; i < fileStats.length; i++) { 
			Path f = fileStats[i].getPath(); 
			// do something interesting 
		}

		//WritingData
		FSDataOutputStream out = fs.create(p, false); 
		// write some raw bytes 
		out.write(getBytes()); 
		// write an int 
		out.writeInt(getInt()); 
		... 
		out.close();
		
		//
	
Distributed cache
-The Distributed Cache provides an API to push data to all slave nodes
-Transfer"happens"behind"the"scenes"before"any"task"is"executed"
-Data"is"only"transferred"once"to"each"node,"rather""
-Note:"Distributed"Cache"is"read/only"
-Files"in"the"Distributed"Cache"are"automaEcally"deleted"from"slave" nodes"when"the"job"?nishes"

		Configuration conf = new Configuration(); 
		DistributedCache.addCacheFile(new URI("/myapp/lookup.dat"),conf); 
		DistributedCache.addFileToClassPath(new Path("/myapp/mylib.jar"),conf); 
		DistributedCache.addCacheArchive(new URI("/myapp/map.zip",conf)); 
		DistributedCache.addCacheArchive(new URI("/myapp/mytar.tar",conf)); 
		DistributedCache.addCacheArchive(new URI("/myapp/mytgz.tgz",conf)); 
		DistributedCache.addCacheArchive(new URI("/myapp/mytargz.tar.gz",conf));
		
		
LocalJobRunner mode
Configuration conf = new Configuration(); 
conf.set("mapred.job.tracker", "local"); 
conf.set("fs.default.name", "file:///");

No reducers
job.setNumReduceTasks(0); 
-Anything written using the Context.write method in the Mapper will be written to HDFS
	-Rather"than"written"as"intermediate"data"
	-One file"per"Mapper"will"be"written"

***How"Many"Reducers"Do"You"Need?"
-Default is single reducer
-Example: a job must output one file per day of the week
	-Key"will"be"the"weekday"
	-Seven"Reducers"will"be"specified"
	-A"Partitioner"will"be"written"which"sends"one"key"to"each"Reducer"
- you should take into account the number of Reduce slots likely to be available on the cluster
- Create a class that extends Partitioner 
- Override the getPartition method
		-Return"an"int"between"0"and"one"less"than"the"number"of"Reducers"
		-e.g.,"if"there"are"10"Reducers,"return"an"int"between"0"and"9"

URLs
-----
JobTracker webUI
-http://<job_tracker_address>:50030/ 
-http://localhost:50030/     | if"running"in"pseudo/distributed"mode"

NameNode webUI
-http://<name_node_address>:50070/ 


URLs
----
-Spark UI: http://localhost:4040 [ http://<driver-node>:4040 ]
- cluster  manager’s  web  UI  should  appear  at @ http://masternode:8080 
	and show all your workers.
-spark.yarn.historyServer.address : http://lnxcdh21.emeter.com:18088/history


yarn logs -applicationId <appid> --appOwner <userid>
resource manager UI -> nodes page -> particular node -> particular container

Oozie urls

oozie job -oozie http://lnxcdh21.emeter.com:11000/oozie/ -kill 0000029-160511162000176-oozie-oozi-W
oozie job -oozie http://lnxcdh21.emeter.com:11000/oozie/ -info 0000029-160511162000176-oozie-oozi-W
oozie job -oozie http://lnxcdh21.emeter.com:11000/oozie/ -log 0000029-160511162000176-oozie-oozi-W

JobTracker webUI
-http://<job_tracker_address>:50030/ 
-http://localhost:50030/     | if"running"in"pseudo/distributed"mode"

NameNode webUI
-http://<name_node_address>:50070/ 

HBase WebUI
http://master.foo.com:60010

HBaseMaster:  ports: HBaseMaster @60000 | <ip>:60000
RegionServer: http://<region-server-address>:60030

*********************************************************
 
*********************************************************
Hbase
-----
********************************************************
??Spark 2.0 feature
??What diff b/w dataSet & dataFrame
	-A Dataset is a strongly-typed, immutable collection of objects that are mapped to a relational schema.
	-At the core of the Dataset API is a new concept called an encoder, which is responsible for converting 
	 between JVM objects and tabular representation.
	-Spark’s internal Tungsten binary format
	-
??Spark 2.0 new features
??map() Vs flatMap()
??CheckPointing
??groupBy Vs groupByKey


*********************************************************
Hadoop
------
Node nomenculature in bigData		

Hdfs:
	NameNode (SecondaryNameNode) & dataNode
HBase:
	HMaster & RegionServer 
	Region

Spark:

Yarn:
		Two  types  of  long-running  daemon:  
		a  resource manager (one per cluster) to manage the use of resources across the cluster, and 
		node managers running on all the nodes in the cluster to launch and monitor containers.

		MapReduce 1 Vs MapReduce 2:
				In MapReduce 1, there are two types of daemon that control the job execution process:
				-a jobtracker and 
				-one or more tasktrackers

				YARN these responsibilities are handled by separate entities: 
				-the resource manager and 
				-an application master (one for each MapReduce job).
				
				In YARN, the equivalent role is the timeline server, which stores application history.
	
ZooKeeper:


? how to choose the number of map tasks for a given job. 
	
? how to choose the number of reduce tasks for a given job.
	The number of mappers launched is roughly equal to the input size divided by dfs.block.size (the default block 
	 size is 64 MB)
? How much memory does NameNode need

? Anatomy of a MapReduce Job Run(on page 185), with security perspective.

? Anatomy of a spark Job

? Anatomy of Oozie workFlow

? Compression and Serialization stuff(parquet & protocolBuffers)

? Spark on YARN(on page 571)

? Distributed Cache on page 274

? Why Not Use Java Object Serialization? 
		(pg 126/154 Definitive guide)
		Java comes with its own serialization mechanism, called Java Object Serialization (often
		referred to simply as “Java Serialization”), that is tightly integrated with the language, so
		it’s natural to ask why this wasn’t used in Hadoop. Here’s what Doug Cutting said in
		response to that question:
		Why didn’t I use Serialization when we first started Hadoop? Because it looked big and
		hairy and I thought we needed something lean and mean, where we had precise control
		over exactly how objects are written and read, since that is central to Hadoop. With
		Serialization you can get some control, but you have to fight for it.
		The logic for not using RMI [Remote Method Invocation] was similar. Effective, high-
		performance inter-process communications are critical to Hadoop. I felt like we’d need
		to precisely control how things like connections, timeouts and buffers are handled, and
		RMI gives you little control over those.
		The problem is that Java Serialization doesn’t meet the criteria for a serialization format
		listed earlier: compact, fast, extensible, and interoperable.

?

serialize -> store in file based structures like sequence file

--------------------------------------------------------------------------------------------------------------
Project consist of 
-MapReduce(It is this that spark is intending to replace)
	
	DataFlow in map reduce
	(Map)
	-Hadoop runs the job by dividing it into tasks, of which there are two types:
	  map tasks and reduce tasks. The tasks are scheduled using YARN and run on nodes in
	  the cluster
	
	-Hadoop creates one map task for each split, which runs the user-defined
	 map function for each record in the split.
	
	-A good split size tends to be the size of an HDFS block, which is 128(??or 64) MB by default	
		-optimal split size is the same as the block size: it is the
	 largest size of input that can be guaranteed to be stored on a single node. 
	
	-Data locality optimization: Hadoop does its best to run the map task on a node where the input data resides in
	 HDFS, because it doesn’t use valuable cluster bandwidth.
	
	-****(imp)Map tasks write their output to the local disk, not to HDFS. Why is this? 
		-storing it in HDFS with replication would be overkill
	
	-If the node running the map task fails before the map
	 output has been consumed by the reduce task, then Hadoop will automatically rerun
     the map task on another node to re-create the map output.	
    
    -Three kinds of map job execution possible
    	-Data-local (a), rack-local (b), and off-rack (c) map tasks 

	(Reduce)
    -Reduce tasks don’t have the advantage of data locality; the input to a single reduce task
	 is normally the output from all mappers.	
		-map outputs have to be transferred across the network to the node where the reduce task is running
	
	-The output of the reduce is normally stored in HDFS for reliability. 	 
	
	-Each HDFS block of the reduce output, the first replica is stored on the local node,
	
	-When there are multiple reducers, the map tasks partition their output, each creating
	 one partition for each reduce task. There can be many keys (and their associated values)
	 in each partition, but the records for any given key are all in a single partition.
	
	-Map -> combine -> reduce
	
	(Combiner)
	-The data flow between map and reduce tasks is collo-quially known as the shuffle, 
	 as each reduce task is fed by many map tasks
		-Refer diagram pg 62 - Hadoop definitive guide
	
	-Hadoop allows the user to specify a combiner function to be run on the map output, and the combiner function’s
	 output forms the input to the reduce function.
	
	-Hadoop does not provide a guarantee of how many times it will call it for
		a particular map output record, if at all. In other words, calling the combiner function
		zero, one, or many times should produce the same output from the reducer.
	
	-The combiner function doesn’t replace the reduce function. 
		
	-But it can help cut down the amount of data shuffled between the mappers and the reducers,
	 and for this reason alone it is always worth considering whether you can use a combiner
	 function in your MapReduce job
		
	Job job = new Job();
    job.setJarByClass(MaxTemperatureWithCombiner.class);
    job.setJobName("Max temperature");
    FileInputFormat.addInputPath(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    
    job.setMapperClass(MaxTemperatureMapper.class);
    job.setCombinerClass(MaxTemperatureReducer.class);
    job.setReducerClass(MaxTemperatureReducer.class);
    job.setOutputKeyClass(Text.class);	
	
	
-Hadoop Streaming
	-
	-Hadoop Streaming uses Unix standard streams
		as the interface between Hadoop and your program, so you can use any language that
		can  read  standard  input  and  write  to  standard  output  to  write  your  MapReduce
		program. 
		
		
-------------------------------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------------------------

--------------------------------------------------------------------------------------------------------------			

*********************************************************
R script
--------

In real life scenarios, you would train the model using one “tagged” dataset, verify the quality, 
save the trained model and use it to predict with different datasets serving as input.



.RDA 
-----
load("filename.RDA")

(.RDA (or .rda) is short for .RData (or .rdata :-).  It is the usual
file format for saving R objects to file (with save() or
save.image()).)
----

> > vars<-load('~/523/Data.rda') 
> > vars 
> [1] "seg_vegNew" 
> >print(seg_vegNew) 

Try to "ls(seg_vegNew)" 
R> my.x <- get('x', seg_vegNew) 

You should see a list of variables in it ... say "x" was one of them, one 
way that you can then get access to it is like so: 

R> my.x <- get('x', seg_vegNew) 


RP_MODEL.rda
------------
library(RODBC)
library(randomForest)
library(nza)
library(nzr)
library(tree)
RP_Model <- "/home/pipe/ajit/rdemo/RP_MODEL.rda"
vars <- load (RP_Model)
vars
	
> print(RP_Model)

Call:
 randomForest(formula = as.factor(training_data$TARGET) ~ ., data = training_data[,      !names(training_data) %in% c("SDP_WID", "ANALYSIS_END_DATE",          "ANALYSIS_START_DATE", "SDP_SEGMENT_CONFIG_WID")], ntree = 50) 
               Type of random forest: classification
                     Number of trees: 50
No. of variables tried at each split: 3

        OOB estimate of  error rate: 33.11%
Confusion matrix:
      Clean Theft class.error
Clean 13411  5483   0.2901979
Theft  6616 11030   0.3749292



-ntree	  sets	  how	  large	  your	  Random	  Forest	  is	  going	  to	  be.	  Or	  in	  other	
  words,	how	  many	  trees	  should	  be	  contained	  in	  your	   ensemble.

-There are two main types of decision trees:
	-Classification trees : Predicted outcome is the class the data belongs.
	-Regression trees : Predicted outcome is continuous variable e.g. a real number such as the price of a commodity.
	
	
	
---
> ls(RP_Model)
 [1] "call"            "classes"         "confusion"       "err.rate"       
 [5] "forest"          "importance"      "importanceSD"    "inbag"          
 [9] "localImportance" "mtry"            "ntree"           "oob.times"      
[13] "predicted"       "proximity"       "terms"           "test"           
[17] "type"            "votes"           "y"     
-----

Below is an example of building a decision tree for predicting income, based on age, sex 
and numbers of hours per week.
> # build a tree using built-in analytics
> adultTree = nzDecTree(income~age+sex+HOURSPERWEEK, data=nzadult)

The fitted model can be applied to another dataset. If the dataset is stored in a database table, massive data 
transfer can be avoided by using the overloaded function predict() to [ Refer to http://archive.ics.uci.edu/ml/datasets/Adult for more information]
Wrappers for Built-in Analytics perform classification inside the NPS system.
> # use the previously created tree for prediction 
> adultPred = predict(adultTree, nzadult)

> # bring the results of the prediction to R using as.data.frame 
> head(as.data.frame(adultPred)) 



nz.data.frame
--------------

The most important and frequent construct is the object of the class nz.data.frame. The 
function nz.data.frame() creates a pointer to a table located on the NPS system. This point-
er can later be used to run data transformations with nzApply or nzRun or data mining al-
gorithms. It does not store any data in local memory but rather provides metadata that can 
be used to determine the correct table subset (columns and/or rows) where user code 
should run. It is the standard output of the majority of data manipulation functions in the 
nzLibrary for R.
> nzConnect("admin", "password", "TT4-R040", "mm") 
# get the data 
> nzadult = nz.data.frame("adult") 
> nzadult 
SELECT  AGE,WORKCLASS,FNLWGT,EDUCATION,EDUCATIONNUM,MARITALSTATUS,OCCUPATION, 
RELATIONSHIP,RACE,SEX,CAPITALGAIN,CAPITALLOSS,HOURSPERWEEK,NATIVECOUNTRY, 
INCOME,ID  FROM  adult
The nz.data.frame class implements a number of methods for extracting a subset of its 
data, gathering meta-info similar to data.frame, and working with parallel data-processing 
algorithms. The table schema is read from the NPS system and all necessary metadata is 
available to the user.


[,] and $
---------

A subset of columns and/or rows may be specified using the [,] operator.
A limitation is that rows cannot be referenced by their numbers since there is no continuous 
row numbering on the NPS system. Instead, you must specify value-based conditions, for 

example, 
d [,1]>10 which means "all rows where the value of the first column is greater than 10." 
The $ operator may also be used to select an nz.data.frame column.

head, tail
----------

In order to get a sample of the data, the head() and tail() functions may be useful. The 
functions pull the specified data from the start or end of the dataset, respectively.

as.nz.data.frame
----------------

Another useful data manipulation function is as.nz.data.frame. It creates an 
nz.data.frame object from a different R object. Then, an NPS system table is created and 
the passed data is inserted into this table. The created object points to the newly created 
NPS system table.
This example shows how an nz.data.frame object can be created from another R object, in 
this case from a data.frame iris. 




?? How a model is created 
?? How model evaluation is done i.e. how the the best model is selected
?? If a model is hardCoded, then how will it learn over data set 
?? What are the properties that were considered in creating RandomForest like 
	-depth of tree
	-no of nodes
	-......
?? training data


URLs

R 
http://www.toptal.com/machine-learning/machine-learning-theory-an-introductory-primer
https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm

Quick-R good for quick ref
http://www.statmethods.net/index.html

training Tutorial
https://www.kaggle.com/c/bike-sharing-demand/forums/t/11525/tutorial-0-433-score-with-randomforest-in-r/80074

*********************************************************
Serialization:
http://docs.oracle.com/javase/8/docs/platform/serialization/spec/serialTOC.html

*********************************************************



*********************************************************
BigData

*********************************************************
 Hive
------

Hive is not designed for online transaction processing and does not offer real-time queries and row level updates. 
It is best used for batch jobs over large sets of immutable data (like web logs).

hive-site.xml  file  to  $SPARK_HOME/conf, 

imports		
// Import Spark SQL
import org.apache.spark.sql.hive.HiveContext;
// Or if you can't have the hive dependencies
import org.apache.spark.sql.SQLContext;
// Import the JavaSchemaRDD
import org.apache.spark.sql.SchemaRDD;
import org.apache.spark.sql.Row;

Notes:
-Cache table as
	hiveCtx.cacheTable("tableName")
	In Spark 1.2, the regular  cache()  method on RDDs also results in a cacheTable()



1)
JavaSparkContext ctx = new JavaSparkContext(...);
SQLContext sqlCtx = new HiveContext(ctx);
OR 
HiveContext hiveCtx = new HiveContext(sc);


2)
//Read from Hive table
HiveContext hiveCtx = new HiveContext(sc);
SchemaRDD rows = hiveCtx.sql("SELECT key, value FROM mytable");
JavaRDD<Integer> keys = rows.toJavaRDD().map(new Function<Row, Integer>() {
  public Integer call(Row row) { 
  	return row.getInt(0); 
  	}
});


3) Jason example
SchemaRDD input = hiveCtx.jsonFile(inputFile);
// Register the input schema RDD
input.registerTempTable("tweets");
// Select tweets based on the retweetCount
SchemaRDD topTweets = hiveCtx.sql("SELECT text, retweetCount FROM tweets ORDER BY retweetCount LIMIT 10");

4) ParquetFile
saveAsParquetFile()

rest almost same as above load from Parquet file and create TempTable of it
and use it to display data

class HappyPerson implements Serializable {
	  private String name;
	  private String favouriteBeverage;
	  public HappyPerson() {}
	  public HappyPerson(String n, String b) {
	    name = n; favouriteBeverage = b;
	  }
	  public String getName() { return name; }
	174      |      Chapter 9: Spark SQL  public void setName(String n) { name = n; }
	  public String getFavouriteBeverage() { return favouriteBeverage; }
	  public void setFavouriteBeverage(String b) { favouriteBeverage = b; }
	};
	...
	ArrayList<HappyPerson> peopleList = new ArrayList<HappyPerson>();
	peopleList.add(new HappyPerson("holden", "coffee"));
	JavaRDD<HappyPerson> happyPeopleRDD = sc.parallelize(peopleList);
	SchemaRDD happyPeopleSchemaRDD = hiveCtx.applySchema(happyPeopleRDD,
	  HappyPerson.class);
	happyPeopleSchemaRDD.registerTempTable("happy_people");

-Pseudodistributed mode is nearly identical; it’s effectively a one-node cluster.

Hive properties & commands:

-hive.metastore.warehouse.dir : The default value for this property is /user/hive/warehouse

-.hiverc file: Best to put commands like this in the $HOME/.hiverc file, which will be processed when Hive starts.

-If you omit the EXTERNAL keyword and the original table is external, the
	new table will also be external. If you omit EXTERNAL and the original
	table is managed, the new table will also be managed. However, if you
	include the EXTERNAL keyword and the original table is managed, the new
	table will be external. Even in this scenario, the  LOCATION clause will
	still be optional.

-hive> SHOW PARTITIONS employees;

-hive> set hive.mapred.mode=strict;

-hadoop distcp /data/log_messages/2011/12/02 s3n://ourbucket/logs/2011/12/02
	DistCp Version 2 (distributed copy) is a tool used for large inter/intra-cluster copying. 
	It uses MapReduce to effect its distribution, error handling and recovery, and reporting. 
	It expands a list of files and directories into input to map tasks, each of which will 
	copy a partition of the files specified in the source list.

-partition problem:
	When we executes select * from user; nothing appears.
	Why?
	I spent a long time searching for an answer.
	Finally, solved.
	Because when external table is declared, default table path is changed to specified 
	location in hive metadata which contains in metastore, but about partition, nothing 
	is changed, so, we must manually add those metadata.
	
	
	ALTER TABLE user ADD PARTITION(date='2010-02-22');
	Every time a new data=... folder (partition) is created, we must manually alter the 
	table to add partition information.

-we can get a partition’s location as follows:
	hive> DESCRIBE EXTENDED log_messages PARTITION (year=2012, month=1, day=2);
	...
	location:s3n://ourbucket/logs/2011/01/02,

-DESCRIBE EXTENDED table command lists the input and output formats,
	the SerDe, and any SerDe properties in the DETAILED TABLE INFORMATION.
	
-The CLUSTERED BY … INTO … BUCKETS clause, with an optional SORTED BY … clause is used
	to optimize certain kinds of queries.
	CREATE EXTERNAL TABLE IF NOT EXISTS stocks (
	  exchange        STRING,
	  symbol          STRING,
	  ymd             STRING,
	  price_open      FLOAT,
	  price_high      FLOAT,
	  price_low       FLOAT,
	  price_close     FLOAT,
	  volume          INT,
	  price_adj_close FLOAT)
	CLUSTERED BY (exchange, symbol)
	SORTED BY (ymd ASC)
	INTO 96 BUCKETS
	ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
	LOCATION '/data/stocks';
	
-IF EXISTS
	DROP TABLE IF EXISTS employees;

-Drop table: For managed tables, the table metadata and data are deleted.
	For external tables, the metadata is deleted but the data is not.

- Hadoop Trash feature [fs.trash.interval]:  which is not on by
	default,  the  data  is  moved  to  the  .Trash  directory  in  the  distributed
	filesystem for the user, which in HDFS is /user/$USER/.Trash. To enable
	this feature, set the property fs.trash.interval to a reasonable positive
	number. It’s the number of minutes between “trash checkpoints”; 1,440
	would be 24 hours. While it’s not guaranteed to work for all versions of
	all distributed filesystems,

-ALTER TABLE  modifies  table  metadata  only.  The  data  for  the  table  is untouched. 

- change a partition location
	ALTER TABLE log_messages PARTITION(year = 2011, month = 12, day = 2)
	SET LOCATION 's3n://ourbucket/logs/2011/01/02';
	
	This command does not move the data from the old location, nor does it delete the old data.

-Changing Columns
	You can rename a column, change its position, type, or comment:
	ALTER TABLE log_messages
	CHANGE COLUMN hms hours_minutes_seconds INT
	COMMENT 'The hours, minutes, and seconds part of the timestamp'
	AFTER severity;

-Alter table properties
	ALTER TABLE log_messages SET TBLPROPERTIES (
	 'notes' = 'The process id is no longer captured; this column is always NULL');

-The ALTER TABLE … TOUCH

- hive.files.umask.value that defines  a  umask  value  used  to  set  the  default  
	permissions  of  newly  created  files,  by masking bits

- hive.metastore.authorization.storage.checks: is true, Hive prevents a user from dropping a table 
	when the user does not have permission to delete the underlying files that back the table

- hive.metastore.execute.setugi: true, When running in secure mode.

- hive.security.authorization.enabled: Enable or disable the hive client authorization

-

-

-

-

-

-

 Data Types and File Formats
 HiveQL: Data Definition
 HiveQL: Queries (optional)
 Schema Design
 Other File Formats and Compression
 Developing 
 Security
 
*********************************************************
Hadoop Training: Mohit 

hadoop 1= only read
hadoop 2= append

hadoop adviced to work in same data centre (e.g over internet)
cassandra is adviced to work in diff data centre

skiplist
Namenode is (1)editLog- only added ()
 			(2)concurrentSkipList: sortedMap CSLM
			
			FsImage: serialized concurrntSkipList as FsImage
			BlockPoolMap stores which block coming from which node. BlockId is the key of the map

scheduler			
			
File write is 2 step 
File read is 1 step process

In production secondary node is started, goes to nameNode and download editLog




costOfLock
HashTable is used by reducer
sequenceFile:
map file: sorted seq file with index file (index key)
AVRO: allows avro schema 

?How last 1 hr editLog is downloaded is it hourly based File
:: rolled in 20 min.. so 1 hr can be downloaded
?why desirealize fsImage we already have updatedConcrntSkipListMap
::can be done
?after crash recovery who becomes secondayNameNode
::
?
::
?
::
?
::
?
::
?
::
?
::
?
::
?
::
?
::
?

*********************************************************
Kerberos

Kerberos uses symmetric cryptography to authenticate clients to services and vice versa. 


*********************************************************

*********************************************************

*********************************************************

*********************************************************

*********************************************************

*********************************************************

*********************************************************
classLoader
https://www.youtube.com/watch?v=t8sQw3pGJzM&list=PLXvUQtFPPOS-y9yuG0KiAb5pORGBKXjz9

Vijay saraswat java is not type safe 1997

sheng liang and gilad bracha. Dynamic class loading in JVM

NoClassDefFound
find *.jar -exec jar -tf '{}'\; | grep myClass
A.class.get().getResourece().

NoSuchMethodError
javap -private myclass

Types (Hierarchy) of Java Class Loaders

Java class loaders can be broadly classified into below categories:

Bootstrap Class Loader
	Bootstrap class loader loads java’s core classes like java.lang, java.util etc. These are classes that 
	are part of java runtime environment. Bootstrap class loader is native implementation and so they may 
	differ across different JVMs.
	Bootstrap class loader don't have any parents, if you call String.class.getClassLoader() it will 
	return null and any code based on that may throw NullPointerException in Java. Bootstrap class loader 
	is also known as Primordial ClassLoader in Java.
	Bootstrap class loader, which is implemented in native language mostly in C,


Extensions Class Loader
	JAVA_HOME/jre/lib/ext contains jar packages that are extensions of standard core java classes. Extensions 
	class loader loads classes from this ext folder. Using the system environment propery java.ext.dirs you 
	can add ‘ext’ folders and jar files to be loaded using extensions class loader.
	Extension ClassLoader delegates class loading request to its parent, Bootstrap and if unsuccessful, 
	loads class form jre/lib/ext directory or any other directory pointed by java.ext.dirs system property. 
	Extension ClassLoader in JVM is implemented by  sun.misc.Launcher$ExtClassLoader.

System or Application class loader
	Java classes that are available in the java classpath are loaded using System class loader.
	Java classes is called System or Application class loader and it is responsible for loading 
	application specific classes from CLASSPATH environment variable, -classpath or -cp command 
	line option, Class-Path attribute of Manifest file inside JAR. 

How classLoader works in Java
		Java class loaders are used to load classes at runtime. ClassLoader in Java works on three principle: 
		delegation, visibility and uniqueness. Delegation principle forward request of class loading to parent 
		class loader and only loads the class, if parent is not able to find or load class. Visibility principle 
		allows child class loader to see all the classes loaded by parent ClassLoader, but parent class loader 
		can not see classes loaded by child. Uniqueness principle allows to load a class exactly once, which 
		is basically achieved by delegation and ensures that child ClassLoader doesn't reload the class already 
		loaded by parent.

Remember Classpath is used to load class files while PATH is used to locate executable like javac or java command.
		
Read more: http://javarevisited.blogspot.com/2012/12/how-classloader-works-in-java.html#ixzz4chLMpCod
***********************************************************


find . -exec jar -tf '{}'\; | grep "sparkProperties"



??????????
-linear-probe hash maps
****************************************************************************************************
Algorithms:

decide between big O = f(n) = O(g(n)) i.e. fn is < or takes less time
omega => f(n) = omega(g(n)) i.e. fn is > or g(n) takes less time 
theta => f(n) = theta(g(n)) i.e. fn is = or equals


Rules are defined
- Multiplicative constants can be omitted: 14n2 becomes n2
-n^a dominates n^b if a > b : for instance, n^2 dominates n
- polynomial dominates any logarithm: n dominates (log n)^ 3
-Any exponential dominates any polynomial: 3^n dominates n^5 (it even dominates 2^n )
-n! & 2^n are theta

-The moral: in big- T terms, the sum of a geometric series is simply the ?rst term if the series is
strictly decreasing, the last term if the series is strictly increasing, or the number of terms if the
series is unchanging.

more rules:
http://www.chegg.com/homework-help/algorithms-1st-edition-solutions-9780073523408


-	It is a basic property of decimal numbers that
	The sum of any three single-digit numbers is at most two digits long.
	The rule is correct(not only for decimal) for any base b = 2
	
-	How many digits are needed to represent the number N = 0 in base b ?
	With k digits in base b we can express numbers up to bk - 1 
	999 = 10^3 - 1

-	The rule for converting logarithms from base a to base b : logb N = (loga N)/(loga b)
	
-	Modular arithmetic is a system for dealing with restricted ranges of integers. 

-	Euclid’s algorithm for greatest common divisor:
	Given two integers a and b , it ?nds the largest integer that divides both of
	them, known as their greatest common divisor (gcd).
	The most obvious approach is to ?rst factor a and b , and then multiply together their
	common factors. For instance, 1035 = 32 · 5 · 23 and 759 = 3 · 11 · 23 , so their gcd is 3 · 23 = 69 .
	
	Rule: If x and y are positive integers with x = y , then gcd(x, y) = gcd(x mod y, y) .
	
-	Divide-and-conquer algorithms 
	recursion: Divide-and-conquer algorithms often follow a generic pattern:  they tackle a problem of size
	n by recursively solving, say, a subproblems of size n/b and then combining these answers in
	O(nd) time, for some a, b, d > 0
	
	- BinarySearch: O(n log n)
	- MergeSort: O(n log n)
	
	
-Recursive call are expensive than iterative calls

Divide & conquer Algo
Greedy Algo
Dynamic programming 
Linear programming and reduction
NP complete solutions
Quantum algo

	
	
	
-Binary search work on Arrays but not on Linked list	

BinarySearch- log n

Selection sort: find min no and swap with sorted list first element. Now move +1 in list. O(n^2)
avg case:
worst case:

Insertion sort: eg arrange  cards in deck.  O(n^2)
avg case: average-case complexity for insertion sort is (N-1)*N/4
worst case: O(n^2)

Bubble sort: O(n^2)

avg case:
worst case:

Insertion sort better than Selection sort is better than bubble sort
Insertion sort<selection sort<bubble sort
Because in sorted list insertion sort is linear

MergeSort:
avg case: 
worst case: O(nlogn)

Quick sort:
avg case: O(nlogn)
worst case: O(n^2) 

O(n^2) is infeasible for n > 10000				

log n < n < n log n < n^2 < n^3 < 2^n < n!

stable sorting : spreadSheet where 1 column sorting does not affect sorting of other column
use Merge sort with careful at merging arrays: only move A[i] <= A[j]
	
TODO: bubble sort, Heapsort

Graphs:
traversals:
	-BFS:visited + parent info. gives shortest path
		 -Level by level exploration
		 -edges explored from BFS forms a tree
		 -acyclic graph = connected with n-1 edges, it is also known as tree	
	
	-DFS: Explore each vertex as soon it is visited, DFS numbering 
		path discovered by DFS are not shortest unlike BFS  so why use DFS as it gives:
		-DFS numbering: get no when we started in node and when we get out of node. pre[] & post[]
		pre and post can be used to find:
		-cycle in graph
		-cut vertex i.e. removal of node disconnects a graph
		-Undirected:Non tree edge generates a cycle
		-directed graph: Non tree edge
			-Forward edge: using pre post [u     v]
											[v v]
			-backward edge: only backward edge forms cycle
								[u v]
							  [v 	v ]	
			-cross edge
					disjoint set 
						[a 	b] 
						[c 	d]
		-DFS numbering can be used to compute strongly connected graph(if edge b/w i to j then j to i is strongly conected)
		 all pairs of nodes in SSC are strongly connected
		-DFS helps identifying articulating point i.e. removing such vertex disconnects graph OR
		 bridges i.e. removing such edge disconnects graph
		
		-In this graph, an even number of vertices (the four vertices numbered 2, 4, 5, and 6) 
		 have odd degrees. The sum of the degrees of the vertices is 2 + 3 + 2 + 3 + 3 + 1 = 14, twice the number of edges.
	-Directed acyclic graph: no cycles	
		
degree: no of nodes connected to vertex
	-inDegree: no of edges moving in to vertex
	-outDegree: no of edges moving out of vertex
	
DAG: directed acyclec graph
topological ordering: use indegree to sort. start with 0 and enumerate it, then delete. Repeat and get the topological sorting


******************************************************************************************************************
Micro services: http://martinfowler.com/articles/microservices.html
Monolithic: a monolithic application built as a single unit.
-Client
-DB
-Server

Monolithic applications can be successful, but increasingly people are feeling frustrations with them - 
especially as more applications are being deployed to the cloud . Change cycles are tied together - 
a change made to a small part of the application, requires the entire monolith to be rebuilt and deployed. 
Over time it's often hard to keep a good modular structure, making it harder to keep changes that ought to 
only affect one module within that module. Scaling requires scaling of the entire application rather than parts 
of it that require greater resource

-code change / deploy cycle
-hard to keep a good modular structure
-scaling problem

MicroServices:
	architecture puts each element of functionality into a seperate service and scales by 
	distributing these services across servers as needed ie scale only those MS which require more resources.
-building applications as suites of services
-Services are independently deployable and scalable
-each service also provides a firm module boundary
-even allowing for different services to be written in different programming languages

-definition is that a component is a unit of software that is independently replaceable and upgradeable.
-aim of a good microservice architecture is to minimize these through cohesive service boundaries and 
 evolution mechanisms in the service contracts.

-Applications built from microservices aim to be as decoupled and as cohesive as possible - they own their 
 own domain logic and act more as filters in the classical Unix sense

Conway's Law:
Any organization that designs a system (defined broadly) will produce a design whose structure is a 
copy of the organization's communication structure.

-- Melvyn Conway, 1967


-battle-tested code 

??Patterns like Tolerant Reader and Consumer-Driven Contracts are often applied to microservices. 
??The second approach in common use is messaging over a lightweight message bus. The infrastructure 
  chosen is typically dumb (dumb as in acts as a message router only) - simple implementations 
  such as RabbitMQ or ZeroMQ don't do much more than provide a reliable asynchronous fabric


******************************************************************************************************************

Problems:

Graph:
	1)	consider the task of coloring a political map.  What is the minimum number of colors needed, 
		with the obvious restriction that neighboring countries should have different colors?
		
		A graph with one vertex for each country (1 is Brazil, 11 is Argentina) and edges
		between neighbors. It contains exactly the information needed for coloring, and nothing more.
		The precise goal is now to assign a color to each vertex so that no edge has endpoints of the
		same color.	
		
	2) Examination scheduling with min no of slots: 
		Use one vertex for each exam and put an edge between two vertices if there is a con?ict, 
		that is, if there is somebody taking both endpoint exams
		
	3) Cities and flights: Nodes are cities and edges are flights
	
	
***********************************************************************
Programming hive - 59 


***********************************************************************
11/4/16  TDD training 	
	

--------------------DS questions----------------
Q2May: Print Concatenation of Zig-Zag String in ‘n’ Rows

Given a string and number of rows ‘n’. Print the string formed by concatenating n rows when input string is written in row-wise Zig-Zag fashion.

Examples:

Input: str = "ABCDEFGH"
       n = 2
Output: "ACEGBDFH"
Explanation: Let us write input string in Zig-Zag fashion
             in 2 rows.
A   C   E   G   
  B   D   F   H
Now concatenate the two rows and ignore spaces 
in every row. We get "ACEGBDFH"

Input: str = "GEEKSFORGEEKS"
       n = 3
Output: GSGSEKFREKEOE
Explanation: Let us write input string in Zig-Zag fashion
             in 3 rows.
G       S       G       S
  E   K   F   R   E   K
    E       O       E
Now concatenate the two rows and ignore spaces 
in every row. We get "GSGSEKFREKEOE"

Ans: Use 2D array 
		row[no of rows][length of string]
		Loop till length of string i.e i=0 till i< str.length
		  char letter = processingLetter
		  int rowToWrite = getRowToWrite(i)
		  Loop till no of rows i.e n times j=0 to j<no of rows
			if (i == j)
			  row[j][i] = letter
			else
			  row[j][i] = " "  
		  End Loop
		End Loop
		
	getRowToWrite (){
		//Todo
	}




Q2May Add 1 to a number represented as linked list
	Number is represented in linked list such that each digit corresponds to a node in linked list. 
	Add 1 to it. For example 1999 is represented as (1-> 9-> 9 -> 9) and adding 1 to it should change 
	it to (2->0->0->0)

Ans: Use doubly link lst & stack.

Create doubly LL with values
Push nos to stack
Traverse to last link list node
Loop till stack is not empty
  pop value add to LL value
  add popped value to LL node value
  If result > 9 
      push carry to stack
  if LL prev node is Null i.e. first node
      pop all elements and add to LL first node  
Loop end

Ans 2: 
-reverse the LL
-carry - noToAdd
-Loop whuke carry = 0
-	add no to node
-	If sum > 10 
-		carry 10 - sum
-   Node = next node
-End Loop


Q1 3 May
------
Reverse words in a given string
Example: Let the input string be “i like this program very much”. The function should change the string to “much very program this like i”

Solution:
String s = "i like this program very much";

printReverse(String s){

	String[] split = s.split(" ");
	int length = split.length - 1;
	while(length >= 0){
		System.out.print(split[length] + " ");
		length--;
	}
		
}
o/p - much very program this like i 		

alternate:
reverse(char[], startIndex, endIndex){

}


Q2 3May
-------
Rearrange a linked list such that all even and odd positioned nodes are together

Rearrange a linked list in such a way that all odd position nodes are together and all even positions node are together,

Examples:

Input:   1->2->3->4
Output:  1->3->2->4

Input:   10->22->30->43->56->70
Output:  10->30->56->22->43->70

Solution2:
nodeEven = root; nodeRootOdd = root.next;
while(nodeEven.next != null){
  nodeOdd = nodeEven.next; tmpNode = nodeOdd.next
  nodeEven .next = tmpNode;
  nodeOdd.next = tmpNode.next;
  evenNode = tmpNode.next;
  if (null != evenNode ) tmpNode.next = evenNode.next.next;
}
odd.next = nodeRootOdd;

---------------------------------------

If (node size >= 3) {
  nodeEven = root; nodeRootOdd = root.next;
  while(nodeEven.next != null){
    nodeOdd = nodeEven.next; 
    nodeEven.next = nodeOdd.next;
    nodeEven = nodeOdd.next;
    nodeOdd.next = nodeEven.next
  }
nodeEven.next = nodeRootOdd;
}
print LL;



***********************************************
Java Security:  
-------------

Google book Inside Security: https://books.google.co.in/books?id=XfWlYWVzo20C&printsec=frontcover&source=gbs_ge_summary_r&cad=0#v=onepage&q&f=true

Once a class has been loaded into the virtual machine , its checked by following:

1) Class loaders:

2) ByteCode Verifier: Bytecode Verification
	When a class loader presents the bytecodes of a newly loaded Java platform class to the virtual machine,
	these bytecodes are first inspected by a verifier. The verifier checks that the instructions cannot 
	perform actions that are obviously damaging. All classes except for system classes are verified. 
	You can, however, deactivate verification with the undocumented -noverify option.

	For example,  -noverify (or -Xverify:none) option
	
	java -noverify Hello
	
	Here are some of the checks that the verifier carries out:

	Variables are initialized before they are used.
	Method calls match the types of object references.
	Rules for accessing private data and methods are not violated.
	Local variable accesses fall within the runtime stack.
	The runtime stack does not overflow.
	If any of these checks fails, then the class is considered corrupted and will not be loaded

	You might wonder, however, why a special verifier checks all these features. After all, the compiler 
	would never allow you to generate a class file in which an uninitialized variable is used or in which 
	a private data field is accessed from another class. Indeed, a class file generated by a compiler for 
	the Java programming language always passes verification. 

	However, the bytecode format used in the class files is well documented, and it is an easy matter for 
	someone with some experience in assembly programming and a hex editor to manually produce a class file 
	that contains valid but unsafe instructions for the Java virtual machine. Once again, keep in mind that 
	the verifier is always guarding against maliciously altered class files, not just checking the class 
	files produced by a compiler.
	
3) Security Manager
	Once a class has been loaded into the virtual machine and checked by the verifier, the second security 
	mechanism of the Java platform springs into action: the security manager
	The security manager is a class that controls whether a specific operation is permitted. Operations 
	checked by the security manager include the following:
	
			Creating a new class loader
			Exiting the virtual machine
			Accessing a field of another class by using reflection
			Accessing a file
			Opening a socket connection
			Starting a print job
			Accessing the system clipboard
			Accessing the AWT event queue
			Bringing up a top-level window
			
	The default behavior when running Java applications is that no security manager is installed, so all 
	these operations are permitted. The applet viewer, on the other hand, enforces a security policy 
	that is quite restrictive.		
	
-Anatomy of java program	
	-The bytecode verifier
	 The bytecode verifier ensures that Java class files follow the rules of the Java language. In terms of
	 resources, the bytecode verifier helps enforce memory protections for all Java programs. As the figure
	 implies, not all classes are subject to bytecode verification.	
	 
	-The class loader
	 One or more class loaders load all Java classes. Programatically, the class loader can set permissions
	 for each class it loads. 
	 
	-The access controller
	 The access controller allows (or prevents) most access from the core API to the operating system,
	 based upon policies set by the end user or system administrator.
	
	-The security manager
	 The security manager is the primary interface between the core API and the operating system; it has
	 the ultimate responsibility for allowing or preventing access to all system resources. However, it
	 exists mostly for historical reasons; it defers its actions to the access controller.

	-The security package 
	 The security package (that is, classes in the java.security package as well as those in the
 	 security extensions) allows you to add security features to your own application as well as providing
     the basis by which Java classes may be signed

	-The key database
	 The key database is a set of keys used by the security infrastructure to create or verify digital
	 signatures. In the Java architecture, it is part of the security package, though it may be manifested as
	 an external file or database.
	
	
- Security Debugging : java.security.debug : all / access / jar / policy / scl
	For example, to see the permissions granted by the secure class loader and see a stack trace when a permission
	check fails, you would specify -Djava.security.debug=scl,access,failure


-JSSE: java secure socket extension, provides SSL encryption facilities
	-SSL: secure socket layer
-JCE: java cryprography extension
-JAAS: java authentication and authorization service 


	
audit log
concurrentSkipList
BigTable


-Djava.library.path=/path will be handled in java bytecode level.
 LD_LIBRARY_PATH is Linux specific and is an environment variable pointing to directories where the dynamic 
 loader should look for shared libraries.

-If a library is not explcitely loaded from java, i.e. a dependent library has to be used, then LD_LIBRARY_PATH 
 has to be used to have that library available in the jvm
-There are three options for including the native library: 

1. Add to java.library.path JVM argument 
2. Add to LD_LIBRARY_PATH environment variable 
3. Copy library file to within a path that already exists within the 
library path 


The java.library.path is defined in the jvm.config file and contains 
paths delimited by the "," character. 

The LD_LIBRARY_PATH is an environment variable which can be set one- 
time by using a command such as "export LD_LIBRARY_PATH=/opt/ 
fusionreactor/etc/lib:${LD_LIBRARY_PATH}". Alternatively, on RedHat 
based distributions (which CentOS is) you can also configure 
permanently for all users in the "/etc/ld.so.conf.d/local.conf" file. 

Finally, an alternative is to copy the appropriate library file for 
your system (libFusionReactor.so / libFusionReactor-linux-x86_64.so) 
to a folder already within the library path (eg /opt/jrun4/bin)  

-PATH, CLASSPATH and LD_LIBRARY_PATH
The CLASSPATH environment variable tells the Java virtual machine where to find the class libraries, such as the jdmktk.jar file.

The PATH environment variable specifies the location of executable files, for example, the proxygen tool and the mibgen compiler.

If needed, the LD_LIBRARY_PATH environment variable specifies the location of native libraries, for example, the libstat.so kernel statistics library

Keystore, certificates
-----------------------


Keystore is used by a server to store private keys, and truststore is used by third party client to store public keys provided by server to access. I have did that in my production application. Below are the steps for generating java certificates for SSL communication:

1. Generate a certificate using keygen command in windows:

keytool -genkey -keystore server.keystore -alias mycert-20161109 -keyalg RSA -keysize 2048 -validity 3950

2. Self certify the certificate:

keytool -selfcert -alias mycert-20161109 -keystore server.keystore -validity 3950

3. Export certificate to folder:

keytool -export -alias mycert-20161109 -keystore server.keystore -rfc -file mycert-20161109.cer

4. Import Certificate into client Truststore:

keytool -importcert -alias mycert-20161218 -file C:\certs\mycert-20161218.cer -keystore .truststore



--------------------------------JVM perf training----------------------------------

Microbenchmarking -JMH
----------------------
cmpxchg

jdk 
native -> private lib -> public java.util....
JMH
asm

asm lang lib
put library at - --- /jre/lib/amd64/server/hsdis-amd64.so

Perf killers
-memory access
-locks

OS-running and blocking Q
LOCKS ARE BAD compare and exchange are better than lock
-(cost of retrial)
-starvation

New CPU instruction added to 8: witness XAdd(memLocation, increment)  [wait free ]


incrAndGet used compAndXchange in jdk 7 but uses XAdd in JDK8

Macro profilers
---------------

savePoint: time when all info are taken for JVM. All threads are stopped .. for GC etc. Glopal flag is checked by all threads. 
-s/w prof overhead in profiling time. No applicable for production
-no idea of h/w, filesystem... what is slow disk , filsystem
-sampling profiler: b/w save point so no idea when problem occured.


-perf is good profiler
-Need java and system profiler together
-jStack 

System profiler:
-perf_events aka perf: no s/w instrumentation, only H/w ins. Should be used in linux. No of event > no of PMC

CPU with spcl types of register
normal register
PMC: perf monitor Register

-events like cpu, pagefault cache-miss.. etc 

-JDK 8 added -fno-omit-frame-pointer -> preserve frame pointer. RBP register used as the base pointer of the stack
-overhead is < 1%. So use it freely

-Start java program with this option and then use perf. -X:+PerfFramePointer
-show perf the debugSymbol file to convert address Vs methodName at the address

rxNetty better than tomCat.

-perf & analyser

-front end cycle - instruction eneded up in RS
-back end cycle

RS: reservationStation
ROB: reorderingBuffer

-pushing into diff cache line
	-paddedAtomic class
	
-memory and obj layout: 	

likwid - tool to measure CPU reg
likwid -topology

java -XX:+PrintFlagsFinal
java -XX:+PrintFlagsFinal -XX:+UnlockDiagnosticVMOptions
java -XX:+PrintFlagsFinal -XX:+UnlockDiagnosticVMOptions -XX:+UnlockDiagnosticVMOptions 

PrintAssembly

Need Hsdis.dll / .so in location


Tools: 
-JCStress: concurrency stress: best concrr stress 
-perf: best profiling 
-perf report
-analyser
-jmh
-likwid (need to knw the kernel version)
-mpstat: (microprocessor )mpstat -P ALL
-vmstat: thread block status (park Vs unpark)
-ganglia + perf [hadoop / spark]
-solaris analyser (best): get some xtra from perf map the h/w counter with code
-JOL(openJDK):memory and obj layout 
-JOC: java only profiler
-Tracers
		-sa(jdk):tools to look at methodtable: part of JDK sa. Best debugger to look at heap and non heap portion. Need HSDB...
		-stap: system tap. Best traces. Bit complex(written in script version of C). Use /Good for heap profile. 
			-stap -L 'kernel.function("vfs.read")'
			-goto source page on net documentation... alll ex
			-pageCache
			-GC
				-stap -L 
				
		-lttng:linux tracing toolkit new gen. Tracer. 
		-ebps: extented berkley packet filters
-jcmd: collect all info  related to pid. with prop flagsAll gives all flag of running app
	-{manageble} - can be on / off remotely ie. can be changed on running application
-jmap: heapDump
-jvm tools: jcmd, jps
-jinfo <pid>
-GC: printGCDetails + printGCtimestamp / dateStamp + GCLogs
-jstat: jstat -gcutil <PID> <refreshNoOfMilliSec> 
-jhat
-read heapDump:
	-sa
-visualvm - load heapDump -> classes , sort on memory -> instances, show nearest GC root. attach source code to Visualvm
-gdb: coredump: need OS tool
-for window windbg
-jcore: create 
Java only profilers:
--------------------
-jmc: java only profiling(no OS etc) - paid:  Sampling profile. does not instrumenation so can be used at production
	- on/off no xtra cost
	- only on oracle jdk and not openJdk
-honestProfiler: java only, good. Open source
-JFR: dont profile at savepoint Java flight recorder. 
	-start dump and stop. 
****-vmstat: snapshot of machine.
	-1st thng for profiling
	-si so > 0 not performing well 

-iostat

-all my write directly go to pageCache and not to buffer
-debug symbol: linuxDebugSymbols


uname -r : linux ver. 
right debug symbols are not installed 

VMSTAT
Perf
analyzer / jmc


r run
b block
si swap in
so swap out



context swtch high
	- too many threads and not match to th eh/w
	- too many locks
Formula for no of threads:

Nthreads = Ncpu * Ucpu * (1 + w/c) wait to compute
iterations .. bind no of threads to CPU
start with noThread = noCpu
otherwise lot of contextSwitching

JMX - threadMxBean  	use JConsole


onErrorHeapDump 
jcmd (online, overhead)



hadoop etc 
negio
perf + ganglia

-bind the thread to CPU
-taskset -c 1,5: stick app to cpu core 1 7 5

volatile:
- variables cannot be re ordered
- var cannot be cached
- var follow happens before relationship
  lock addl $0x0


-sp stack pointer
rsp - 64 bit stack pointer


Atomic varaibles / classes:
1) reordering, caching, happens before semantics of atomic class are identical of a volatile class
2) this guarantee does not apply to certain case method like lazy set, weak comparator 
3) these spcl functn provide the guarantee of ordering without visibility



mod is slow as div is expensive operation in assmbly .. as CPU 1 port for DIV . Right shift ()
lock

MESI: modfd .. shared ..

false sharing problem??

TO explore:

?? Tungsten is for Spark (CPU opt for spark)
?? RDMA(remote direct .....)




Q
--
?how to read JMH output- 
?How to get to know for wait free alternative, in jdk 
?disable hyperThrd in server ?
?How to use perf for distributed applications: yes
?There is concept of internal and external in JOL

Spark:
?See logs statements in map(funct)
?How to optimize spark application, why slow, not all executors are getting resources



Day 2

JVM
----
-Vtables/methodTable maintained by JVM, every class has acorrsponding methodTable. Building methodTable at time of classLoading
-class has memory structure
-class loading: method structure and memory structure
-stap(impl of solaris dtrace): system tap. Best traces. Bit complex(written in script version of C).  
-sa: tools to look at methodtable: part of JDK sa. Best debugger to look at heap and non heap portion.
-lttng:linux tracing toolkit new gen. Tracer. 
-ebps: extented berkley packet filters
-lttng:linux tracing toolkit new gen 
-threadlocal <...> , to -XX:-UseTLab to switch off default memory allocation. + to on , - to off
-profiler(sampling.. ) vs tracer(deeper dive, like GC called with what param kind of use cases.)
-heap area: yg, tg, pg
-nonHeap area: JVM code, IO buffer
-JVM : 
	-single port for jump, so for loop takes time
	-GC, interpreter,profiler,dynamicCompiler
		-dynmcComp:[-client, -server, -XX:+TC (TieredCompositon)]. 
			-NetBeans uses -client (quickly optimize)
			- -Server
			- -XX:+TC (TieredCompilation): mix of both above. Optional in java 7 i.e false. In java 8 its true. at startup use client and then....
	-cold cachce: dynamicCompiled code kept at cold cache..
	-branch prediction
	-CPU uses spcl interpretation for branch prediction 
	-Jump: all loops(taken care by dynamic compiler) & methods(inline)
	-all method in java are virtual
	-inline virtual functions are difficult, java does that.
	-scala runs on JVM
	-java -XX:+PrintFinalFlags [java -XX:+PrintFlagsFinal -XX:+UnlockDiagnosticVMOptions]
	-JVM escape analysis
	-inlining: static,final,private  always. virtual: often, reflection: sometime
		-javac adds null check ..  that we see when npe is thrown. 
		-JVM rermoves unneccry null check (JVM escape analysis)
		-
	-OSR: on stack replacement technique
	-never executed, zombie, %, !, too big, not enterant(made some assumption but now it does n ot hold)
		- !: exception handling is happening in code. Its expensive
		- n: native method
		- %: OSR - for top level methods like main.. main with loop 
		- zombie: no longer used and candidate for GC
		- made non reentrant:
		- tierCompilation
		- too big
	-JumpTable: if the TypeProfileWidth > 2 , jvm uses it
-HeapDumpAnalysis
	-heapDump(heapdump) vs coreDump(heapdump osdump and non heapdump):

-top	
-JNA: java native API (jna.jar and jna.so)
	-java taskset: bind thrds to cpu
cat /proc/cpuinfo	
-openhft: high frequency trading
	-JTA: java thread affinity: can bind thrds to CPU. 
	Executor.newFixwdThreadPool(4, new AffinityThreadFActory("",SAME_CORE, DIFFERENT_SOCKET,ANY))... binds threads to cpu
	to check 
	Affinity.dumpLocks
	
-nonblockinghashmap better than concurrnt hashMap
-RandomClass uses the lock - use threadLocalRandom does not uses lock
-false sharing: in java counting not with false sharing(if core > 8) LongAdder, LongAccumulator / DoubleAdder, DoubleAccumulator

Some blogs / forums + reference book info + emailId

mohit.riverstone@gmail.com

Q
---
? Some blogs/forums + reference book info
- googleGroups: (mechanical-Sympathy), 
- jeremymanson.blogspot.com
- Cliff click (cliffc.org\blog)
- Dave dice oracle blog(David dice)
- NitSan's: 
- Alexsey shipiney
books
- Java perf tunning: scott oaks... 
- java perf by charlie han	
- openJdk source

?
?
*********************************************************

Sizes of primitive types
boolean & byte -- 1
char & short -- 2
int & float -- 4rfg
long & double -- 8

Each object has a certain overhead for its associated monitor and type information, 
as well as the fields themselves. Beyond that, fields can be laid out pretty much 
however the JVM sees fit 

public class SingleByte
{
    private byte b;
}
vs

public class OneHundredBytes
{
    private byte b00, b01, ..., b99;
}

100 instance of 1 byte VS 1 instance of 100 byte
-On a 32-bit JVM, 
	-I'd expect 100 instances of SingleByte to take 1200 bytes (8 bytes of overhead + 4 bytes for the field due to padding/alignment).
	-1 instance with 100 byte leading to it taking 408 bytes (= 8 bytes overhead + 4 * 100 aligned/padded bytes).

-On a 64-bit JVM, data may differ

**For modern JDK and 64bit architecture object has 12-bytes header and padding by 8 bytes - so minimum object size is 16 bytes. 

*********************************************************

Urls:
HDFS
		
Hadoop operations:
-
Hadoop yarn internals
-
SparkInternals
-
HBase
-
Oozie
-
Hadoop security
-Kerbros
Hadoop serialization issues:

-Kryo
?Why kryo and not Java serialization
?Where to use it


*****************************************************************************************************************


JavaDocs

http://docs.oracle.com/javase/7/docs/technotes/tools/windows/javadoc.html




Java Collections:
-HashMap | ConcurrentHashMap | nonblockingHashmap 
-Generics

Threads:
-ThreadLocal
-Locks
-ConsumerProducer 

-Java security(last)
-DesignPatterns

JDK - 8
-lambda + its internals 
-
 
Hadoop 
-HDFS
-HBase
-Oozie
-spark
	-anatomy of a program/lifeCycle
	-why faster than Mapreduce
-zooKeeper
-Security in all
-serialization/compression
	-protocolBuffer
	-parquet
-UseCase
	-trending etc
-kafka
	-https://www.infoq.com/articles/apache-kafka

-Distributed data structures like locks, queues, barriers, and latches	

To Read
https://www.javacodegeeks.com/2016/08/functional-approach-logging-apache-spark.html



Q&A
1.What is Hadoop framework?
2.On What concept the Hadoop framework works?
3.what is HDFS?
4.what is the patition tolerance in hadoop?
5.what is MAP REDUCE?
6.What Mapper does?
7.what is meaning Replication factor?
8.what is the default replication factor in HDFS?
9.what is the typical block size of an HDFS block?
10.what is a datanode?	
11.what is namenode?	
12.NameNode is single point of failure . what are remedies to overcome  this problem explain?
13.How does master slave architecture in the Hadoop?
14.How many maps are there in a particular Job?
15.What is the Reducer used for?
16.How many instances of JobTracker can run on a Hadoop Cluser?
17.how does job tracker knows whether tasktracker is running or not?
18.What alternate way does HDFS provides to recover data in case a Namenode, 	without backup, fails and cannot be recovered?
19.what is the scalability hadoop explain?
20.where exactly job tracker runs?
21.how does namenode knows whether datanode is running or not?
22.where exactly task tracker runs?
23.what is the size of each split"
24.explain what is FSImage and EditLogs files
25.what is bigdata?


		Paper-2(Hadoop)(10 to 11 am)
1)What is a commodity hardware? Does commodity hardware include RAM?
2)Is Namenode also a commodity?
3)What is a heartbeat in HDFS?
4)Are Namenode and job tracker on the same host?
5)If a particular file is 50 mb, will the HDFS block still consume 64 mb as the default size?
6)A user is like you or me, who has some query or who needs some kind of data.Is client the end user in HDFS?
7)On what basis data will be stored on a rack?
8)What is a Secondary Namenode? Is it a substitute to the Namenode?
9)Why ‘Reading‘ is done in parallel and ‘Writing‘ is not in HDFS?
10)What is a JobTracker in Hadoop? How many instances of JobTracker run on a Hadoop Cluster?
11)What is a Task Tracker in Hadoop? How many instances of TaskTracker run on a Hadoop Cluster
12)What is configuration of a typical slave node on Hadoop cluster? How many JVMs run on a slave node?
13)Does MapReduce programming model provide a way for reducers to communicate with each other? In a MapReduce job can a reducer communicate )with another reducer?
15)What is the meaning of speculative execution in Hadoop? Why is it important?
16)If reducers do not start before all mappers finish then why does the progress on MapReduce job shows something like Map(50%) Reduce(10%)? Why reducers progress percentage is displayed when mapper is not finished yet?

17)What is the difference between a Hadoop database and Relational Database?
What is Hadoop framework?
18)On What concept the Hadoop framework works?
19)what is HDFS?
20)what is MAP REDUCE?
21)What Mapper does?
22)What is the InputSplit in map reduce software?
23)what is meaning Replication factor?
24)what is the default replication factor in HDFS?
25)Explain the WordCount implementation via Hadoop framework ?
26)Which interface needs to be implemented to create Mapper and Reducer for the Hadoop?
27)What is the InputFormat ?
28)How does Mappers run() method works?
29)what is the typical block size of an HDFS block?
34)How many maps are there in a particular Job?
36)What are the primary phases of the Reducer?
37)Explain the shuffle?	
40)It can be possible that a Job has 0 reducers?
41)What happens if number of reducers are 0?
42)How many instances of JobTracker can run on a Hadoop Cluser?
43)Explain the Reducer?s Sort phase?
44)Explain the core methods of the Reducer?
47)What is the use of Context object?
52)What are the key features of HDFS?
53)What is Fault Tolerance?

		Paper-3(Hadoop)(10 to 11 am)
1)What is a commodity hardware? Does commodity hardware include RAM?
2)Is Namenode also a commodity?
3)What is a heartbeat in HDFS?
4)Are Namenode and job tracker on the same host?
5)If a particular file is 50 mb, will the HDFS block still consume 64 mb as the default size?
6)A user is like you or me, who has some query or who needs some kind of data.Is client the end user in HDFS?
7)On what basis data will be stored on a rack?
8)What is a Secondary Namenode? Is it a substitute to the Namenode?
9)Why ‘Reading‘ is done in parallel and ‘Writing‘ is not in HDFS?
10)What is a JobTracker in Hadoop? How many instances of JobTracker run on a Hadoop Cluster?
11)What is a Task Tracker in Hadoop? How many instances of TaskTracker run on a Hadoop Cluster
12)What is configuration of a typical slave node on Hadoop cluster? How many JVMs run on a slave node?
13)Does MapReduce programming model provide a way for reducers to communicate with each other? In a MapReduce job can a reducer communicate )with another reducer?
15)What is the meaning of speculative execution in Hadoop? Why is it important?
16)If reducers do not start before all mappers finish then why does the progress on MapReduce job shows something like Map(50%) Reduce(10%)? Why reducers progress percentage is displayed when mapper is not finished yet?
17)What is the difference between a Hadoop database and Relational Database?
What is Hadoop framework?
18)On What concept the Hadoop framework works?
19)what is HDFS?
20)what is MAP REDUCE?
21)What Mapper does?
22)What is the InputSplit in map reduce software?
23)what is meaning Replication factor?
24)what is the default replication factor in HDFS?
25)Explain the WordCount implementation via Hadoop framework ?
26)Which interface needs to be implemented to create Mapper and Reducer for the Hadoop?
27)What is the InputFormat ?
28)How does Mappers run() method works?
29)what is the typical block size of an HDFS block?
34)How many maps are there in a particular Job?
36)What are the primary phases of the Reducer?
37)Explain the shuffle?	
40)It can be possible that a Job has 0 reducers?
41)What happens if number of reducers are 0?
42)How many instances of JobTracker can run on a Hadoop Cluser?
43)Explain the Reducer?s Sort phase?
44)Explain the core methods of the Reducer?
47)What is the use of Context object?
52)What are the key features of HDFS?
53)What is Fault Tolerance?

HDFS Questions

1. Filesystems that manage the storage across a network of machines are called 
A. Distributed Filesystems 
B. Clusters
C. Partitions
D.Server


2. Hadoop comes with a distributed filesystem called
A.NDFS
B.GFS
C.HDFS
D.HFS


3.HDFS stands for
A. Hadoop Dynamic Filsystem
B. Hadoop Debug Filesystem
C. Hadoop Dangerous Filesystem
D. None of the above


4. HDFS is a filesystem designed for
A. Storing large files
B. Commodity Hardware
C. To Support Nutch
D. All Correct


5. Hadoop clusters running today that stores
A. bytes of data
B. gigabytes of data
C. petabytes of data
D. megabytes of data


6. HDFS is built around the idea that the most efficient data processing pattern is a
A. write-once, read-many-times
B. write-many-times, read-many-times
C. write-once, read-once
D. write-many-times, read-once


7. Commodity Hardware means more expensive
A. True
B. False


8. Which is not in HDFS?
A. Namenode
B. Secondary Datanode
C. Datanode
D. Secondary Namenode


9. Namenode holds ................. in memeory
A. Text files
B. Images
C. Metadata
D. Rawdata


10. As a rule of thumb, each file, directory, and block takes about 
A. 150 bytes
B. 4 kb
C. 64 mb
D. 1 byte
E. None of the above


11. What is the  purpose of  Block in HDFS?
A. We can modify data
B. We can't see data
C. We can see, but cannot modify data
D. There is no block in HDFS


12. What is Block?
A. A logical storage
B. A physical storage
C. It is not storage
D. None of the above


13. The typical Block size is?
A. 128 mb
B. 256 mb
C. 64 mb
D. 512 mb


14. The default block size is?
A. 128 mb
B. 512 mb
C. 256 mb
D. 64 mb



15. Blocks are a fixed size?
A. True
B. False


16. Will list the blocks that make up each file in the filesystem?
A. hadoop fsck / -files -blocks
B. hadoop fs -ls
C. hadoop blocks / -ls
D. hadoop fs -mkdir


17. An HDFS cluster has two types of nodes operating in a master-worker pattern:
A. Namenode- Master, Datanode-Worker
B. Namenode- Worker, Datanode- Master
C. Namenode- Master, Secondary Namenode- worker
D. None of the above


18. The namenode manages the filesystem
A. Blocks
B. Namespace
C. Raw data
D. Secondary Namenode


19. Without the namenode,
A. The filesystem can run
B. The filesystem cannot run
C. The filesystem can run, but cannot access
D. A & C


20. What is Secondary Namenode?
A. It is backup node for Namenode
B. It is optional node in HDFS
C. Without Secondary Namenode, Filesystem cannot work
D. We can replace Namenode with Secondary Namenode


21. Namenode is Single point of failure
A. True
B. False


22. Minimum replication factor in HDFS is
A. 1
B. 0
C. -1
D. 3


23. Default replication factor in HDFS is
A. 2
B. 3
C. 1
D. None


24. Maximum replication factor in HDFS is
A. 64
B. 512
C. 256
D. 128
E. None


25. Hadoop is written in
A. C++
B. Erlang
C. Bigdata
D. Java


26. Whats the default  port where the dfs namenode web ui will listen on
A. 8080
B. 5070
C. 50070
D.8085


27. HDFS is block structured file system" means 
A. In HDFS individual files are broken into blocks of a fixed size
B. in HDFS blocks are partitioned into individual files
C. None


28. You can choose replication factor per directory ?
A. True
B. False


29. You can choose replication factor per file in a directory?
A. True
B. False




30. You can choose replication factor per block of a file 
A. True
B. False


31. To list the the number of files in a HDFS directory?
A. hadoop fs ls
B. hadoop ls -fs
C. hadoop fs -ls
D. ls


32. To create a directory in HDFS?
A. hadoop fs mkdir
B. hadoop fs -mkdir
C. mkdir
D. hadoop fs -cat


33. To copy file from your local directory to HDFS?
A. hadoop fs -put <hdfsfile> <localfile>
B. hadoop fs -copy <localfile> <hdfsfile>
C. hadoop fs -put <localfile> <hdfsfile>
D. hadoop fs -copyFromLocal <hdfsfile> <localfile>


34. Can we do online transactions(OLTP) with HDFS?
A. True
B. False


35. chmod command for
A. to change directory to file
B. to change file permissions only
C. to change directory permissions only
D. B & C


36. can we create a file in HDFS?
A. Yes
B. No


37. Can we create a directory in HDFS?
A. Yes
B. No



38. How to create a file in HDFS?
A. hadoop fs -mkfile
B. hadoop fs mkfile
C. cannot create file
D. None


39. How to create a directory in HDFS?
A. hadoop fs -mkdir
B. hadoop fs mkdir
C. hadoop fs -ls
D.cannot create directory


40. What will get by this command 'hadoop dfsadmin -safemode get'.
A. Namenode will ON
B. Datanode will ON
C. Namenode will OFF
D. display status Namenode


41. Which command to switch Namenode safemode status as ON
A. hadoop dfsadmin -safemode on
B. hadoop dfsadmin -safemode get
C. hadoop dfsadmin -safemode enter
D. None


42. Which command to switch Namenode safemode status as OFF
A. hadoop dfsadmin -safemode off
B. hadoop dfsadmin -safemode leave
C. hadoop dfsadmin -safemode get
D. None


43. Which command to get Namenode safemode status?
A. hadoop dfsadmin -safemode off
B. hadoop dfsadmin -safemode leave
C. hadoop dfsadmin -safemode get
D. None


44. hadoop fs -rm command for
A. to delete a directory from HDFS
B. to delete multiple files from HDFS
C. to delete a file from HDFS
D. All



45. hadoop fs -rmr command for
A. to delete a directory from HDFS
B. to delete multiple files from HDFS
C. to delete a file from HDFS
D. A & C


46. How to get report for HDFS?
A. hadoop dfsadmin -info
B. hadoop dfsadmin -report
C. hadoop dfsadmin -get report
D. All


47. What is Heartbeat time for Datanode?
A. 5 seconds
B. 6 seconds
C. 3 seconds
D. 10 seconds


48. Can we change Heartbeat time for Datanode?
A. Yes
B. No, It is not possible


49. In what time Secondary Namenode will communicate with Namenode?
A. 60 Minutes
B. 30 Minutes
C. 10 Minutes
D. 5 Minutes


50. Apache Hadoop is Open Source project to support Lucene and Nutch web search engines.
A. True
B.. False


51. In which file we will configure replication factor?
A. conf/core-site.xml
B. conf/mapred-site.xml
C. conf/hdfs-site.xml
D. conf/hadoop-env.xml


52. In which file we will configure hdfs?
A. conf/core-site.xml
B. conf/mapred-site.xml
C. conf/hdfs-site.xml
D. conf/hadoop-env.xml

53. In which file we will configure mapreduce?
A. conf/core-site.xml
B. conf/mapred-site.xml
C. conf/hdfs-site.xml
D. conf/hadoop-env.xml


54. In which file we will configure block size?
A. conf/core-site.xml
B. conf/mapred-site.xml
C. conf/hdfs-site.xml
D. conf/hadoop-env.xml


55. In which file we will configure JAVA_HOME path?
A. conf/core-site.xml
B. conf/mapred-site.xml
C. conf/hdfs-site.xml
D. conf/hadoop-env.xml


56. In which directory, default configuration files available?
A. bin
B. lib
C. conf
D. src


57. What is the command to format Namenode?
A. hadoop fs namenode -format
B. hadoop namenode -format
C. hadoop fs -format
D. hadoop namenode -clean


58. What is the command to start all daemon services?
A. start-dfs.sh
B. start-mapred.sh
C. start-all.sh
D. None


59. What is the command to start  HDFS services?
A. start-dfs.sh
B. start-mapred.sh
C. start-all.sh
D. None



60. What is the command to start mapreduce services?
A. start-dfs.sh
B. start-mapred.sh
C. start-all.sh
D. None


61. What is the command to stop all daemon services?
A. stop-dfs.sh
B. stop-mapred.sh
C. stop-all.sh
D. None


62. What is the command to stop HDFS services?
A. stop-dfs.sh
B. stop-mapred.sh
C. stop-all.sh
D. None


63. What is the command to stop mapreduce services?
A. stop-dfs.sh
B. stop-mapred.sh
C. stop-all.sh
D. None


64. By which command we will test, which services are running?
A. cps
B. mps
C. jps
D. run service


65. To install hadoop cluster, we need to have java in machine?
A. Yes
B. No


66. Abbreviation for IPV6 is?
A. Intranet protocol variety 6
B. Internet protocol vision 6
C. Internet protocol version 6
D. None




67. What are pre-requisites to avoid Namenode failures?
A. High Configured Machine
B. Take backup
C. Limit Scaling
D. All
E. A&C


68. In what conditions Namenode  will fail?
A. Software/Hardware failures
B. Unlimited Scaling
C. Running more number jobs
D. All


69. What will happen if Namenode fail?
A. Nothing will happen
B. All processes will stop
C. Datanode will take responsibility
D. B & C


70. Is there any backup node for Namenode in HDFS?
A. Yes
B. No


71. How to display file data in terminal?
A. hadoop fs -display 
B. hadoop fs -view
C. hadoop fs -cat
D. hadoop fs -gedit


72. How to Copy HDFS files to Local Disk?
A. hadoop fs -put <hdfsfile> <localfile>
B. hadoop fs -copy <hdfsfile> <localfile>
C. hadoop fs -copyToLocal <localfile> <hdfsfile>
D. hadoop fs -get <hdfsfile> <localfile>


73. How to move HDFS files to Local Disk?
A. hadoop fs -put <hdfsfile> <localfile>
B. hadoop fs -move <hdfsfile> <localfile>
C. hadoop fs -moveToLocal <hdfsfile> <localfile>
D. hadoop fs -get <hdfsfile> <localfile>



74. How many datanodes we need for typical hadoop cluster?
A. 3
B. 4
C. 5
D. 6


75. HDFS is like a
A. Database
B. Operating System
C. Datawarehouse
D. None


76.HDFS stands for
A. Hadoop Dynamic Filsystem
B. Hadoop Debug Filesystem
C. Hadoop Distributed Filesystem
D. None of the above


77. HDFS is a filesystem designed for
A. Storing large files
B. Commodity Hardware
C. To Support Nutch
D. All Correct


78. Hadoop clusters running today that stores
A. bytes of data
B. gigabytes of data
C. petabytes of data
D. megabytes of data


79. Default replication factor in HDFS is
A. 1
B. 2
C. 3
D. None


80. Maximum replication factor in HDFS is
A. 64
B. 128
C. 256
D. 512
E. None



81. Hadoop is written in
A. C++
B. Java
C. Bigdata
D. C Language


82. What's the default  port where the dfs namenode web ui will listen on
A. 8080
B. 5070
C. 8085
D. 50070


83. You can choose replication factor per block of a file 
A. True
B. False


84. To list the the number of files in a HDFS directory?
A. hadoop fs ls
B. hadoop ls -fs
C. hadoop fs -ls
D. ls


85. To create a directory in HDFS?
A. hadoop fs mkdir
B. hadoop fs -mkdir
C. mkdir
D. hadoop fs -cat


86. To copy file from your local directory to HDFS?
A. hadoop fs -put <hdfsfile> <localfile>
B. hadoop fs -copy <localfile> <hdfsfile>
C. hadoop fs -put <localfile> <hdfsfile>
D. hadoop fs -copyFromLocal <hdfsfile> <localfile>


87. What is Heartbeat time for Datanode?
A. 5 seconds
B. 6 seconds
C. 3 seconds
D. 10 seconds


88. Can we change Heartbeat time for Datanode?
A. Yes
B. No, It is not possible

89. In what time Secondary Namenode will communicate with Namenode?
A. 60 Minutes
B. 30 Minutes
C. 10 Minutes
D. 5 Minutes


90. Apache Hadoop is Open Source project to support Lucene and Nutch web search engines.
A. True
B.. False


---
HDFS Questions

1. Filesystems that manage the storage across a network of machines are called 
A. Distributed Filesystems 
B. Clusters
C. Partitions
D.Server
Answer: A

2. Hadoop comes with a distributed filesystem called
A.NDFS
B.GFS
C.HDFS
D.HFS
Answer: C

3.HDFS stands for
A. Hadoop Dynamic Filsystem
B. Hadoop Debug Filesystem
C. Hadoop Dangerous Filesystem
D. None of the above
Answer: D

4. HDFS is a filesystem designed for
A. Storing large files
B. Commodity Hardware
C. To Support Nutch
D. All Correct
Answer: D

5. Hadoop clusters running today that stores
A. bytes of data
B. gigabytes of data
C. petabytes of data
D. megabytes of data
Answer: C

6. HDFS is built around the idea that the most efficient data processing pattern is a
A. write-once, read-many-times
B. write-many-times, read-many-times
C. write-once, read-once
D. write-many-times, read-once
Answer: A

7. Commodity Hardware means more expensive
A. True
B. False
Answer: B

8. Which is not in HDFS?
A. Namenode
B. Secondary Datanode
C. Datanode
D. Secondary Namenode
Answer: B

9. Namenode holds ................. in memeory
A. Text files
B. Images
C. Metadata
D. Rawdata
Answer: C

10. As a rule of thumb, each file, directory, and block takes about 
A. 150 bytes
B. 4 kb
C. 64 mb
D. 1 byte
E. None of the above
Answer: A

11. What is the  purpose of  Block in HDFS?
A. We can modify data
B. We can't see data
C. We can see, but cannot modify data
D. There is no block in HDFS
Answer: C

12. What is Block?
A. A logical storage
B. A physical storage
C. It is not storage
D. None of the above
Answer: A

13. The typical Block size is?
A. 128 mb
B. 256 mb
C. 64 mb
D. 512 mb
Answer: A

14. The default block size is?
A. 128 mb
B. 512 mb
C. 256 mb
D. 64 mb
Answer: D 


15. Blocks are a fixed size?
A. True
B. False
Answer: A

16. Will list the blocks that make up each file in the filesystem?
A. hadoop fsck / -files -blocks
B. hadoop fs -ls
C. hadoop blocks / -ls
D. hadoop fs -mkdir
Answer: A

17. An HDFS cluster has two types of nodes operating in a master-worker pattern:
A. Namenode- Master, Datanode-Worker
B. Namenode- Worker, Datanode- Master
C. Namenode- Master, Secondary Namenode- worker
D. None of the above
Answer: A

18. The namenode manages the filesystem
A. Blocks
B. Namespace
C. Raw data
D. Secondary Namenode
Answer: B

19. Without the namenode,
A. The filesystem can run
B. The filesystem cannot run
C. The filesystem can run, but cannot access
D. A & C
Answer: B

20. What is Secondary Namenode?
A. It is backup node for Namenode
B. It is optional node in HDFS
C. Without Secondary Namenode, Filesystem cannot work
D. We can replace Namenode with Secondary Namenode
Answer: B

21. Namenode is Single point of failure
A. True
B. False
Answer: A

22. Minimum replication factor in HDFS is
A. 1
B. 0
C. -1
D. 3
Answer: A

23. Default replication factor in HDFS is
A. 2
B. 3
C. 1
D. None
Answer: B

24. Maximum replication factor in HDFS is
A. 64
B. 512
C. 256
D. 128
E. None
Answer: B

25. Hadoop is written in
A. C++
B. Erlang
C. Bigdata
D. Java
Answer: D

26. Whats the default  port where the dfs namenode web ui will listen on
A. 8080
B. 5070
C. 50070
D.8085
Answer: C

27. HDFS is block structured file system" means 
A. In HDFS individual files are broken into blocks of a fixed size
B. in HDFS blocks are partitioned into individual files
C. None
Answer: A

28. You can choose replication factor per directory ?
A. True
B. False
Answer: A

29. You can choose replication factor per file in a directory?
A. True
B. False
Answer: A



30. You can choose replication factor per block of a file 
A. True
B. False
Answer: B

31. To list the the number of files in a HDFS directory?
A. hadoop fs ls
B. hadoop ls -fs
C. hadoop fs -ls
D. ls
Answer: C

32. To create a directory in HDFS?
A. hadoop fs mkdir
B. hadoop fs -mkdir
C. mkdir
D. hadoop fs -cat
Answer: B

33. To copy file from your local directory to HDFS?
A. hadoop fs -put <hdfsfile> <localfile>
B. hadoop fs -copy <localfile> <hdfsfile>
C. hadoop fs -put <localfile> <hdfsfile>
D. hadoop fs -copyFromLocal <hdfsfile> <localfile>
Answer: A

34. Can we do online transactions(OLTP) with HDFS?
A. True
B. False
Answer: B

35. chmod command for
A. to change directory to file
B. to change file permissions only
C. to change directory permissions only
D. B & C
Answer: D

36. can we create a file in HDFS?
A. Yes
B. No
Answer: B

37. Can we create a directory in HDFS?
A. Yes
B. No
Answer: A


38. How to create a file in HDFS?
A. hadoop fs -mkfile
B. hadoop fs mkfile
C. cannot create file
D. None
Answer: C

39. How to create a directory in HDFS?
A. hadoop fs -mkdir
B. hadoop fs mkdir
C. hadoop fs -ls
D.cannot create directory
Answer: A

40. What will get by this command 'hadoop dfsadmin -safemode get'.
A. Namenode will ON
B. Datanode will ON
C. Namenode will OFF
D. display status Namenode
Answer: D 

41. Which command to switch Namenode safemode status as ON
A. hadoop dfsadmin -safemode on
B. hadoop dfsadmin -safemode get
C. hadoop dfsadmin -safemode enter
D. None
Answer: C

42. Which command to switch Namenode safemode status as OFF
A. hadoop dfsadmin -safemode off
B. hadoop dfsadmin -safemode leave
C. hadoop dfsadmin -safemode get
D. None
Answer: B

43. Which command to get Namenode safemode status?
A. hadoop dfsadmin -safemode off
B. hadoop dfsadmin -safemode leave
C. hadoop dfsadmin -safemode get
D. None
Answer: C

44. hadoop fs -rm command for
A. to delete a directory from HDFS
B. to delete multiple files from HDFS
C. to delete a file from HDFS
D. All
Answer: C


45. hadoop fs -rmr command for
A. to delete a directory from HDFS
B. to delete multiple files from HDFS
C. to delete a file from HDFS
D. A & C
Answer: D

46. How to get report for HDFS?
A. hadoop dfsadmin -info
B. hadoop dfsadmin -report
C. hadoop dfsadmin -get report
D. All
Answer: B

47. What is Heartbeat time for Datanode?
A. 5 seconds
B. 6 seconds
C. 3 seconds
D. 10 seconds
Answer: C

48. Can we change Heartbeat time for Datanode?
A. Yes
B. No, It is not possible
Answer: B

49. In what time Secondary Namenode will communicate with Namenode?
A. 60 Minutes
B. 30 Minutes
C. 10 Minutes
D. 5 Minutes
Answer: A

50. Apache Hadoop is Open Source project to support Lucene and Nutch web search engines.
A. True
B.. False
Answer: A

51. In which file we will configure replication factor?
A. conf/core-site.xml
B. conf/mapred-site.xml
C. conf/hdfs-site.xml
D. conf/hadoop-env.xml
Answer: C

52. In which file we will configure hdfs?
A. conf/core-site.xml
B. conf/mapred-site.xml
C. conf/hdfs-site.xml
D. conf/hadoop-env.xml
Answer: A
53. In which file we will configure mapreduce?
A. conf/core-site.xml
B. conf/mapred-site.xml
C. conf/hdfs-site.xml
D. conf/hadoop-env.xml
Answer: B

54. In which file we will configure block size?
A. conf/core-site.xml
B. conf/mapred-site.xml
C. conf/hdfs-site.xml
D. conf/hadoop-env.xml
Answer: C

55. In which file we will configure JAVA_HOME path?
A. conf/core-site.xml
B. conf/mapred-site.xml
C. conf/hdfs-site.xml
D. conf/hadoop-env.xml
Answer: D

56. In which directory, default configuration files available?
A. bin
B. lib
C. conf
D. src
Answer: D

57. What is the command to format Namenode?
A. hadoop fs namenode -format
B. hadoop namenode -format
C. hadoop fs -format
D. hadoop namenode -clean
Answer: B

58. What is the command to start all daemon services?
A. start-dfs.sh
B. start-mapred.sh
C. start-all.sh
D. None
Answer: C

59. What is the command to start  HDFS services?
A. start-dfs.sh
B. start-mapred.sh
C. start-all.sh
D. None
Answer: A


60. What is the command to start mapreduce services?
A. start-dfs.sh
B. start-mapred.sh
C. start-all.sh
D. None
Answer: B

61. What is the command to stop all daemon services?
A. stop-dfs.sh
B. stop-mapred.sh
C. stop-all.sh
D. None
Answer: C

62. What is the command to stop HDFS services?
A. stop-dfs.sh
B. stop-mapred.sh
C. stop-all.sh
D. None
Answer: A

63. What is the command to stop mapreduce services?
A. stop-dfs.sh
B. stop-mapred.sh
C. stop-all.sh
D. None
Answer: B

64. By which command we will test, which services are running?
A. cps
B. mps
C. jps
D. run service
Answer: C

65. To install hadoop cluster, we need to have java in machine?
A. Yes
B. No
Answer: A

66. Abbreviation for IPV6 is?
A. Intranet protocol variety 6
B. Internet protocol vision 6
C. Internet protocol version 6
D. None
Answer: C



67. What are pre-requisites to avoid Namenode failures?
A. High Configured Machine
B. Take backup
C. Limit Scaling
D. All
E. A&C
Answer: D

68. In what conditions Namenode  will fail?
A. Software/Hardware failures
B. Unlimited Scaling
C. Running more number jobs
D. All
Answer: D

69. What will happen if Namenode fail?
A. Nothing will happen
B. All processes will stop
C. Datanode will take responsibility
D. B & C
Answer: B

70. Is there any backup node for Namenode in HDFS?
A. Yes
B. No
Answer: B

71. How to display file data in terminal?
A. hadoop fs -display 
B. hadoop fs -view
C. hadoop fs -cat
D. hadoop fs -gedit
Answer: C

72. How to Copy HDFS files to Local Disk?
A. hadoop fs -put <hdfsfile> <localfile>
B. hadoop fs -copy <hdfsfile> <localfile>
C. hadoop fs -copyToLocal <localfile> <hdfsfile>
D. hadoop fs -get <hdfsfile> <localfile>
Answer: D

73. How to move HDFS files to Local Disk?
A. hadoop fs -put <hdfsfile> <localfile>
B. hadoop fs -move <hdfsfile> <localfile>
C. hadoop fs -moveToLocal <hdfsfile> <localfile>
D. hadoop fs -get <hdfsfile> <localfile>
Answer: C


74. How many datanodes we need for typical hadoop cluster?
A. 3
B. 4
C. 5
D. 6
Answer: B

75. HDFS is like a
A. Database
B. Operating System
C. Datawarehouse
D. None
Answer: B

76.HDFS stands for
A. Hadoop Dynamic Filsystem
B. Hadoop Debug Filesystem
C. Hadoop Distributed Filesystem
D. None of the above
Answer: C

77. HDFS is a filesystem designed for
A. Storing large files
B. Commodity Hardware
C. To Support Nutch
D. All Correct
Answer: D

78. Hadoop clusters running today that stores
A. bytes of data
B. gigabytes of data
C. petabytes of data
D. megabytes of data
Answer: C

79. Default replication factor in HDFS is
A. 1
B. 2
C. 3
D. None
Answer: C

80. Maximum replication factor in HDFS is
A. 64
B. 128
C. 256
D. 512
E. None
Answer: D


81. Hadoop is written in
A. C++
B. Java
C. Bigdata
D. C Language
Answer: B

82. What's the default  port where the dfs namenode web ui will listen on
A. 8080
B. 5070
C. 8085
D. 50070
Answer: D

83. You can choose replication factor per block of a file 
A. True
B. False
Answer: B

84. To list the the number of files in a HDFS directory?
A. hadoop fs ls
B. hadoop ls -fs
C. hadoop fs -ls
D. ls
Answer: C

85. To create a directory in HDFS?
A. hadoop fs mkdir
B. hadoop fs -mkdir
C. mkdir
D. hadoop fs -cat
Answer: B

86. To copy file from your local directory to HDFS?
A. hadoop fs -put <hdfsfile> <localfile>
B. hadoop fs -copy <localfile> <hdfsfile>
C. hadoop fs -put <localfile> <hdfsfile>
D. hadoop fs -copyFromLocal <hdfsfile> <localfile>
Answer: A

87. What is Heartbeat time for Datanode?
A. 5 seconds
B. 6 seconds
C. 3 seconds
D. 10 seconds
Answer: C

88. Can we change Heartbeat time for Datanode?
A. Yes
B. No, It is not possible
Answer: B
89. In what time Secondary Namenode will communicate with Namenode?
A. 60 Minutes
B. 30 Minutes
C. 10 Minutes
D. 5 Minutes
Answer: A

90. Apache Hadoop is Open Source project to support Lucene and Nutch web search engines.
A. True
B.. False
Answer: A

----


•	What is Hadoop? Brief about the components of  Hadoop.
•	What are the Hadoop daemon processes tell the components of Hadoop and functionality?
•	Tell steps for configuring Hadoop?
•	What is architecture of HDFS and flow?
•	Can we have more than one configuration setting for Hadoop cluster how can you switch between these configurations?
•	What will be your troubleshooting approach in Hadoop?
•	What are the exceptions you have come through while working on Hadoop, what was your approach for getting rid of those exceptions or errors?

•	How will you proceed if Namenode is down?
•	What will be your approach if Datanode is down?
•	How can you start the cluster?
•	How can you stop the cluster?
•	What is dfs.name.dir and dfs.data.dir is used for?
•	What is SSH?
•	What is password less SSH?
•	Why do we need password less SSH in Hadoop?
•	How can you transfer configuration files of Hadoop from one system to another system
•	Have you ever come across bind exception while configuring cluster? How you solved it? Why it comes
•	When do you get connection refused error? How can you solve this problem
•	Have you ever come across "no route to host" error? If yes how you solved?
•	What is socket timeout error? And where it effect in Hadoop cluster?
•	How unknown host errors make sense to you?
•	Could not replicate data have you ever come across this error if yes what is the probable reason behind it?
•	What is heap memory? How we use it in Hadoop cluster?
•	What is a zombie process in Linux?
•	What zero size file problem in Hadoop and what is the reason behind it?
•	What is over replicated, under replicated blocks give some scenarios
•	Have you ever come across " too many file open error"
•	How communication between clients-->Namenode->Datanode happens? Explain the steps by which file is send from client the HDFS.
•	what configuration settings will you implement for HBase and hive with Hadoop
•	Have you ever come across HMaster not running ?? If yes what was the reason and how you solved it?
•	What are the daemon processes for HBase?
•	What may be the problem behind region servers being not started.
•	How many way you can execute queries in hive?
•	Have you ever come across error when master initializes, but region servers do not. What solution did you synthesized from that?
•	What is the role of JVM in Hadoop
•	Have you heard about JPS command? How can you use it and what you need to install before using that command?
•	 How can you configure a client for HBase? If yes what were the settings you used?
•	What is zookeeper?
•	How we can load table in hive?
•	How can we start hive server?
•	What is thrift server?
•	Can you tell me basic syntax for connecting hive through JDBC ?
•	What are no SQL databases how they differ from relational database, what are the NSQL databases you know?
•	Have you ever come across java.lang.outofmemoryerror?
•	How will you check if Hadoop Daemons(Namenode, Datanode, Jobtracker, Tasktracker, and Secondrynamenode) are running?


•	Q1. What are the default configuration files that are used in Hadoop
As of 0.20 release, Hadoop supported the following read-only default configurations
- src/core/core-default.xml
- src/hdfs/hdfs-default.xml
- src/mapred/mapred-default.xml
•	Q2. How will you make changes to the default configuration files
Hadoop does not recommends changing the default configuration files, instead it recommends making all site specific changes in the following files
- conf/core-site.xml
- conf/hdfs-site.xml
- conf/mapred-site.xml
•	Unless explicitly turned off, Hadoop by default specifies two resources, loaded in-order from the classpath:
- core-default.xml : Read-only defaults for hadoop.
- core-site.xml: Site-specific configuration for a given hadoop installation.
•	Hence if same configuration is defined in file core-default.xml and src/core/core-default.xml then the values in file core-default.xml (same is true for other 2 file pairs) is used.
•	Q3. Consider case scenario where you have set property mapred.output.compress to true to ensure that all output files are compressed for efficient space usage on the cluster. If a cluster user does not want to compress data for a specific job then what will you recommend him to do ?
Ask him to create his own configuration file and specify configuration mapred.output.compress to false and load this file as a resource in his job.
•	Q4. In the above case scenario, how can ensure that user cannot override the configuration mapred.output.compress to false in any of his jobs
This can be done by setting the property final to true in the core-site.xml file
•	Q5. What of the following is the only required variable that needs to be set in file conf/hadoop-env.sh for hadoop to work
-	HADOOP_LOG_DIR
-	JAVA_HOME
- HADOOP_CLASSPATH
The only required variable to set is JAVA_HOME that needs to point to directory
•	Q6. List all the daemons required to run the Hadoop cluster
- NameNode
- DataNode
- JobTracker
- TaskTracker
•	Q7. Whats the default port that jobtrackers listens to
50030
•	Q8. Whats the default port where the dfs namenode web ui will listen on
50070
Posted by Aman at 9:04 AM 0 comments
Email ThisBlogThis!Share to TwitterShare to Facebook
Sunday, November 28, 2010
Java interview questions for Hadoop developer Part 3
Q21. Explain difference of Class Variable and Instance Variable and how are they declared in Java
Class Variable is a variable which is declared with static modifier.
Instance variable is a variable in a class without static modifier.
The main difference between the class variable and Instance variable is, that first time, when class is loaded in to memory, then only memory is allocated for all class variables. That means, class variables do not depend on the Objets of that classes. What ever number of objects are there, only one copy is created at the time of class loding.
•	Q22. Since an Abstract class in Java cannot be instantiated then how can you use its non static methods
By extending it
•	Q23. How would you make a copy of an entire Java object with its state?
Have this class implement Cloneable interface and call its method clone().
•	Q24. Explain Encapsulation,Inheritance and Polymorphism
Encapsulation is a process of binding or wrapping the data and the codes that operates on the data into a single entity. This keeps the data safe from outside interface and misuse. One way to think about encapsulation is as a protective wrapper that prevents code and data from being arbitrarily accessed by other code defined outside the wrapper.
Inheritance is the process by which one object acquires the properties of another object.
The meaning of Polymorphism is something like one name many forms. Polymorphism enables one entity to be used as as general category for different types of actions. The specific action is determined by the exact nature of the situation. The concept of polymorphism can be explained as “one interface, multiple methods”.
•	Q25. Explain garbage collection?
Garbage collection is one of the most important feature of Java.
Garbage collection is also called automatic memory management as JVM automatically removes the unused variables/objects (value is null) from the memory. User program cann’t directly free the object from memory, instead it is the job of the garbage collector to automatically free the objects that are no longer referenced by a program. Every class inherits finalize() method from java.lang.Object, the finalize() method is called by garbage collector when it determines no more references to the object exists. In Java, it is good idea to explicitly assign null into a variable when no more in us
•	Q26. What is similarities/difference between an Abstract class and Interface?
Differences- Interfaces provide a form of multiple inheritance. A class can extend only one other class.
- Interfaces are limited to public methods and constants with no implementation. Abstract classes can have a partial implementation, protected parts, static methods, etc.
- A Class may implement several interfaces. But in case of abstract class, a class may extend only one abstract class.
- Interfaces are slow as it requires extra indirection to find corresponding method in in the actual class. Abstract classes are fast.
Similarities
- Neither Abstract classes or Interface can be instantiated
•	Q27. What are different ways to make your class multithreaded in Java
There are two ways to create new kinds of threads:
- Define a new class that extends the Thread class
- Define a new class that implements the Runnable interface, and pass an object of that class to a Thread’s constructor.
•	Q28. What do you understand by Synchronization? How do synchronize a method call in Java? How do you synchonize a block of code in java ?
Synchronization is a process of controlling the access of shared resources by the multiple threads in such a manner that only one thread can access one resource at a time. In non synchronized multithreaded application, it is possible for one thread to modify a shared object while another thread is in the process of using or updating the object’s value. Synchronization prevents such type of data corruption.
- Synchronizing a method: Put keyword synchronized as part of the method declaration
- Synchronizing a block of code inside a method: Put block of code in synchronized (this) { Some Code }
•	Q29. What is transient variable?
Transient variable can’t be serialize. For example if a variable is declared as transient in a Serializable class and the class is written to an ObjectStream, the value of the variable can’t be written to the stream instead when the class is retrieved from the ObjectStreamthe value of the variable becomes null.
•	Q30. What is Properties class in Java. Which class does it extends?
The Properties class represents a persistent set of properties. The Properties can be saved to a stream or loaded from a stream. Each key and its corresponding value in the property list is a string
•	Q31. Explain the concept of shallow copy vs deep copy in Java
In case of shallow copy, the cloned object also refers to the same object to which the original object refers as only the object references gets copied and not the referred objects themselves.
In case deep copy, a clone of the class and all all objects referred by that class is made.
•	Q32. How can you make a shallow copy of an object in Java
Use clone() method inherited by Object class
•	Q33. How would you make a copy of an entire Java object (deep copy) with its state?
Have this class implement Cloneable interface and call its method clone().

•	What is a JobTracker in Hadoop? How many instances of JobTracker run on a Hadoop Cluster?
•	JobTracker is the daemon service for submitting and tracking MapReduce jobs in Hadoop. There is only One Job Tracker process run on any hadoop cluster. Job Tracker runs on its own JVM process. In a typical production cluster its run on a separate machine. Each slave node is configured with job tracker node location. The JobTracker is single point of failure for the Hadoop MapReduce service. If it goes down, all running jobs are halted. JobTracker in Hadoop performs following actions(from Hadoop Wiki:)
•	Client applications submit jobs to the Job tracker.
•	The JobTracker talks to the NameNode to determine the location of the data
•	The JobTracker locates TaskTracker nodes with available slots at or near the data
•	The JobTracker submits the work to the chosen TaskTracker nodes.
•	The TaskTracker nodes are monitored. If they do not submit heartbeat signals often enough, they are deemed to have failed and the work is scheduled on a different TaskTracker.
•	A TaskTracker will notify the JobTracker when a task fails. The JobTracker decides what to do then: it may resubmit the job elsewhere, it may mark that specific record as something to avoid, and it may may even blacklist the TaskTracker as unreliable.
•	When the work is completed, the JobTracker updates its status.
•	

•	Client applications can poll the JobTracker for information.

•	How JobTracker schedules a task?
•	The TaskTrackers send out heartbeat messages to the JobTracker, usually every few minutes, to reassure the JobTracker that it is still alive. These message also inform the JobTracker of the number of available slots, so the JobTracker can stay up to date with where in the cluster work can be delegated. When the JobTracker tries to find somewhere to schedule a task within the MapReduce operations, it first looks for an empty slot on the same server that hosts the DataNode containing the data, and if not, it looks for an empty slot on a machine in the same rack.
•	What is a Task Tracker in Hadoop? How many instances of TaskTracker run on a Hadoop Cluster
•	A TaskTracker is a slave node daemon in the cluster that accepts tasks (Map, Reduce and Shuffle operations) from a JobTracker. There is only One Task Tracker process run on any hadoop slave node. Task Tracker runs on its own JVM process. Every TaskTracker is configured with a set of slots, these indicate the number of tasks that it can accept. The TaskTracker starts a separate JVM processes to do the actual work (called as Task Instance) this is to ensure that process failure does not take down the task tracker. The TaskTracker monitors these task instances, capturing the output and exit codes. When the Task instances finish, successfully or not, the task tracker notifies the JobTracker. The TaskTrackers also send out heartbeat messages to the JobTracker, usually every few minutes, to reassure the JobTracker that it is still alive. These message also inform the JobTracker of the number of available slots, so the JobTracker can stay up to date with where in the cluster work can be delegated.
•	What is a Task instance in Hadoop? Where does it run?
•	Task instances are the actual MapReduce jobs which are run on each slave node. The TaskTracker starts a separate JVM processes to do the actual work (called as Task Instance) this is to ensure that process failure does not take down the task tracker. Each Task Instance runs on its own JVM process. There can be multiple processes of task instance running on a slave node. This is based on the number of slots configured on task tracker. By default a new task instance JVM process is spawned for a task.
•	How many Daemon processes run on a Hadoop system?
•	Hadoop is comprised of five separate daemons. Each of these daemon run in its own JVM. Following 3 Daemons run on Master nodes NameNode - This daemon stores and maintains the metadata for HDFS. Secondary NameNode - Performs housekeeping functions for the NameNode. JobTracker - Manages MapReduce jobs, distributes individual tasks to machines running the Task Tracker. Following 2 Daemons run on each Slave nodes DataNode – Stores actual HDFS data blocks. TaskTracker - Responsible for instantiating and monitoring individual Map and Reduce tasks.
•	What is configuration of a typical slave node on Hadoop cluster? How many JVMs run on a slave node?
•	Single instance of a Task Tracker is run on each Slave node. Task tracker is run as a separate JVM process.
•	Single instance of a DataNode daemon is run on each Slave node. DataNode daemon is run as a separate JVM process.
•	One or Multiple instances of Task Instance is run on each slave node. Each task instance is run as a separate JVM process. The number of Task instances can be controlled by configuration. Typically a high end machine is configured to run more task instances.
•	What is the difference between HDFS and NAS ?
•	The Hadoop Distributed File System (HDFS) is a distributed file system designed to run on commodity hardware. It has many similarities with existing distributed file systems. However, the differences from other distributed file systems are significant. Following are differences between HDFS and NAS
•	In HDFS Data Blocks are distributed across local drives of all machines in a cluster. Whereas in NAS data is stored on dedicated hardware.
•	HDFS is designed to work with MapReduce System, since computation are moved to data. NAS is not suitable for MapReduce since data is stored seperately from the computations.
•	HDFS runs on a cluster of machines and provides redundancy usinga replication protocal. Whereas NAS is provided by a single machine therefore does not provide data redundancy.
•	How NameNode Handles data node failures?
•	NameNode periodically receives a Heartbeat and a Blockreport from each of the DataNodes in the cluster. Receipt of a Heartbeat implies that the DataNode is functioning properly. A Blockreport contains a list of all blocks on a DataNode. When NameNode notices that it has not recieved a hearbeat message from a data node after a certain amount of time, the data node is marked as dead. Since blocks will be under replicated the system begins replicating the blocks that were stored on the dead datanode. The NameNode Orchestrates the replication of data blocks from one datanode to another. The replication data transfer happens directly between datanodes and the data never passes through the namenode.
•	Does MapReduce programming model provide a way for reducers to communicate with each other? In a MapReduce job can a reducer communicate with another reducer?
•	Nope, MapReduce programming model does not allow reducers to communicate with each other. Reducers run in isolation.
•	Can I set the number of reducers to zero?
•	Yes, Setting the number of reducers to zero is a valid configuration in Hadoop. When you set the reducers to zero no reducers will be executed, and the output of each mapper will be stored to a separate file on HDFS. [This is different from the condition when reducers are set to a number greater than zero and the Mappers output (intermediate data) is written to the Local file system(NOT HDFS) of each mappter slave node.]
•	Where is the Mapper Output (intermediate kay-value data) stored ?
•	The mapper output (intermediate data) is stored on the Local file system (NOT HDFS) of each individual mapper nodes. This is typically a temporary directory location which can be setup in config by the hadoop administrator. The intermediate data is cleaned up after the Hadoop Job completes.
•	What are combiners? When should I use a combiner in my MapReduce Job?
•	Combiners are used to increase the efficiency of a MapReduce program. They are used to aggregate intermediate map output locally on individual mapper outputs. Combiners can help you reduce the amount of data that needs to be transferred across to the reducers. You can use your reducer code as a combiner if the operation performed is commutative and associative. The execution of combiner is not guaranteed, Hadoop may or may not execute a combiner. Also, if required it may execute it more then 1 times. Therefore your MapReduce jobs should not depend on the combiners execution.
•	What is Writable & WritableComparable interface?
•	org.apache.hadoop.io.Writable is a Java interface. Any key or value type in the Hadoop Map-Reduce framework implements this interface. Implementations typically implement a static read(DataInput) method which constructs a new instance, calls readFields(DataInput) and returns the instance.
•	org.apache.hadoop.io.WritableComparable is a Java interface. Any type which is to be used as a key in the Hadoop Map-Reduce framework should implement this interface. WritableComparable objects can be compared to each other using Comparators.
•	What is the Hadoop MapReduce API contract for a key and value Class?
•	The Key must implement the org.apache.hadoop.io.WritableComparable interface.
•	The value must implement the org.apache.hadoop.io.Writable interface.
•	What is a IdentityMapper and IdentityReducer in MapReduce ?
•	org.apache.hadoop.mapred.lib.IdentityMapper Implements the identity function, mapping inputs directly to outputs. If MapReduce programmer do not set the Mapper Class using JobConf.setMapperClass then IdentityMapper.class is used as a default value.
•	org.apache.hadoop.mapred.lib.IdentityReducer Performs no reduction, writing all input values directly to the output. If MapReduce programmer do not set the Reducer Class using JobConf.setReducerClass then IdentityReducer.class is used as a default value.

•	What is the meaning of speculative execution in Hadoop? Why is it important?
•	Speculative execution is a way of coping with individual Machine performance. In large clusters where hundreds or thousands of machines are involved there may be machines which are not performing as fast as others. This may result in delays in a full job due to only one machine not performaing well. To avoid this, speculative execution in hadoop can run multiple copies of same map or reduce task on different slave nodes. The results from first node to finish are used.
•	When is the reducers are started in a MapReduce job?
•	In a MapReduce job reducers do not start executing the reduce method until the all Map jobs have completed. Reducers start copying intermediate key-value pairs from the mappers as soon as they are available. The programmer defined reduce method is called only after all the mappers have finished.
•	If reducers do not start before all mappers finish then why does the progress on MapReduce job shows something like Map(50%) Reduce(10%)? Why reducers progress percentage is displayed when mapper is not finished yet?
•	Reducers start copying intermediate key-value pairs from the mappers as soon as they are available. The progress calculation also takes in account the processing of data transfer which is done by reduce process, therefore the reduce progress starts showing up as soon as any intermediate key-value pair for a mapper is available to be transferred to reducer. Though the reducer progress is updated still the programmer defined reduce method is called only after all the mappers have finished.
•	What is HDFS ? How it is different from traditional file systems?
•	HDFS, the Hadoop Distributed File System, is responsible for storing huge data on the cluster. This is a distributed file system designed to run on commodity hardware. It has many similarities with existing distributed file systems. However, the differences from other distributed file systems are significant.
•	HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware.
•	HDFS provides high throughput access to application data and is suitable for applications that have large data sets.
•	HDFS is designed to support very large files. Applications that are compatible with HDFS are those that deal with large data sets. These applications write their data only once but they read it one or more times and require these reads to be satisfied at streaming speeds. HDFS supports write-once-read-many semantics on files.
•	What is HDFS Block size? How is it different from traditional file system block size?
•	In HDFS data is split into blocks and distributed across multiple nodes in the cluster. Each block is typically 64Mb or 128Mb in size. Each block is replicated multiple times. Default is to replicate each block three times. Replicas are stored on different nodes. HDFS utilizes the local file system to store each HDFS block as a separate file. HDFS Block size can not be compared with the traditional file system block size.
•	What is a NameNode? How many instances of NameNode run on a Hadoop Cluster?
•	The NameNode is the centerpiece of an HDFS file system. It keeps the directory tree of all files in the file system, and tracks where across the cluster the file data is kept. It does not store the data of these files itself. There is only One NameNode process run on any hadoop cluster. NameNode runs on its own JVM process. In a typical production cluster its run on a separate machine. The NameNode is a Single Point of Failure for the HDFS Cluster. When the NameNode goes down, the file system goes offline. Client applications talk to the NameNode whenever they wish to locate a file, or when they want to add/copy/move/delete a file. The NameNode responds the successful requests by returning a list of relevant DataNode servers where the data lives.
•	What is a DataNode? How many instances of DataNode run on a Hadoop Cluster?
•	A DataNode stores data in the Hadoop File System HDFS. There is only One DataNode process run on any hadoop slave node. DataNode runs on its own JVM process. On startup, a DataNode connects to the NameNode. DataNode instances can talk to each other, this is mostly during replicating data.
•	How the Client communicates with HDFS?
•	The Client communication to HDFS happens using Hadoop HDFS API. Client applications talk to the NameNode whenever they wish to locate a file, or when they want to add/copy/move/delete a file on HDFS. The NameNode responds the successful requests by returning a list of relevant DataNode servers where the data lives. Client applications can talk directly to a DataNode, once the NameNode has provided the location of the data.
•	How the HDFS Blocks are replicated?
•	HDFS is designed to reliably store very large files across machines in a large cluster. It stores each file as a sequence of blocks; all blocks in a file except the last block are the same size. The blocks of a file are replicated for fault tolerance. The block size and replication factor are configurable per file. An application can specify the number of replicas of a file. The replication factor can be specified at file creation time and can be changed later. Files in HDFS are write-once and have strictly one writer at any time. The NameNode makes all decisions regarding replication of blocks. HDFS uses rack-aware replica placement policy. In default configuration there are total 3 copies of a datablock on HDFS, 2 copies are stored on datanodes on same rack and 3rd copy on a different rack.
•	
Can you think of a questions which is not part of this post? Please don't forget to share it with me in comments section & I will try to include it in the list. 






•	NoSQL Interview Questions

•	Describe CAP theorem (Brewer's theorem)
•	What is CAP theorem? Describe how weakening consistency constraints can yield highly available distributed systems (e.g. databases). Give an example.
•	Consistent Hashing
•	Describe consistent hashing and the advantages that it has over traditional hashing techniques. How can this technique help while scaling distributed systems, for example, distributed hash tables?
•	Dynamo Vs BigTable
•	Contrast and compare the features of Google's BigTable and Amazon's Dynamo databases
•	Different types of NoSQL databases
•	Explain the properties, advantages and drawbacks of these different types of NoSQL databases. Give examples of each type of database e.g. CouchDB is an example of a document Oriented Database.
•	Document Oriented Database
•	Ordered Key/Value Store
•	Eventually consistent Key/Value Store
•	Graph Database
•	Object Database



-----------

Spark

Big Data enthusiasts have certified Apache Spark as the hottest data compute engine for Big Data in the world. It is fast ejecting MapReduce and Java from their positions, and job trends are reflecting this change. According to a survey by TypeSafe, 71% of global Java developers are currently evaluating or researching Spark, and 35% of them have already started using it. Spark experts are currently in demand, and in the days to follow, the number of Spark-related job opportunities is only expected to go through the roof.

The Edureka course on Apache Spark and Scala helps you master large-scale data processing though Spark Streaming, Spark SQL, MLlib, GraphX, among others. The course aims to enhance your career path as a Big Data developer through live projects and interactive tutorials by industry experts.



This is the perfect time to prepare for a Spark interview. We have curated a list Spark interview questions and answers to help you breeze through your interview and have a successful career around Spark. If you have any specific questions, we encourage you to write them in the comments section. Our experts will be happy to answer them for you.

All the best!

1. What is Apache Spark?
Wikipedia defines Apache Spark “an open source cluster computing framework originally developed in the AMPLab at University of California, Berkeley but was later donated to the Apache Software Foundation where it remains today. In contrast to Hadoop’s two-stage disk-based MapReduce paradigm, Spark’s multi-stage in-memory primitives provides performance up to 100 times faster for certain applications. By allowing user programs to load data into a cluster’s memory and query it repeatedly, Spark is well-suited to machine learning algorithms.”

Spark is essentially a fast and flexible data processing framework. It has an advanced execution engine supporting cyclic data flow with in-memory computing functionalities. Apache Spark can run on Hadoop, as a standalone system or on the cloud. Spark is capable of accessing diverse data sources including HDFS, HBase, Cassandra among others

2. Explain the key features of Spark.
• Spark allows Integration with Hadoop and files included in HDFS.

• It has an independent language (Scala) interpreter and hence comes with an interactive language shell.

• It consists of RDD’s (Resilient Distributed Datasets), that can be cached across computing nodes in a cluster.

• It supports multiple analytic tools that are used for interactive query analysis, real-time analysis and graph processing. Additionally, some of the salient features of Spark include:

Lighting fast processing: When it comes to Big Data processing, speed always matters, and Spark runs Hadoop clusters way faster than others. Spark makes this possible by reducing the number of read/write operations to the disc. It stores this intermediate processing data in memory.

Support for sophisticated analytics: In addition to simple “map” and “reduce” operations, Spark supports SQL queries, streaming data, and complex analytics such as machine learning and graph algorithms. This allows users to combine all these capabilities in a single workflow.

Real-time stream processing: Spark can handle real-time streaming. MapReduce primarily handles and processes previously stored data even though there are other frameworks to obtain real-time streaming.  Spark does this in the best way possible.

3. What is “RDD”?
RDD stands for Resilient Distribution Datasets: a collection of fault-tolerant operational elements that run in parallel. The partitioned data in RDD is immutable and is distributed in nature.

4. How does one create RDDs in Spark?
In Spark, parallelized collections are created by calling the SparkContext “parallelize” method on an existing collection in your driver program.

                val data = Array(4,6,7,8)

                val distData = sc.parallelize(data)

Text file RDDs can be created using SparkContext’s “textFile” method. Spark has the ability to create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, among others. Spark supports text files, “SequenceFiles”, and any other Hadoop “InputFormat” components.

                 val inputfile = sc.textFile(“input.txt”)

5. What does the Spark Engine do?
Spark Engine is responsible for scheduling, distributing and monitoring the data application across the cluster.

6. Define “Partitions”.
A “Partition” is a smaller and logical division of data, that is similar to the “split” in Map Reduce. Partitioning is the process that helps derive logical units of data in order to speed up data processing.

Here’s an example:  val someRDD = sc.parallelize( 1 to 100, 4)

Here an RDD of 100 elements is created in four partitions, which then distributes a dummy map task before collecting the elements back to the driver program.

7. What operations does the “RDD” support?
Transformations
Actions
8. Define “Transformations” in Spark.
“Transformations” are functions applied on RDD, resulting in a new RDD. It does not execute until an action occurs. map() and filer() are examples of “transformations”, where the former applies the function assigned to it on each element of the RDD and results in another RDD. The filter() creates a new RDD by selecting elements from the current RDD.

9. Define “Action” in Spark.
An “action” helps in bringing back the data from the RDD to the local machine. Execution of “action” is the result of all transformations created previously. reduce() is an action that implements the function passed again and again until only one value is left. On the other hand, the take() action takes all the values from the RDD to the local node.

10. What are the functions of “Spark Core”?
The “SparkCore” performs an array of critical functions like memory management, monitoring jobs, fault tolerance, job scheduling and interaction with storage systems.

It is the foundation of the overall project. It provides distributed task dispatching, scheduling, and basic input and output functionalities. RDD in Spark Core makes it fault tolerance. RDD is a collection of items distributed across many nodes that can be manipulated in parallel. Spark Core provides many APIs for building and manipulating these collections.

11. What is an “RDD Lineage”?
Spark does not support data replication in the memory. In the event of any data loss, it is rebuilt using the “RDD Lineage”. It is a process that reconstructs lost data partitions.

12. What is a “Spark Driver”?
“Spark Driver” is the program that runs on the master node of the machine and declares transformations and actions on data RDDs. The driver also delivers RDD graphs to the “Master”, where the standalone cluster manager runs.

13. What is SparkContext?
“SparkContext” is the main entry point for Spark functionality. A “SparkContext” represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster.

14. What is Hive on Spark?
Hive is a component of Hortonworks’ Data Platform (HDP). Hive provides an SQL-like interface to data stored in the HDP. Spark users will automatically get the complete set of Hive’s rich features, including any new features that Hive might introduce in the future.

The main task around implementing the Spark execution engine for Hive lies in query planning, where Hive operator plans from the semantic analyzer which is translated to a task plan that Spark can execute. It also includes query execution, where the generated Spark plan gets actually executed in the Spark cluster.

15. Name a few commonly used Spark Ecosystems.
Spark SQL (Shark)
Spark Streaming
GraphX
MLlib
SparkR
16. What is “Spark Streaming”?
Spark supports stream processing, essentially an extension to the Spark API. This allows stream processing of live data streams. The data from different sources like Flume and HDFS is streamed and processed to file systems, live dashboards and databases. It is similar to batch processing as the input data is divided into streams like batches.

Business use cases for Spark streaming: Each Spark component has its own use case. Whenever you want to analyze data with the latency of less than 15 minutes and greater than 2 minutes i.e. near real time is when you use Spark streaming

17. What is “GraphX” in Spark?
“GraphX” is a component in Spark which is used for graph processing. It helps to build and transform interactive graphs.

18. What is the function of “MLlib”?
“MLlib” is Spark’s machine learning library. It aims at making machine learning easy and scalable with common learning algorithms and real-life use cases including clustering, regression filtering, and dimensional reduction among others.

19. What is “Spark SQL”?
Spark SQL is a Spark interface to work with structured as well as semi-structured data. It has the capability to load data from multiple structured sources like “textfiles”, JSON files, Parquet files, among others. Spark SQL provides a special type of RDD called SchemaRDD. These are row objects, where each object represents a record.

Here’s how you can create an SQL context in Spark SQL:

        SQL context: scala> var sqlContext=new SqlContext

        HiveContext: scala> var hc = new HIVEContext(sc)

20. What is a “Parquet” in Spark?
“Parquet” is a columnar format file supported by many data processing systems. Spark SQL performs both read and write operations with the “Parquet” file.

21. What is an “Accumulator”?
“Accumulators” are Spark’s offline debuggers. Similar to “Hadoop Counters”, “Accumulators” provide the number of “events” in a program.

Accumulators are the variables that can be added through associative operations. Spark natively supports accumulators of numeric value types and standard mutable collections. “AggregrateByKey()” and “combineByKey()” uses accumulators.

22. Which file systems does Spark support?
Hadoop Distributed File System (HDFS)
Local File system
S3
23. What is “YARN”?
“YARN” is a large-scale, distributed operating system for big data applications. It is one of the key features of Spark, providing a central and resource management platform to deliver scalable operations across the cluster.

24. List the benefits of Spark over MapReduce.
Due to the availability of in-memory processing, Spark implements the processing around 10-100x faster than Hadoop MapReduce.
Unlike MapReduce, Spark provides in-built libraries to perform multiple tasks form the same core; like batch processing, steaming, machine learning, interactive SQL queries among others.
MapReduce is highly disk-dependent whereas Spark promotes caching and in-memory data storage
Spark is capable of iterative computation while MapReduce is not.
Additionally, Spark stores data in-memory whereas Hadoop stores data on the disk. Hadoop uses replication to achieve fault tolerance while Spark uses a different data storage model, resilient distributed datasets (RDD). It also uses a clever way of guaranteeing fault tolerance that minimizes network input and output.

25. What is a “Spark Executor”?
When “SparkContext” connects to a cluster manager, it acquires an “Executor” on the cluster nodes. “Executors” are Spark processes that run computations and store the data on the worker node. The final tasks by “SparkContext” are transferred to executors.

26. List the various types of “Cluster Managers” in Spark.
The Spark framework supports three kinds of Cluster Managers:

Standalone
Apache Mesos
YARN
27. What is a “worker node”?
“Worker node” refers to any node that can run the application code in a cluster.

28. Define “PageRank”.
“PageRank” is the measure of each vertex in a graph.

29. Can we do real-time processing using Spark SQL?
Not directly but we can register an existing RDD as a SQL table and trigger SQL queries on top of that.

30. What is the biggest shortcoming of Spark?
Spark utilizes more storage space compared to Hadoop and MapReduce.

Also, Spark streaming is not actually streaming, in the sense that some of the window functions cannot properly work on top of micro batching.

Got a question for us? Please mention it in the comments section and we will get back to you.

-------

https://0x0fff.com/spark-misconceptions/

-----

HBase

Interview Question for – HBase

Q1 What are the different types of tombstone markers in HBase for deletion?

Answer: There are 3 different types of tombstone markers in HBase for deletion-

1)Family Delete Marker- This markers marks all columns for a column family.

2)Version Delete Marker-This marker marks a single version of a column.

3)Column Delete Marker-This markers marks all the versions of a column.

Q2 When should you use HBase and what are the key components of HBase?

Answer: HBase should be used when the big data application has –

1)A variable schema

2)When data is stored in the form of collections

3)If the application demands key based access to data while retrieving.

Key components of HBase are –

Region- This component contains memory data store and Hfile.

Region Server-This monitors the Region.

HBase Master-It is responsible for monitoring the region server.

Zookeeper- It takes care of the coordination between the HBase Master component and the client.

Catalog Tables-The two important catalog tables are ROOT and META.ROOT table tracks where the META table is and META table stores all the regions in the system.

Q3 Explain the difference between HBase and Hive.

Answer: HBase and Hive both are completely different hadoop based technologies-Hive is a data warehouse infrastructure on top of Hadoop whereas HBase is a NoSQL key value store that runs on top of Hadoop. Hive helps SQL savvy people to run MapReduce jobs whereas HBase supports 4 primary operations-put, get, scan and delete. HBase is ideal for real time querying of big data where Hive is an ideal choice for analytical querying of data collected over period of time.

Q4 What is Row Key?

Answer: Every row in an HBase table has a unique identifier known as RowKey. It is used for grouping cells logically and it ensures that all cells that have the same RowKeys are co-located on the same server. RowKey is internally regarded as a byte array.

Q5 Explain the difference between RDBMS data model and HBase data model.

Answer: RDBMS is a schema based database whereas HBase is schema less data model.

RDBMS does not have support for in-built partitioning whereas in HBase there is automated partitioning.

RDBMS stores normalized data whereas HBase stores de-normalized data.

Q6 What are the different operational commands in HBase at record level and table level?

Answer: Record Level Operational Commands in HBase are –put, get, increment, scan and delete.

Table Level Operational Commands in HBase are-describe, list, drop, disable and scan.

Q7 Explain about the different catalog tables in HBase?

Answer: The two important catalog tables in HBase, are ROOT and META. ROOT table tracks where the META table is and META table stores all the regions in the system.

Q8 Explain the process of row deletion in HBase.

Answer: On issuing a delete command in HBase through the HBase client, data is not actually deleted from the cells but rather the cells are made invisible by setting a tombstone marker. The deleted cells are removed at regular intervals during compaction.

Q9 What is column families? What happens if you alter the block size of ColumnFamily on an already populated database?

Answer: The logical deviation of data is represented through a key known as column Family. Column families consist of the basic unit of physical storage on which compression features can be applied. In an already populated database, when the block size of column family is altered, the old data will remain within the old block size whereas the new data that comes in will take the new block size. When compaction takes place, the old data will take the new block size so that the existing data is read correctly.

Q10 Explain about HLog and WAL in HBase.

Answer: All edits in the HStore are stored in the HLog. Every region server has one HLog. HLog contains entries for edits of all regions performed by a particular Region Server.WAL abbreviates to Write Ahead Log (WAL) in which all the HLog edits are written immediately.WAL edits remain in the memory till the flush period in case of deferred log flush.

Q11 what is NoSql?

Answer: Apache HBase is a type of “NoSQL” database. “NoSQL” is a general term meaning that the database isn’t an RDBMS which supports SQL as its primary access language, but there are many types of NoSQL databases: BerkeleyDB is an example of a local NoSQL database, whereas HBase is very much a distributed database. Technically speaking, HBase is really more a “Data Store” than “Data Base” because it lacks many of the features you find in an RDBMS, such as typed columns, secondary indexes, triggers, and advanced query languages, etc.

Q12 What is regionserver?

Answer: It is a file which lists the known region server names.

Q13 Give the name of the key components of Hbase

Answer: The key components of Hbase are Zookeeper, RegionServer, Region, Catalog Tables and Hbase Master.

Q14 What is the reason of using Hbase?

Answer: Hbase is used because it provides random read and write operations and it can perform number of operation per second on a large data sets.

Q15 Define standalone mode in Hbase?

Answer: It is a default mode of Hbase .In standalone mode, HBase does not use HDFS—it uses the local filesystem instead—and it runs all HBase daemons and a local ZooKeeper in the same JVM process.

Q16 Which operating system is supported by Hbase?

Answer: Hbase supports those OS which supports java like windows, Linux.

Q17 What are the main features of Apache HBase?

Answer: Apache HBase has many features which supports both linear and modular scaling,HBase tables are distributed on the cluster via regions, and regions are automatically split and re-distributed as your data grows(Automatic sharding).HBase supports a Block Cache and Bloom Filters for high volume query optimization(Block Cache and Bloom Filters).

Q18 What is the difference between HDFS/Hadoop and HBase?

Answer: HDFS doesn’t provides fast lookup records in a file,IN Hbase provides fast lookup records for large table.

Q19 What are datamodel operations in HBase?

Answer: 1)Get(returns attributes for a specified row,Gets are executed via HTable.get)

2)put(Put either adds new rows to a table (if the key is new) or can update existing rows (if the key already exists). Puts are executed via HTable.put (writeBuffer) or HTable.batch (non-writeBuffer))

3)scan(Scan allow iteration over multiple rows for specified attributes)

4)Delete(Delete removes a row from a table. Deletes are executed via HTable.delete)

HBase does not modify data in place, and so deletes are handled by creating new markers called tombstones. These tombstones, along with the dead values, are cleaned up on major compaction.

Q20 How many filters are available in Apache HBase?

Answer: Total we have 18 filters are support to hbase.They are:

ColumnPrefixFilter

TimestampsFilter

PageFilter

MultipleColumnPrefixFilter

FamilyFilter

ColumnPaginationFilter

SingleColumnValueFilter

RowFilter

QualifierFilter

ColumnRangeFilter

ValueFilter

PrefixFilter

SingleColumnValueExcludeFilter

ColumnCountGetFilter

InclusiveStopFilter

DependentColumnFilter

FirstKeyOnlyFilter

KeyOnlyFilter

Q21 Does HBase support SQL?

Answer: Not really. SQL-ish support for HBase via Hive is in development, however Hive is based on MapReduce which is not generally suitable for low-latency requests.By using Apache Phoenix  can retrieve data from hbase by using sql queries.

Q22 Is there any difference between HBase datamodel and RDBMS datamodel?

Answer: In Hbase,data is stored as a table(have rows and columns) similar to RDBMS but this is not a helpful analogy. Instead, it can be helpful to think of an HBase table as a multi-dimensional map.

Q23 What is Apache HBase?

Answer: Apache Hbase is one the sub-project of  Apache Hadoop,which was designed for NoSql database(Hadoop Database),bigdata store and a distributed, scalable.Use Apache HBase when you need random, realtime read/write access to your Big Data.A table which contain billions of rows X millions of columns -atop clusters of commodity hardware. Apache HBase is an open-source, distributed, versioned, non-relational database modeled after Google’s Bigtable. Apache HBase provides Bigtable-like capabilities  run on top of Hadoop and HDFS.

Q24 What is the use of shutdown command?

Answer: It is used to shut down the cluster.

Q25  How to delete the table with the shell?

Answer: To delete table first disable it then delete it.

Q26 What is the full form of MSLAB?

Answer: MSLAB stands for Memstore-Local Allocation Buffer.

Q27 What is REST?

Answer: Rest stands for Representational State Transfer which defines the semantics so that the protocol can be used in a generic way to address remote resources. It also provides support for different message formats, offering many choices for a client application to communicate with the server.

Q28 Mention what are the key components of Hbase?

Answer:Zookeeper: It does the co-ordination work between client and Hbase Maser

Hbase Master: Hbase Master monitors the Region Server

RegionServer: RegionServer monitors the Region

Region: It contains in memory data store(MemStore) and Hfile.

Catalog Tables: Catalog tables consist of ROOT and META

Q29 In Hbase what is column families?

Answer: Column families comprise the basic unit of physical storage in Hbase to which features like compressions are applied.

Q30 Explain how does Hbase actually delete a row?

Answer: In Hbase, whatever you write will be stored from RAM to disk, these disk writes are immutable barring compaction. During deletion process in Hbase, major compaction process delete marker while minor compactions don’t. In normal deletes, it results in a delete tombstone marker- these delete data they represent are removed during compaction.

Also, if you delete data and add more data, but with an earlier timestamp than the tombstone timestamp, further Gets may be masked by the delete/tombstone marker and hence you will not receive the inserted value until after the major compaction.

Q31 What Is The Difference Between HBase and Hadoop/HDFS?

Answer: HDFS : is a distributed file system that is well suited for the storage of large files. It\’s documentation states that it is not, however, a general purpose file system, and does not provide fast individual record lookups in files.

HBase: on the other hand, is built on top of HDFS and provides fast record lookups (and updates) for large tables. This can sometimes be a point of conceptual confusion. HBase internally puts your data in indexed “StoreFiles” that exist on HDFS for high-speed lookups.

Q32 How many Operational command in Hbase?

Answer: There are five main command in HBase.

Get
Put
Delete
Scan
Increment
Q33 Why cant I iterate through the rows of a table in reverse order?

Answer: Because of the way HFile works: for efficiency, column values are put on disk with the length of the value written first and then the bytes of the actual value written second. To navigate through these values in reverse order, these length values would need to be stored twice (at the end as well) or in a side file. A robust secondary index implementation is the likely solution here to ensure the primary use case remains fast.

Q34 Explain what is Hbase?

Answer: Hbase is a column-oriented database management system which runs on top of HDFS (Hadoop Distribute File System). Hbase is not a relational data store, and it does not support structured query language like SQL.

In Hbase, a master node regulates the cluster and region servers to store portions of the tables and operates the work on the data.

Q35 How to connect to Hbase?

Answer: A connection to Hbase is established through Hbase Shell which is a Java API.

Q36 Why we describe HBase Schema less?

Answer: Other than the column family name, HBase doesn’t require you to tell it anything about your data ahead of time. That’s why HBase is often described as a schema-less database.

Q37 What is Hfile?

Answer: All columns in a column family are stored together in the same low level storage file, called an Hfile.

Q38 How data is written into HBase?

Answer: When data is updated it is first written to a commit log, called a write-ahead log (WAL) in HBase, and then stored in the in-memory memstore. Once the data in memory has exceeded a given maximum value, it is flushed as an HFile to disk. After the flush, the commit logs can be discarded up to the last unflushed modification.

Q39 How data is read back from HBase?

Answer: Reading data back involves a merge of what is stored in the memstores, that is, the data that has not been written to disk, and the on-disk store files. Note that the WAL is never used during data retrieval, but solely for recovery purposes when a server has crashed before writing the in-memory data to disk.

Q40 What is the role of Zookeeper in Hbase?

Answer: The zookeeper maintains configuration information, provides distributed synchronization, 
and also maintains the communication between clients and region servers.

Q41 What are the different types of filters used in Hbase?

Answer: Filters are used to get specific data form a Hbase table rather than all the records.

They are of the following types.

Column Value Filter

Column Value comparators

KeyValue Metadata filters.

RowKey filters.

Q42 How does Hbase provide high availability?

Answer: Hbase uses a feature called region replication. In this feature for each region of a table, there will be multiple replicas that are opened in different RegionServers. The Load Balancer ensures that the region replicas are not co-hosted in the same region servers.

Q43 Explain what is the row key?

Answer: Row key is defined by the application. As the combined key is pre-fixed by the rowkey, it enables the application to define the desired sort order. It also allows logical grouping of cells and make sure that all cells with the same rowkey are co-located on the same server.

Q44 What are the different compaction types in Hbase?

Answer: There are two types of compaction. Major and Minor compaction. In minor compaction, the adjacent small HFiles are merged to create a single HFile without removing the deleted HFiles. Files to be merged are chosen randomly.

In Major compaction, all the HFiles of a column are emerged and a single HFiles is created. The delted HFiles are discarded and it is generally triggered manually.

Q45 What is TTL (Time to live) in Hbase?

Answer: TTL is a data retention technique using which the version of a cell can be preserved till a specific time period.Once that timestamp is reached the specific version will be removed

Q46 In Hbase what is log splitting?

Answer: When a region is edited, the edits in the WAL file which belong to that region need to be replayed. Therefore, edits in the WAL file must be grouped by region so that particular sets can be replayed to regenerate the data in a particular region. The process of grouping the WAL edits by region is called log splitting.

Q47 Why MultiWAL is needed?

Answer: With a single WAL per RegionServer, the RegionServer must write to the WAL serially, because HDFS files must be sequential. 
This causes the WAL to be a performance bottleneck.

Q48 What are the different Block Caches in Hbase?

Answer: HBase provides two different BlockCache implementations: the default on-heap LruBlockCache and the BucketCache, 
which is (usually) off-heap.

Q49 Can you create HBase table without assigning column family.

Answer:  No, Column family also impact how the data should be stored physically in the HDFS file system, 
hence there is a mandate that you should always have at least one column family. We can also alter the 
column families once the table is created.

Q50 What is HFile ?

Answer: The HFile is the underlying storage format for HBase.

HFiles belong to a column family and a column family can have multiple HFiles.

But a single HFile can’t have data for multiple column families


-------------
Spring guides
https://spring.io/guides

Spring boot
http://docs.spring.io/spring-boot/docs/2.0.0.BUILD-SNAPSHOT/reference/htmlsingle/
Getting started: https://spring.io/guides/gs/spring-boot/
Samples: https://github.com/spring-projects/spring-boot/tree/master/spring-boot-samples

Cloud foundry:











ML

Machine  learning  algorithms  attempt  to  make  predictions  or  decisions  based  on
training  data.

There  are  multiple  types  of  learning  problems,  including  
classification- There  are  multiple  types  of  learning  problems,  including  classification, regression, or clustering,
regression-
clustering,


classification, which involves identifying which of several categories an
item  belongs  to  (e.g.,  whether  an  email  is  spam  or  non-spam),  based  on  labeled
examples of other items (e.g., emails known to be spam or not).

Classification  and  regression  are  two  common  forms of supervised learning, where
algorithms attempt to predict a variable from features of objects using labeled train-
ing data

Clustering
Clustering is the unsupervised learning task that involves grouping objects into clus-
ters of high similarity. Unlike the supervised tasks seen before, where data is labeled,
clustering can be used to make sense of unlabeled data. It is commonly used in data
exploration (to find what a new dataset looks like) and in anomaly detection (to iden-
tify points that are far from any cluster).

RandomForestModel??
MLLibRegressionModel??

*****************************************
Classification  and  regression  are  two  common  forms  of  supervised  learning,  where
algorithms attempt to predict a variable from features of objects using labeled train-
ing data (i.e., examples where we know the answer). 

 The difference between them is
the type of variable predicted: in classification, the variable is discrete (i.e., it takes on
a finite set of values called classes); for example, classes might be  spam  or  nonspam  for
emails, or the language in which the text is written. In regression, the variable predic-
ted is continuous (e.g., the height of a person given her age and weight).

Decision trees and random forests
Decision trees are a flexible model that can be used for both classification and regres-
sion. They represent a tree of nodes, each of which makes a binary decision based on
a feature of the data (e.g., is a person’s age greater than 20?),


***************************************

All learning algorithms require defining a set of features for each item, which will be
fed  into  the  learning  function

At the end, the
algorithm  will  return  a  model  representing  the  learning  decision

This  model  can  now  be  used  to  make  predictions  on  new  points



dataTypes
Vector
A mathematical vector. MLlib supports both dense vectors, where every entry is
stored,  and  sparse  vectors,  where  only  the  nonzero  entries  are  stored  to  save
space.

LabeledPoint
A labeled data point for supervised learning algorithms such as classification and
regression. Includes a feature vector and a label 


OUPCTN NeighborHood 
Feature
Compares SDP usage pattern with its neighbours.
 * Neighbours are identified by zip + 3 codes, If not found then zip + 2 considered 
 Finally the SDPs with correlation coefficient of less than -0.75 compared to the neighbors and 
 usage of at least 20% of the neighborhood usage (zip+3 or zip+2) are inserted into RP_FEATURE table. 
 The actual_value is the correlation coefficient.

UI
The SDP usage comparison to neighbors' chart displays the average interval usage (in kWh) of the SDP and 
compares it to three metrics of interval usage for the neighborhood during the investigation period. 
The three interval usage metrics of the neighborhood that are computed are: 
average interval usage, 
average interval usage for 75% of the neighbors, 
average interval usage for 95% of the neighbors.


NTB

Run the Night Time Bypass algorithm.  
Number of days in the past X days which
the total usage at night (for example 7pm and 5am ) is less than 0.32 
and during the day (for example 8am to 4pm) is greater than 3.2
and account is active and MAX usage during day window / AVG usage during day window >= 1.5
Z_RP_SP_ALGORITHM_NTB
count(distinct night_usage_date) as ACTUAL_VALUE


UWNAA
SDP with no active accounts 
inactive account window should meet or exceed parameter THRESHOLD_SUM_INACTIVE_DAYS
During this inactive window period is usage is > than threshold
Sum of duration of all intervals

--------------

wiki***OozieJobExecutor is the gateway to any Hadoop job in EIP. Oozie job executor is a 24x7 EIP application 
so it's spring application context contains all the EIP common property sets like DB Datasources, logger, ebo etc.
These environment scope properties can be picked from the ApplicationContext and propagated to Oozie jobs in Hadoop

The OozieExecutor class uses the OozieClient Java API to read system properties and submit the java jobs to the Oozie engine

propertiesManager.buildSparkProperties(properties, conf);
OozieClient oozieClient = new OozieClient(oozieClientUrl);
Properties props = oozieClient.createConfiguration();

props.putAll(properties);
String jobId = oozieClient.run(props);

Oozie is a  workflow scheduler system, manage the timely execution of thousands of workflows of dependent jobs in
a Hadoop cluster.

OozieJob executor while launching the job access the ${EIP_HOME}/.keytabs/<org>.keytab to authenticate and the 
obtain the Hbase Auth Token.

This Hbase Auth Token is then Base 64 encoded and sent as job properties to Spark/MR executors where is can be 
decoded and used for Hbase authentication. Auth tokens for HDFS and YARN are implicitly passed by Qozie

<action name="LoadIntervalReads">
		<shell xmlns="uri:oozie:shell-action:0.1">
			<job-tracker>${jobTracker}</job-tracker>
			<name-node>${nameNode}</name-node>
			<configuration>
				<property>
					<name>mapred.job.queue.name</name>
					<value>${queueName}</value>
				</property>
			</configuration>
			<exec>spark-submit</exec>
			<argument>--master</argument>
			<argument>yarn</argument>
			<argument>--deploy-mode</argument>
			<argument>cluster</argument>
			<argument>--principal</argument>
			<argument>${orgKerberosPrincipalWithRealm}</argument>
			<argument>--keytab</argument>
			<argument>${orgKerberosPrincipal}.keytab</argument>
			<argument>--class</argument>
			<argument>com.emeter.loader.acloader.lp.LpIntervalLoader</argument>
			<argument>--num-executors</argument>
			<argument>${numOfExecutors}</argument>
			<argument>${sparkAppJar}</argument>
			<argument>${sparkProperties}</argument>
			<file>${orgKerberosPrincipal}.keytab#${orgKerberosPrincipal}.keytab</file>
		</shell>
		<ok to="MoveProcessed"/>
		<error to="kill"/>
	</action>

oozie.wf.application.path

Web
Apache Oozie is a system for running workflows of dependent jobs

Oozie has been
designed to scale, and it can manage the timely execution of thousands of workflows in
a Hadoop cluster.

Oozie runs
as a service in the cluster, and clients submit workflow definitions for immediate or later
execution.

We have taken advantage of JSP Expression Language (EL) syntax in several places in
the  workflow  definition. 

manually running oozie
we export the  OOZIE_URL environment variable to tell the  oozie
command which Oozie server to use 






MTM wiki
Oozie
Oozie is a  workflow scheduler system to manage Apache Hadoop jobs.
Oozie is integrated with the rest of the Hadoop stack supporting several types of Hadoop jobs out of the box
This involves creating the Oozie workflow files and submitting them to the scheduling engine through the OozieExecutor

Oozie Job executor is a Jobserver that is responsible to launch all the Hadoop Jobs from eip

OozieJob executor while launching the job access the ${EIP_HOME}/.keytabs/<org>.keytab to authenticate and the 
obtain the Hbase Auth Token.

This Hbase Auth Token is then Base 64 encoded and sent as job properties to Spark/MR executors where is can be 
decoded and used for Hbase authentication. Auth tokens for HDFS and YARN are implicitly passed by Qozie

HbaseDAO has the capability to authenticate using keytab or HBASE_AUTH_TOKEN
either keep keytab file <user>.keytab in ${EIP_HOME}/.keytabs
or
Make sure UseeGroupInformation object available to HbaseDAO has hbase_auth_token before it connects to hbase.

----

Kerberos

Cloudera uses Kerberos to secure the hadoop cluster. After we enable Kerberos security, 
access to all the Hadoop services is authenticated using Kerberos.

During the installation process installer creates keytabs for all the necessary principals inside ${EIP_HOME}/.keytabs folder
Hence all the eip processes can access these keytabs to authenticate from Kerberos Authentication server.

Generating a Kerberos Keytab File
run ktutil tool from shell:
ktutil

1.  Authentication.  The  client  authenticates  itself  to  the  Authentication  Server  and
receives a timestamped Ticket-Granting Ticket (TGT).
2.  Authorization. The client uses the TGT to request a service ticket from the Ticket-
Granting Server.
3.  Service request. The client uses the service ticket to authenticate itself to the server
that is providing the service the client is using. In the case of Hadoop, this might
be the namenode or the resource manager.

Together, the Authentication Server and the Ticket Granting Server form the Key Dis-
tribution Center (KDC). 

To use Kerberos authentication with Hadoop, you need to install, configure, and run a KDC (Hadoop does
not come with one)

A keytab is a file that stores passwords 

Instead of  using  the  three-step  Kerberos  ticket  exchange  protocol  to  authenticate  each  call,
which would present a high load on the KDC on a busy cluster, Hadoop uses delegation
tokens to allow later authenticated access without having to contact the KDC again.

A delegation token is generated by the server (the namenode, in this case) and can be
thought of as a shared secret between the client and the server. On the first RPC call to
the namenode, the client has no delegation token, so it uses Kerberos to authenticate.
As a part of the response, it gets a delegation token from the namenode. In subsequent
calls it presents the delegation token, which the namenode can verify (since it generated
it using a secret key), and hence the client is authenticated to the server.

Kerberos builds on symmetric key cryptography and requires a trusted third party, and optionally may use 
public-key cryptography during certain phases of authentication.[1] Kerberos uses UDP port 88 by default.



CDH
Cloudera’s  Distribution  Including  Apache  Hadoop  (hereafter  CDH)  is  an  integrated
Apache  Hadoop–based  stack  containing  all  the  components  needed  for  production,
tested and packaged to work together.

-Lifecycle of spark program
	-create RDD(parallelize / external data) in driver program
	-Lazy transform them to new RDDs using transformations like filter() or map()
	-Cache any intermediate RDDs that can be reused
	-Launch actions like count() and collect() to kick of parallel computation. Optimized and executed by spark
	

Fast:
1) keeps intermediate data in memory unlike Hadoop MR it does not write to HDFS 10-100x speed
2) MapRed has hardcoded map and reduce slots so CPU usage is not 100%. In spark has generic slots that can be used either by map/reduce
3) Empty slots for map or reduce are not filled aggressively in Map-reduce (Face book noticed that and used corona to 
   agressively start next map/reduce job). In spark it does
4) parallelism in MapRed - is by process Id on a node (???). Slots in MapRed is called processId
   parallelism in spark - is by threads on a executor. So better. Spark calls them cores. So eg. 6 cores are started in a Executor 	
   
HBase: The Hadoop Database   
HBase is a distributed, persistent, strictly consistent storage system
with near-optimal write—

MapReduce works by breaking the processing into two phases: the map phase and the
reduce phase. Each phase has key-value pairs as input and output, the types of which
may be chosen by the programmer. 
Input is split in input-spilts for each task to be used by its mapping function....(usually a block size 128 MB default)

Map task have advntage of data locality
Reduce task doesnot have it


The term sharding describes the logical separation of records into horizontal partitions.
The idea is to spread data across multiple storage files—or servers—as opposed to
having each stored contiguously.
The separation of values into those partitions is performed on fixed boundaries: you
have to set fixed rules ahead of time to route values to their appropriate store. With it
comes the inherent difficulty of having to reshard the data when one of the horizontal
partitions exceeds its capacity.
Resharding is a very costly operation, since the storage layout has to be rewritten. This
entails defining new boundaries and then horizontally splitting the rows across them.
Massive copy operations can take a huge toll on I/O performance as well as temporarily
elevated storage requirements. And you may still take on updates from the client ap-
plications and need to negotiate updates during the resharding process.


-----------------------------------------------------------------------------
computation 

Michael Armbrust

Structuring Apache Spark 2.0: SQL, DataFrames, Datasets And Streaming - by Michael Armbrust

Aspects of Big Data
-three Vs of big data: data velocity, variety, and complexity, in addition to volume.

How Big Data Differs from Traditional BI
-BI:Traditional BI methodology works on the principle of assembling all the enterprise data in a central server. The data 
 	is generally analyzed in an offline mode. The online transaction processing (OLTP) transactional data is transferred to 
 	a denormalized environment called as a data warehouse. The data is usually structured in an RDBMS with very little 
 	unstructured data.
-BigData:Data is retained in a distributed file system instead of on a central server.
		The processing functions are taken to the data rather than data being taking to the functions.
		Data is of different formats, both structured as well as unstructured.
		Data is both real-time data as well as offline data.
		Technology relies on massively parallel processing (MPP) concepts. 
 
How Big Is the Opportunity?
-	The amount of data is growing all around us every day, coming from various channels (see Figure 1-1).  
	As 70 percent of all data is created by individuals who are customers of some enterprise or the other, organizations 
	cannot ignore this important source of feedback from the customer as well as insight into customer behavior.
-	
----------------------------------------------------------------------------------



Cluster capacity planning:
-Need toknow what services u need for the cluster as spark, oozie etc
-Then comes capacity planning
	-Two things drive the capacity planning for bigData projects
		-How much data u keep in hadoop
		-How much data u want to process in daily basis
12 Master and 18 slave	
	
Services				Master				Share Slave?				Workers/Slaves				Utility/Edge/Gateway
HDFS					2(NameNode			Yes(DataNode)				18
YARN + MR2				2(ResouceManager)	Yes(NodeManager)			18
Hive					1 or 2				No							0
Spark					withYarn:			Yes(Executors but will be under yarn)0	
HBase					3(HMaster, as HA is more imp)Yes(RegionServer)	18
Impala					1 or 2				Yes(ImpalaD)				18
Kafka					3					No							0

						~12 masters
						run on 12 physical
						servers except 
						zookeepers

Master
-----
Storage for master is not very imp except for the Gateway. 
Master configuration is straight forward
Lets say each master has 
-64 GB of RAM | eg NameNode give 8 GB to OS and rest to Master (NameNode)
-16 cores CPU



Slaves
------
Let Each slave has 64 GB RAM and 16 core and 16TB of storage
overall = 16*18=288 TB total storage

With HDFS relication default factor of 3, i.e. 288/3 = ~90 TB of data can be saved
	
With 16 cores CPU	

Slave resource distribution is not straight forward as it has to look
Each slave has 

Service					RAM(64)									core	
-OS						6 GB is good starting point for slave	1
-HDFS | dataNode		1 GB(hrtBt to NameNode + writing to otherDataNode in case of replicFac) 		1 core
-Yarn+MR2 | NodeManager	1 GB(hrtbt to res Manager, does not process data, facilitate Mapper/reducer tasks) 1 core 	
-Impala | ImpalaD 		8GB(is memory intensive)	2 or 4 cores
-HBase | RegionServer	4GB		2 or 4 cores
-other services like mail, encryption and decryption

Have allocated 20 GB of RAM and 7 cores for each slave
44 GB RAM & 9 cores left on worker Node

-unix lscpu command to get CPU cores 

Resource manager run on 8088 port | <ip>:8088/cluster

-vcore multiplier: for more I/O value sld be ~4 | for more CPU intensive value = 1
-44 GB RAM & 9 cores left on worker Node
	-44*18=792 GB of total RAM for worker nodes
	-9*18*2(vcoreMultiplier for moderate load)=324 of total cores for all nodes
	
	***So 44 GB of memory & 9 cores are required as overall memory and oveerall cores for Yarn(NodeManager)
	depicted by 
	-Java heap size of NodeManager: 1 GB(initially decided)
	-****containerMemory(yarn.nodeManager.resource.memory): 44 GB(as calculated)
	-yarn.nodeManager.resource.memory = 44 GB of memory
	-yarn.nodeManager.resource.cpu-vcores: 27 for each node as 9*3 (2 i changed to 3 for moderate node)
		-This tells how many cores to give by containers managed by Yarn
	min/max 
	-yarn.scheduler.minimum-allocation-mb: min memory of a contatiner  e.g. 1 GB
	-yarn.scheduler.increment-allocation-mb: incremental memory of a contatiner	1 GB 
	-yarn.scheduler.maximum-allocation-mb: max memory of a contatiner  4 GB
	
	-yarn.scheduler.minimum-allocation-vcores: min vcore	1
	-yarn.scheduler.increment-allocation-vcores: incremental vcore	1
	-yarn.scheduler.maximum-allocation-vcores: max vcore	3
	
-So if all configurations done properly, on ResourceManager you can see
 Total memory as 792 and VCore as 324 then all configrations are correct 
 else some issue with the configuration
 
-I/O is data copy from HardDisk to memory, when I/O happens CPU issues I/O request
  but I/O handler does the I/O. Once data is loaded then CPU is used.
 
 -When u want ur machine to be used at full power, the context switch b/w processes 
  should not happen. In such case where CPU intensive processes are running context 
  switch will be costly and through put will be down
 -In I/O context switch will happen.
	

-mapredure configuration properties:
	-mapreduce.map.memory-mb
	-xx... vcore
	
-*** you have to satisfy the restrictions enforced by YARN for jobs like Mapper/Reducer eg given 5 GB instead of 4
	without calculation then we run into issues like out of mem 
	
-refer yarn tunning on cloudera. cdh_ig_yarn_tunning
+ https://www.youtube.com/watch?v=arDBjDO2QcA 
UTUbe


parquet-column-1.5.0-cdh5.4.1.jar
parquet-hadoop-1.5.0-cdh5.4.1.jar
parquet-avro-1.5.0-cdh5.4.1.jar
parquet-cascading-1.5.0-cdh5.4.1.jar
parquet-common-1.5.0-cdh5.4.1.jar
parquet-encoding-1.5.0-cdh5.4.1.jar
parquet-format-2.1.0-cdh5.4.1-javadoc.jar
parquet-format-2.1.0-cdh5.4.1-sources.jar
parquet-format-2.1.0-cdh5.4.1.jar
parquet-generator-1.5.0-cdh5.4.1.jar
parquet-hadoop-bundle-1.5.0-cdh5.4.1.jar
parquet-jackson-1.5.0-cdh5.4.1.jar
parquet-pig-1.5.0-cdh5.4.1.jar
parquet-pig-bundle-1.5.0-cdh5.4.1.jar
parquet-protobuf-1.5.0-cdh5.4.1.jar
parquet-scala_2.10-1.5.0-cdh5.4.1.jar
parquet-scrooge_2.10-1.5.0-cdh5.4.1.jar
parquet-test-hadoop2-1.5.0-cdh5.4.1.jar
parquet-thrift-1.5.0-cdh5.4.1.jar
parquet-tools-1.5.0-cdh5.4.1.jar
		
		
		hadoop.version=2.6.0
		hbase.version=1.0.0
		cdh.version=cdh5.4.1
		hive.version=1.1.0
		cdh.protobuf.version=2.5.0
		cdh.jets3t.version=0.9.0
		cdh.avro.version=1.7.6-cdh5.4.1
		cdh.hadoop.version=2.6.0-cdh5.4.1
		cdh.slf4j.version=1.7.5
		servlet-version=3.0.20100224
		zookeeper.version=3.4.5
		spark.version=1.3.0
		oozie.version=4.1.0



EPIC ideas
-Implementing search integration in HBase with Lucene.
-DataBrics spark notebook like concept 
-
**********************************************************************************************************************************************************************
J2EE
-----

RestFul services
----------------



JSR107 - java caching



Data Structures
---------------
Binary tree | BST

				1
			2		3
		4		5		

(a) Inorder (Left, Root, Right) : 4 2 5 1 3
(b) Preorder (Root, Left, Right) : 1 2 4 5 3
(c) Postorder (Left, Right, Root) : 4 5 2 3 1

Breadth First or Level Order Traversal : 1 2 3 4 5

InOrder
-------
	Algorithm Inorder(tree)
	   1. Traverse the left subtree, i.e., call Inorder(left-subtree)
	   2. Visit the root.
	   3. Traverse the right subtree, i.e., call Inorder(right-subtree)

	Uses of Inorder
	In case of binary search trees (BST), Inorder traversal gives nodes in non-decreasing order.
   
Preorder Traversal:
--------------------
	Algorithm Preorder(tree)
	   1. Visit the root.
	   2. Traverse the left subtree, i.e., call Preorder(left-subtree)
	   3. Traverse the right subtree, i.e., call Preorder(right-subtree)    

	Uses of Preorder
	Preorder traversal is used to create a copy of the tree. Preorder traversal is also used to get prefix expression on of an expression tree

Postorder Traversal:
--------------------

Algorithm Postorder(tree)
   1. Traverse the left subtree, i.e., call Postorder(left-subtree)
   2. Traverse the right subtree, i.e., call Postorder(right-subtree)
   3. Visit the root.

Uses of Postorder
Postorder traversal is used to delete the tree


Java Questions:
1) Max no of distinct nodes in BST | https://www.careercup.com/question?id=5631266591342592

	use hashSet to add nodes while traversing the BST
	return the size of hashSet

	public static int maxDistinct(Tree root){
	Set<String> uniq = new HashSet<>();
	if(root == null){
	return 0;	
	}
	return getMax(root, uniq);
	}

	private static int getMax(Tree root, Set uniq){
	if(root == null){
	return uniq.size();
	}
	int l = 0;
	int r = 0;
	if(uniq.add(root.data)){
	l = getMax(root.left, uniq);
	r = getMax(root.right, uniq);
	uniq.remove(uniq.size()-1);
	}
	return Math.max(l,r);
	}

2) Sorted n elements, distribute them in K equal weighing bucket | https://www.careercup.com/question?id=5725965217955840	

	
--------------HashMap----------------------------------
https://tekmarathon.com/2013/03/11/creating-our-own-hashmap-in-java/
https://tekmarathon.com/2012/12/04/hashmap-internal-implementation-analysis-in-java/
***http://coding-geek.com/how-does-a-hashmap-work-in-java/

-resizable array of hash buckets, each consisting of a chain of Map.Entry elements

Do you foresee any problem with this resizing of hashmap in java?  Thread safety issue in HashMap
	Since java is multi threaded it is very possible that more than one thread might be using same hashmap and then they both realize 
	the need for re-sizing the hashmap at the same time which leads to race condition.

	What is race condition with respect to hashmaps? When two or more threads see the need for resizing the same hashmap, 
	they might end up adding the elements of old bucket to the new bucket simultaneously and hence might lead to infinite loops

A HashMap stores data into multiple singly linked lists of entries (also called buckets or bins). 
-Key immutability
-So what’s the big difference with JAVA 7? Well, Nodes can be extended to TreeNodes. A TreeNode is a red-black tree structure 
 that stores really more information so that it can add, delete or get an element in O(log(n)).
 Red black trees are self-balancing binary search trees. Their inner mechanisms ensure that their length is always in log(n) 
 despite new adds or removes of nodes. The main advantage to use those trees is in a case where many data are in the same 
 index (bucket) of the inner table, the search in a tree will cost O(log(n)) whereas it would have cost O(n) with a linked list.

static final int MAXIMUM_CAPACITY = 1 << 30;  | in power of 2
static final int DEFAULT_INITIAL_CAPACITY = 16; | in power of 2
static final float DEFAULT_LOAD_FACTOR = 0.75f; | double the size when LF reaches .75

STEP1: Create a simple data structure with key, value and which can also extend as linked list i.e keep reference of next node
class Entry {
            final String key;
            String value;
            Entry next;

            Entry(String k, String v) {
                  key = k;
                  value = v;
            }

            public String getValue() {
                  return value;
            }

            public void setValue(String value) {
                  this.value = value;
            }

            public String getKey() {
                  return key;
            }
      }

use entry[] table : to store entry values
getHash(): give the hashValue of the key.
getBucket(): give the bucket index for the hashValue for the key.
put(key, value): go to bucket index, get the element and add to the head of the linklist else add the element.
get(key): get Hash of the key and use it get the bucket index

Basic example : https://www.careercup.com/question?id=5115994033881088


	   
--------------ConcurrentHashMap------------------------
concurrencyLevel - the estimated number of concurrently updating threads. The implementation performs internal 
sizing to try to accommodate this many threads.

-resizable array of hash buckets, each consisting of a chain of Map.Entry elements

In the ConcurrentHashMap Api , you will find the following constants.

static final int DEFAULT_INITIAL_CAPACITY = 16;
static final int DEFAULT_CONCURRENCY_LEVEL = 16;

initial capacity parameter and concurrency level parameters of ConcurrentHashMap constructor (or Object) are  set to 16 by default.

Thus, instead of a map wide lock, ConcurrentHashMap maintains  a list of 16 locks by default ( number of locks equal to the initial capacity , 
which is by default  16) each of which is used to lock on a single bucket of the Map.This indicates that 16 threads (number of threads equal 
to the concurrency level , which is by  default 16) can modify the collection at the same time , given ,each thread works on different bucket. 
So unlike hashtable, we perform any sort of operation ( update ,delete ,read ,create) without locking on entire map in ConcurrentHashMap.

??Interviewer : Can two threads update the ConcurrentHashMap simultaneously ?
	Yes it is possible that two threads can simultaneously write on the ConcurrentHashMap. ConcurrentHashMap 
	default implementation allows 16 threads to read and write in parallel. But in the worst case scenario , 
	when two objects lie in the same segment or same partition, then parallel write would not be possible.	
	
??Interviewer : Why ConcurrentHashMap does not allow null keys and null values ?
	According to the author of the ConcurrentHashMap (Doug lea himself)
	The main reason that nulls aren't allowed in ConcurrentMaps (ConcurrentHashMaps, ConcurrentSkipListMaps) is 
	that ambiguities that may be just barely tolerable in non-concurrent maps can't be accommodated. The main one 
	is that if map.get(key) returns null, you can't detect whether the key explicitly maps to null vs the key isn't mapped.
	In a non-concurrent map, you can check this via map.contains(key), but in a concurrent one, the map might have 
	changed between calls.
	
	In simple words, 
	The code is like this : 
	if (map.containsKey(k)) {
	   return map.get(k);
	} else {
	   throw new KeyNotPresentException();
	}
	It might be possible that key k might be deleted in between the get(k) and containsKey(k) calls. As a result , 
	the code will return null as opposed to KeyNotPresentException (Expected Result if key is not present). 	

-ConcurrentHashMap uses a fixed pool of locks that form a partition over the collection of buckets.	
-Map.Entry elements used by ConcurrentHashMap
	protected static class Entry implements Map.Entry {
		protected final Object key;
		protected volatile Object value;
		protected final int hash;
		protected final Entry next;
		...
	}

-Retrieval operations
Retrieval operations proceed by first finding the head pointer for the desired bucket (which is done without 
locking, so it could be stale), and traversing the bucket chain without acquiring the lock for that bucket. 
If it doesn't find the value it is looking for, it synchronizes and tries to find the entry again.

-	
--------------LinkedHashMap-------------------
As LinkedHashMap will provide us the retrieval order as insertion order of elements, it needs to keep track of last
inserted object. It has two references head and tail which will keep track of the latest object inserted and the 
first object inserted.When first object i.e. cbr250r is added then head and tail will both point to same object.

Hash table and linked list implementation of the Map interface, with predictable iteration order. This implementation 
differs from HashMap in that it maintains a doubly-linked list running through all of its entries. This linked list 
defines the iteration ordering, which is normally the order in which keys were inserted into the map

--------------ArrayList-------------------
http://netjs.blogspot.in/2015/08/how-arraylist-works-internally-in-java.html
[] Objects


--------------ConcurrentSkipListMaps-------------------
By specification, ConcurrentHashMap does no guarantee the runtime of its operations. Wherein ConcurrentSkipListMap guarantees 
O(log(n)) performance for most of its operations.
ConcurrentHashMap allows to modify the number of threads to tune the concurrency behaviour wherein ConcurrentSkipListMap 
does not allow to modify the concurrent thread count.
ConcurrentHashMap is not NavigableMap and also not a SortedMap, But ConcurrentSkipListMap is both a NavigableMap and a SortedMap.
ConcurrentSkipListMap is a skip list and CocurrentHashMap is not.

--------------BlockingQueue----------------------------

--------------ProducerConsumer-------------------------


-----------------DS interview Questions------------------
1) M X N matrix with 0s and 1s all sorted i.e. 
00001
00111
00011

Find no of occurances of 1 in linear complexity O(n)

Soln:
Traverse thru first row, call recusively getNeighbour by adding 1 to row. If no is 1 return with 0s occurances

2) [1,2,6,7,4,8,10] array of nos
for any input, fetch the pair of nos from array whose sum is equal to input

Soln: 
Iterate over array
Store input % a[i] and store result in Map<> key=remainder and value = index of array/no itself
Now for subsequent run check a[i] is contained in Map<>, if yes, the a[i] and the map value are the pair
else update Map with % values

3) Sum of two linkList (No reversing is allowed)

4) What is a BlockingQueue

5) Hadoop architecture

6) Spark job anatomy

- spark context

7) Log file with eventType 

8) Phone no in any format how would sort them i.e. compare them

9) Hadoop security

10) TopN design patten in Java / in Spark / cassandra

11) Check pointing in HDFS
 
12) Create a genericMethod printArray to accept and printArray of Integers & String

13) Comparator to sort score if score is same then sort by alphabetical
----------------------------------------------------------------------------------------------------------
MicroServices:
https://www.amazon.in/Top-Microservices-Interview-Questions-Answers-ebook/dp/B01J39LNDG?_encoding=UTF8&%2AVersion%2A=1&%2Aentries%2A=0&portal-device-attributes=desktop
 
 
----------------------------------------------------
IntelliJ
-ctrl+shft+A : all shortcuts

------------------------------------------------------------------------
UUID OR GUID
http://www.baeldung.com/java-uuid


The standard representation of the UUID uses hex digits (octets): 
123e4567-e89b-12d3-a456-556642440000

The Nil UUID is a special form of UUID in which all bits are set to zero.

Java provides methods for getting variant and version of UUID:

UUID uuid = UUID.randomUUID();
int variant = uuid.variant();
int version = uuid.version();

These are 5 different versions for variant 2 UUIDs: Time Based (UUIDv1), DCE Security (UUIDv2), Name Based (UUIDv3 and UUIDv5), Random (UUIDv4).

Java provides an implementation for the v3 and v4, but also provides a constructor for generating any type of UUID:

UUID uuid = new UUID(long mostSigBits, long leastSigBits);
