-HBase- WAL concept, Transaction, all questions 
-HBase- WAL concept, Transaction, all questions 
-HDFS- editLog concept + questions
-Spark- Revision + questions
-Hadoop questions
-Kerberos implementation
-Protobuf + parquet implementation
-Spring security
-HashMap, concurrentHashMap, arrayBlockingQueue
-Carrier cup questions 
-

lightening spark (4/6) 
Advanced Spark internal (mohit) (5/6)
Advanced Hbase internal (mohit) (6/6)
Advanced Hive internal (mohit)(7/6)
Advanced Oozie internal (mohit)(8/6)
Hadoop real world solutions cook book(9-11/6)
Hadoop map reduce cook book (12-15/6)
Hadoop security() (16-17/6)

Core java2 (17-20) [Serialization + collections + Threading]
Java security(21-23)
JVM internals


28/6 - update resume & LinkedIn
Questions + Design patterns(24-30/6)


Algo books:
Algorithm design by jon kleinberg and eva tardos
Algorithms by Sanjoy dasgupta


YouTube: GoogleTalksArchive

JVM clustering concepts
(Terracotta video: https://www.youtube.com/watch?v=_aCPwRZBLAU) 
-codeless clustering
-heap level instrumentation at VM level
-byteCode injections
-configuration file to tell which thread, object, lock to run in cluster vs local only
-stateless - dump every thing in DB and no data in memory
-stateful - data in memory and is shared

-ProducerConsumer: http://examples.javacodegeeks.com/core-java/util/concurrent/locks-concurrent/condition/java-util-concurrent-locks-condition-example/
https://www.youtube.com/watch?v=FLcXf9pO27w



wall time 
Todos 
-custom Lock
-Fairness example
-Nested Monitor Lockout
-http://www.ibm.com/developerworks/library/j-jtp10264/ (shambhu)
-Java security
-https://0x0fff.com/spark-misconceptions/
-caching LRU etc

-------------------------------Misc---------------------------------------------------
Two terms for the same thing

"Map" is used by Java, C++
"Dictionary" is used by .Net, Python
"Associative array" is used by Javascript, PHP	

---------------------------------------------------------------------------------------
Java Questions
-****************************************************
ConcurrentHashMap
-ConcurrentHashMap  allows concurrent add  and  updates  that lock only certain parts of the internal data structure. Thus,
-If many keys return the same hashcode, performance will deteriorate because buckets are implemented as Lists with O(n) retrieval
-In Java 8, when the buckets become too big, they’re dynamically replaced with sorted trees, which have O(log(n)) retrieval
-Note that this is possible only when the keys are Comparable (for example, String or Number classes).
-JDK8 update:
	-forEach Performs a given action for each (key, value)
	-reduce Combines all (key, value) given a reduction function into a result
	-search Applies a function on each (key, value) until the function produces a non-null result
	-keySet that returns a view of the ConcurrentHashMap as a Set

-****************************************************-****************************************************
Bit operations:
---------------
Left shift: Multiplication
<< (signed left shift)
<<< (un-signed left shift)
Since each left shift has the effect of doubling the original value, programmers frequently use this 
fact as an efficient alternative to multiplying by 2
		byte a = 64; 
		i = a << 2; 
		// two left shift | 64 (0100 0000) twice results in i containing the value 256 (1 0000 0000)

Signed left shift uses multiplication... So this could also be calculated as 2 * (2^1) = 4. Another example [2 << 11] = 2 *(2^11) = 4096

Right shift:
>> (Signed right shift) : If the number is negative, then 1 is used as a filler and if the number is positive, then 0 is used as a filler
>>> (Unsigned right shift) In Java, the operator ‘>>>’ is unsigned right shift operator. It always fills 0 irrespective of the sign of the 
	number

Signed right shift uses division... So this could also be calculated as 4 / (2^1) = 2 Another example [4096 >> 11] = 4096 / (2^11) = 2



Threads:
? Volatile:  Why would you need volatile keyword when member varaibles are shared across threads and will have same value.
			Ans: If your computer contains more than one CPU, each thread may run on a different CPU
			Java memory model and CPU architecture, main memory->cache->register. Value is flushed to main memory 
			from cache. It may be possible 1 thread update shared var value but its still in cache another thread 
			may read stale value from main memory(RAM). Volatile ensures any update is directly written to main memory.
			It ensures every read and write to volatile variable will be done to main memory than cache.

? Monitor Vs semaphore Vs mutex
		Mutex:
		Used to provide mutual exclusion i.e. ensures at most one process can do something (like execute a 
		section of code, or access a variable) at a time.
		A famous analogy is the bathroom key in a Starbucks; only one person can acquire it, 
		therefore only that one person may enter and use the bathroom. Everybody else who wants to 
		use the bathroom has to wait till the key is available again.

		Monitor:
		Provides mutual exclusion to an object i.e. at any point in time, at most one process may access any 
		of the object's members/ execute any of its methods. This is ideologically similar to a mutex for an 
		entire OOP instance*; no part of the instance can be touched by more than one process at a time.

		Semaphore:
		Is a counter which grants count number**of accesses to a resource at a time. So if a semaphore has 
		initial count = 5, the resource it protects may be accessed by at most 5 requestors*** at a time; 
		the 6th requestor onwards will have to wait for an empty 'slot'.
		Think of a patient's hospital room that has 5 seats; the nurse allows visitors to enter until all 
		5 seats are occupied. Any other visitors must wait outside until a seat becomes vacant, at which 
		point the nurse allows one to enter.

		Semaphores are typically used as a signaling mechanism between processes.

? Is synchronized block reentrant - yes
	Synchronized blocks in Java are reentrant. This means, that if a Java thread enters a synchronized block of code, 
	and thereby take the lock on the monitor object the block is synchronized on, the thread can enter other Java 
	code blocks synchronized on the same monitor object.
? Process Vs thread
? Serialization does not preserve memory address in Java


Locks:
	Locking default rules	
	- Always lock during updates to object fields. 
	- Always lock during access of possibly updated object fields. 
	- Never lock when invoking methods on other objects.

	-Many concurrent programs use final extensively, in part as helpful, automatically enforced documentation 
	 of design decisions that reduce the need for synchronization
	 
	- 


-****************************************************-****************************************************
Unsafe
------

sun.misc.Unsafe class is intended to be only used by core Java classes which is why its authors made its 
only constructor private and only added an equally private singleton instance. The public getter for this 
instances performs a security check in order to avoid its public use:

public static Unsafe getUnsafe() {
  Class cc = sun.reflect.Reflection.getCallerClass(2);
  if (cc.getClassLoader() != null)
    throw new SecurityException("Unsafe");
  return theUnsafe;
}

This looked-up class is then checked for its ClassLoader where a null reference is used to 
represent the bootstrap class loader on a HotSpot virtual machine

You could force the VM to load your application classes using the bootstrap class loader by adding 
it to the -Xbootclasspath

-****************************************************-****************************************************
Create an Instance of a Class Without Calling a Constructor

Using the Unsafe, we can create an instance of ClassWithExpensiveConstructor (or any of its subclasses) 
without having to invoke the above constructor, simply by allocating an instance directly on the heap:


-****************************************************-****************************************************
unsafe - https://dzone.com/articles/understanding-sunmiscunsafe

Native Memory Allocation (using native (off-heap) memory in Java )

Did you ever want to allocate an array in Java that should have had more than Integer.MAX_VALUE entries? 
You can create such an array by allocating native memory

Native memory allocation is used by for example direct byte buffers that are offered in Java's NIO packages.
Other than heap memory, native memory is not part of the heap area and can be used non-exclusively 
for example for communicating with other processes. As a result, Java's heap space is in competition 
with the native space: the more memory you assign to the JVM, the less native memory is left.

By calling Unsafe#allocateMemory(long), the virtual machine allocates the requested amount of native 
memory for you. After that, it will be your responsibility to handle this memory correctly.

Be aware that directly allocated memory is always native memory and therefore not garbage collected. 
You therefore have to free memory explicitly as demonstrated in the above example by a call to Unsafe#freeMemory(long).

-****************************************************-****************************************************

-****************************************************-****************************************************

-****************************************************-****************************************************

-****************************************************-****************************************************

-****************************************************-****************************************************
Java code to get Free memory:
-----------------------------
http://stackoverflow.com/questions/17374743/how-can-i-get-the-memory-that-my-java-program-uses-via-javas-runtime-api


Context Switching Overhead
--------------------------
When a CPU switches from executing one thread to executing another, the CPU needs to save the local data, 
program pointer etc. of the current thread, and load the local data, program pointer etc. of the next thread 
to execute. This switch is called a "context switch"
https://en.wikipedia.org/wiki/Context_switch

Context switching overhead
-thread needs some resources from the computer
-thread needs some memory to keep its local stack.

Java object size:
computing an object's size is using the Instrumented class from Java's attach API which offers a dedicated 
method for this purpose called getObjectSize.





JDK8 features:
***************
Functional in functional programming means "using functions as first class values," it often has
a secondary nuance of "no interaction during execution between components."


1) Methods and lambdas as first-class citizens:  being able to pass methods around at run-time, and 
	hence making them first-class citizens


		Passing the method reference
		-----------------------------


		Java 8 method reference :: syntax (meaning use this method as a value);
								--
		Java 8 when you write File::isHidden you create a method reference, which can similarly be passed around                        

		Lambdas -- anonymous functions
		----------------------------
		****Lambda expressions can only appear in places where they will be assigned to a variable whose type 
			is a functional interface. For example:
			Runnable r = () -> { System.out.println("hello"); };
			
		For example, you can now write (int x) -> x + 1 to mean "the function that, when called with argument x, returns the 
		value x + 1."
		?? -  	You might wonder why this is necessary because you could define a method add1 inside a
				class MyMathsUtils and then write MyMaths-Utils::add1! Yes, you could, but the new lambda
				syntax is more concise for cases where you don’t have a convenient method and class available.
				
		Predicate: a method is passed as predicate. Means something function-like that takes a value for an argument and returns 
		true or false
		
		Which above style to choose:
		----------------------------
		So you don’t even need to write a method definition that’s used only once; the code is crisper
		and clearer because you don’t need to search to find the code you’re passing. But if such a
		lambda exceeds a few lines in length (so that its behavior isn’t instantly clear), then you should
		instead  use  a  method  reference  to  a  method  with  a  descriptive  name  instead  of  using  an
		anonymous lambda. Code clarity should be your guide.
		
		Behaviour parameterization(passing block of code) is flexible than value parametrzn 
		as value parmeterization is rigid
		
		-Lambda syntax example:
		
				StateOwner stateOwner = new StateOwner();

				stateOwner.addStateListener(
					(oldState, newState) -> System.out.println("State changed")
				);
				The lambda expressions is this part:

				(oldState, newState) -> System.out.println("State changed")
				The lambda expression is matched against the parameter type of the addStateListener() method's parameter. 
				If the lambda expression matches the parameter type (in this case the StateChangeListener interface) , 
				then the lambda expression is turned into a function that implements the same interface as that parameter.

				Java lambda expressions can only be used where the type they are matched against is a single method interface. 
				In the example above, a lambda expression is used as parameter where the parameter type was the StateChangeListener 
				interface. This interface only has a single method. Thus, the lambda expression is matched successfully against 
				that interface.

		-Matching Lambdas to Interfaces
				
				A single method interface is also sometimes referred to as a functional interface. Matching a Java lambda 
				expression against a functional interface is divided into these steps:

				Does the interface have only one method?
				Does the parameters of the lambda expression match the parameters of the single method?
				Does the return type of the lambda expression match the return type of the single method?

				If the answer is yes to these three questions, then the given lambda expression is matched successfully 
				against the interface.
	
		(parameters) -> expression
		or (note the curly braces for statements)
		(parameters) -> { statements; }
		
		return is a control-flow statement. To make this lambda valid, curly braces are required as
		follows: (Integer i) -> {return "Alan" + i;}. 
		
		{} has statements and no {} states expressions
		Note: For {} i.e. multi statement use explicit return statement. 


		-Use case Examples of lambdas
				A boolean expression 			(List<String> list) -> list.isEmpty()
				Creating objects 				() -> new Apple(10)
				Consuming from an object 		(Apple a) -> {
														System.out.println(a.getWeight());
												}
				Select/extract from an object 	(String s) -> s.length()
				Combine two values 				(int a, int b) -> a * b
				Compare two objects 			(Apple a1, Apple a2) -> a1.getWeight().compareTo(a2.getWeight())



	-Because  of  the  idea  of  target  typing,  the  same  lambda  expression  can  be  associated  with
	 different functional interfaces if they have a compatible abstract method signature.
	 
	- Refactoring code:
		 -converting anonymous classes to lambda expressions
		 	Runnable r = () -> sysout("Hello");
		 	-catch: First,  the meanings  of this  and super  are  different for anonymous  classes  and lambda expressions.	
		 			Inside an anonymous class, this refers to the anonymous class itself, but
					inside  a  lambda  it  refers  to  the  enclosing  class
					
					Second,  anonymous  classes  are  allowed to shadow variables from the enclosing class. Lambda expressions can’t 
	
	-Compilation & byteCode info for Lambda
		-javap -c -v ClassName
	
		-anonymous classes have some undesirable characteristics that impact the performance of applications
			-The compiler generates a new class file for each anonymous class.
			-Each new anonymous class introduces a new subtype for a class or interface.
		
		-InvokeDynamic to the rescue	
			-The creation of an extra class has been replaced with an invokedynamic instruction
			-The typical use for this instruction is something like the following:
				def add(a, b) { a + b }
				Here the types of a and b aren’t known at compile time and can change from time to time. For
				this reason, when the JVM executes an invokedynamic for the first time, it consults a bootstrap
				method, implementing the language-dependent logic that determines the actual method to be
				called. The bootstrap method returns a linked call site. 
			
			-A lambda expression is translated into bytecode by putting its body into one of a static method
			 created at runtime. A stateless lambda, one that captures no state from its enclosing scope
			
			-More at http://cr.openjdk.java.net/~briangoetz/lambda/lambda-translation.html

2) Streams

	Using the Streams API, you don’tneed to think in terms of loops at all. The data processing happens internally inside the library.
	We call this idea internal iteration.


3) Default method

	The Java 8 solution is to break the last link an interface can now contain method signatures for
	which an implementing class doesn’t provide an implementation! So who implements them?
	The missing method bodies are given as part of the interface (hence default implementations)
	rather than in the implementing class.
	
	?? But  wait  a  second a single class can  implement  multiple  interfaces,  right?  So  if  you  have
		multiple default  implementations  in  several  interfaces,  does  that mean  you have  a  form  of
		multiple inheritance in Java? Yes, to some extent! We show in chapter 9 that there are some
		restrictions that prevent issues such as the infamous diamond inheritance problem in C++.


4) Static methods in Interface

5) Optional<T> class
	
	Can help you avoid NullPointer exceptions.  It’s  a  container  object  that  may  or  not  contain  a  value
	
	
6)	(structural) Pattern  matching.

6)

7)

8) Date issues:
---------------
Date:
		Doesn’t represent a date but a point in time with milliseconds precision.  years start from 1900, whereas the months start at index 0
		March 18, 2014
		Date date = new Date(114, 2, 18);
		toString method of the Date class could be quite misleading. It also includes the JVM’s default time zone

Calendar:
		months also start at index 0
		(at leastCalendar got rid of the 1900 offset for the year)

Finally, both Date and Calendar are mutable classes

DateFormat:  DateFormat also comes with its own set of problems. For example, it isn’t thread-safe

JDK 8 
java.time package includes
many new classes to help you: LocalDate, LocalTime, LocalDateTime, Instant, Duration, and Period
LocalDate: all instances are immutable. Has no info about timeZone


--------------------------------JDK 8 in details-----------------------------------------------

Passing code with behavior parameterization
-------------------------------------------

Behavior parameterization is a software development pattern

-------------------------------------------------------Core Java --------------------------------------------------

Anonymous classes
------------------
Anonymous classes are like the local classes (a class defined in a block) that you’re already
familiar with in Java. But anonymous classes don’t have a name. They allow you to declare and
instantiate  a  class  at  the  same  time.


Inner class Vs static inner classes
-----------------------------------
A nested class is a member of its enclosing class. Non-static nested classes (inner classes) have access to other 
members of the enclosing class, even if they are declared private. Static nested classes do not have access to other 
members of the enclosing class.
...

Note: A static nested class interacts with the instance members of its outer class (and other classes) just 
like any other top-level class. In effect, a static nested class is behaviorally a top-level class that has 
been nested in another top-level class for packaging convenience.
There is no need for LinkedList.Entry to be top-level class as it is only used by LinkedList 
(there are some other interfaces that also have static nested classes named Entry, such as Map.Entry - same concept). 
And since it does not need access to LinkedList's members, it makes sense for it to be static - it's a much cleaner approach.




Polymophism
-----------
abstraction Vs encapsulation
? Differences between Abstraction and Encapsulation
Abstraction and encapsulation are complementary concepts. On the one hand, abstraction focuses on the behavior of an object.
On the other hand, encapsulation focuses on the implementation of an object’s behavior.  Encapsulation is usually achieved by
hiding information about the internal state of an object and thus, can be seen as a strategy used in order to provide abstraction.


? What is functional interface ?

A functional interface is any interface that contains only one abstract method. (A functional interface 
may contain one or more default methods or static methods.) Because a functional interface contains only 
one abstract method, you can omit the name of that method when you implement it.

? What is the Difference between JDK and JRE ?

The Java Runtime Environment (JRE) is basically the Java Virtual Machine (JVM) where your Java programs are being executed.
It also includes browser plugins for applet execution. The Java Development Kit (JDK) is the full featured Software Development
Kit for Java, including the JRE, the compilers and tools (like JavaDoc, and Java Debugger), in order for a user to develop, compile
and execute Java applications.


? What is the difference between a synchronized method and a synchronized
block ?
In Java programming, each object has a lock.  A thread can acquire the lock for an object by using the synchronized keyword.
The synchronized keyword can be applied in a method level (coarse grained lock) or block level of code (?ne grained lock).


? How do you ensure that N threads can access N resources without deadlock ?
A very simple way to avoid deadlock while using N threads is to impose an ordering on the locks and force each thread to follow
that ordering. Thus, if all threads lock and unlock the mutexes in the same order, no deadlocks can arise.


? What is difference between fail-fast and fail-safe ?
The Iterator’s fail-safe property works with the clone of the underlying collection and thus, it is not affected by any modification
in the collection.  All the collection classes in java.util package are fail-fast, while the collection classes in java.util.concurrent
are fail-safe. Fail-fast iterators throw a ConcurrentModificationException, while fail-safe iterator never throws such
an exception.


? What is difference between Array and ArrayList ? When will you use Array over
ArrayList ?
The Array and ArrayList classes differ on the following features:
•  Arrays can contain primitive or objects, while an ArrayList can contain only objects.
•  Arrays have fixed size, while an ArrayList is dynamic.
•  An ArrayList provides more methods and features, such as addAll, removeAll, iterator, etc.
•  For a list of primitive data types, the collections use autoboxing to reduce the coding effort.  However, this approach makes
them slower when working on ?xed size primitive data types.


? What is difference between ArrayList and LinkedList ?
Both the ArrayList and LinkedList classes implement the List interface, but they differ on the following features:
•  An ArrayList is an index based data structure backed by an Array. It provides random access to its elements with a performance
equal to O(1). On the other hand, a LinkedList stores its data as list of elements and every element is linked to its previous and
next element. In this case, the search operation for an element has execution time equal to O(n).
•  The Insertion, addition and removal operations of an element are faster in a LinkedList compared to an ArrayList, because
there is no need of resizing an array or updating the index when an element is added in some arbitrary position inside the
collection.
•  A LinkedList consumes more memory than an ArrayList, because every node in a LinkedList stores two references, one for its
previous element and one for its next element.



? What is Java Priority Queue ?
The PriorityQueue is an unbounded queue, based on a priority heap and its elements are ordered in their natural order. At the time
of its creation, we can provide a Comparator that is responsible for ordering the elements of the PriorityQueue. A PriorityQueue
doesn’t allow null values, those objects that doesn’t provide natural ordering, or those objects that don’t have any comparator
associated with them.  Finally, the Java PriorityQueue is not thread-safe and it requires O(log(n)) time for its enqueing and
dequeing operations.

? What do you know about the big-O notation and can you give some examples
with respect to different data structures ?
The Big-O notation simply describes how well an algorithm scales or performs in the worst case scenario as the number of ele-
ments in a data structure increases. The Big-O notation can also be used to describe other behavior such as memory consumption.
Since the collection classes are actually data structures, we usually use the Big-O notation to chose the best implementation to
use, based on time, memory and performance. Big-O notation can give a good indication about performance for large amounts
of data.


? What’s the difference between Enumeration and Iterator interfaces ?
Enumeration is twice as fast as compared to an Iterator and uses very less memory. However, the Iterator is much safer compared
to Enumeration, because other threads are not able to modify the collection object that is currently traversed by the iterator. Also,
Iterators allow the caller to remove elements from the underlying collection, something which is not possible with Enumerations.

? What is the difference between HashSet and TreeSet ?
The HashSet is Implemented using a hash table and thus, its elements are not ordered. The add, remove, and contains methods of
a HashSet have constant time complexity O(1). On the other hand, a TreeSet is implemented using a tree structure. The elements
in a TreeSet are sorted, and thus, the add, remove, and contains methods have time complexity of O(logn).

-class loader hierarchy
-WHY NEED CUSTOM CLASS LOADER:

-Serialization & DeSerialization
	Externalizable 
	-Issues with Java serialization(EffectiveJava):
		-A major cost of implementing  Serializable  is that it decreases the flexibility to 
		 change a class's implementation   once   it   has   been   released.
		
		-A second cost of implementing  Serializable  is that it increases the likelihood of bugs 
		 and  security  holes. 
		 	-deserialization  is  a  “hidden  constructor”  with  all  of  the  same  issues  as  other constructors.
		 	 Because there is no explicit constructor, it is easy to forget that you must ensure 
			 that deserialization guarantees all of the invariants established by real constructors and that it 
			 does  not  allow  an  attacker  to  gain  access  to  the  internals  of  the  object  under  construction.
		 	
		-A third  cost  of  implementing  Serializable   is  that  it  increases  the  testing  burden 
		 associated with releasing a new version of a class.
		
		-Classes designed for inheritance (Item 15) should rarely implement  Serializable , and 
		 interfaces should rarely extend it. 
			- it  may  be  impossible  to  write  a  serializable 
				subclass.  Specifically,  it  will  be  impossible  if  the  superclass  does  not  provide  an  accessible 
				parameterless  constructor.  Therefore  you  should  consider  providing  a  parameterless 
				constructor  on  nonserializable  classes  designed  for  inheritance. 
		
		-Inner  classes  (Item  18)  should  rarely,  if  ever,  implement  Serializable .
			-Serializable .   They  use compiler-generated  synthetic  fields  to  store  references  to  
			 enclosing  instances  and  to  store values  of  local  variables  from  enclosing  scopes.  
			 How  these  fields  correspond  to  the  class definition  is  unspecified,  as  are  the  
			 names  of  anonymous  and  local  classes.\
			 
		-A static member class can, however, implement  Serializable.  
		
		-Do  not  accept  the  default  serialized  form  without  first  considering  whether  it  is appropriate.
			-Accepting the default serialized form should be a conscious decision on your 
				part  that  this  encoding  is  reasonable  from  the  standpoint  of  flexibility,  performance,  and 
				correctness.  Generally  speaking,  you  should  accept  the  default  serialized  form  only  if  it  is 
				largely  identical  to  the  encoding  that  you  would  choose  if  you  were  designing  a  custom 
				serialized form. 
			-The  default  serialized  form  of  an  object  is  a  reasonably  efficient  encoding  of  the  physical 
				representation  of  the  object  graph  rooted  at  the  object.  In  other words, it describes  the data 
				contained in the object and in every object that is reachable from this object. It also describes 
				the  topology  by  which  all  of  these  objects  are  interlinked.  The  ideal  serialized  form  of  an 
				object  contains  only  the  logical  data  represented  by  the  object.  It  is  independent  of  the 
				physical representation. 
			-The   default   serialized   form   is   likely   to   be   appropriate   if   an   object's   physical 
				representation is identical to its logical content.
				//Good candidate for default serialized form 
				public class Name implements Serializable { 
				    /** 
				     * Last name.  Must be non-null. 
				     * @serial 
				     */ 
				    private String lastName; 
				 
				    /** 
				     * First name.  Must be non-null. 
				     * @serial 
				     */ 
				    private String firstName; 
				    /** 
				     * Middle initial, or '\u0000' if name lacks middle initial. 
				     * @serial 
				     */ 
				    private char   middleInitial; 
				 
				    ... // Remainder omitted 
				}
			-The  presence  of  the  @serial   tag  tells  the  Javadoc  utility  to  place  this  documentation  on  a 
			 special page that documents serialized forms.	
				
			-Even  if  you  decide  that  the  default  serialized  form  is  appropriate,  you  often  must 
				provide a  readObject  method to ensure invariants and security. 	
			-In the case of  Name,  the readObject  method could ensure that  lastName  and  firstName  were non-null.	
			
			- If  an  instance  is  serialized  in  a  later  version  and  deserialized  in  an  earlier 
				version, the added fields will be ignored. Had the earlier version's  readObject  method failed 
				to invoke  defaultReadObject, the deserialization would fail with a  StreamCorruptedException.  		

? Why versioning of file is required in serilztn and deserialztn

When Java objects use serialization to save state in files, or as blobs in databases, the potential arises that the version of a 
class reading the data is different than the version that wrote the data.
A compatible change is a change that does not affect the contract between the class and its callers.	

SerialVersionUID is a must in serialization process. But it is optional for the developer to add it in java source file. 
If you are not going to add it in java source file, serialization runtime will generate a serialVersionUID and associate 
it with the class. The serialized object will contain this serialVersionUID along with other data.	
Even though serialVersionUID is a static field, it gets serialized along with the object. 
This is one exception to the general serialization rule that, “static fields are not serialized”

Javadocs says,

“the default serialVersionUID computation is highly sensitive to class details that may vary depending 
on compiler implementations, and can thus result in unexpected InvalidClassExceptions during deserialization”

Now you know why we should declare a serialVersionUID.

-Serialization does not preserve memory address in Java


-Immutable class

-marker interface

-memory leak
-How would you improve performance of a Java application
	-Manage Pools of reusable objects – thread, JDBC connection pool
	-Optimize I/O – use buffers
	-Avoid N/W trips
	-Memory mamangement
	-Vectors Vs ArrrayList, lazy initialization
	-Static - always make it a point to nullify the references as soon as you reach at a point in your code where 
	 the use of the static member is over.

? What is structure of Java Heap ? What is Perm Gen space in Heap ?
The JVM has a heap that is the runtime data area from which memory for all class instances and arrays is allocated. It is created
at the JVM start-up. Heap memory for objects is reclaimed by an automatic memory management system which is known as a
garbage collector.  Heap memory consists of live and dead objects.  Live objects are accessible by the application and will not
be a subject of garbage collection. Dead objects are those which will never be accessible by the application, but have not been
collected by the garbage collector yet.  Such objects occupy the heap memory space until they are eventually collected by the
garbage collector.



50 
What is Singleton class?

XML
-xsd
-xpath

collection
-----------\
-Every object has a default hash code. That hash code is derived from the object's memory address.
-Strings (s and t) have the same hash value because, for strings, the hash values are derived from their contents.


-Linked list & ArrayList: ordered data structure
-Linked lists and arrays let you specify in which order you want to arrange the elements.

HashTable
-HashTable: If you don't care about the ordering of the elements, then there are data structures 
		  that let you find elements much faster. No control over the order.
		  -Hashtable methods are synchronized.
		  -A hash table is an array of linked lists. Each list is called a bucket. 
		  -Hash table computes an integer, called the hash code, for each object
		  -Some researchers believe that it is a good idea to make the size of the hash table a prime number 
		   to prevent a clustering of keys.
		  -Load factor determines when a hash table is rehashed
		  -HashTable can be used to implement other DS like Set	

-Set: A set is a collection of elements without duplicates
	-HashSet:
	-LinkedHashSet(since 1.4) that keeps track of the order in which the elements are added to the set.
		-The iterator of a LinkedHashSet visits the elements in insertion order. That gives you an ordered collection with fast element lookup.
		- 
	-The TreeSet class is similar to the hash set, with one added improvement. A tree set is a sorted collection.	
		-You insert elements into the collection in any order. When you iterate through the collection, the values 
		 are automatically presented in sorted order
		-Every time an element is added to a tree, it is placed into its proper sorting position. 
		 Therefore, the iterator always visits the elements in sorted order
	
	
-ConcurrentModificationException

-Thread
	Thread states
	Diff b/w sleep and wait


Spring 
-bean lifecycle
-dependency injection 
-autowiring
-With imp


J2EE

Diff b/w app server and webServer

*********************************************************

functional interface
----------------------
A functional interface is any interface that contains only one abstract method. (A functional interface 
may contain one or more default methods or static methods.) Because a functional interface contains only 
one abstract method, you can omit the name of that method when you implement it.

interface CheckPerson {
    boolean test(Person p);
}

The JDK defines several standard functional interfaces, which you can find in the package java.util.function
For example, you can use the Predicate<T> interface in place of CheckPerson. This interface contains the method boolean test(T t)
The interface Predicate<T> is an example of a generic interface.


public static void printPersonsWithPredicate(
    List<Person> roster, Predicate<Person> tester) {
    for (Person p : roster) {
        if (tester.test(p)) {
            p.printPerson();
        }
    }
}


---Same as above---- P is predicate geeric interface

printPersonsWithPredicate(
    roster,
    p -> p.getGender() == Person.Sex.MALE
        && p.getAge() >= 18
        && p.getAge() <= 25
);
*********************************************************
Lambda Expressions:
------------------
https://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html
functional interface: The JDK defines several standard functional interfaces, which you can find in the 
package java.util.function


The syntax of anonymous classes is bulky considering that the CheckPerson interface contains only one method. 
In this case, you can use a lambda expression instead of an anonymous class
*********************************************************
Design

-Implementing  Serializable   is  a  serious commitment that should be made with care. 
 Extra caution is warranted if a class is designed for  inheritance .
 
-Take special care when making a class serializable if that class will be used in inheritance  
	-Classes designed for inheritance (Item 15) should rarely implement  Serializable , and 
	 interfaces should rarely extend it.
	-Therefore  you  should  consider  providing  a  parameterless constructor  on  nonserializable  
	 classes  designed  for  inheritance

-	 
	 
Design patterns
--------------------
visitor pattern is a common pattern used to walk through a family of classes


-Design by Contract (DBC) 

Is as simple as possible
Is as clear as possible
Has no ambiguity
Is completely accurate
Allows the reader to completely ignore implementation details (unless there is a bug)
Pours understanding into the mind of the reader as quickly as possible, with little chance for misunderstanding

The fundamental idea of Design By Contract is to treat the services offered by a class or interface as a contract 
between the class (or interface) and its caller
Contract as being made of two parts:
	requirements upon the caller made by the class
	promises made by the class to the caller

Requirements must be stated in javadoc, and may be enforced by throwing checked or unchecked exceptions 
when the stated conditions are violated.
Promises are stated in javadoc.


-Program to an interface and not to an implementation.
-Favor object composition over inheritance.

UML:
class diagrams: 
	-member's visibility, 
		-where + means public, - means private, and # means protected. 
	-In UML, methods whose names are written in italics are abstract
	-Static methods are shown underlined.
	
	-Inheritance is represented using a solid line and a hollow triangular arrow.
	-An interface looks much like inheritance, except that the arrow has a dotted line tail.
		-name <<interface>> is shown enclosed within double angle brackets
	-Composition: how objects are contained in other objects. For example, a Company might include one Employee and one Person 
		-By solid line with no arrow head
		-The lines between classes show that there can be 0 to 1 instances of Person in Company and 0 to 1 
		 instances of Employee in Company.
		-object composition as a single line with either an * or 0, *
		-
	-Aggregation
		-Some writers use a hollow and a solid diamond arrowhead to indicate containment of aggregates
	
	-Composition Vs Aggregation
		-Simple rules:

		A "owns" B = Composition : B has no meaning or purpose in the system without A
		A "uses" B = Aggregation : B exists independently (conceptually) from A
		Example 1:

		A Company is an aggregation of People. A Company is a composition of Accounts. 
		When a Company ceases to do business its Accounts cease to exist but its People continue to exist.

		Example 2: (very simplified)

		A Text Editor owns a Buffer (composition). A Text Editor uses a File (aggregation). 
		When the Text Editor is closed, the Buffer is destroyed but the File itself is not destroyed.
		
	-UML notations for different kind of dependency between two classes enter image description here
	
		association: solidLine with arrow
		dependency: dottedLine with arrow
		inheritence: solidLine with hollow traingular arrow head
		interface/impl: dotted line hollow traingular arrow head
		Aggregation: solidLine with hollow diamond head
		Composition: solidLine with black filled diamond head
		
	
Structural
----------
-proxy: Proxy is used to control access to its implementation
-state: State allows you to change the implementation dynamically

	-Proxy is simply a special case of State. 
	-Both Proxy and State provide a surrogate class that you use in your code. 
	 The real class that does the work is hidden behind this surrogate class
	-Structurally, the difference between Proxy and State is simple: a Proxy 
	 has only one implementation, while State has more than one 
	-Of course, it isn’t necessary that Implementation have the same 
	 interface as Proxy; as long as Proxy is somehow “speaking for” the class 
	 that it is referring method calls to then the basic idea is satisfied. 
	
	-The common uses for Proxy as described in Design Patterns are: 
		1. 	Remote proxy. This proxies for an object in a different address space. 
			A remote proxy is created for you automatically by the RMI compiler rmic as it creates stubs and skeletons. 
		2.	Virtual proxy. This provides “lazy initialization” to create expensive objects on demand. 
		3.	Protection proxy. Used when you don’t want the client programmer to have full 
			access to the proxied object. 
		4.	Smart reference. To add additional actions when the proxied object is accessed. 
			For example, or to keep track of the number of references that are held for a 
			particular object, in order to implement the copy-on-write idiom and prevent 
			object aliasing. A simpler example is keeping track of the number of calls to a 
			particular method. 
	 

-dynamicProxies
	-Spring AOP uses it extensively, it internally creates a dynamic proxy for different AOP constructs
	-For any frameworks needing to support interface and annotation based features A real proxied 
	 instance need not even exist, a dynamic proxy can recreate the behavior expected of an interface, 
	 based on some meta-data provided through annotations

*********************************************************
Pipeline & Streaming
--------------------


*********************************************************
Collection
----------

*********************************************************
Threads
--------
missed signal - while loop instead of if
slipped signal - check twice or use 1 syncronized block
spurious wakeup: syncronized on member variable of String const literal, notify will wake other unwanted threads.. more read
deadlock:
starvation: fairness
Blocked Vs Waiting
Reentrant
Locks should be Reentrance. Thread should be given lock on object when already have lock on same object,
	in cases requesting lock again will block the thread and deadlock will arise.
A reentrance lockout: If a thread calls lock() twice without calling unlock() in between, the second call to lock() will block. 
					  A reentrance lockout has occurred.

synchronizers: locks, semaphores, blocking queue etc


Concurrency:

-Parallel Workers
-Assembly Line (event driven)
-Function parallelism:
	(JDK 7)ForkAndJoinPool which can help you implement something similar to functional parallelism.

-If a resource is created, used and disposed within the control of the same thread, and never 
	escapes the control of this thread, the use of that resource is thread safe.
	
synchronized
------------
The object taken in the parentheses by the synchronized construct is called a monitor object


Volatile:
--------
Why would you need volatile keyword when member varaibles are shared across threads and will have same value.
Ans: If your computer contains more than one CPU, each thread may run on a different CPU
Java memory model and CPU architecture, main memory->cache->register. Value is flushed to main memory 
from cache. It may be possible 1 thread update shared var value but its still in cache another thread 
may read stale value from main memory(RAM). Volatile ensures any update is directly written to main memory.
It ensures every read and write to volatile variable will be done to main memory than cache.

When a thread writes to a volatile variable, then not just the volatile variable itself is written to main memory. 
Also all other variables changed by the thread before writing to the volatile variable are also flushed to main memory. 
When a thread reads a volatile variable it will also read all other variables from main memory which were flushed 
to main memory together with the volatile variable.

Developers may use this extended visibility guarantee to optimize the visibility of variables between threads. 
Instead of declaring each and every variable volatile, only one or a few need be declared volatile

Volatile works without synchronized as long as only Thread 1 calls put() and only Thread 2 calls take().

Java volatile happens before guarantee:
the volatile keyword comes with a "happens before guarantee". The happens before guarantee guarantees 
that read and write instructions of volatile variables cannot be reordered. Instructions before and after 
can be reordered, but the volatile read/write instruction cannot be reordered with any instruction 
occurring before or after it.


Thread signaling - wait, notify(), notifyAll() 
----------------------------------------------
http://tutorials.jenkov.com/java-concurrency/thread-signaling.html

Missed signals - if
Spurious Wakeups - use while loop instead of if 

Multiple Threads Waiting for the Same Signals
The while loop is also a nice solution if you have multiple threads waiting, which are all awakened 
using notifyAll(), but only one of them should be allowed to continue. Only one thread at a time will 
be able to obtain the lock on the monitor object, meaning only one thread can exit the wait() call 
and clear the wasSignalled flag. Once this thread then exits the synchronized block in the doWait() 
method, the other threads can exit the wait() call and check the wasSignalled member variable inside 
the while loop. However, this flag was cleared by the first thread waking up, so the rest of the awakened 
threads go back to waiting, until the next signal arrives.


Don't call wait() on constant String's or global objects: Don't use global objects, string constants 
etc. for wait() / notify() mechanisms. Use an object that is unique to the construct using it 

The problem with calling wait() and notify() on the empty string, or any other constant string is, 
that the JVM/Compiler internally translates constant strings into the same object. That means, that 
even if you have two different MyWaitNotify instances, they both reference the same empty string instance. 
This also means that threads calling doWait() on the first MyWaitNotify instance risk being awakened by 
doNotify() calls on the second MyWaitNotify instance.


Remember, that even if the 4 threads call wait() and notify() on the same shared string instance, 
the signals from the doWait() and doNotify() calls are stored individually in the two MyWaitNotify 
instances. A doNotify() call on the MyWaitNotify 1 may wake threads waiting in MyWaitNotify 2, but 
the signal will only be stored in MyWaitNotify 1.

At first this may not seem like a big problem. After all, if doNotify() is called on the second 
MyWaitNotify instance all that can really happen is that Thread A and B are awakened by mistake. 
This awakened thread (A or B) will check its signal in the while loop, and go back to waiting because 
doNotify() was not called on the first MyWaitNotify instance, in which they are waiting. This situation 
is equal to a provoked spurious wakeup. Thread A or B awakens without having been signaled. But the code 
can handle this, so the threads go back to waiting.

The problem is, that since the doNotify() call only calls notify() and not notifyAll(), only one thread 
is awakened even if 4 threads are waiting on the same string instance (the empty string). So, if one of 
the threads A or B is awakened when really the signal was for C or D, the awakened thread (A or B) will 
check its signal, see that no signal was received, and go back to waiting. Neither C or D wakes up to check 
the signal they had actually received, so the signal is missed. This situation is equal to the missed signals 
problem described earlier. C and D were sent a signal but fail to respond to it.

If the doNotify() method had called notifyAll() instead of notify(), all waiting threads had been awakened 
and checked for signals in turn. Thread A and B would have gone back to waiting, but one of either C or D 
would have noticed the signal and left the doWait() method call. The other of C and D would go back to 
waiting, because the thread discovering the signal clears it on the way out of doWait().


You may be tempted then to always call notifyAll() instead notify(), but this is a bad idea performance wise.
There is no reason to wake up all threads waiting when only one of them can respond to the signal.

Slipped signal:
Slipped conditions means, that from the time a thread has checked a certain condition until it acts upon it, 
the condition has been changed by another thread so that it is errornous for the first thread to act

Reentrant
Synchronized blocks in Java are reentrant. This means, that if a Java thread enters a synchronized block of 
code, and thereby take the lock on the monitor object the block is synchronized on, the thread can enter 
other Java code blocks synchronized on the same monitor object


Thread local
------------
http://stackoverflow.com/questions/1202444/how-is-javas-threadlocal-implemented-under-the-hood
thinking in java + concurrency in practice

A second way to prevent tasks from colliding over shared resources is to eliminate the 
sharing of variables. Thread local storage is a mechanism that automatically creates 
different storage for the same variable, for each different thread that uses an object. Thus, if 
you have five threads using an object with a variable x, thread local storage generates five 
different pieces of storage for x. 

The ThreadLocal class internally maintains a table associating data (Object references) with 
Thread instances. 

Most applications of ThreadLocals construct one instance per thread.
ThreadLocal variables are normally declared as static

ThreadLocal can also be useful for constructing per-thread resource pools


class ServiceUsingThreadLocal {           // Fragments 
 static ThreadLocal output = new ThreadLocal(); 
 
 public void service() { 
  try { 
    final OutputStream s = new FileOutputStream("..."); 
    Runnable r = new Runnable() { 
      public void run() { 
        output.set(s); 
        try { doService(); } 
        catch (IOException e) { ... } 
        finally { 
          try { s.close(); } 
          catch (IOException ignore) {} 
        } 
      } 
    }; 
    new Thread(r).start(); 
  } 
  catch (IOException e) { ...} 
 } 
 
 void doService() throws IOException { 
  ((OutputStream)(output.get())).write(...); 
  // ... 
 } 
} 

Since values set on a ThreadLocal object only are visible to the thread who set the value, 
no thread can set an initial value on a ThreadLocal using set() which is visible to all threads.

Instead you can specify an initial value for a ThreadLocal object by subclassing ThreadLocal 
and overriding the initialValue() method. Here is how that looks:

private ThreadLocal myThreadLocal = new ThreadLocal<String>() {
    @Override protected String initialValue() {
        return "This is the initial value";
    }
};


Now all threads will see the same initial value when calling get() before having called set() .


-	ThreadLocal objects are usually stored as static fields. When you create a ThreadLocal 
	object, you are only able to access the contents of the object using the get( ) and set( ) 
	methods. 

-	increment( ) and get( ) are not synchronized, because ThreadLocal guarantees that no race condition can occur.  

-	Thread local storage is a mechanism that automatically creates 
	different storage for the same variable.
	
-	Think of ThreadLocal<T> as holding a Map<Thread,T> that stores thread specific values
-	InheritableThreadLocal: Passing threadLocal storage values to child processes.

Benifits:
-	Should not be used in pool threads to communicate values between tasks
-	A disadvantage but one must be really careful with cleaning up ThreadLocals properly since whatever data you 
	put in there stays there as long as the thread lives unless it is explicitly removed. This is especially 
	troublesome in an environment where the threads are reused using thread pools so some garbage data maybe 
	attached to thread unless it's cleaned properly.
-	ThreadLocal offers a number of benefits. It is often the easiest way to render a stateful class thread-safe, 
	or to encapsulate non-thread-safe classes so that they can safely be used in multithreaded environments. 
	Using ThreadLocal allows us to bypass the complexity of determining when to synchronize in order to achieve 
	thread-safety, and it improves scalability because it doesn't require any synchronization. In addition to 
	simplicity, using ThreadLocal to store a per-thread-singleton or per-thread context information has a valuable 
	documentation perk -- by using a ThreadLocal, it's clear that the object stored in the ThreadLocal is not 
	shared between threads, simplifying the task of determining whether a class is thread-safe or not.	

e.g.
-	Transaction management in J2EE for example is done with ThreadLocals
- 	Frequent use of a non-threadsafe utility object that has a (relatively) high cost of construction (such as SimpleDateFormat)

*********************************************************
Memoization or caching

JDK 8 new methods
Map: getOrDefault, putIfAbsent
collection: removeIf() -> remove all elements in a collection that match a predicate.
List: replaceAll() -> similar to the map method in a stream, but it mutates the elements of the List.
						In contrast, the map method produces new elements.
					List<Integer> numbers = ....
					numbers.replaceAll(x -> x * 2);
					....
					
					

*********************************************************
Java memory model ||  memory->cache->Register
-----------------
	http://tutorials.jenkov.com/java-concurrency/java-memory-model.html
	The Java memory model specifies how the Java virtual machine works with the computer's memory (RAM). 
	The Java virtual machine is a model of a whole computer so this model naturally includes a 
	memory model - AKA the Java memory model.

	The Java memory model used internally in the JVM divides memory between thread stacks and the heap

	All local variables of primitive types ( boolean, byte, short, char, int, long, float, double) are 
	fully stored on the thread stack and are thus not visible to other threads. One thread may pass a 
	copy of a pritimive variable to another thread, but it cannot share the primitive local variable itself
		 -----------------
	CPU->|Register->cache|---->Memory
		 -------CPU-------
	On the hardware, both the thread stack and the heap are located in main memory
	Parts of the thread stacks and heap may sometimes be present in CPU caches and 
	in internal CPU registers


	When objects and variables can be stored in various different memory areas in the computer, 
	certain problems may occur. The two main problems are:
		-Visibility of thread updates (writes) to shared variables.
		-Race conditions when reading, checking and writing shared variables.

Stack and heap:
http://www.kdgregory.com/index.php?page=java.refobj

-	Local variables (& parameters) are still stored on the stack, but they hold a pointer to the object, not the object itself, in heap

-GC 
	-Mark & Sweep
		-Phase 1, Mark: The GC starts from root and walks thru the graph marking all objects
		-Phase 2, Sweep: Anything which is not marked in1st phase is eligible for collection. If GC obj has finalize
				It is added to finalization queue and if not space is made available
		-Phase 3, Compact:Some GC have this step and objects are moved to coalesce free space left behind. 	 		
	
	***Root: local variable & parameter in stack, static class member variables, operands of the 
			currently executing expression
			
	
	So what are the "roots"? In a simple Java application, they're method arguments and local variables 
	(stored on the stack), the operands of the currently executing expression (also stored on the stack), 
	and static class member variables.
	
	
-	In programs that use their own classloaders, such as app-servers, the picture gets muddy: only classes loaded 
	by the system classloader (the loader used by the JVM when it starts) contain root references. Any classloaders 
	that the application creates are themselves subject to collection, once there are no more references to them. 
	This is what allows app-servers to hot-deploy: they create a separate classloader for each deployed application, 
	and let go of the classloader reference when the application is undeployed or redeployed.

-	You may be wondering what happens if you have a circular reference: object A contains a reference to 
	object B, which contains a reference back to A. The answer is that a mark-sweep collector isn't fooled: 
	if neither A nor B can be reached by a chain of strong references, then they're eligible for collection.

-	However, memory isn't the only resource that might need to be cleaned up. 
	Consider FileOutputStream: when you create an instance of this object, it allocates a file handle from 
	the operating system. If you let all references to the stream go out of scope before closing it, 
	what happens to that file handle? The answer is that the stream has a finalizer method: a method that's 
	called by the JVM just before the garbage collector reclaims the object. In the case of FileOutputStream, 
	the finalizer closes the stream, which releases the file handle back to the operating system — and also 
	flushes any buffers, ensuring that all data is properly written to disk.

-	While finalizers seem like an easy way to clean up after yourself, they do have some serious limitations. 
	First, you should never rely on them for anything important, since an object's finalizer may never be 
	called — the application might exit before the object is eligible for garbage collection. There are some 
	other, more subtle problems with finalizers.
	
-	object life-cycle, without reference objects
	created - initialized - inUse - Unreachable - finalized

- 	Three new stages in the object life cycle: softly-reachable, weakly-reachable, and phantom-reachable

															-SoftlyReachable - 
															|		|	 	   |
	created - initialized - StronglyReachable---------------|-------|----------|-> Finalize
															WeaklyReachable - 		|
																					|
																					|					
															-PhantomReachable <-----	
	
	You use Reference objects when you want to continue to hold on to a reference to that 
	object you want to reach that object but you also want to allow the garbage collector to 
	release that object.
	
	You accomplish this by using a Reference object as an intermediary (a proxy) between you 
	and the ordinary reference. In addition, there must be no ordinary references to the object 
	(ones that are not wrapped inside Reference objects). If the garbage collector discovers that 
	an object is reachable through an ordinary reference, it will not release that object. 

	-softly reachable
		The object is the referent of a SoftReference, and there are no strong references to it. 
		The garbage collector will attempt to preserve the object as long as possible, but will collect 
		it before throwing an OutOfMemoryError.
	-weakly reachable
		The object is the referent of a WeakReference, and there are no strong or soft references 
		to it. The garbage collector is free to collect the object at any time, with no attempt to 
		preserve it. In practice, the object will be collected during a major collection, but may 
		survive a minor collection.
	-phantom reachable
		The object is the referent of a PhantomReference, and it has already been selected for collection 
		and its finalizer (if any) has run. The term “reachable” is really a misnomer in this case, as 
		there's no way for you to access the actual object.
	
	-References and Referents
		A reference object is a layer of indirection between your program code and some other object, called 
		a referent. Each reference object is constructed around its referent, and the referent cannot be changed
		
		-Soft references are for implementing memory-sensitive caches. 
		-Weak references are for implementing "canonicalizing mappings" where instances of objects can be 
		 simultaneously used in multiple places in a program, to save storage
		 
		 canonicalized mappings: http://wiki.c2.com/?CanonicalizedMapping
			A "canonicalized" mapping is where you keep one instance of the object in question in memory and 
			all others look up that particular instance via pointers or somesuch mechanism. 
			This is where weaks references can help.
			
			The short answer is that WeakReference objects can be used to create pointers to objects in your 
			system while still allowing those objects to be reclaimed by the garbage-collector once they 
			pass out of scope.
			
			WeakHashMap works exactly like HashMap, except that the keys (not the values!) 
			are referred to using weak references. If a WeakHashMap key becomes garbage, its entry is removed 
			automatically.

	-WeakHashMap | https://community.oracle.com/blogs/enicholas/2006/05/04/understanding-weak-references
		
		One of the requirement was to use serialNo for a Widget, the widget class was not extensible(declared final)
		so we had to use a hashMap for that
		serialNumberMap.put(widget, widgetSerialNumber);
		
		To solve the "widget serial number" problem above, the easiest thing to do is use the built-in 
		WeakHashMap class. WeakHashMap works exactly like HashMap, except that the keys (not the values!) 
		are referred to using weak references. If a WeakHashMap key becomes garbage, its entry is removed 
		automatically.
		
		But we have to exactly when we do not need the widget as above is a strong reference and we need to remove 
		the entry from the Map. 		
		WeakReference<Widget> weakWidget = new WeakReference<Widget>(widget);
		
		And then elsewhere in the code you can useweakWidget.get() to get the actual Widgetobject. Of course the 
		weak reference isn't strong enough to prevent garbage collection, so you may find (if there are no strong 
		references to the widget) that weakWidget.get()suddenly starts returning null.
		
		This generally means that some sort of cleanup is required;WeakHashMap, for example, has to remove such 
		defunct entries to avoid holding onto an ever-increasing number of deadWeakReferences
		
		The weakHashMap keys are automatically wrapped in WeakReferences by the map. The trigger to allow cleanup 
		is that the key is no longer in use. 
		The Key class must have a hashCode( ) and an equals( ) since it is being used as a key

		example:
		public class WeakHashMapTest {
			public static void main(String[] args) {
				Map hashMap= new HashMap();
		        Map weakHashMap = new WeakHashMap();
		        String keyHashMap = new String("keyHashMap");
		        String keyWeakHashMap = new String("keyWeakHashMap");
		        hashMap.put(keyHashMap, "Ankita");
		        weakHashMap.put(keyWeakHashMap, "Atul");
		        System.gc();
		        System.out.println("Before: hash map value:"+hashMap.get("keyHashMap")+" and weak hash map value:"+weakHashMap.get("keyWeakHashMap"));
		        keyHashMap = null;
		        keyWeakHashMap = null;
		        System.gc();  
		        System.out.println("After: hash map value:"+hashMap.get("keyHashMap")+" and weak hash map value:"+weakHashMap.get("keyWeakHashMap"));
		    }
		}
		//output:
		Before: hash map value:Ankita and weak hash map value:Atul
		After: hash map value:Ankita and weak hash map value:null		
		
		

-	they're method arguments and local variables (stored on the stack), the operands of the currently executing expression 
	(also stored on the stack), and static class member variables
	
-	In programs that use their own classloaders, such as app-servers, the picture gets muddy: only classes 
	loaded by the system classloader (the loader used by the JVM when it starts) contain root references. 
	Any classloaders that the application creates are themselves subject to collection, once there are no 
	more references to them. This is what allows app-servers to hot-deploy: they create a separate classloader 
	for each deployed application, and let go of the classloader reference when the application is undeployed 
	or redeployed.

-	You may be wondering what happens if you have a circular reference: object A contains a reference to object B, 
	which contains a reference back to A. The answer is that a mark-sweep collector isn't fooled: if neither 
	A nor B can be reached by a chain of strong references, then they're eligible for collection

-	Garbage collector runs in its own thread, and doesn't care what your code is doing

Object Life Cycle (without Reference Objects)
----------------------------------------------
JDK 1.2 introduced the java.lang.ref package, and three new stages in the object life cycle: 

-softly reachable
	The object is the referent of a SoftReference, and there are no strong references to it. The garbage 
	collector will attempt to preserve the object as long as possible, but will collect it before 
	throwing an OutOfMemoryError.
	
	The JDK documentation says that soft references are appropriate for a memory-sensitive cache: each of the 
	cached objects is accessed through a SoftReference, and if the JVM decides that it needs space, then it 
	will clear some or all of the references and reclaim their referents
	
	To be useful in this role, however, the cached objects need to be pretty large — on the order of several 
	kilobytes each. Useful, perhaps, if you're implementing a fileserver that expects the same files to be 
	retrieved on a regular basis, or have large object graphs that need to be cached. But if your objects are 
	small, then you'll have to clear a lot of them to make a difference, and the reference objects will add 
	overhead to the whole process.
	
	Soft Reference as Circuit Breaker
	
	A better use of soft references is to provide a "Circuit Breaker" for memory allocation: put a soft 
	reference between your code and the memory it allocates, and you avoid the dreaded OutOfMemoryError.
	
	1: List<List<Object>> results = new LinkedList<List<Object>>();
			
	2: SoftReference<List<List<Object>>> ref
        = new SoftReference<List<List<Object>>>(new LinkedList<List<Object>>());
        
       List<List<Object>> results = ref.get();
		if (results == null)
			throw new TooManyResultsException(rowCount);
		else
        	results.add(row); 
	
	While those expensive operations happen, the only reference to the list is via the SoftReference. 
	If you run out of memory the reference will be cleared, and the list will become garbage. It means 
	that the method throws, but the effect of that throw can be confined. And perhaps the calling code can 
	recreate the query with a retrieval limit.
	
	Once the expensive operations complete, you can hold a strong reference to the list with relative impunity. 
	However, note that I use a LinkedList for my results rather than an ArrayList: I know that linked lists 
	grow in increments of a few dozen bytes, which is unlikely to trigger OutOfMemoryError. By comparison, 
	if an ArrayList needs to increase its capacity, it must create a new array to do so. In a large list, 
	this could mean a multi-megabyte allocation.
	
	Also note that I set the results variable to null after adding the new element; this is one of the 
	few cases where doing so is justified. Although the variable goes out of scope at the end of the loop, 
	the garbage collector might not know that (because there's no reason for the JVM to clear the variable's 
	slot in the call stack). So, if I didn't clear the variable, it would be an unintended strong reference 
	during the subsequent pass through the loop.
	
	??
	Finally, think carefully about non-obvious strong references. For example, you might want to add a 
	circuit breaker while constructing XML documents using the DOM. However, each node in a DOM holds 
	a reference to its parent, in effect holding a reference to every other node in the tree. And if 
	you use a recursive call to build that document, your stack might be full of references to individual nodes.
	
-weakly reachable
	The object is the referent of a WeakReference, and there are no strong or soft references to it. 
	The garbage collector is free to collect the object at any time, with no attempt to preserve it. 
	In practice, the object will be collected during a major collection, but may survive a minor collection.
	
	There are two main uses: 
	-	associating objects that have no inherent relationship
	-	reducing duplication via a canonicalizing map.
	
	??
	The Problem With ObjectOutputStream
	
-phantom reachable
	The object is the referent of a PhantomReference, and it has already been selected for collection and 
	its finalizer (if any) has run. The term “reachable” is really a misnomer in this case, as there's no way 
	for you to access the actual object, but it's the terminology that the API docs use 

	Finallizer issue:
	
	-if all objects eligible for collection have finalizers, then the collection will have no effect: those 
	objects remain in memory awaiting finalization
	
	-A finalizer might never be invoked
	-Finalizers can create another strong reference to an object
	
	-finalization happens on its own thread, independent of the garbage collector's thread
	
	
These states only apply to objects eligible for collection in other words, those with no strong references

Phantom references
------------------


JVM
1) Mark
2) Sweep
3) Compact (optional)

*********************************************************
Garbage collection
-------------------

*********************************************************
hadoop
------

http://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html

URLs
----
-Spark UI: http://localhost:4040 [ http://<driver-node>:4040 ]
- cluster  manager’s  web  UI  should  appear  at @ http://masternode:8080 
	and show all your workers.
-spark.yarn.historyServer.address : http://lnxcdh21.emeter.com:18088/history


yarn logs -applicationId <appid> --appOwner <userid>
resource manager UI -> nodes page -> particular node -> particular container

Oozie urls

oozie job -oozie http://lnxcdh21.emeter.com:11000/oozie/ -kill 0000029-160511162000176-oozie-oozi-W
oozie job -oozie http://lnxcdh21.emeter.com:11000/oozie/ -info 0000029-160511162000176-oozie-oozi-W
oozie job -oozie http://lnxcdh21.emeter.com:11000/oozie/ -log 0000029-160511162000176-oozie-oozi-W

JobTracker webUI
-http://<job_tracker_address>:50030/ 
-http://localhost:50030/     | if"running"in"pseudo/distributed"mode"

NameNode webUI
-http://<name_node_address>:50070/ 

HBase WebUI
http://master.foo.com:60010
http://lnxcdh01.emeter.com:60010

HBaseMaster:  ports: HBaseMaster @60000 | <ip>:60000
RegionServer: http://<region-server-address>:60030

*********************************************************
What"is"Common"Across"Hadoop/able"Problems?"
Nature of the data
		-Volume"
		-Velocity"
		-Variety"
Nature of the analysis
		-Batch"processing"
		-Parallel"execuBon"
		-Distributed"data"


InputFile->InputFormat->partition split1->RecordReader->Mapper->Combiner->Partitioner->Reducer->OutputFormat->OutputFile
						---------------		----------	------	-----		-----		----
The%Mapper%
	–?Each"Map"task"(typically)"operates"on"a"single"HDFS" block"
	–?Map"tasks"(usually)"run"on"the"node"where"the" block"is"stored"

	- Combiners
		-VERY IMPORTANT: The Combiner may run once, or more than once, on  the output from any given Mapper
		-Combiner and Reducer code are o[en idenHcal 
			-Technically,"this"is"possible"if"the"operation"performed"is"commutative" and"associative"
			-Input"and"output"data"types"for"the"Combiner/Reducer"must"be"	identical"

				job.setMapperClass(WordMapper.class); 
				job.setReducerClass(SumReducer.class); 
				job.setCombinerClass(SumReducer.class); 


Shuffle and  Sort  is done by partitioners
	-Sorts"and"consolidates"intermediate"data"from"all" mappers"
	-Happens"after"all"Map"tasks"are"complete"and" before"Reduce"tasks"start"
	-Partitioner: The Partitioner determines which Reducer each intermediate key and its associated values goes to
	-The default Partitioner is the HashPartitioner 
		-Uses"the"Java"hashCode"method"
		-Guarantees"all"pairs"with"the"same"key"go"to"the"same"Reducer"
	

The Reducer
	–?Operates"on"shuffled/sorted"intermediate"data" (Map"task"output)"
	–?Produces"final"output"

Shuffle and sort
	After the Map phase is over, all intermediate values for a given intermediate key are grouped together

Each key and value list is passed to a Reducer
	-All"values"for"a"parAcular"intermediate"key"go"to"the"same"Reducer"
	-The"intermediate"keys/value"lists"are"passed"in"sorted"key"order
	
	
MRv2%daemons%
–?ResourceManager"–"one"per"cluster". Starts"ApplicaGonMasters,"allocates"resources"on"slave"nodes"
–?ApplicationMaster"–"one"per"job". Requests"resources,"manages"individual"Map"and"Reduce"tasks"
–?NodeManager"–"one"per"slave"node". Manages"resources"on"individual"slave"nodes"
–?JobHistory"–"one"per"cluster". Archives"jobs’"metrics"and"metadata"	

HDFS%daemons%
–?NameNode"–"holds"the"metadata"for"HDFS"". Typically"two"on"a"producGon"cluster:"one"acGve,"one"standby"
–?DataNode"–"holds"the"actual"HDFS"data. One"per"slave"node"

HDFS programtically
		Configuration conf = new Configuration(); 
		FileSystem fs = FileSystem.get(conf);
		Path p = new Path("/path/to/my/file");

-The conf object has read in the Hadoop con?guraHon ?les, and therefore  knows the address of the NameNode 
-A File in HDFS is represented by a Path object

-Directory listings
		Path p = new Path("/my/path"); 
		Configuration conf = new Configuration(); 
		FileSystem fs = FileSystem.get(conf); 
		
		//Directory listings
		FileStatus[] fileStats = fs.listStatus(p); 
		for (int i = 0; i < fileStats.length; i++) { 
			Path f = fileStats[i].getPath(); 
			// do something interesting 
		}

		//WritingData
		FSDataOutputStream out = fs.create(p, false); 
		// write some raw bytes 
		out.write(getBytes()); 
		// write an int 
		out.writeInt(getInt()); 
		... 
		out.close();
		
		//
	
Distributed cache
-The Distributed Cache provides an API to push data to all slave nodes
-Transfer"happens"behind"the"scenes"before"any"task"is"executed"
-Data"is"only"transferred"once"to"each"node,"rather""
-Note:"Distributed"Cache"is"read/only"
-Files"in"the"Distributed"Cache"are"automaEcally"deleted"from"slave" nodes"when"the"job"?nishes"

		Configuration conf = new Configuration(); 
		DistributedCache.addCacheFile(new URI("/myapp/lookup.dat"),conf); 
		DistributedCache.addFileToClassPath(new Path("/myapp/mylib.jar"),conf); 
		DistributedCache.addCacheArchive(new URI("/myapp/map.zip",conf)); 
		DistributedCache.addCacheArchive(new URI("/myapp/mytar.tar",conf)); 
		DistributedCache.addCacheArchive(new URI("/myapp/mytgz.tgz",conf)); 
		DistributedCache.addCacheArchive(new URI("/myapp/mytargz.tar.gz",conf));
		
		
LocalJobRunner mode
Configuration conf = new Configuration(); 
conf.set("mapred.job.tracker", "local"); 
conf.set("fs.default.name", "file:///");

No reducers
job.setNumReduceTasks(0); 
-Anything written using the Context.write method in the Mapper will be written to HDFS
	-Rather"than"written"as"intermediate"data"
	-One file"per"Mapper"will"be"written"

***How"Many"Reducers"Do"You"Need?"
-Default is single reducer
-Example: a job must output one file per day of the week
	-Key"will"be"the"weekday"
	-Seven"Reducers"will"be"specified"
	-A"Partitioner"will"be"written"which"sends"one"key"to"each"Reducer"
- you should take into account the number of Reduce slots likely to be available on the cluster
- Create a class that extends Partitioner 
- Override the getPartition method
		-Return"an"int"between"0"and"one"less"than"the"number"of"Reducers"
		-e.g.,"if"there"are"10"Reducers,"return"an"int"between"0"and"9"

URLs
-----
JobTracker webUI
-http://<job_tracker_address>:50030/ 
-http://localhost:50030/     | if"running"in"pseudo/distributed"mode"

NameNode webUI
-http://<name_node_address>:50070/ 


URLs
----
-Spark UI: http://localhost:4040 [ http://<driver-node>:4040 ]
- cluster  manager’s  web  UI  should  appear  at @ http://masternode:8080 
	and show all your workers.
-spark.yarn.historyServer.address : http://lnxcdh21.emeter.com:18088/history


yarn logs -applicationId <appid> --appOwner <userid>
resource manager UI -> nodes page -> particular node -> particular container

Oozie urls

oozie job -oozie http://lnxcdh21.emeter.com:11000/oozie/ -kill 0000029-160511162000176-oozie-oozi-W
oozie job -oozie http://lnxcdh21.emeter.com:11000/oozie/ -info 0000029-160511162000176-oozie-oozi-W
oozie job -oozie http://lnxcdh21.emeter.com:11000/oozie/ -log 0000029-160511162000176-oozie-oozi-W

JobTracker webUI
-http://<job_tracker_address>:50030/ 
-http://localhost:50030/     | if"running"in"pseudo/distributed"mode"

NameNode webUI
-http://<name_node_address>:50070/ 

HBase WebUI
http://master.foo.com:60010

HBaseMaster:  ports: HBaseMaster @60000 | <ip>:60000
RegionServer: http://<region-server-address>:60030

*********************************************************
spark examples from dataBricks:
https://github.com/databricks/learning-spark/tree/master/src/main/java/com/oreilly/learningsparkexamples/java

Hadoop | jobHistory | http://lnxcdh21.emeter.com:19888/jobhistory

Spark - History Server | http://lnxcdh21.emeter.com:18088/
-----
spark Tutorial : https://www.youtube.com/watch?v=7ooZ4S7Ay6Y

from same speaker: Myth of sparks
? Why we need spark
	-In distributed programming using java, we end up writing many MR jobs or pipelines of MR
	  to acheive the desired result i.e difficulty in programming directly in Hadoop
	-Performance bottlenecks, batch processing does not fit all use-cases. Spark not only 
	  provides batch processing but micro batch processing.
	-***Better support for Iterative jobs, typically for machine learning where data is loaded once.
	  Spark caching with LRU eviction is good. 
	-***Spark offers lazy initialization
	  Optimize the job before executing it. Only when a action is called Spark analyzes how best\
	  the optimization can be done
	-***In memory Caching. 
	  Scan HDD once and then scan the RAM
	-Efficient pipelining:
	  Avoids data hitting the HDD by all means
	
	Spark pillars
	--------------
	-Two main abstractions of spark
	 -RDD: resilient distributed dataSets
	 	-Simple: collection of dataset splitted in partitions across nodes of cluster
	 	-Complex: is an interface for data transformation
	 			 -RDD refers to data stored either in HDD store like HBase, cassandra, HDFS	
	 			  OR in cache memory
	 			  OR in another RDD
	 			-Partitions are recomputes on cache eviction i.e fault tolerant.
	 			-MetaData stored in interface
	 				-partitions	
	 					-set of data splits associated with this RDD
	 					-stores references to the partitions, 
	 					-can be a reference to a specific input split residing in HDFS 
	 					-can be a refernce to a cache item
	 				-dependencies 
	 					-List of parent RDD invloved in creation of this RDD
	 				-Compute: 
	 					-Re-compute a partition, if something is wrong or evicted in HDD
	 					-Depends on dependencies
	 					-if u have to recompute a partition after a shuffle, then u have to execute the whole shuffle
	 				-Preferred locations:
	 					-split has the info where this data belongs to. spark uses this info for use data locally concept
	 				-partitioner
	 	-Transformation: is lazy computation. only meta data is changed and computation actally starts.
	 		-everything is rdd in spark, datframe has rdd...
	 	-Action: computation starts after an action is called. sparks start with last rdd and goes to first rdd.
	 	
	 -DAG: Direct acyclic graph
	 	-Sequence of computation performed on data
	 	-Node: of the graph is rdd partition
	 	-Edge:is the transformation that is actually executed.
	 	-Acyclic: is a graph cannot return to previous partition. 
	 		-Spark is written in scala where all objects are immutable
	 		-If u have a partition u cannot update a partition but u create another RDD
	 	-Direct:it is direct as each transformation is from one partition to another
	 
	 
	 -IF u lose single partition from cache then spark will go back to part partition 
	  to re-create the partition and if that also not avlbl then go back to its parent
	  partition and so on...

Spark architecture: Two types of nodes
	-Driver Node: Runs driver. 
		Driver:
		-Has the sparkContext(all meta information about the cluster).
		-Is the entry point for spark shell
		-Translates RDD into execution graph
		-Brings up Spark Web UI.
		-
		
	-Worker Node: Runs no. of executors. 
		-Have cache. 
		-Each worker has no of tasks
		-Executors:
			-Stores data in cache in JVM / HDD
			-Reads data from sources, so driver does not read data unless you 
			 call sc.paralleize.. there u read data in driver
		-Each node can have many executors
		-Each executor can start multiple tasks as tasks are related to no of cores in the machine.
		
	
	-Executor Memory
		-10% of heap: JVM heap: for itself: 10% is reserved to avoid OutOfMem
		-90% of heap: safe: 90% of memory is dedicated to processing tasks
			-20% of safe: Shuffle: 
				-to sort the data in partition u need to have some buffer space. This is the buffer space
				-Used for append only map used for shuffling data
			-60% of safe: Storage
				-20% of storage: Unroll: Place where all desierailization happens. 
				 Cache can contain data and serialized data. Need some place to 
				 deserialize and transform data back to java objects.
				 Partitions are also enrolled here one-by-one. Unrolling is converting to java object.
				-80% Storage:
	
	-Application
		-Single instance of sparkContext that store some data, logic to process it and can schedule
		 series of jobs sequentially or parallel(sparkContext is threadSafe).
		 -Each job is set of transformations on RDD
		 	-Each job is split in stages
		 
		-When u run spark shell, application starts and when u close spark shell, application stops
		
	-Job: Each app is split in jobs
	-Stage: Each Job is split in stages. Set of transformations can be pipelined by a single worker.
		-Usually it is app transformations b/w  read, shuffle, action, save
	-Task: Each Stage is split in tasks. Execution of stage on single partition is called task.
	
	Spark memory management: https://0x0fff.com/spark-memory-management/
	-Memory: Spark considers memory as cache with LRU eviction rules.
	 If disk is available, the data is evicted to disk
	-rdd.cache().count(): only cache is just update in metaData but using
	 count as action actually evaluates the RDD and caches it.
	 
	-SparkDataFrame:
		-used as interface for all the languages and not specific to one languages
		-It is a RDD with schema - fields names, field dataTypes & statistics
		-RDD of row objects. 
		-Data is stored in row columnar format. row chunksize set by spark.sql.inMemoryColumnarStorage.batchSize
		-Delivers fast performance for small subsets 
		-DataBricks look dataFrame as future of spark
		-
	
	****spark2.0 adavanced concept | https://www.youtube.com/watch?v=1a4pgYzeFwE
	Structuring Apache Spark 2.0: SQL, DataFrames, Datasets And Streaming - by Michael Armbrust
	https://www.youtube.com/watch?v=pZQsDloGB4w
	
	
	
	-RDD disadvantage 
		-Dependencies: are list of dependencies that one RDD need
		-Partitions:Given these dependencies how am I going to split the work or computation
		-The data is unstructured 
		-By structure u can limit what u can express.
		
		
		-The computation is opaque, spark has no idea what the operation wld b join, etc .. just return a iterator
		-The data is also opaque, serialize into bytes , can't look into columns and do some optimization like compression etc
		-caching a RDD needs more memory that dataSet
		-serialization performance matters in spark as it makes suffling fast. 
		
		-Idea is tell spark more about the structure of data so that spark can use optimization and 
		 execute your code more efficiently.
		- Hence we need in some structure i.e take ur data and arrange them in some plan 	
		
		-spark is able to understand the computation
		
		-catalyst is the name of query optimizer(catalyst optimizer) that runs inside spark
		
		
		1.x -> dataFrame & dataSet were 2 diff | not to break code compatibility
		2.x-> made same 
		*****
							syntaxError					AnalysisError
		-SQL				runtime						runtime	
		-DataFrame			compileTime					runtime
		-DataSet			compileTime					compileTime
		
		
		DLS: domain-specific language (DSL) 
		
		-DataFrames are faster than RDD in general and now DataSet are the fastest due to optimizations
		-DF can make intelligent decisions and speed up things
		-DrawBack lost type safety. When we do .collect etc what we get is RDD of row which is not type safe.
		-You can anytime move back from DF to RDD to gain typeSafety but you lose all optimization.
		-To overcome above drawback and also not to lose  optimization, dataSet are used
		-catalyst optimizer provides all these optimizations
		-DataSets are
			-Extension to dataFrame APIs i.e operate on data thru sqlContext
			-Conceptually similar to RDD i.e. get back the lambdas and type
			-DataSet uses Tungesten's fast in-memory encoding
				-started managing spark memory(JVM objects and serialized objects) to off heap as 
				 in heap it is bound for garbage collection.
				-Also has columnar based storage that allows to acess fast and efficiently
				-In order to do optimization it has to know about the data.
				-It can expose expressions and fields to the DF query planner, where the 
				 optimizer(catalyst optimizer) can use to make decisions.
			-Provides interoperability with DF which RDD does not.
			-Avlbl from spark1.6 as experimental API
			-DataSet code is more visually compact(less typing) and	will tend to execute faster than 
			 than RDD counterpart because of Tungesten's encoding and Encoders 
			-When u want to store Person dataSet can store its efficient serialized from using encoders
			 and when u need baak dataSet it can deserialize the data back from the compact format(off-heap).
				**some doubt on serialization/deser, so use term compact form rather than ser/deser
			-Greatly efficient when using caching as less memory is used.. hence less spill overs...
			-The data compacted in tungesten format is 2 times compact than kryo..
			-***Datasets are similar to RDDs, however, instead of using Java serialization or Kryo they use 
			 a specialized Encoder to serialize the objects for processing or transmitting over the network. 
			 While both encoders and standard serialization are responsible for turning an object into bytes,
			 encoders are code generated dynamically and use a format that allows Spark to perform many 
			 operations like filtering, sorting and hashing without deserializing the bytes back into an object.
		
		-Limitation (1.6 they are marked experimental)
		
		Example
		import java.util.Arrays;
		import java.util.Collections;
		import java.io.Serializable;
		
		import org.apache.spark.api.java.function.MapFunction;
		import org.apache.spark.sql.Dataset;
		import org.apache.spark.sql.Row;
		import org.apache.spark.sql.Encoder;
		import org.apache.spark.sql.Encoders;
		
		public static class Person implements Serializable {
		  private String name;
		  private int age;
		
		  public String getName() {
		    return name;
		  }
		
		  public void setName(String name) {
		    this.name = name;
		  }
		
		  public int getAge() {
		    return age;
		  }
		
		  public void setAge(int age) {
		    this.age = age;
		  }
		}
		
		// Create an instance of a Bean class
		Person person = new Person();
		person.setName("Andy");
		person.setAge(32);
		
		// Encoders are created for Java beans
		Encoder<Person> personEncoder = Encoders.bean(Person.class);
		Dataset<Person> javaBeanDS = spark.createDataset(
		  Collections.singletonList(person),
		  personEncoder
		);
		javaBeanDS.show();
		
		
		//OR read from textFile
		// Create an RDD of Person objects from a text file
		JavaRDD<Person> peopleRDD = spark.read()
		  .textFile("examples/src/main/resources/people.txt")
		  .javaRDD()
		  .map(new Function<String, Person>() {
		    @Override
		    public Person call(String line) throws Exception {
		      String[] parts = line.split(",");
		      Person person = new Person();
		      person.setName(parts[0]);
		      person.setAge(Integer.parseInt(parts[1].trim()));
		      return person;
		    }
  		});
		Dataset<Person> javaBeanDS = spark.createDataset(
				 peopleRDD,
				 personEncoder
		);
		// +---+----+
		// |age|name|
		// +---+----+
		// | 32|Andy|
		// +---+----+
		
		// Encoders for most common types are provided in class Encoders
		Encoder<Integer> integerEncoder = Encoders.INT();
		Dataset<Integer> primitiveDS = spark.createDataset(Arrays.asList(1, 2, 3), integerEncoder);
		Dataset<Integer> transformedDS = primitiveDS.map(new MapFunction<Integer, Integer>() {
		  @Override
		  public Integer call(Integer value) throws Exception {
		    return value + 1;
		  }
		}, integerEncoder);
		transformedDS.collect(); // Returns [2, 3, 4]
		
		// DataFrames can be converted to a Dataset by providing a class. Mapping based on name
		String path = "examples/src/main/resources/people.json";
		Dataset<Person> peopleDS = spark.read().json(path).as(personEncoder);
		peopleDS.show();
		// +----+-------+
		// | age|   name|
		// +----+-------+
		// |null|Michael|
		// |  30|   Andy|
		// |  19| Justin|
		// +----+-------+
		
	-GZIP file: cannot split a gzip file while reading so only one partition can be used.
	-DataFrame.explain - explain the query optmz plan
	-DataFrame.explain..true - explain the query optmz for each stage or so
	-
	
? When use spark / MapReduce
	-Spark reduces development time
	-Spark has MLib implementation
	
? Difference between cache and persist
	-With cache(), you use only the default storage level MEMORY_ONLY.
	-With persist(), you can specify which storage level you want,(rdd-persistence).
	-Use persist() if you want to assign another storage level than MEMORY_ONLY to the RDD (which storage level to choose)
	
? Which API in spark use shuffle: Examples @ http://backtobazics.com/big-data/spark/apache-spark-reducebykey-example/
	-flatMap: not uses shufle
	-flatMapToPair: not uses shufle
	-reduceByKey: forces shuffle
	-groupByKey: It uses shuffle
		groupBy is a transformation operation in Spark hence it is lazily evaluated
		It is a wide operation as it shuffles data from multiple partitions and create another RDD
		It is a costly operation as it doesn’t us combiner local to a partition to reduce the data transfer
		Not recommended to use when you need to do further aggregation on grouped data
	-filter(): not shuffling data
	
? In a typical MR how many times the data is put in HDD 1,2,3 or 4
	-Three or more [rare case two, when data of single reducer fits into memory of this reducer]

? How shuffle works in Spark: https://0x0fff.com/spark-architecture-shuffle/
	-prior to spark 1.2.0, shuffle was a problem in spark. It was implemented as Hash shuffle
	-Now it is sort shuffle as default: it uses append only map in shuffle memory area shown above. 10% 20% etc
	-Tungsten sort: optimized one (dataBrics is working on it). Now available with spark 1.5 & >
	
? How many MR jobs are required by hive
	-Typically Hive queries perform 3-5 MR jobs
	-And each MR writes data 3 or more times in disk
	-

? Which sort algorithm is used in spark
	-Timsort is a hybrid stable sorting algorithm, derived from merge sort and insertion sort, 
	 designed to perform well on many kinds of real-world data.

? appendOnlyMap: spark/scala impl

myths of spark:
https://0x0fff.com/spark-misconceptions/



		-Spark is an in-memory technology
		-Spark performs 10x-100x faster than Hadoop
		-Spark introduces completely new approach for data processing on the market

		What do we see in Spark? It has no option for in-memory data persistence, it has pluggable connectors 
		for different persistent storage systems like HDFS, Tachyon, HBase, Cassandra and so on, but it does 
		not have native persistence code, neither for in-memory nor for on-disk storage. Everything it can do 
		is to cache the data, which is not the “persistence”. Cached data can be easily dropped and recomputed 
		later based on the other data available in the source persistent store available through connector.

		Given the fact that Spark allows you to use in-memory cache with the LRU eviction rules, you might 
		still assume that it is in-memory technology, at least when the data you are processing fits in memory. 

		And even more. Do you think that Spark processes all the transformations in memory? You would be disappointed, 
		but the heart of Spark, “shuffle”, writes data to disks. If you have a “group by” statement in your 
		SparkSQL query or you are just transforming RDD to PairRDD and calling on it some aggregation by key, 
		you are forcing Spark to distribute data among the partitions based on the hash value of the key. 

		The “shuffle” process consists of two phases, usually referred as “map” and “reduce”.
		
		So if you have an RDD of M partitions and you transform it to pair RDD with N partitions, 
		there would be M*N files created on the local filesystems in your cluster, holding all the 
		data of the specific RDD. There are some optimizations available to reduce amount of files. 
		Also there are some work undergo to pre-sort them and then “merge” on “reduce” side, but 
		this does not change the fact that each time you need to “shuffle” you data you are putting 
		it to the HDDs.
		
		So finally, Spark is not an in-memory technology. It is the technology that allows you to 
		efficiently utilize in-memory LRU cache with possible on-disk eviction on memory full condition. 
		It does not have built-in persistence functionality (neither in-memory, nor on-disk). And it 
		puts all the dataset data on the local filesystems during the “shuffle” process.
		
		
		Next misconception is that “Spark performs 10x-100x faster than Hadoop”. Let’s refer to one 
		of the early presentations on this topic: http://laser.inf.ethz.ch/2013/material/joseph/LASER-Joseph-6.pdf. 
		It states as a goal of Spark to support iterative jobs, typical for machine learning. If you 
		refer to the Spark main page on Apache website, you would again see an example of 
		where the Spark shines:

		And again, this example is about the machine learning algorithm called “Logistic Regression”. 
		What is the essential part of the most machine learning algorithms? They are repeatedly iterating 
		over the same dataset many times. And here is where Spark in-memory cache with LRU eviction 
		really shines! 
		
		****In general, Spark is faster than MapReduce because of:
		
		-Faster task startup time. Spark forks the thread, MR brings up a new JVM
		-Faster shuffles. Spark puts the data on HDDs only once during shuffles, MR do it 2 times
		-Faster workflows. Typical MR workflow is a series of MR jobs, each of which persists data to 
			HDFS between iterations. Spark supports DAGs and pipelining, which allows it to execute 
			complex workflows without intermediate data materialization (unless you need to “shuffle” it)
		-Caching. It is doubtful because at the moment HDFS can also utilize the cache, but in general 
			Spark cache is quite good, especially its SparkSQL part that caches the data in optimized column-oriented form


		They are good in implementing the idea of efficient LRU cache and data processing pipelining, 
		but they are not alone. If you would be open-minded thinking about this problem, you would 
		notice that in general they are implementing almost the same concepts that were earlier introduced 
		by MPP databases: 

		Let’s start with fault tolerance. “Cache” concept assumes that data is persisted somewhere else. 
		If it is not this way, this is not the “cache”, but yet another persistent store. In case of HDFS, 
		caching data does not mean removing it from HDFS, so you have full fault tolerance – if the cache 
		entry is evicted, it can be easily read from HDFS (one of N copies of your data there). 
		So HDFS cache is fully fault-tolerant
		
		***About lineage – Hadoop and MapReduce concept does not include multi-step computation model, 
		so the data is persisted after each MapReduce job. Within the map or reduce task it is again 
		lineaged – when you lose one map task you would recompute it on another node with the same 
		input split (you don’t have to restart everything), if the reducer fails it would copy map 
		outputs once again on another node and execute there.
		
		So again, Spark is a great engine for distributed compute, but it is good because of 2 things: 
		caching and pipelining, and both of them are not new concepts. But I agree that Spark provides 
		unique blend of them
		
		****Spark cannot be 10x and even 100x faster than traditional MR, I just say that you won’t 
		have 10x and even 2x+ improvement by just moving to Spark – only certain cases would benefit 
		from using it, but on average moving to Spark won’t magically make your system blazing fast 

		
		***Spark is not designed to be a persistent storage engine. What they call “persistence” is effectively caching, because:
		1. The data is available to your session only. You cannot share cached RDD with other sessions
		2. The data lifetime is limited by your session runtime. If your session is failed or terminated, you lose your cached data
		In case you have some experience with databases, this caching works more or less like temporary tables
		
		
		
Hadoop is 3 apache proj
	-HDFS
	-Yarn
	-MapReduce (spark core is a contender of replacing it)
	
RDDs can be created:
1) Loading external data Set : sc.textFile()
2) Distributing collection of objects : sc.parallelize


-

JavaRDD<String> inputRDD = sc.textFile("Log.txt");
JavaRDD<String> errorsRDD = inputRDD.filter(
  new Function<String, Boolean>() {
    public Boolean call(String x) { return x.contains("error"); }
  }
});

-RDD (Resilient Distributed Datasets):  
	a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel


-An RDD in Spark is simply an immutable distributed collection of objects. Each RDD is split into multiple partitions, 
 which may be computed on different nodes of the cluster
-Transformation and action: If you are ever confused whether a given function is a transformation or an action, 
 you can look at its return type: transformations return RDDs, whereas actions return some other data type.
-cache() is the same as calling persist() with the default storage level
-parallelize() The simplest way to create RDDs is to take an existing collection in your program and pass it to SparkContext’s parallelize() method 
	JavaRDD<String> lines = sc.parallelize(Arrays.asList("pandas", "i like pandas"));
-Spark keeps track of the set of dependencies between different RDDs, called the lineage graph.

-Although transformations are lazy, you can force Spark to execute them at any time by running an action, such as count(). This is an easy way to test out just part of your program.
-Both anonymous inner classes and lambda expressions can reference any final variables in the method enclosing them, so you can pass these variables to Spark just as in Python and Scala.
-Note that distinct() is expensive
-The performance of intersection() is much worse than union()
-Cartesian product is very expensive for large RDDs.
-The persist() call on its own doesn’t force evaluation.
-RDDs come with a method called unpersist() that lets you manually remove them from the cache.
-Spark keep dependencies between different RDDs, called the lineage graph
-Both  anonymous  inner  classes  and  lambda  expressions  can  reference any  final  variables in the 
 method enclosing them, so you can pass these variables to Spark
-function: 
	Function<T,R> : R call(T) : Take in one input and return one output
	Function2<T1, T2, R>: R call(T1,T2) : 2 input and 1 o/p in operations like aggregate(), fold()
	FlatMapFunction<T,R>: Iterable<R> call(T) : 1 input and 0 or more o/p for flatMap()
-The  map()  transformation takes in a function and applies it to each element in the RDD with
	the result of the function being the new value of each element in the resulting RDD.
	Its useful to note that map() ’s return type does not have  to be the same as its input type
-.flatMap():Spark RDD flatMap function returns a new RDD by first applying a function to all elements of this RDD, 
			and then flattening the results
			
			Sometimes we want to produce multiple output elements for each input element. 
			The operation to do this is called flatMap()
			
			Its useful to note that map() ’s return type does not have to be the same as its input type
			
			-It is a narrow operation as it is not shuffling data from one partition to multiple partitions
			-Output of flatMap is flatten
			-flatMap parameter function should return array, list or sequence 
			
			Ex.
				public class FlatMapExample {
					public static void main(String[] args) throws Exception {
						JavaSparkContext sc = new JavaSparkContext();

						// Parallelized with 2 partitions
						JavaRDD<String> rddX = sc.parallelize(
								Arrays.asList("spark rdd example", "sample example"),
								2);

						// map operation will return List of Array in following case
						JavaRDD<String[]> rddY = rddX.map(e -> e.split(" "));
						List<String[]> listUsingMap = rddY.collect();

						// flatMap operation will return list of String in following case
						JavaRDD<String> rddY2 = rddX.flatMap(e -> Arrays.asList(e.split(" ")));
						List<String> listUsingFlatMap = rddY2.collect();
					}
				}			
				
	
			
-The filter() transformation takes in a function and returns an RDD that only has elements that pass the filter()  function.

-.persist(): When we ask Spark to persist an RDD, the nodes that compute the RDD store their partitions.

****
spark architecture:
https://0x0fff.com/spark-architecture/

-  So if you want to know how much data you can cache in Spark, you should take the sum of all the heap sizes for all the executors, 
	multiply it by safetyFraction and by storage.memoryFraction, and by default it is 0.9 * 0.6 = 0.54 or 54% of the total heap size 
	you allow Spark to use

-key/value RDD:


operations: Basic RDD transformations on an RDD containing {1, 2, 3, 3}


1-RDD transformations 

map() - Apply a function to each element in the RDD and return an RDD of the result.
flatmap() - Apply a function to each element in the RDD and return an RDD of the contents of the iterators returned. Often used to extract words
			-  as “flattening” the iterators returned to it, so that instead of ending up with an RDD of lists we have an RDD of the elements in those lists.
filter() - 
distinct()
sample(withReplacement, fraction, [seed]) - 

2-RDD transformations 

subtract()
union()
intersection()
cartesian()

RDD actions

.first()
collect() - Return all elements from the RDD.
count() - Number of elements in the RDD.
countByValue() - Number of times each element occurs in the RDD
take(num) - Return num elements from the RDD.
top(num) - Return the top num elements the RDD.
takeOrdered(num)(ordering) - Return num elements based on provided ordering.
takeSample(withReplacement, num, [seed]) - Return num elements at random.
reduce(func) - Combine the elements of the RDD together in parallel (e.g., sum).
fold(zero)(func) - Same as reduce() but with the provided zero value
aggregate(zeroValue)(seqOp, combOp) - Similar to reduce() but used to return a different type.
foreach(func) - Apply the provided function to each element of the RDD.


fold()

-require that the return type of our result be the same type as that of the elements in the RDD we are operating over



spark Tutorial : https://www.youtube.com/watch?v=7ooZ4S7Ay6Y
--------------

Spark:
	scheduling 
	monitring 
	distributing

Spark universe
	
1) spark core (at centre)

2) spark library: push computation to core
	SQL - hive queries work automatically
	Streaming: flume and kafka buffer straming
	Mlib
	GraphX
	BlinkDB: query result in x sec with y error rate..
	TackYon: data distribution 

3) Resource managers
	Node	
	yarn
	Mesos
	spark standalone node

4) File systems
	HDFS
	HBASE
	MongoDB
	...

Oozie shoots MR jobs in order in a cluster

Oozie urls

oozie job -oozie http://lnxcdh21.emeter.com:11000/oozie/ -kill 0000029-160511162000176-oozie-oozi-W
oozie job -oozie http://lnxcdh21.emeter.com:11000/oozie/ -info 0000029-160511162000176-oozie-oozi-W
oozie job -oozie http://lnxcdh21.emeter.com:11000/oozie/ -log 0000029-160511162000176-oozie-oozi-W

JobTracker webUI
-http://<job_tracker_address>:50030/ 
-http://localhost:50030/     | if"running"in"pseudo/distributed"mode"

NameNode webUI
-http://<name_node_address>:50070/ 

Data read speed
1) RAM 10GB/s
2) HDD 100MB/s
3) External 
4) N/w slow

White paper : 
	-spark 
	-RDD
	-streaming
	-sparkSql
	-blinkDB
	-graphX


	RDD
----------
-Partitions: More partitions leads to more parallelism. 
-For each partition we need a task/thread to complete operations on that RDD
-RDD can be created:
	-parallize a collection 
		e.g.: sc.parallelize(Arrays.asList("fish","cat","dogs"));
		Not used outside testing or prototyping
	-read from file, cluster etc
		e.g. sc.textFile("xxx.txt")
	-Types:
		HadoopRDD
		FilteredRDD
		JdbcRDD
		ShuffledRDD
		
		....
		....

-RDD interface has:
	-Partitions
	-dependencies
	-
-Base RDD (with some partitions)- transforms to new RDD (it gets same no of partitions) -> Action like collect() [give me data back at driver] 
-All tranformations in RDD are lazy. So it keep on creating meta data that this 
	RDD depends on this etc. But no read happen.
-Dont call collect action on RDD of TB size, out of memory 
-Which RDD to cache: intermediate RDD used many times, should be cached (cleaned RDD shld be cached)
-caching a RDD is also lazy. Cachced in JVM (cached RDD are visible in spark UI)
-when RDD is cached, where its gets cached ?
-LifeCycle of spark program
	-create RDD(parallelize / external data) in driver program
	-Lazy transform them to new RDDs using transformations like filter() or map()
	-Cache any intermediate RDDs that can be reused
	-Launch actions like count() and collect() to kick of parallel computation. Optimized and executed by spark

-Transformations(lazy): it work on every item of RDD e.g. map() howeever some transformations work on per partition base e.g. 
	open DB connection save and then close DB connection. 
-Actions: 
-download spark scala source code to understand transormations and actions in details. API comments explains better


.count() -> no f items in RDD
.collect() -> collect all items from RDDs and bring back to driver



Fast:
1) keeps intermediate data in memory unlike Hadoop MR it does not write to HDFS 10-100x speed
2) MapRed has hardcoded map and reduce slots so CPU usage is not 100%. In spark has generic slots 
	that can be used either by map/reduce
3) Empty slots for map or reduce are not filled aggressively in Map-reduce (Face book noticed that and used corona to 
   agressively start next map/reduce job). In spark it does
4) parallelism in MapRed - is by process Id on a node (???). Slots in MapRed is called processId
   parallelism in spark - is by threads on a executor. So better. Spark calls them cores. So eg. 6 cores are started in a Executor 



Spark architecture:

1) Local mode	shell starts JVM runs and start
		executor and
		driver process
		
	Jvm has slots called cores	eg. 6 cores i.e. 6 threads can run simultenously 	
2) Standalone mode: Driver tells the master that need some worker JVM to run my tasks
3) Yarn
4) 


spark mailing lists	 - q are answered quickly




Table 3-4. Basic actions on an RDD containing {1, 2, 3, 3}


Spark performance enhancements tips:
-use mapPartitions works per-partition to avoid setup wprk again & again
-
-
-
-
	
URLs
----
-Spark UI: http://localhost:4040 [ http://<driver-node>:4040 ]
- cluster  manager’s  web  UI  should  appear  at @ http://masternode:8080 
	and show all your workers.
-spark.yarn.historyServer.address : http://lnxcdh21.emeter.com:18088/history


yarn logs -applicationId <appid> --appOwner <userid>
resource manager UI -> nodes page -> particular node -> particular container

Oozie urls

oozie job -oozie http://lnxcdh21.emeter.com:11000/oozie/ -kill 0000029-160511162000176-oozie-oozi-W
oozie job -oozie http://lnxcdh21.emeter.com:11000/oozie/ -info 0000029-160511162000176-oozie-oozi-W
oozie job -oozie http://lnxcdh21.emeter.com:11000/oozie/ -log 0000029-160511162000176-oozie-oozi-W

JobTracker webUI
-http://<job_tracker_address>:50030/ 
-http://localhost:50030/     | if"running"in"pseudo/distributed"mode"

NameNode webUI
-http://<name_node_address>:50070/ 

HBase WebUI
http://master.foo.com:60010

HBaseMaster:  ports: HBaseMaster @60000 | <ip>:60000
RegionServer: http://<region-server-address>:60030


Questions
---------
?coalesce : to unite so as to form one mass
?lineage graph : Spark dependencies between different RDDs, called the lineage graph
?Task Vs Executors in SPark, can a executor run multiple tasks

*********************************************************
Spark MLib
----------
-spark.ml, which aims to provide a uniform set of high-level APIs that help users create and tune practical machine learning pipelines.
-spark.mllib will later be deprecated and expected ml will be used. (My conclusion)
-MLlib uses the linear algebra package Breeze, which depends on netlib-java for optimised numerical processing

-MLlib’s  design  and  philosophy  are  simple:  it  lets  you  invoke  various  algorithms  on
 distributed datasets, representing all data as RDDs but at the end of the day, it is simply 
 a set of functions to  call  on  RDDs.

-Regression refers to predicting a numeric quantity like size or income or temperature
-Classification refers to predicting a label or category,like spam or picture of a cat
-Clustering

e.g. spam filter
-RDD: Create RDD of strings representing your messages
-Feature Creation: Run one of MLlib’s feature extraction algorithms to convert text into numerical
 features (suitable for learning algorithms); this will give back an RDD of vectors.
-Training model: Call a classification algorithm (e.g logistic  regression) on the RDD of vectors.
 this will give back a model object that can be used to classify new points
-Evaluation: Evaluate the model on a test dataset using one of MLlib’s evaluation functions
-Prediction: Predict test data using created model

-System requirement:
	gfortran runtime library
	
-Data Types: package org.apache.spark.mllib
	
	-Vector: Vectors can be constructed with the  mllib.linalg.Vectors  class
		Dense: stores every entry in an array of floating point numbers.
		Sparse: only the nonzero entries and their entries are stored to save space.
		
		-Vector denseVec1 = Vectors.dense(1.0, 2.0, 3.0);
		 Vector denseVec2 = Vectors.dense(new double[] {1.0, 2.0, 3.0});
		-Vector sparseVec1 = Vectors.sparse(4, new int[] {0, 2}, new double[]{1.0, 2.0});

	
	-LabeledPoint: A labeled data point for supervised learning algorithms such as classification and
				   regression. Includes a feature vector and a label (which is a floating-point value).
				   Located in the  mllib.regression  package.
					
	-Rating: A rating of a product by a user, used in the  mllib.recommendation  package for
             product recommendation.
	
	-Model: Each  Model   is  the  result  of  a  training  algorithm,  and  typically  has  a  predict()
			method  for  applying  the  model  to  a  new  data  point  or  to  an  RDD  of  new  data
			points.
			
Most algorithms work directly on RDDs of  Vector s,  LabeledPoint s, or  Rating s			


-Feature extraction: The  mllib.feature  package contains several classes for common feature transformations
					 Create Feature vectors from text


-LIBSVM is a library for Support Vector Machines (SVMs).					 



			************************************


Machine Learning
----------------


Machine learning explores the construction and study of algorithms that can learn from and make predictions on data.
Such algorithms operate by building a model from example inputs in order to make data-driven predictions or decisions
rather than following strictly static program instructions.


R randomForest implements Breiman’s random forest algorithm for classification and regression.

The wrongness measure is known as the cost function (a.k.a., loss function)

George E. P. Box that "all models are wrong, but some are useful"

predict.randomForest :: predict method for random forest objects


Classification Problems

Regression machine learning systems: 
------------------------------------
	Systems where the value being predicted falls somewhere on a continuous spectrum. These systems help us 
	with questions of How much? or How many?
Classification machine learning systems: 
----------------------------------------
	Systems where we seek a yes-or-no prediction, such as Is this tumer cancerous?, Does this cookie meet our quality standards?, and so on.


What is Random Forests?
		Segment and cluster
		Suited for wide data
		Advantages of Random Forests
		Case Study example

-CART (Classification and Regression Trees)
-RandomForests are constructed from decision trees.
-Random Forests is a tool that leverages the power of many decision trees, judicious randomization, and ensemble 
	learning to produce astonishingly accurate predictive models, insightful variable importance rankings, missing value imputations, 
	novel segmentations, and laser-sharp reporting on a record-by-record basis for deep data understanding. 
-Random Forests are collections of decision trees that together produce predictions and deep insights into the structure of data 



decision tree
--------------
https://www.youtube.com/watch?v=eKD5gxPPeY0
http://spark.apache.org/docs/latest/mllib-decision-tree.html



-Decision trees are widely used since they are easy to interpret, handle categorical features, extend to the 
 multiclass classification setting, do not require feature scaling, and are able to capture non-linearities and 
 feature interactions.

-MLlib supports decision trees for binary and multiclass classification and for regression, 
 using both continuous and categorical features. The implementation partitions data by rows, allowing 
 distributed training with millions of instances

-Basic algorithm: 
 The decision tree is a greedy algorithm that performs a recursive binary partitioning of the feature space. 
 The tree predicts the same label for each bottommost (leaf) partition. Each partition is chosen greedily by 
 selecting the best split from a set of possible splits, in order to maximize the information gain at a tree node. 
 
-Node impurity
 The node impurity is a measure of the homogeneity of the labels at the node. The current implementation 
 provides two impurity measures for classification (Gini impurity and entropy) 

		mpurity			Task			Formula			Description
		Gini impurity	Classification	?Ci=1fi(1-fi)	fi is the frequency of label i at a node and C is the number of unique labels.
		Entropy			Classification	?Ci=1-filog(fi)	fi is the frequency of label i at a node and C is the number of unique labels.	

		? is 

-Information gain 
 The information gain is the difference between the parent node impurity and the weighted sum of the two child node impurities

-Stopping rule
 The recursive tree construction is stopped at a node when one of the following conditions is met:
 
 1) The node depth is equal to the maxDepth training parameter.
 2) No split candidate leads to an information gain greater than minInfoGain.
 3) No split candidate produces child nodes which each have at least minInstancesPerNode training instances.
 
-Problem specification parameters: These parameters describe the problem you want to solve and your dataset. 
 They should be specified and do not require tuning.
 
 -algo: Classification or Regression 
 -numClasses: Number of classes (for Classification only)
 -categoricalFeaturesInfo: Specifies which features are categorical and how many categorical values each 
  of those features can take. This is given as a map from feature indices to feature arity (number of categories). 
  Any features not in this map are treated as continuous.
  
  E.g., Map(0 -> 2, 4 -> 10) specifies that feature 0 is binary (taking values 0 or 1) and that 
  feature 4 has 10 categories (values {0, 1, ..., 9}). 	
 
 
-Stopping criteria parameters: These parameters determine when the tree stops building (adding new nodes). 
 When tuning these parameters, be careful to validate on held-out test data to avoid overfitting.
 
 	-maxDepth: Maximum depth of a tree. Deeper trees are more expressive (potentially allowing higher accuracy), 
		but they are also more costly to train and are more likely to overfit.
	-minInstancesPerNode: For a node to be split further, each of its children must receive at least this 
		number of training instances. This is commonly used with RandomForest since those are often trained 
		deeper than individual trees.
	-minInfoGain: For a node to be split further, the split must improve at least this 
		much (in terms of information gain).
 
-Tunable parameters: These parameters may be tuned. Be careful to validate on held-out test data when tuning in order to avoid overfitting
 	-maxBins: Number of bins used when discretizing continuous features.
		Increasing maxBins allows the algorithm to consider more split candidates and make fine-grained 
		split decisions. However, it also increases computation and communication.
		Note that the maxBins parameter must be at least the maximum number of categories 
		M for any categorical feature.	
 	-maxMemoryInMB: Amount of memory to be used for collecting sufficient statistics.
	-subsamplingRate: Fraction of the training data used for learning the decision tree.
	-impurity: Impurity measure (discussed above) used to choose between candidate splits. 
	 This measure must match the algo parameter.


RandomForest : Random forests are ensembles of decision trees.
------------
-An ensemble method is a learning algorithm which creates a model composed of a set of other base models. 
 MLlib supports two major ensemble algorithms: 
 	GradientBoostedTrees (takes longer as train each tree at a time) 
 	RandomForest(can train multiple trees in parallel)

-GBT VS RandomForest 	
	-Less vs more time
	-Training more trees in a Random Forest reduces the likelihood of overfitting
	-Random Forests can be easier to tune since performance improves monotonically with the 
	 number of trees (whereas performance can start to decrease for GBTs if the number of trees grows too large).

-Basic algorithm
	-Random forests train a set of decision trees separately, so the training can be done in parallel. 
	-The algorithm injects randomness into the training process so that each decision tree is a bit different. 
	-Combining the predictions from each tree reduces the variance of the predictions, improving the performance on test data.
	
-Prediction: To make a prediction on a new instance, a random forest must aggregate the predictions 
			 from its set of decision trees. This aggregation is done differently for classification and regression.
			 
	-Classification: Majority vote. Each tree’s prediction is counted as a vote for one class. 
	 The label is predicted to be the class which receives the most votes.			 
 	-Regression: Averaging. Each tree predicts a real value. The label is predicted to be the average 
 	 of the tree predictions.
 	 
-Tunning parameters
	-numTrees: Number of trees in the forest.
		-Increasing the number of trees will decrease the variance in predictions, improving the model’s test-time accuracy.
		-Training time increases roughly linearly in the number of trees
	-maxDepth: Maximum depth of each tree in the forest.
		-Increasing the depth makes the model more expressive and powerful. However, deep 
		 trees take longer to train and are also more prone to overfitting
		-In general, it is acceptable to train deeper trees when using random forests than when using a single decision tree. 
		 One tree is more likely to overfit than a random forest (because of the variance 
		 reduction from averaging multiple trees in the forest).
	
	Below parameters can be tuned to speed up training (generally not required to tune)
	-subsamplingRate: This parameter specifies the size of the dataset used for training each tree in 
	 the forest, as a fraction of the size of the original dataset. The default (1.0) is recommended, 
	 but decreasing this fraction can speed up training.	
	-featureSubsetStrategy: Number of features to use as candidates for splitting at each tree node. 
	 The number is specified as a fraction or function of the total number of features 
	 
-Precision and recall
 Precision is actually a common metric for binary classification problems, where there are two category values, 
 not several. In a binary classification problem, where there is some kind of positive and negative class, 
 precision is the fraction of examples that the classifier marked positive that are actually positive. 
 It is often accompanied by the metric recall. This is the fraction of all examples that are actually 
 positive that the classifier marked positive.

 For example, say there are 20 actually positive examples in a data set of 50 examples. 
 The classifier marks 10 of the 50 as positive, and of those 10, 4 are actually positive 
 (correctly classified). Precision is 4/10 = 0.4 and recall is 4/20 = 0.2 in this case.
 
 


Data Types:
----------

-Vector
	-Dense
	-sparse
	
-LabeledPoint: A labeled point is a local vector, either dense or sparse, associated with a label/response	

-MLlib supports reading training examples stored in LIBSVM format
 each line represents a labeled sparse feature vector using the following format:	
 e.g. 
 label index1:value1 index2:value2 ...
 
 
 Transformation
		 filter
		 map
		 union

 
 Action
		 count - which  returns  the  count  as  a  number
		 take -  retrieve a small number of elements in the RDD
		 collect (not to use on large data sets) - function to retrieve the entire RDD
		 saveAs...
 
*********************************************************
Hbase
-----
HBase:
	HMaster & RegionServer 
	Region

(Table, RowKey, Family, Column, Timestamp) -> Value
SortedMap<RowKey, List<SortedMap<Column, List<Value, Timestamp>>>>

The  first  SortedMap  is  the  table,  containing  a  List  of  column  families.  The  families
contain another SortedMap, which represents the columns, and their associated values.
These values are in the final List that holds the value and the timestamp it was set.

Secondary index
Predicate deleting
Comparison b/w Big table and HBase


Table 1-1. Differences in naming
HBase 				Bigtable
Region 				Tablet
RegionServer 		Tablet server
Flush 				Minor compaction
Minor compaction 	Merging compaction
Major compaction 	Major compaction
Write-ahead log 	Commit log
HDFS 				GFS
Hadoop MapReduce 	MapReduce
MemStore 			memtable
HFile 				SSTable
ZooKeeper 			Chubby

HBase UI:
		master host at port 60010 (HBase region servers use 60030 by de-
		fault). If the master is running on a host named master.foo.com on the default port, to
		see the master’s home page you can point your browser at http://master.foo.com:60010
		
		
HBase: The Hadoop Database


/conf/ - HBase conf files
---------------------------
HBase will not see these properties unless you do one of the following:
	Add a pointer to your HADOOP_CONF_DIR to the HBASE_CLASSPATH environment variable in hbase-env.sh.	


zoo.cfg Versus hbase-site.xml
		if  there  is  a  zoo.cfg  on  the  classpath
		(meaning it can be found by the Java process), it takes precedence over all settings in
		hbase-site.xml—but only those starting with the hbase.zookeeper.property prefix, plus
		a few others.

Property 					zoo.cfg + hbase-site.xml 				hbase-site.xml only
hbase.zookeeper.quorum 		Constructed from server.n lines as		Used as specified.
							specified in zoo.cfg. Overrides any
							setting in hbase-site.xml.

hbase.zookeeper.property.* 	All values from zoo.cfg override any	Used as specified.	
							value specified in hbase-site.xml.

zookeeper.* 				Only taken from hbase-site.xml. 		Only taken from hbase-site.xml.

rsync:	There  are  many  ways  to  synchronize  your  configuration  files  across
		your cluster. The easiest is to use a tool like rsync.  

hbase-site.xml and hbase-default.xml
	hbase-default.xml. Configurations that users would rarely change can exist only

	The servers always read the hbase-default.xml file first and subsequently merge it with
	the hbase-site.xml file content—if present. The properties set in hbase-site.xml always
	take precedence over the default values loaded from hbase-default.xml.

	Any  modifications  in  your  site  file  require  a  cluster  restart  for  HBase.

	-When you add Hadoop configuration files to HBase, they will always take the lowest priority.
	 An example of such an HDFS client property is dfs.replication. If, for example, you
	 want to run with a replication factor of 5, HBase will create files with the default of 3
	 unless you override them in HBase configuration file.
	 
	
hbase-env.sh
	You set HBase environment variables in this file. Examples include options to pass to
	the JVM when an HBase daemon starts. Changes here will require a cluster restart for HBase to notice the change.
	
	
regionserver

		This file lists all the known region server names. It is a flat text file that has one hostname
		per line. The list is used by the HBase maintenance script to be able to iterate over all
		the servers to start the region server process.

log4j.properties
	Changing Logging Levels on page 466 for information on this topic, and Analyzing the Logs on page 468



?HBase WAL and memStore concept
?How transaction and lock is handled in HBase

HBase Web-UI |  The HBase Master user interface
		HBase also starts a web-based user interface (UI) listing vital attributes. By default, it
		is deployed on the master host at port 60010 (HBase region servers use 60030 by default). 
		If the master is running on a host named master.foo.com on the default port, to
		see the master’s home page you can point your browser at 
		http://master.foo.com:60010

The catalog and user tables list details about the available tables. 


Here is a summary of the points we just discussed:
		Create HTable instances only once, usually when your application starts.
		Create a separate HTable instance for every thread you execute (or use HTablePool).
		Updates are atomic on a per-row basis.		
	
A row in HBase is identified by a unique row key and—as is the case with most values in HBase—this is a Java byte[] array.

hbase(main):001:0> create 'test', 'cf1'             
hbase(main):002:0> put 'test', 'row1', 'cf1', 'val1'
hbase(main):004:0> scan 'test'                      
hbase(main):005:0> scan 'test', { VERSIONS => 3 }   	

For both operations, scan and get, you only get the latest (also referred to as the newest) version, 
because HBase saves versions in time descending order and is set to return only one version by default.

write-buffer
The HBase API comes with a built-in client-side write buffer that collects put operations
	By default, the client-side buffer is not enabled. You activate the buffer by setting auto-
	flush to false, by invoking: 
		table.setAutoFlush(false)
	
	Once you have activated the buffer, you can store data into HBase as shown in Single Puts
		void flushCommits() throws IOException | The flushCommits() method ships all the modifications to the remote server(s)
	
	The API tracks how much data you are buffering by counting the required heap size of every instance
	you have added. This tracks the entire overhead of your data, also including necessary
	internal data structures. Once you go over a specific limit, the client will call the flush
	command for you implicitly.
	
	You can control the configured maximum allowed client side write buffer size with these calls:
		long getWriteBufferSize()
		void setWriteBufferSize(long writeBufferSize) throws IOException
	
	The default size is a moderate 2 MB (or 2,097,152 bytes) and assumes you are inserting
	reasonably small records into HBase.

		hbase-site.xml configuration file—for example, adding:
		<property>
		  <name>hbase.client.write.buffer</name>
		  <value>20971520</value>
		</property>
		This will increase the limit to 20 MB.
		
		
	The buffer is only ever flushed on two occasions:
		Explicit flush: Use the flushCommits() call to send the data to the servers for permanent storage.
		Implicit flush: This is triggered when you call put() or setWriteBufferSize(). Both calls compare
						the currently used buffer size with the configured limit and optionally invoke the
						flushCommits() method.
						
		In  case  the  entire  buffer  is  disabled,  setting  setAutoFlush(true) will force the client 
		to call the flush method for every invocation of put().
		Another  call  triggering  the  flush  implicitly  and  unconditionally  is  the  close()
		method of HTable.					
		
	 Result: keyvalues=NONE: Fetching intermediate data(enable buffer), 
	 	It is caused by the fact that the client write buffer is an in-memory structure that is literally holding back
		any unflushed records. Nothing was sent to the servers yet, and therefore you cannot access it.
		
	
	Write buffer content, you would find that ArrayList<Put> getWriteBuffer() can be used to get the internal  
	list of buffered  Put  instances  you  have  added  so  far.
			Exactly that list that makes HTable not safe for multithreaded use. 
	
	Also note that a bigger buffer takes more memory--on both the client and server side since the 
	server instantiates the passed write buffer to process it

	On the other hand, a larger buffer size reduces the number of RPCs made. For an estimate 
	of server-side memory-used, evaluate 
	hbase.client.write.buffer  *  hbase.regionserver.handler.count  * number of region server.
	
	If you only store large cells, the local  buffer  is  less  useful,  since  the  transfer  is  then  dominated  by
	the transfer time. In this case, you are better advised to not increase the client buffer size.
	
	Batch Operations:
		List of Puts
		void put(List<Put> puts) throws IOException
		
		Exception case: one of put has bogus data 
		Add a Put with a nonexistent family to the list.
		The call to put() fails with the following (or similar) error message:
		org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException:
		
		The  failed  ones  are returned  and  the  client  reports  the  remote  error  using  the  
		RetriesExhausted WithDetailsException.
		
		Those Put instances that have failed on the server side are kept in the local write buffer.
		They will be retried the next time the buffer is flushed. You can also access them using
		the getWriteBuffer() method of HTable and take, for example, evasive actions.

	 	you cannot control the order in which the puts are applied on the server side, which implies that the order
		in which the servers are called is also not under your control.
		
	Atomic compare-and-set
		There is a special variation of the put calls that warrants its own section: check and put. 	
		boolean checkAndPut(byte[] row, byte[] family, byte[] qualifier, byte[] value, Put put) throws IOException
		
		Such guarded operations are often used in systems that handle, for example, account
		balances, state transitions, or data processing.
		
		A special type of check can be performed using the checkAndPut() call:
		only update if another value is not already present. This is achieved by
		setting the value parameter to null.	
	
		The compare-and-set operations provided by HBase rely on checking
		and modifying the same row! As with other operations only providing
		atomicity guarantees on single rows
		
	get()
		A get() operation is bound to one specific row, but can retrieve any number of columns and/or cells contained therein.
		Get addFamily(byte[] family)
		Get addColumn(byte[] family, byte[] qualifier)
		Get setTimeRange(long minStamp, long maxStamp) throws IOException
		Get setTimeStamp(long timestamp)
		Get setMaxVersions()
		Get setMaxVersions(int maxVersions) throws IOException	
		
		By default, MaxVersion is set to 1, meaning that the get() call returns the most current match only
		
		Access  to  the  raw,  low-level  KeyValue  instances  is  provided  by  the  raw()  method,
		returning  the  array  of  KeyValue  instances  backing  the  current  Result  instance

		If the Result instance is empty, the output will be: keyvalues=NONE
		
		List of Gets
			List<Get> gets = new ArrayList<Get>();	
			Result[] results = table.get(gets);
			For any bogus row, exception is thrown and entire opertion is aborted
			
		
		Serialization in HC		
			TypeManager: 
			/**
				 * <pre>
				 * Serializes the passed dataType into a byte[]. length parameter is only used for variable length 
				 * data types (i.e. currently only string). 
				 * 1.	If for a string data type length is passed then the complete string is converted to a byte[] 
				 * 		of the size (length). The byte from the string are copied
				 *     	in the byte[] from the beginning and trailing bytes are left null (empty).
				 * 2. If for a string data type length is null or negative then the complete string is converted 
				 * 	  to a null terminated byte[] containing string. In this case the
				 *    length of byte array is (length of encoded string byte[] +1 null byte)
				 * </pre>
			 */
		
		Delete
			Single Deletes			
					Delete(byte[] row)
					Delete(byte[] row, long timestamp, RowLock rowLock)
					
					deleteFamily(): remove an entire column family
					deleteColumns():exactly one column and deletes either all versions of that cell when no 
									timestamp is given, or all matching and older versions when a timestamp is specified
					deleteColumn(): deletes either the most current or the specified version, that is, the 
									one with the matching timestamp.
									
			List of Deletes
			
			Atomic compare-and-delete
				Should the test fail, nothing is deleted and the
				call returns a false. If the check is successful, the delete is applied and true is returned.
			
			---------- WAL Memstore concept
			Implementation
			Bigtable [...] allows clients to reason about the locality properties of the data represented
			in the underlying storage.
			The data is stored in store files, called HFiles, which are persistent and ordered immut-
			able maps from keys to values. Internally, the files are sequences of blocks with a block
			index stored at the end. The index is loaded when the HFile is opened and kept in
			Building Blocks    |    23memory. The default block size is 64 KB but can be configured differently if required.
			The store files provide an API to access specific values as well as to scan ranges of values
			given a start and end key.
			Implementation is discussed in great detail in Chapter 8. The text here
			is an introduction only, while the full details are discussed in the refer-
			enced chapter(s).
			Since every HFile has a block index, lookups can be performed with a single disk seek.
			First, the block possibly containing the given key is determined by doing a binary search
			in the in-memory block index, followed by a block read from disk to find the actual key.
			The store files are typically saved in the Hadoop Distributed File System (HDFS), which
			provides a scalable, persistent, replicated storage layer for HBase. It guarantees that
			data  is  never  lost  by  writing  the  changes  across  a  configurable  number  of  physical
			servers.
			When data is updated it is first written to a commit log, called a write-ahead log (WAL)
			in HBase, and then stored in the in-memory memstore. Once the data in memory has
			exceeded a given maximum value, it is flushed as an HFile to disk. After the flush, the
			commit logs can be discarded up to the last unflushed modification. While the system
			is flushing the memstore to disk, it can continue to serve readers and writers without
			having to block them. This is achieved by rolling the memstore in memory where the
			new/empty one is taking the updates, while the old/full one is converted into a file.
			Note that the data in the memstores is already sorted by keys matching exactly what
			HFiles represent on disk, so no sorting or other special processing has to be performed.

			----------Delete concept
			Because store files are immutable, you cannot simply delete values by removing the
			key/value pair from them. Instead, a delete marker (also known as a tombstone marker)
			is written to indicate the fact that the given key has been deleted. During the retrieval
			process, these delete markers mask out the actual values and hide them from reading
			clients.
			Reading data back involves a merge of what is stored in the memstores, that is, the data
			that has not been written to disk, and the on-disk store files. Note that the WAL is
			never used during data retrieval, but solely for recovery purposes when a server has
			crashed before writing the in-memory data to disk.
			Since flushing memstores to disk causes more and more HFiles to be created, HBase
			has a housekeeping mechanism that merges the files into larger ones using compac-
			tion. There are two types of compaction: minor compactions and major compactions.
			The former reduce the number of storage files by rewriting smaller files into fewer but
			larger ones, performing an n-way merge. Since all the data is already sorted in each
			HFile, that merge is fast and bound only by disk I/O performance.
			The major compactions rewrite all files within a column family for a region into a single
			new one. They also have another distinct feature compared to the minor compactions:
			based on the fact that they scan all key/value pairs, they can drop deleted entries in-
			cluding their deletion marker. Predicate deletes are handled here as well—for example,
			removing values that have expired according to the configured time-to-live or when
			there are too many versions.
			----------

Three major components to HBase: the client library, one master server, and
			many region servers. The region servers can be added or removed while the system is
			up and running to accommodate changing workloads. The master is responsible for
			assigning regions to region servers and uses Apache ZooKeeper, a reliable, highly avail-
			able, persistent and distributed coordination service

			zookeeper uses zab protocol
			
			The  master  server  is  also  responsible  for  handling  load  balancing  of  regions  across
			region servers, to unload busy servers and move regions to less occupied ones. The
			master is not part of the actual data storage or retrieval path. It negotiates load balancing
			and maintains the state of the cluster, but never provides any data services to either the
			region servers or the clients, and is therefore lightly loaded in practice. In addition, it
			takes care of schema changes and other metadata operations, such as creation of tables
			and column families.
			
			“Region Lookups” on page 345	
			
			
	BatchOperations
			void batch(List<Row> actions, Object[] results) 
			  throws IOException, InterruptedException
			Object[] batch(List<Row> actions) 
			  throws IOException, InterruptedException

			Be aware that you should not mix a Delete and Put operation for the
			same row in one batch call. The operations will be applied in a different
			order that guarantees the best performance, but also causes unpredictable results.

			When you use the batch() functionality, the included Put instances will
			not be buffered using the client-side write buffer. The batch() calls are
			synchronous and send the operations directly to the servers; no delay
			or  other  intermediate  processing  is  used.  This  is  obviously  different
			compared to the put() calls, so choose which one you want to use care-
			fully.	
			The client-side write buffer is not used.

			Difference is that

			void batch(List<Row> actions, Object[] results) 
			  throws IOException, InterruptedException
			gives you access to the partial results, while
			[Gives access to the results of all succeeded operations, and the remote exceptions
			 for those that failed.]

			Object[] batch(List<Row> actions) 
			  throws IOException, InterruptedException

			  [Only returns the client-side exception; no access to partial results is possible.]

			does not! The latter throws the exception and nothing is returned to you since the
			control flow of the code is interrupted before the new result array is returned.
			The former function fills your given array and then throws the exception.
			
	Row Locks
		The region servers provide a row lock feature ensuring that only a client holding the matching lock can modify a row.			
		
		You  should  avoid  using  row  locks  whenever  possible.  Just  as  with
		RDBMSes,  you  can  end  up  in  a  situation  where  two  clients  create  a
		deadlock by waiting on a locked row, with the lock held by the other
		client.
		
		To reiterate: do not use row locks if you do not have to. And if you do, use them sparingly!
		
		Put(byte[] row)
		which is not providing a RowLock instance parameter, the servers will create a lock on
		your behalf, just for the duration of the call. In fact, from the client API you cannot
		even retrieve this short-lived, server-side lock instance.
		
		RowLock lockRow(byte[] row) throws IOException
		void unlockRow(RowLock rl) throws IOException
		
		While a lock on a row is held by someone "whether by the server briefly or a client
		explicitly" all other clients trying to acquire another lock on that very same row will
		stall, until either the current lock has been released, or the lease on the lock has expired.
		key  to  the  hbase-site.xml  file  and  setting  the  value  to  a  different,  millisecond-based timeout:
		The  default  timeout  on  locks  is  one  minute, 
		<property>
		  <name>hbase.regionserver.lease.period</name>
		  <value>120000</value>
		</property>
		
		Do Gets require Locks
			This is legacy and actually not used at server side
			Servers instead apply a multiversion concurrency control-style* mechanism ensuring 
			that row-level read operations, such as get() calls, never return half-written data
			for example, what is written by another thread or client
			
			Think of this like a small-scale transactional system: only after a mutation has been
			applied to the entire row can clients read the changes. While a mutation is in progress,
			all reading clients will be seeing the previous state of all columns

		UnknownRowLockException
			When you try to use an explicit row lock that you have acquired earlier but failed to
			use within the lease recovery time range, you will receive an error from the servers, in
			the  form  of  an  UnknownRowLockException.
			
		Scans
			it is time to take a look at  scans,  a  technique  akin  to  cursors†  in  database  
			systems,  which  make  use  of  the underlying sequential, sorted storage layout 
			HBase is providing.
			
			ResultScanner getScanner(Scan scan) throws IOException
			ResultScanner getScanner(byte[] family) throws IOException
			ResultScanner getScanner(byte[] family, byte[] qualifier) throws IOException	
			
			Scan()
			Scan(byte[] startRow, Filter filter)
			Scan(byte[] startRow)
			Scan(byte[] startRow, byte[] stopRow)
			
			The start row is always inclusive, while the end row is exclusive. This is
			often expressed as [startRow, stopRow) in the interval notation.
			
			The scan will match the first row key that is equal to or larger than the given start row. 
			If no start row was specified, it will start at the beginning of the table.
			
			It will also end its work when the current row key is equal to or greater than the optional
			stop row. If no stop row was specified, the scan will run to the end of the table.

			Once you have configured the Scan instance, you can call the HTable method, named
			getScanner(), to retrieve the ResultScanner instance
			
		The ResultScanner Class
			Scans do not ship all the matching rows in one RPC to the client, but instead do this
			on a row basis. This obviously makes sense as rows could be very large and sending
			thousands, and most likely more, of them in one call would use up too many resources,
			and take a long time.
			
			The ResultScanner converts the scan into a get-like operation, wrapping the Result
			instance for each row into an iterator functionality.
			
			Result next() throws IOException
			Result[] next(int nbRows) throws IOException
			void close()
			
			The next() calls return a single instance of Result representing the next available row.
			Alternatively, you can fetch a larger number of rows using the next(int nbRows) call
			
			Make sure you release a scanner instance as quickly as possible. An open scanner holds
			quite a few resources on the server side, which could accumulate to a large amount of 
			heap space being occupied.
			
			each call to next() will be a separate RPC for each row
			
			it would make sense to fetch more than one row per RPC if possible. This is called scanner caching
			and is disabled by default.
			
			You can enable it at two different levels: on the table level, to be effective for all scan
			instances, or at the scan level, only affecting the current scan.
			hbase-site.xml configuration file:
			<property>
			  <name>hbase.client.scanner.caching</name>
			  <value>10</value>
			</property>
			They work the same way as the table-wide settings, giving you control over how many
			rows are retrieved with every RPC.
			
			When the time taken to transfer the rows to the client, or to process the data on the client, exceeds 
			the configured scanner lease threshold, you will  end  up  receiving  a  lease  expired  error,  in  
			the  form  of  a ScannerTimeoutException being thrown.
			
			Consider the leasePeriod during scan as due to time taken to fetch or thread sleep, if wake up time > lease period 
			then there would be runtime exception, ScannerTimeoutException, UnknownScannerException.
				UnknownScannerException: 	It means that the next() call is using a
				-----------------------		scanner ID that has since expired and been removed in due course. In other words, the
											ID your client has memorized is now unknown to the region servers—which is the name
											of the exception.
			
			You might be tempted to add the following into your code:
					Configuration conf = HBaseConfiguration.create()
					conf.setLong(HConstants.HBASE_REGIONSERVER_LEASE_PERIOD_KEY, 120000)
			assuming this increases the lease threshold (in this example, to two minutes). But that is not going to 
			work as the value is configured on the remote  region  servers,  not  your  client  application.
			Your  value  is  not being sent to the servers, and therefore will have no effect.
			If you want to change the lease period setting you need to add the appropriate configuration key to the 
			hbase-site.xml file on the region servers while not forgetting to restart them for the changes to take effect!
			
			ScannerCaching
			Caching doesn't help in bulky data transfers (rows) as they do not fit in memory.
			Use Batching for that.
			As opposed to caching, which operates on a row level, batching works on the column level instead.
			For example, setting the scan to use setBatch(5) would return five columns per Result instance.

					When a row contains more columns than the value you used for the
					batch,  you  will  get  the  entire  row  piece  by  piece,  with  each  next
					Result returned by the scanner.
		
					The last Result may include fewer columns, when the total number of
					columns in that row is not divisible by whatever batch it is set to. For
					example, if your row has 17 columns and you set the batch to 5, you get
					four  Result  instances,  with  5,  5,  5,  and  the  remaining  two  columns
					within.			
			
					The combination of scanner caching and batch size can be used to control the number
					of  RPCs  required  to  scan  the  row  key  range  selected
			
					Refer caching/batch example on page 161 - HBase definitive guide
					
					**RPCs calculations**
					---------------------
					To compute the number of RPCs required for a scan, you need to first
					multiply the number of rows with the number of columns per row (at
					least some approximation). Then you divide that number by the smaller
					value of either the batch size or the columns per row. Finally, divide that
					number by the scanner caching value. In mathematical terms this could
					be expressed like so:
					RPCs = (Rows * Cols per Row) / Min(Cols per Row, Batch Size) / Scanner Caching
					
					In addition, RPCs are also required to open and close the scanner. You
					would need to add these two calls to get the overall total of remote calls
					when dealing with scanner
					
					For some ex, The small batch value causes the servers to group three columns into one Result, 
								 while the scanner caching of six causes one RPC to transfer six rows
								 
								 
					
			This parameter is specifying the maximum size a region within the table can grow to
			Maximum file size is actually a misnomer, as it really is about the
			maximum size of each store, that is, all the files belonging to each
			column family. If one single column family exceeds this maximum
			size, the region is split. Since in practice, this involves multiple files,
			the better name would be maxStoreSize.
			
			Read-only
			By default, all tables are writable, but it may make sense to specify the read-only
			option for specific tables. If the flag is set to true, you can only read from the table
			
			HBase uses one of two different approaches  to  save  write-ahead-log  entries  to  disk.
			You  either  use  deferred  log flushing or not. This is a boolean option and is, by default, set to false
			
			HTableDescriptor
			HColumnDescriptor
			Column::   family:qualifier
			
			Compression
			HBase has pluggable compression algorithm support (you can find more on this
			topic in  “Compression” on page  424) that allows you to choose the best com-pression—or none—
			for the data stored in a particular column family.
			
			Bloom filter
			An advanced feature available in HBase is Bloom filters,§ allowing you to improve
			lookup   times   given   you   have   a   specific   access   pattern   
			(see   “Bloom   Fil-ters” on page 377 for details). Since they add overhead in terms of storage and
			memory, they are turned off by default.
					none
					row
					rowcal
			
			Disabling the table first tells every region server to flush any uncommitted changes to
			disk, close all the regions, and update the .META. table to reflect that no region of this
			table is not deployed to any servers


		Catalog tables: .META[holds references to all user table regions] and -ROOT[holds ref to all .META regions]
		UserTables: 
		zk_dump: 
		
		B+ trees:
		B+ trees have some specific features that allow for efficient insertion, lookup, and de-
		letion of records that are identified by keys. They represent dynamic, multilevel indexes
		with lower and upper bounds as far as the number of keys in each segment (also called
		page) is concerned
		That is also the reason why you will find
		an OPTIMIZE TABLE command in most layouts based on B+ trees—it basically rewrites
		the table in-order so that range queries become ranges on disk again
		
		
		Log-Structured Merge-Trees
		Log-structured  merge-trees,  also  known  as  LSM-trees,  follow  a  different  approach.
		Incoming data is stored in a logfile first, completely sequentially. Once the log has the
		modification saved, it then updates an in-memory store that holds the most recent
		updates for fast lookup.
		
		The store files are arranged similar to B-trees, but are optimized for sequential disk
		access where all nodes are completely filled and stored as either single-page or multi-
		page blocks.

		Updating the store files is done in a rolling merge fashion, that is, the system
		packs existing on-disk multipage blocks together with the flushed in-memory data until
		the block reaches its full capacity, at which point a new one is started.
		
		Merging writes out a new block with the combined result. Eventually,
		the trees are merged into the larger blocks
		
		Lookups are done in a merging fashion in which the in-memory store is searched first,
		and then the on-disk store files are searched next.
		
		Predicate deletion:		
		An additional feature of the background processing for housekeeping is the ability to
		support predicate deletions. These are triggered by setting a time-to-live (TTL) value
		that  retires  entries,  for  example,  after  20  days.  The  merge  processes  will  check  the
		predicate and, if true, drop the record from the rewritten blocks.

		There are two different database para-digms: one is seek and the other is transfer.
		seek: used by B+ trees
		transfer: used by LSM trees
		
		B+ trees work well until there are too many modifications, because
		they force you to perform costly optimizations to retain that advantage for a limited
		amount of time. The more and faster you add data at random locations, the faster the
		pages become fragmented again. Eventually, you may take in data at a higher rate than
		the optimization process takes to rewrite the existing files. The updates and deletes are
		done at disk seek rates, rather than disk transfer rates.
		
		LSM-trees work at disk transfer rates and scale much better to handle large amounts
		of data. They also guarantee a very consistent insert rate, as they transform random
		writes into sequential writes using the logfile plus in-memory store. 
		
		****
		The general communication flow is that a new client contacts the ZooKeeper ensemble
		(a separate cluster of ZooKeeper nodes) first when trying to access a particular row. It
		does so by retrieving the server name (i.e., hostname) that hosts the -ROOT- region from
		ZooKeeper. With this information it can query that region server to get the server name
		that hosts the .META. table region containing the row key in question. Both of these
		details are cached and only looked up once. Lastly, it can query the reported .META.
		server and retrieve the server name that has the region containing the row key the client
		is looking for.
		
		The  HMaster  is  responsible  for  assigning  the  regions  to  each  HRegion
		Server  when  you  start  HBase.  This  also  includes  the  special  -ROOT-
		and .META. tables.

		HBase provides two special catalog tables, called -ROOT- and .META..*
		The -ROOT- table is used to refer to all regions in the .META. table. The design considers
		only one root region, that is, the root region is never split to guarantee a three-level, B+
		tree-like lookup scheme: the first level is a node stored in ZooKeeper that contains the
		location of the root table’s region—in other words, the name of the region server host-
		ing that specific region. The second level is the lookup of a matching meta region from
		the -ROOT- table, and the third is the retrieval of the user table region from the .META.
		table.
		
		
		HRegionServer->HRegion->Store(instance for each HColumnFamily. Store are  lightweight  
		wrappers  around  the  actual  storage  file called  HFile.) A  Store  also  has  a  MemStore, 
		and  the  HRegionServer  a  shared  HLog  instance.
		
		****
		The first step is to write the data to the write-ahead log (the WAL), represented by the HLog class.
		The WAL is a standard Hadoop SequenceFile and it stores HLogKey instances
		Once the data is written to the WAL, it is placed in the MemStore. At the same time, it
		is checked to see if the MemStore is full and, if so, a flush to disk is requested
		The request is served by a separate thread in the HRegionServer
		
		preflushing: RegionServer checks data in memstore with value configured against 
					hbase.hregion.preclose.flush.size(set to 5 MB by default).
					
					Stopping the region servers forces all memstores to be written to disk,
					no matter how full they are compared to the configured maximum size, set with the
					hbase.hregion.memstore.flush.size property (the default is 64 MB)
		during the preflush, the server and its regions are still available		
		
		HDFS defaults storage @ /hbase ..
				The first set of files are the write-ahead log files handled by the HLog instances, created
				in  a  directory  called  .logs  underneath  the  HBase  root  directory.  The  .logs  directory
				contains a subdirectory for each HRegionServer.
				****All regions from that region server share the same HLog files.

				When a logfile is are no longer needed because all of the contained edits have been
				persisted into store files, it is decommissioned into the .oldlogs directory under the root
				HBase directory.

				The old logfiles are deleted by the master after 10 minutes (by default), set with the
				hbase.master.logcleaner.ttl
				
		  The hbase.id and hbase.version files contain the unique ID of the cluster, and the file
		  format version:

		  Every table in HBase has its own directory. Each table directory contains a top-level 
		  file named .tableinfo, which stores the serialized HTableDescriptor
		  
		 HFile Format
		  The  actual  storage  files  are  implemented  by  the  HFile  class,  which  was  specifically
		  created to serve one purpose: store HBase’s data efficiently. They are based on 
		  Hadoop’s TFile class,? and mimic the SSTable format used in Google’s Bigtable architecture.
		  
		 {NAME => 'testtable', FAMILIES => [{NAME => 'colfam1', 
		  BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', 
		  COMPRESSION \=> 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', 
		  IN_MEMORY => 'false', BLOCKCACHE => 'true'}]} 
			
		  Larger block size is preferred if files are primarily for sequentialaccess.
		  Smaller blocks are good for random access	
		
		  When you are using a compression algorithm you will not have much control over block size	
		 
		  ****			 
		  HBase default block size: 64KB
		  HDFS default block size: 64MB

		  One thing you may notice is that the default block size for files in HDFS is 64 MB,
		  which is 1,024 times the HFile default block size. As such, the HBase storage file blocks
		  do not match the Hadoop blocks. In fact, there is no correlation between these two
		  block types. HBase stores its files transparently into a filesystem. The fact that HDFS
		  uses blocks is a coincidence. And HDFS also does not know what HBase stores; it only
		  sees binary files. 
		
		  WAL figure: Page 364
		  
		  The process is as follows: first the client initiates an action that modifies data. This can
		  be, for example, a call to put(), delete(), and increment(). Each of these modifications
		  is wrapped into a KeyValue object instance and sent over the wire using RPC calls. The
		  calls are (ideally) batched to the HRegionServer that serves the matching regions.
		  Once the KeyValue instances arrive, they are routed to the HRegion instances that are
		  responsible for the given rows. The data is written to the WAL, and then put into the
		  MemStore of the actual Store that holds the record. This is, in essence, the write path of
		  HBase.
		  
		  The master is responsible for monitoring the servers using ZooKeeper, and if it detects
		  a server failure, it immediately starts the process of recovering its logfiles, before reas-
		  signing the regions to new servers.
		  
		  That is where the architecture of HBase comes into play. There are no index files that
		  allow such direct access of a particular row or column. The smallest unit is a block in
		  an  HFile,  and  to  find  the  requested  data  the  RegionServer  code  and  its  underlying
		  Store instances must load a block that could potentially have that data stored and scan
		  through it. And that is exactly what a Scan does anyway.
		  
		HBase provides two special catalog tables, called -ROOT- and .META..*
		The -ROOT- table is used to refer to all regions in the .META. table. The design considers
		only one root region, that is, the root region is never split to guarantee a three-level, B+
		tree-like lookup scheme: the first level is a node stored in ZooKeeper that contains the
		location of the root table’s region—in other words, the name of the region server host-
		ing that specific region. The second level is the lookup of a matching meta region from
		the -ROOT- table, and the third is the retrieval of the user table region from the .META.
		table.			  
		  
		 Lookup-1
		 	a read of the zooKeeper node to find the root table region.
		 Lookup-2
		 	From -ROOT-, meta table region 
		 Lookup-3
		 	From .META., user table region is known
		
		
		Zookeeper:
		HBase creates a list of znodes under its root node. The default is /hbase and is configured
		with the zookeeper.znode.parent property. Here is the list of the contained znodes and
		their purposes.
		


-RokKey & schema design
	-column key: The columns are the typical HBase combination of a column family name and a 
	 column qualifier, forming the column key.
	-The cells are sorted in descending order of the timestamp so that the latest cell is always first
	-The KeyValues are sorted by row key first, and then by column key 
	-The timestamp or version of a cell is farther to the right, it is another important selection criterion
	-The store files retain the timestamp range for all stored cells,
	 so if you are asking for a cell that was changed in the past two hours, but a particular
	 store file only has data that is four or more hours old it can be skipped completely.
	-How you should store your data. The two choices are tall-narrow and flat-wide. 
		-TallNarrow: Less columns and many rows
		-FlatWide: More columns and less rows
			-keyValue: rowKey->columnFamilyName->columnQualifier->timestamp->Value
			-email key example with userId & messageId
				-FlatWide example
				 <userId> : <colfam> : <messageId> : <timestamp> : <email-message>
				 12345 : data : 5fc38314-e290-ae5da5fc375d : 1307097848 : "Hi Lars, ..."
				-TallNarrow example
				 <userId>-<messageId> 			  :	<colfam> : <qualifier> : <timestamp> : <email-message>
				 12345-5fc38314-e290-ae5da5fc375d : data 	 : 			   : 1307097848  : "Hi Lars, ..."
				
				This results in a table that is easily splittable, with the additional benefit of having 
				a more fine-grained query granularity.	 
	
	-partial key scans - Key design
		-You can specify a start and end key that is set to the exact key
			-The start key of a scan is inclusive, while the stop key is exclusive
		-Consider  the  following  row  key structure:
		 <userId>-<date>-<messageId>-<attachmentId>
		-Make sure that you pad the value of each field in the composite row key so that the 
		 lexicographical (binary, and ascending) sorting works as expected.
		-Command Description
				<userId> Scan over all messages for a given user ID.
				<userId>-<date> Scan over all messages on a given date for the given user ID.
				<userId>-<date>-<messageId> Scan over all parts of a message for a given user ID and date.
				<userId>-<date>-<messageId>-<attachmentId>   Scan over all attachments of a message for a given 
				                                             user ID and date. 
		-One major drawback to composite key is atomicity. Since the data is now spanning many rows for a
		 single inbox, it is not possible to modify it in one operation.
		-Using  the  composite  row  key  with  the  user  ID  and  date  gives  you  a  natural  order,
		 displaying the newest messages first, sorting them in descending order by date.
		 
	-Time series data key
		-Its salient feature is that its row key represents the event time. This imposes a problem 
		 with the way HBase is arranging its rows
		-The sequential, monotonously increasing nature of time series data causes all incoming
		 data to be written to the same region.
			-To overcome this problem by ensuring that data is spread over all region servers
			 instead. This can be done, for example, by prefixing the row key with a nonsequential prefix
			-Salting
				-You can use a salting prefix to the key that guarantees a spread of all rows across
				 all region servers.
				-byte prefix = (byte) (Long.hashCode(timestamp) % <number of region servers>);
				 byte[] rowkey = Bytes.add(Bytes.toBytes(prefix), Bytes.toBytes(timestamp);
			-
	-Randomization
		-When your data is not scanned in ranges but accessed randomly, you can use this strategy.	
		-Random keys avoid region hot-spots
		
	-SecondaryIndexes
		-offer sorting by different fields so that the user can switch at will
		-		 
		 
	-??ReadPath		
Questions:
----------
bloomFilters??

??The Region Life Cycle:  See “The Region Life Cycle” on page 348 for details.
HBaseMaster:  ports: HBaseMaster @60000 | <ip>:60000
RegionServer: http://<region-server-address>:60030


??versioning and timestamps
??regionHotspots:
??HBase node decomissioning:
-Node decommision
	-To avoid any problems b/w HBase load balancer and Master Node,by disabling the balancer first.
		-hbase(main):001:0> balance_switch false => disable loadbalancer
		-hbase(main):001:0> balance_switch true => enable loadbalancer
	
	-HBase directory on the particular server run
	$ ./bin/hbase-daemon.sh stop regionserver
	-First close all regions and then shut itself down
	-Its ephemeral(lasting for short time) node in the zookeeper will expire

-GraceFul decommision
	- want to decommission a loaded region server, run the following:
	$ ./bin/graceful_stop.sh HOSTNAME
		hostname is the host that you want to decommision
	
	-The graceful_stop.sh script will move the regions off the decommissioned region server
	 one at a time to minimize region churn. It will verify the region deployed in the new
	 location before it moves the next region, and so on, until the decommissioned server
	 is carrying no more regions.

	- The master will notice the region server gone but all regions will have already been 
	 redeployed, and because the region server went down cleanly, there will be no WALs to split.

-Rolling restarts
	A primitive rolling restart might be effected by running something like the following:
	$ for i in `cat conf/regionservers|sort`; do ./bin/graceful_stop.sh --restart --reload --debug $i; done &> /tmp/log.txt &

-Master update:

		****Must do the rolling start of the servers

		-Unpack the release 
		-Run hbck to ensure the cluster is consistent:
			$ ./bin/hbase hbck
			Effect repairs if inconsistent.	 

		-Restart the master:
			$ ./bin/hbase-daemon.sh stop master; ./bin/hbase-daemon.sh start master

		-Disable the region balancer:
			$ echo "balance_switch false" | ./bin/hbase shell

		-Run the graceful_stop.sh script per region server. For example:
			$ for i in `cat conf/regionservers|sort`; do ./bin/graceful_stop.sh --restart --reload --debug $i; done &> /tmp/log.txt &	

			If you are running Thrift or REST servers on the region server, pass the --thrift
			or --rest option, 

		-Restart Master again
			$ ./bin/hbase-daemon.sh stop master; ./bin/hbase-daemon.sh start master

		- Run hbck to ensure the cluster is consistent.
		
-Addition of new servers - Local 
	-Local Master [60000 for RPC and 60010 for the web-based UI]
	
			HBase offers is built-in scalability. As the load on your cluster 
			increases, you need to be able to add new servers to compensate for the new requirements

			-Addition of local Master server:
			 $ ./bin/local-master-backup.sh start 1

			The number at the end of the command signifies an offset that is added to the default 
			ports of 60000 for RPC and 60010 for the web-based UI. In this example, a new master
			process would be started that reads the same configuration files as usual, but would
			listen on ports 60001 and 60011, respectively

			-Starting more than one is also possible:
			 $./bin/local-master-backup.sh start 1 3 5

			-Logs for offset
			 logs/hbase-${USER}-1-master-${HOSTNAME}.log

			-stop backup master:
			 $ ./bin/local-master-backup.sh stop 1
	
	-Local region server [60200 for RPC, and 60300 for the web UIs] 
		-$ ./bin/local-regionservers.sh start 1

	
	**You do not have to start with an offset of 1. Since these are added to the base port numbers, 
	  you are free to specify any offset you prefer.
	  
-Addition of new servers - Cluster [Fully distributed cluster]	  
	
	-Master
	The master process uses ZooKeeper to negotiate which is the currently active master:
	there is a dedicated ZooKeeper znode that all master processes race to create, and the
	first one to create it wins.
	The /hbase/master znode is ephemeral i.e short lived
	
	When the master process that created the znode fails, ZooKeeper
	will notice the end of the session with that server and remove the znode accordingly,
	triggering the election process

	Starting a server on multiple machines requires that it is configured just like the rest of the  HBase  cluster
	
	$ ./bin/hbase-daemon.sh start master
	
	$ ./bin/hbase-daemon.sh start master --backup
	
	Find the active master server:  
	http://hostname:60010 URL on all possible master servers to find the active one
	
	The start-hbase.sh script starts the primary master, the region servers, and eventually the backup masters. 
	Alternatively,  you  can  invoke  the  hbase-backup.sh  script  to  initiate  the  start  of  the backup masters.
	
		
	-Adding RegionServer
		-The  first  thing  you  should  do  is  to  edit  the  regionservers file in the conf directory
		-Once you have updated the file, you need to copy it across all machines in the cluster.
		 You also need to ensure that the newly added machine has HBase installed, and that
		 the configuration is current.
		-One option is to run the start-hbase.sh script on the master machine. It will skip all machines that have
		 a process already running. Since the new machine fails this check, it will appropriately
		 start the region server daemon.
		-Another option is the launcher script directly on the new server
			-$ ./bin/hbase-daemon.sh start regionserver
		-The region server process will start and register itself by creating a znode with its hostname
		 in ZooKeeper. It subsequently joins the collective and is assigned regions.
		

-HBase scripts:
	-./bin/hbase-daemon.sh stop master; ./bin/hbase-daemon.sh start master
	-./bin/hbase hbck
	-./bin/start-hbase.sh 
	
	Local server addition
	-./bin/local-master-backup.sh start 1   => also stop
	-$ ./bin/local-regionservers.sh start 1	=> also stop


-Shifting of data including region
	-Import and Export Tools:  Import and Export MapReduce jobs
		-$ hadoop jar $HBASE_HOME/hbase-0.91.0-SNAPSHOT.jar
			-export: 
				$ hadoop jar $HBASE_HOME/hbase-0.91.0-SNAPSHOT.jar export testtable /user/larsgeorge/backup-testtable
																		  <tableName><HDFS location directory>
			-distcp: hadoop distcp command to move the directory from one cluster to another, and perform the import there. 													  
			-import: 
				hadoop jar $HBASE_HOME/hbase-0.91.0-SNAPSHOT.jar import testtable /user/larsgeorge/backup-testtable	 
			
			****Finally, this Export/Import combination is per-table only. If you have more than one table, 
			you need to run them separately.

		-copyTable
			hbase org.apache.hadoop.hbase.mapreduce.CopyTable --new.name='ravigu:sdp' 'aparsh:sdp'
	
	
-Replication
	-Edit hbase-site.xml in conf dir
	-property: set hbase.replication to true
	-Do not forget to copy the changed configuration file to all machines in your cluster, and to 
	 restart the servers.
	-OR create a new one with the replication scope set to 1
		hbase(main):005:0> create 'testtable2', { NAME => 'colfam1', REPLICATION_SCOPE => 1}
	-OR alter an existing table, you need to disable it before you can do
		
		hbase(main):001:0> create 'testtable1', 'colfam1'
		hbase(main):002:0> disable 'testtable1'
		hbase(main):003:0> alter 'testtable1', NAME => 'colfam1', \ 
		  REPLICATION_SCOPE => '1'
		hbase(main):004:0> enable 'testtable1' 
	
	-A slave—here also called a peer
	 hbase(main):006:0> add_peer '1', 'slave-zk1:2181:/hbase'
	 hbase(main):007:0> start_replication
	 
			 [a slave—here also called a peer]	 
			 The first command adds the ZooKeeper quorum details for the peer cluster so that
			 modifications can be shipped to it subsequently. The second command starts the actual
			 shipping of modification records to the peer cluster. For this to work as expected, you
			 need to make sure that you have already created an identical copy of the table on the
			 peer cluster

	hbase(main):008:0> stop_replication
	hbase(main):009:0> remove_peer '1'
	
		Removing a peer and stopping the translation is equally done, using the reverse commands:
		
		
	-HBase does include a balancer. Note however that it balances based on
		number of regions, not their size or traffic. So it is still possible that
		a regionserver ends up with more larger/busier regions than other
		regionservers. If you notice this behaviour, you can always manually
		reassign a region using the HBase Shell.

		http://hbase.apache.org/book/node.management.html#lb	
	
-Default ports used by the HBase daemons
Node type 		Port 		Description
Master 			60000 		The RPC port the master listens on for client requests. Can be configured with the hbase.master.port configuration property.
Master 			60010 		The web-based UI port the master process listens on. Can be configured with the hbase.master.info.port configuration property.
Region server 	60020 		The RPC port the region server listens on for client requests. Can be configured with the hbase.regionserver.port configuration property.
Region server 	60030 		The web-based UI port the region server listens on. Can be configured with the hbase.regionserver.info.port configuration property.	


-Analyzing the HBase Logs on page 468:
	-editing the log4j.properties file in the conf directory
		-log4j.logger.org.apache.hadoop.hbase=INFO
	-various default HBase, ZooKeeper, and Hadoop logfiles
		Server type 				Logfile
		HBase Master 				$HBASE_HOME/logs/hbase-<user>-master-<hostname>.log
		HBase RegionServer 			$HBASE_HOME/logs/hbase-<user>-regionserver-<hostname>.log
		ZooKeeper 					Console log output only
		NameNode 					$HADOOP_HOME/logs/hadoop-<user>-namenode-<hostname>.log
		DataNode 					$HADOOP_HOME/logs/hadoop-<user>-datanode-<hostname>.log
		JobTracker 					$HADOOP_HOME/logs/hadoop-<user>-jobtracker-<hostname>.log
		TaskTracker 				$HADOOP_HOME/logs/hadoop-<user>-jobtracker-<hostname>.log
	
-TroubleShooting
	-File handles: The ulimit -n for the DataNode processes and the HBase processes should be set high. eg 32000
	
	
		

??Handle shifting server loads. Balance load to newly added node
-http://hbase.apache.org/book.html#node.management
??Garbage Collection Tuning on page 419.
??secondary Indexes
??CoProcessors
??split policy
??Hot deployment / addition of servers in HBase
??HBase load balancing ?
	1)	The Hadoop (HDFS) balancer moves blocks around from one node to another to try to make it so each 
		datanode has the same amount of data (within a configurable threshold). This messes up HBases's 
		data locality, meaning that a particular region may be serving a file that is no longer on 
		it's local host.
		
	2)	HBase's balance_switch balances the cluster so that each regionserver hosts the same number of 
		regions (or close to). This is separate from Hadoop's (HDFS) balancer.
		
		To disable balancer: 
		hbase(main):001:0> balance_switch false
		
		To enable balancer:
		hbase(main):001:0> balance_switch true		
		
		****Contradicting**** 
		The balancer_switch only affects regionserver balance. HBase will automatically balance your 
		regions in the cluster by default, but you can manually run the balancer at any time from the 
		hbase shell.
		
		
		HBase does include a balancer. Note however that it balances based on
		number of regions, not their size or traffic. So it is still possible that
		a regionserver ends up with more larger/busier regions than other
		regionservers. If you notice this behaviour, you can always manually
		reassign a region using the HBase Shell.
		
		http://hbase.apache.org/book/node.management.html#lb
		
	3)	If you are running only HBase, I recommend not running Hadoop's (HDFS) balancer as it will cause 
		certain regions to lose their data locality. This causes any request to that region to have to 
		go over the network to one of the datanodes that is serving it's HFile.
		
		HBase's data locality is recovered though. Whenever compaction occurs, all the blocks are copied 
		locally to the regionserver serving that region and merged. 
		
		All you really need to do to add new nodes to the 
		cluster is add them. Hbase will take care of rebalancing the regions, and once these regions 
		compact data locality will be restored.

??What is thrift in HBase: 
Thrift or REST servers on the region server

??Why HBase ?
??master  failover  scenarios
??regions  being moved from one server to another
??HBase node addition?
??Effective row key design


??Why parquet / seq 
??Faster dataFrame Vs RDD
??HBase key design
??HBase split policy
??HBase logs?

??Spark 2.0 feature
??What diff b/w dataSet & dataFrame
	-A Dataset is a strongly-typed, immutable collection of objects that are mapped to a relational schema.
	-At the core of the Dataset API is a new concept called an encoder, which is responsible for converting 
	 between JVM objects and tabular representation.
	-Spark’s internal Tungsten binary format
	-
??Spark 2.0 new features
??map() Vs flatMap()
??CheckPointing
??groupBy Vs groupByKey


*********************************************************
Hadoop
------
Node nomenculature in bigData		

Hdfs:
	NameNode (SecondaryNameNode) & dataNode
HBase:
	HMaster & RegionServer 
	Region

Spark:

Yarn:
		Two  types  of  long-running  daemon:  
		a  resource manager (one per cluster) to manage the use of resources across the cluster, and 
		node managers running on all the nodes in the cluster to launch and monitor containers.

		MapReduce 1 Vs MapReduce 2:
				In MapReduce 1, there are two types of daemon that control the job execution process:
				-a jobtracker and 
				-one or more tasktrackers

				YARN these responsibilities are handled by separate entities: 
				-the resource manager and 
				-an application master (one for each MapReduce job).
				
				In YARN, the equivalent role is the timeline server, which stores application history.
	
ZooKeeper:


? how to choose the number of map tasks for a given job. 
	
? how to choose the number of reduce tasks for a given job.
	The number of mappers launched is roughly equal to the input size divided by dfs.block.size (the default block 
	 size is 64 MB)
? How much memory does NameNode need

? Anatomy of a MapReduce Job Run(on page 185), with security perspective.

? Anatomy of a spark Job

? Anatomy of Oozie workFlow

? Compression and Serialization stuff(parquet & protocolBuffers)

? Spark on YARN(on page 571)

? Distributed Cache on page 274

? Why Not Use Java Object Serialization? 
		(pg 126/154 Definitive guide)
		Java comes with its own serialization mechanism, called Java Object Serialization (often
		referred to simply as “Java Serialization”), that is tightly integrated with the language, so
		it’s natural to ask why this wasn’t used in Hadoop. Here’s what Doug Cutting said in
		response to that question:
		Why didn’t I use Serialization when we first started Hadoop? Because it looked big and
		hairy and I thought we needed something lean and mean, where we had precise control
		over exactly how objects are written and read, since that is central to Hadoop. With
		Serialization you can get some control, but you have to fight for it.
		The logic for not using RMI [Remote Method Invocation] was similar. Effective, high-
		performance inter-process communications are critical to Hadoop. I felt like we’d need
		to precisely control how things like connections, timeouts and buffers are handled, and
		RMI gives you little control over those.
		The problem is that Java Serialization doesn’t meet the criteria for a serialization format
		listed earlier: compact, fast, extensible, and interoperable.

?

serialize -> store in file based structures like sequence file

--------------------------------------------------------------------------------------------------------------
Project consist of 
-MapReduce(It is this that spark is intending to replace)
	
	DataFlow in map reduce
	(Map)
	-Hadoop runs the job by dividing it into tasks, of which there are two types:
	  map tasks and reduce tasks. The tasks are scheduled using YARN and run on nodes in
	  the cluster
	
	-Hadoop creates one map task for each split, which runs the user-defined
	 map function for each record in the split.
	
	-A good split size tends to be the size of an HDFS block, which is 128(??or 64) MB by default	
		-optimal split size is the same as the block size: it is the
	 largest size of input that can be guaranteed to be stored on a single node. 
	
	-Data locality optimization: Hadoop does its best to run the map task on a node where the input data resides in
	 HDFS, because it doesn’t use valuable cluster bandwidth.
	
	-****(imp)Map tasks write their output to the local disk, not to HDFS. Why is this? 
		-storing it in HDFS with replication would be overkill
	
	-If the node running the map task fails before the map
	 output has been consumed by the reduce task, then Hadoop will automatically rerun
     the map task on another node to re-create the map output.	
    
    -Three kinds of map job execution possible
    	-Data-local (a), rack-local (b), and off-rack (c) map tasks 

	(Reduce)
    -Reduce tasks don’t have the advantage of data locality; the input to a single reduce task
	 is normally the output from all mappers.	
		-map outputs have to be transferred across the network to the node where the reduce task is running
	
	-The output of the reduce is normally stored in HDFS for reliability. 	 
	
	-Each HDFS block of the reduce output, the first replica is stored on the local node,
	
	-When there are multiple reducers, the map tasks partition their output, each creating
	 one partition for each reduce task. There can be many keys (and their associated values)
	 in each partition, but the records for any given key are all in a single partition.
	
	-Map -> combine -> reduce
	
	(Combiner)
	-The data flow between map and reduce tasks is collo-quially known as the shuffle, 
	 as each reduce task is fed by many map tasks
		-Refer diagram pg 62 - Hadoop definitive guide
	
	-Hadoop allows the user to specify a combiner function to be run on the map output, and the combiner function’s
	 output forms the input to the reduce function.
	
	-Hadoop does not provide a guarantee of how many times it will call it for
		a particular map output record, if at all. In other words, calling the combiner function
		zero, one, or many times should produce the same output from the reducer.
	
	-The combiner function doesn’t replace the reduce function. 
		
	-But it can help cut down the amount of data shuffled between the mappers and the reducers,
	 and for this reason alone it is always worth considering whether you can use a combiner
	 function in your MapReduce job
		
	Job job = new Job();
    job.setJarByClass(MaxTemperatureWithCombiner.class);
    job.setJobName("Max temperature");
    FileInputFormat.addInputPath(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    
    job.setMapperClass(MaxTemperatureMapper.class);
    job.setCombinerClass(MaxTemperatureReducer.class);
    job.setReducerClass(MaxTemperatureReducer.class);
    job.setOutputKeyClass(Text.class);	
	
	
-Hadoop Streaming
	-
	-Hadoop Streaming uses Unix standard streams
		as the interface between Hadoop and your program, so you can use any language that
		can  read  standard  input  and  write  to  standard  output  to  write  your  MapReduce
		program. 
		
		
-------------------------------------------------------------------------------------------------------------
-Hadoop Cluster
	-securtiy
		
		1.Authentication.  The  client  authenticates  itself  to  the  Authentication  Server  and
		 receives a timestamped Ticket-Granting Ticket (TGT).
		
		2.  Authorization. The client uses the TGT to request a service ticket from the Ticket-
		Granting Server.
		
		3.  Service request. The client uses the service ticket to authenticate itself to the server
		that is providing the service the client is using. In the case of Hadoop, this might
		be the namenode or the resource manager.
		
		Together, the Authentication Server and the Ticket Granting Server form the Key Dis-
		tribution Center (KDC). 
		
			To use Kerberos authentication with Hadoop, you need to install, configure, and run a KDC (Hadoop does
			not come with one). Your organization may already have a KDC you can use (an Active Directory installation,
			for example); if not, you can set up an MIT Kerberos 5 KDC.
		
		
		-The first step is to enable Kerberos authentication  by  setting  the  
		hadoop.security.authentication  property  in  core-site.xml  to  kerberos.   
		(default  setting  is  simple)

		-We also need to enable service-level authorization by setting  hadoop.security.authorization 
		to true in the same file.
		
		-You may configure access control lists (ACLs) in the
		 hadoop-policy.xml configuration file to control which users and groups have permission
		 to connect to each Hadoop service. 
		
		-The format for an ACL is a comma-separated list of usernames, followed by whitespace,
		 followed   by   a   comma-separated   list   of   group   names
		
		-use the  klist command to see the expiry time of your tickets and kdestroy to invalidate your tickets
		
	-Delegation token
		A delegation token is generated by the server (the namenode, in this case) and can be
		thought of as a shared secret between the client and the server. On the first RPC call to
		the namenode, the client has no delegation token, so it uses Kerberos to authenticate.
		As a part of the response, it gets a delegation token from the namenode. In subsequent
		calls it presents the delegation token, which the namenode can verify (since it generated
		it using a secret key), and hence the client is authenticated to the server.
		When the job has finished, the delegation tokens are invalidated.
	
	-Block access token
		-The client uses the block access token to authenticate
		 itself to datanodes. This is possible only because the namenode shares its secret key used
		 to generate the block access token with datanodes
		-dfs.block.access.token.enable to  true
		
	-The more notable features:	
		-Tasks can be run using the operating system account for the user who submitted
		 the job, rather than the user running the node manager.
		-When tasks are run as the user who submitted the job, the distributed cache (see
		 Distributed Cache” on page 274) is secure. Files that are world-readable are put in
		 a shared cache (the insecure default); otherwise, they go in a private cache, readable
		 only by the owner.
		- The shuffle is secure, preventing a malicious user from requesting another user’s
		 map outputs.
		-
-----------------------------------------------------------------------------------------------------------------------

-Hadoop I/O
	-OLAP is an acronym for Online Analytical Processing. OLAP performs multidimensional analysis of business 
	 data and provides the capability for complex calculations, trend analysis, and sophisticated data modeling.
	 
	-Some common storage formats for Hadoop include: blog: http://blog.matthewrathbone.com/2016/09/01/a-beginners-guide-to-hadoop-storage-formats.html

		Plain text storage (eg, CSV, TSV files)
		Sequence Files
		Avro
		Parquet

	-Choosing an appropriate file format can have some significant benefits:
	
			Faster read times
			Faster write times
			Splittable files (so you don’t need to read the whole file, just a part of it)
			Schema evolution support (allowing you to change the fields in a dataset)
			Advanced compression support (compress the files with a compression codec without sacrificing these features)

	-Avro is not really a file format, it’s a file format plus a serialization and deserialization framework. 
	 With regular old sequence files you can store complex objects but you have to manage the process. 
	 Avro handles this complexity whilst providing other tools to help manage data over time.
	
	-The latest hotness in file formats for Hadoop is columnar file storage.
	 One huge benefit of columnar oriented file formats is that data in the same column tends 
	 to be compressed together which can yield some massive storage optimizations (as data in the same column tends to be similar).

	- compress data in Hadoop.

		File-Level Compression
		Block-Level Compression 
		
		
	-Parquet three concepts: specifying data at bit level, encoding

		-Storage formats, which are binary representations of data. For Parquet this is contained within the 
		 parquet-format GitHub project.
		-Object model converters, whose job it is to map between an external object model and Parquet’s 
		 internal data types. These converters exist in the parquet-mr GitHub project.
		-Object models, which are in-memory representations of data. Avro, Thrift, Protocol Buffers, Hive and Pig 
		 are all examples of object models. Parquet does actually supply an example object model (with MapReduce support)
		 , but the intention is that you’d use one of the other richer object models such as Avro.	
		-Interoperable
		-scan all columnar data in one seek and save a lot of I/O 
		-Storing homogenous data ie. same type data. All columnar data of same type together i.e all ints together 
		-Encoding bunch of values in on chunk
		-Borrowed from Google Dermel paper. 
		-File format to store nested dataStructure
		-Also helpful in CPU bus architectute as less pageFault all column data is available in cache
		
	- Avro, thrift, ProtocolBuffers and Pojos are dataModels
	
	-Parquet is a column-oriented data storage format for Hadoop from Twitter. Column-oriented storage is really nice 
	 for “wide” data, since you can efficiently read just the fields you need.


	-Parquet
		Parquet is built to support very efficient compression and encoding schemes
	
	-parquet Vs protocolBuffers
		-Parquet is a column-oriented data storage format for Hadoop from Twitter. Column-oriented storage is really 
		 nice for “wide” data, since you can efficiently read just the fields you need.
	
		-Protobuf is a data serialization library developed by google. It lets you efficiently and quickly serialize 
		 and deserialize data for transport.
		
		-Avro, Protocol Buffers, and Thrift
			Most applications will prefer to define models using a framework like Avro, Protocol
			Buffers, or Thrift, and Parquet caters to all of these cases. Instead of  ParquetWriter and
			ParquetReader,  use  AvroParquetWriter,  ProtoParquetWriter,  or  ThriftParquet
			Writer, and the respective reader classes.
		
		-Projection schema
	
	-Serialization vs encoding ?? Need more data 
		-Encoding is a process of representation some information is optimal format for storing or transmission data. 
		 And serialization is representation some object or objects for transmission data between some computer programs. 
		 So the difference is:
	
			a. Encoding deals for some data (text, images, videos) but serialization deals with 
			   some objects (data structure) in computer program
			b. As a rule encoding means compression and encryption some data, 
			   serialization doesnot use compression and encryption
	
	-checkSum: error detection during dataTransfer
		-Hadoop uses CRC32
		-HDFS uses a more efficient variant called CRC-32C.
		-Datanodes are responsible for verifying the data they receive before storing the data and its  checksum.
		-A client writing data sends it to a pipeline of datanodes
			(as explained in Chapter 3), and the last datanode in the pipeline verifies the checksum.
			If the datanode detects an error, the client receives a subclass of  IOException
		-Datanode block scanner on page 328:  each datanode runs a  DataBlockScan
			ner in a background thread that periodically verifies all the blocks
		-client detects an error when reading a block, it reports the bad block and the datanode
			it was trying to read from to the namenode before throwing a  ChecksumException. The
			namenode marks the block replica as corrupt so it doesn’t direct any more clients to it
			or try to copy this replica to another datanode
		-You  can  find  a  file’s  checksum  with  hadoop fs -checksum. Check 2 files have same content with their checkSum
		-To disable checkSum:  This is accomplished by using RawLocalFileSystem in place of
		 LocalFileSystem.
	
	-Compression
		
		-A summary of compression formats
				Compression format 	Tool 	Algorithm 	Filename extension    Splittable?
				DEFLATE(a) 			N/A 	DEFLATE 	.deflate 				No
				gzip 				gzip 	DEFLATE 	.gz 					No
				bzip2 				bzip2 	bzip2 		.bz2 					Yes
				LZO 				lzop 	LZO 		.lzo 					No(b)
				LZ4 				N/A 	LZ4 		.lz4 					No
				Snappy 				N/A 	Snappy 		.snappy 				No
		
		-Codecs	
			-A codec is the implementation of a compression-decompression algorithm
			-Hadoop compression codecs
				Compression format 		Hadoop CompressionCodec
				DEFLATE 				org.apache.hadoop.io.compress.DefaultCodec
				gzip 					org.apache.hadoop.io.compress.GzipCodec
				bzip2 					org.apache.hadoop.io.compress.BZip2Codec
				LZO 					com.hadoop.compression.lzo.LzopCodec
				LZ4 					org.apache.hadoop.io.compress.Lz4Codec
				Snappy 					org.apache.hadoop.io.compress.SnappyCodec
				
		-Native libraries
			-For  performance,  it  is  preferable  to  use  a  native  library  for  compression  and
			 decompression. For example, in one test, using the native gzip libraries reduced de-
			 compression times by up to 50% and compression times by around 10% 		
		
		-Compression and Input Splits
			-Use a container file format such as sequence files (see the section on page 127), Avro
			 datafiles (see the section on page 352), ORCFiles (see the section on page 136),
			 or Parquet files (see the section on page 370), all of which support both compression
			 and splitting. A fast compressor such as LZO, LZ4, or Snappy is generally a good
			 choice.
			 
			-For large files, you should not use a compression format that does not support splitting
			 on  the  whole  file,  because  you  lose  locality  and  make  MapReduce  applications  very
			 inefficient.
			 
		-Serialization
			-Serialization is the process of turning structured objects into a byte stream for trans-
			 mission over a network or for writing to persistent storage. Deserialization is the reverse
			 process of turning a byte stream back into a series of structured objects.
			
			-Serialization  is  used  in  two  quite  distinct  areas  of  distributed  data  processing:  for
			 interprocess communication and for persistent storage
			 
			-In Hadoop, interprocess communication between nodes in the system is implemented
			 using remote procedure calls (RPCs). The RPC protocol uses serialization to render the
			 message into a binary stream to be sent to the remote node, 
			 
			-In general, it is desirable that an RPC serialization format is:
				-Compact
				-fast
				-extensible
				-Interoperable: that is why Avro/protocolBuffers 
					For some systems, it is desirable to be able to support clients that are written in
					different languages to the server, so the format needs to be designed to make this
					possible.
			-Hadoop uses its own serialization format, Writables	
			-Chapter 12 for more details
			
		-Writables
			-Java primitive     Writable implementation     Serialized size (bytes)
				boolean    		BooleanWritable    			1
				byte       		ByteWritable       			1
				short      		ShortWritable      			2
				int        		IntWritable        			4
			  					VIntWritable       			1-5
				float      		FloatWritable      			4
				long       		LongWritable       			8
			  					VLongWritable      			1-9
				double     		DoubleWritable     			8
	
		-File-Based Data Structures
			-SequenceFile: providing a persistent data structure for binary key-value pairs.
				-Choose  a  key,  such  as  timestamp  represented  by  a LongWritable, and the value would be a 
				 Writable that represents the quantity being logged.
				-The keys and values stored in a  SequenceFile do not necessarily need to be  Writables.
				 Any types that can be serialized and deserialized by a  Serialization may be used.
				-SequenceFile.Writer, you then write key-value pairs using the append() method.
				-Reading sequence files from beginning to end is a matter of creating an instance of
				 SequenceFile.Reader  and  iterating  over  records  by  repeatedly  invoking  one  of  the
				 next() methods.
				- 
				-SequenceFiles also work well as containers for smaller files.
				-??Processing a whole file as a record on page 228 contains a program to pack files into a  SequenceFile
				-Packing  files  into  a  SequenceFile  makes  storing and processing the smaller files more efficient
				-??io.serializations property 
				-??sync points
					 A sync point is a point in the stream that can be used to resynchronize
					with a record boundary if the reader is “lost”—for example, after seeking to an arbitrary
					position in the stream. Sync points are recorded by SequenceFile.Writer, which in-
					serts a special entry to mark the sync point every few records as a sequence file is being
					written. Such entries are small enough to incur only a modest storage overhead—less
					than 1%. Sync points always align with record boundaries.
					
				-There are two ways to seek to a given position in a sequence file. 
					-The first is the  seek()
						-reader.seek(359); //OK
						
						-Position= // beginning of next record
						 But if the position in the file is not at a record boundary, the reader fails when the  next() method is called:
					     reader.seek(360);
    					 reader.next(key, value); // fails with IOException
					-The second way to find a record boundary makes use of sync points.
					-we can call  sync() with any position in the stream—not necessarily a record boundary—and the reader will reestablish itself at
					 the next sync point so reading can continue:
					-SequenceFile.Writer   has  a  method  called  sync()   for  inserting  a sync point at the current position in the stream.
					-The  hadoop fs command has a  -text option to display sequence files in textual form.
						- hadoop fs -text numbers.seq | head   -----[hdfs dfs -text numbers.seq | head]
				
				-The SequenceFile format
					-A sequence file consists of a header followed by one or more records
					-
-----------------------------------------------------------------------------------------------------------------------------------------------------
	Protocol Buffers
	----------------
	
	-Protobuf is a data serialization library developed by google. It lets you efficiently and quickly serialize 
	 and deserialize data for transport.	 
	 
	
	-Protocol Buffers: https://developers.google.com/protocol-buffers/docs/overview
	
		-a language-neutral, platform-neutral, extensible way of serializing structured data for use in 
		 communications protocols, data storage etc. Think XML, but smaller, faster, and simpler.
		
		-You define how you want your data to be structured once, then you can use special generated 
		 source code to easily write and read your structured data to and from a variety of data 
		 streams and using a variety of languages
		
		-Each protocol buffer message is a small logical record of information, containing a series of name-value pairs
		
		-You should always use lowercase-with-underscores for field names in your .proto files; this ensures 
		 good naming practice in all the generated languages
		
		
		-E.g. 
		 D:\..\em-bt-schema>.\compiler\protoc.exe --proto_path=src --java_out=src src\schema\proto\DeviceEventData.proto
				
				1)
				 message Person {
				   required string name = 1;
				   required int32 id = 2;
				   optional string email = 3;
	
				   enum PhoneType {
					 MOBILE = 0;
					 HOME = 1;
					 WORK = 2;
				   }
	
				   message PhoneNumber {
					 required string number = 1;
					 optional PhoneType type = 2 [default = HOME];
				   }
	
				   repeated PhoneNumber phone = 4;
				 }
				
				
				2)
				package com.emeter.bt.rp;
				
				option java_package = "com.emeter.bt.rp";
				option java_outer_classname = "RPAnalysisData2";
				
				message RPFeatureAnalysis2 {
					repeated RPFeature2 RPFeatures = 3;
				}
				
				message RPFeature2 {
					required int64 featureId = 1;
					required int32 featureCode = 2;
					required string featureEipCode = 3;
					required string featureName = 4;
					optional string featureDescription = 5;
					required double value = 6;
				}
				
				message RPScore2 {
					required string scorerName = 1;
					required double score = 2;
			 	}
			 	
		-a protocol buffer message is a series of key-value pairs.
		
		-Protocol buffers have many advantages over XML for serializing structured data. Protocol buffers:
			-are simpler
			-are 3 to 10 times smaller
			-are 20 to 100 times faster
			-are less ambiguous
			-generate data access classes that are easier to use programmatically 	
			-Automatically-generated serialization and deserialization code avoided the need for hand parsing.
			
			-When this message is encoded to the protocol buffer binary format 
			 (the text format above is just a convenient human-readable representation for debugging and editing), 
			 it would probably be 28 bytes long and take around 100-200 nanoseconds to parse. 
			 The XML version is at least 69 bytes if you remove whitespace, and would take 
			 around 5,000-10,000 nanoseconds to parse.
		
			-When not to use
			 	-However, protocol buffers are not always a better solution than XML for instance, 
			 	 protocol buffers would not be a good way to model a text-based document with markup (e.g. HTML), 
			 	 since you cannot easily interleave structure with text. In addition, XML is human-readable 
			 	 and human-editable; protocol buffers, at least in their native format, are not. XML is also 
			 	 to some extent – self-describing. A protocol buffer is only meaningful if you have the 
			 	 message definition (the .proto file).
			 	
			 	-Protocol Buffers are not designed to handle large messages. As a general rule of thumb, 
			 	 if you are dealing in messages larger than a megabyte each, it may be time to consider an alternate strategy.
		
		-Protocol buffer encoding
		 
		 -Base 128 Varints: 
		  To understand your simple protocol buffer encoding, you first need to understand varints. 
		  Varints are a method of serializing integers using one or more bytes. Smaller numbers take a 
		  smaller number of bytes.
		  
		 -a protocol buffer message is a series of key-value pairs.
		 
		 -The binary version of a message just uses the field's number as the key – the name and declared 
		  type for each field can only be determined on the decoding end by referencing the message type's 
		  definition
		  
		 -Each key in the streamed message is a varint with the value (field_number << 3) | wire_type 
		  in other words, the last three bits of the number store the wire type.
		  
		 -Wire Type	Meaning	Used For
			0	Varint	int32, int64, uint32, uint64, sint32, sint64, bool, enum
			1	64-bit	fixed64, sfixed64, double
			2	Length-delimited	string, bytes, embedded messages, packed repeated fields
			3	Start group	groups (deprecated)
			4	End group	groups (deprecated)
			5	32-bit	fixed32, sfixed32, float 
	
		 -string
		 	Setting the value of b to "testing" gives you:
			
			12 07 74 65 73 74 69 6e 67
			The red bytes(from 74) are the UTF8 of "testing". The key here is 0x12 ? tag = 2, type = 2. 
			The length varint in the value is 7 and lo and behold, we find seven bytes following it – our string.
			
		 -repeatedMsg
		 	22        // tag (field number 4, wire type 2)
			06        // payload size (6 bytes)
			03        // first element (varint 3)
			8E 02     // second element (varint 270)
			9E A7 05  // third element (varint 86942)
		 
		 -While you can use field numbers in any order in a .proto, when a message is serialized 
		  its known fields should be written sequentially by field number, 	
		
		-ProtocolBuffer serialization/Deserialization
			-The protocol buffer compiler creates a class that implements automatic encoding and parsing of 
			 the protocol buffer data with an efficient binary format. The generated class provides getters 
			 and setters for the fields that make up a protocol buffer and takes care of the details of 
			 reading and writing the protocol buffer as a unit.
			 
			-Required Is Forever You should be very careful about marking fields as required. If at some point 
			 you wish to stop writing or sending a required field, it will be problematic to change the field 
			 to an optional field – old readers will consider messages without this field to be incomplete and 
			 may reject or drop them unintentionally. You should consider writing application-specific custom 
			 validation routines for your buffers instead. Some engineers at Google have come to the conclusion 
			 that using required does more harm than good; they prefer to use only optional and repeated. 
			
			-The message classes generated by the protocol buffer compiler are all immutable.
			
			-protocol buffer class has methods for writing and reading messages of your chosen type using the 
			 protocol buffer binary format. These include:
					-byte[] toByteArray();: serializes the message and returns a byte array containing its raw bytes.
					-static Person parseFrom(byte[] data);: parses a message from the given byte array.
					-void writeTo(OutputStream output);: serializes the message and writes it to an OutputStream.
					-static Person parseFrom(InputStream input);: reads and parses a message from an InputStream.
			
			-Protocol Buffers and O-O Design Protocol buffer classes are basically dumb data holders 
			 (like structs in C++); they don't make good first class citizens in an object model.
			
			-If you want to add richer behaviour to a generated class, the best way to do this is to 
			 wrap the generated protocol buffer class in an application-specific class
			
			-You should never add behaviour to the generated classes by inheriting from them. 
			 This will break internal mechanisms and is not good object-oriented practice anyway.
			
			-compatible: In the new version of the protocol buffer:
				-you must not change the tag numbers of any existing fields.
				-you must not add or delete any required fields.
				-you may delete optional or repeated fields.
				-you may add new optional or repeated fields but you must use fresh tag numbers 
				 (i.e. tag numbers that were never used in this protocol buffer, not even by deleted fields).
	
	
	-Interoperability
		-If you’re introducing a new service with one in Java or Go, or even communicating with a backend written in Node, 
		 or Clojure, or Scala, you simply have to hand the proto file to the code generator written in the target language 
		 and you have some nice guarantees about the safety and interoperability between those architectures. The finer points 
		 of platform specific data types should be handled for you in the target language implementation, and you can get 
		 back to focusing on the hard parts of your problem instead of matching up fields and data types in your ad hoc 
		 JSON encoding and decoding schemes.

		When Is JSON A Better Fit?

		There do remain times when JSON is a better fit than something like Protocol Buffers, including situations where:

		You need or want data to be human readable
		Data from the service is directly consumed by a web browser
		Your server side application is written in JavaScript
		You aren’t prepared to tie the data model to a schema
		You don’t have the bandwidth to add another tool to your arsenal
		The operational burden of running a different kind of network service is too great

--------------------------------------------------------------------------------------------------------------
-YARN: This chapter has given a short overview of YARN. For more detail, see Apache Hadoop
		YARN by Arun C. Murthy et al. (Addison-Wesley, 2014).
	
	-YARN provides APIs for requesting and working with cluster resources.
	-Two  types  of  long-running  daemon:  a  resource
		manager (one per cluster) to manage the use of resources across the cluster, and node
		managers running on all the nodes in the cluster to launch and monitor containers.
	-A client contacts the resource manager and asks it to run an application master process
	-The resource manager then finds a node manager that can launch the application master in a container
	-YARN itself does not provide any way for the parts of the application 
	 (client, master, process) to communicate with one another. 
	-Most nontrivial YARN applications use some form of remote communication (such as Hadoop’s RPC
     layer) to pass status updates and results back to the client, but these are specific to the
     application.
    -A request for a set of containers can express the amount of computer resources required for each 
     container (memory and CPU), as well as locality constraints for the containers in that request. 
    -A YARN application can make resource requests at any time while it is running
    	-An  application  can  make  all  of  its  requests  up  front,  or  it  can  take  a  more
		 dynamic approach whereby it requests more resources dynamically to meet the changing needs 
		 of the application.
	-Spark takes the first approach, starting a fixed number of executors on the cluster
	-MapReduce, on the other hand, has two phases: the map
	 task containers are requested up front, but the reduce task containers are not started
	 until later. Also, if any tasks fail, additional containers will be requested so the failed
	 tasks can be rerun.
	-LifeSpan
		-Rather than look at how long the application runs for, it’s useful to categorize applications in
		 terms of how they map to the jobs that users run. 
			-The simplest case is one application per user job, which is the approach that MapReduce takes.
			-The second model is to run one application per workflow or user session. 
		 		-This approach can be more efficient than the first, since containers can
				  be reused between jobs, and there is also the potential to cache intermediate data between jobs.
				  Spark is an example that uses this model.
		-The third model is a long-running application that is shared by different users
			-Used by Apache slider & Impala
			-The always on application master means that users have very low-
				latency responses to their queries since the overhead of starting a new application master
				is avoided.
			 (provide  a  proxy  application  that  the  Impala  daemons  communicate  with  to  request cluster resources.)
	-Building Yarn applications
		
	-MapReduce 1 Vs MapReduce 2:
		In MapReduce 1, there are two types of daemon that control the job execution process:
		-a jobtracker and 
		-one or more tasktrackers

		YARN these responsibilities are handled by separate entities: 
		-the resource manager and 
		-an application master (one for each MapReduce job).
		-nodeManager
		
		In YARN, the equivalent role is the timeline server, which stores application history.
		
		MapReduce 1          YARN
		Jobtracker 		Resource manager, application master, timeline server
		Tasktracker 	Node manager
		Slot 			Container	
		
	-MapReduce is just one YARN application among many	
	
	-Three schedulers are available in YARN: 
		FIFO, 
		Capacity, 
		Fair Schedulers.


--------------------------------------------------------------------------------------------------------------
-HDFS
http://storageconference.org/2010/Papers/MSST/Shvachko.pdf
http://developer.yahoo.net/blogs/hadoop/2008/09/scaling_hadoop_to_4000_nodes_a.html

Hadoop definitive guide:
http://storageconference.org/2010/Papers/MSST/Shvachko.pdf
http://developer.yahoo.net/blogs/hadoop/2008/09/scaling_hadoop_to_4000_nodes_a.html
-The Hadoop Distributed Filesystem (HDFS)
	- Filesystems that manage the storage across a network of machines are called distributed filesystems.
	
	-HDFS is a filesystem designed for storing very large files with streaming data access
	 patterns, running on clusters of commodity hardware.
		-“Very large” in this context means files that are hundreds of megabytes, gigabytes,
			or terabytes in size.
		-streaming data access pattern: write-once,  read-many-times  pattern.  	
		-not very expensive h/w
	
	
	-The number of mappers launched is roughly equal to the input size divided by dfs.block.size (the default block 
	 size is 64 MB).
	
	-Compression(bzip2, gzip, DEFLATE)
		-However, there is a drawback to storing data in HDFS using the compression formats listed 
		 previously. These formats are not splittable. Meaning, once a file is compressed using any 
		 of the codecs that Hadoop provides, the file cannot be decompressed without the whole file 
		 being read.
		-uncompressed file that was 128 MB, this would probably result in two mappers being launched (128 MB/64 MB)
		-files compressed using the bzip2, gzip, and DEFLATE codecs cannot be split, the whole 
		 file must be given as a single input split to the mapper. Using the previous example, if the 
		 input to a MapReduce job was a gzip compressed file that was 128 MB, the MapReduce 
		 framework would only launch one mapper.
	-LZO
		-LZO algorithm was designed to have  
		 fast decompression speeds while having a similar compression speed as compared to 
		 DEFLATE. In addition, thanks to the hard work of the Hadoop community, LZO compressed 
		 files are splittable.
		-core-site.xml to use the LZO codec classes	
			-<property>
				<name>io.compression.codecs</name>
				<value>org.apache.hadoop.io.compress.GzipCodec,
							org.apache.hadoop.io.compress.DefaultCodec,
				org.apache.hadoop.io.compress.BZip2Codec,
				com.hadoop.compression.lzo.LzoCodec,
				com.hadoop.compression.lzo.LzopCodec
				  </value>
				</property>
				<property>
				  <name>io.compression.codec.lzo.class</name>
				  <value>com.hadoop.compression.lzo.LzoCodec</value>
				</property>
			-1)Compress the test dataset:
				$ lzop weblog_entries.txt	
			-2)Put the compressed weblog_entries.txt.lzo file into HDFS:
			-3)Run the MapReduce LZO indexer to index the weblog_entries.txt.lzo file:
				$ hadoop jar /usr/lib/hadoop/lib/hadoop-lzo-0.4.15.jar com.hadoop.
				compression.lzo.DistributedLzoIndexer /test/weblog_entries.txt.lzo
				- DistributedLzoIndexer. This is a MapReduce application that will read 
					one or more LZO compressed files and index the LZO block boundaries of each file
	-SequenceFile
		-flexible format included with the Hadoop distribution.
		-capable of storing both text and binary data
		-SequenceFiles store data as binary key-value pairs.
		-SequenceFiles are able to do this because individual values (or blocks) are compressed, not the entire SequenceFile
		-SequenceFiles have three compression options:
			-Uncompressed: Key-value pairs are stored uncompressed
			-Record compression: The value emitted from a mapper or reducer is compressed
			-Block compression: An entire block of key-value pairs is compressed
				-If  you  are  emitting  sequence  files  for  your  output,  you  can  set  the  mapreduce.out
				 put.fileoutputformat.compress.type property to control the type of compression
				 to use. The default is RECORD, which compresses individual records. Changing this to
				 BLOCK, which compresses groups of records, is recommended because it compresses
				 better (see “The SequenceFile format” on page 133).

	-ProtocolBuffers
	-ApacheThrift
	-ApacheAvro:
		-Apache Avro supports a language-independent file format and includes serialization and
		RPC mechanisms. One of the neat features of Avro is that you do not need to compile any
		type of interface or protocol definition files in order to use the serialization features of
		the framework



	- HDFS is a filesystem designed for storing very large files with streaming data access patterns, running on clusters
	- HDFS is built on efficient data processing pattern is a write-once, read-many-times pattern
	- HDFS is not a good fit
		- low latency data operations (HBase is currently a better choice for low-latency access)
		- lots of small files:
			-	Since the namenode holds filesystem metadata in memory, the limit to the number
				of files in a filesystem is governed by the amount of memory on the namenode. As
				a  rule  of  thumb,  each  file,  directory,  and  block  takes  about  150  bytes.  So,  for
				example, if you had one million files, each taking one block, you would need at
				least 300 MB of memory
		- Multiple writers, arbitrary file modifications
			- Files in HDFS may be written to by a single writer. Writes are always made at the end of the file. 
			  There is no support for multiple writers.
		- A disk has a block size, which is the minimum amount of data that it can read or write.
		  Filesystems for a single disk build on this by dealing with data in blocks, which are an
		  integral multiple of the disk block size.
		- disk block size(512b) -> file system block size (~kbs)-> HDFS block size (128 MB default) 
		- HDFS, too, has the concept of a block, but it is a much larger unit 128 MB by default.(large as to minimize cost of seek)
		- A 1 MB file stored with a block size of 128 MB uses 1 MB of disk space, not 128 MB.
		- Tried to acheive the seek time 1% of the transfer time by keeping 128 MB default size
		- Each HDFS block is replicated to a small number of physically separate machines (typically three).  
		
		- HDFS’s fsck command understands blocks. For example, running:
			% hadoop fsck / -files -blocks
			will list the blocks that make up each file in the filesystem.
	
	NameNode & dataNode
	
	- NameNode: master. manages fileSystem namespace
		-Under federation, each namenode manages a namespace volume, which is made up of the metadata for the namespace, and a 
		 block pool containing all the blocks for the files in the namespace.
		-Information is stored persistently on the local disk in the form of two files: the namespace image and the edit log 
		
	- NameNode resilient to failure: 2 ways
		- To back up the files that make up the persistent state of the filesystem
		- Secondary NameNode:  it does not act as a namenode. Its main role is to periodically merge the namespace 
		  image with the edit log to prevent the edit log from becoming too large.
		  ****edit log
		- It keeps a copy of the merged name-space image, which can be used in the event of the namenode failing.
		- **However, the state of the secondary namenode lags that of the primary, so in the event of total failure of the primary, 
		  data loss is almost certain. The usual course of action in this case is to copy the namenode’s metadata files 
		  that are on NFS to the secondary and run it as the new primary metadata. 
	
	
	-HDFS federation (NameNode)
		-Namenode keeps reference to every file and block in fileSystem in memory
		-On large cluster it becomes a limiting factor (how much memory needed by NameNode)
		-High Availability for nameNode:2.x release series, allows a cluster to scale by adding
			namenodes, each of which manages a portion of the filesystem namespace. For example,
			one namenode might manage all the files rooted under /user, say, and a second name-
			node might handle files under /share.
		-The combination of replicating namenode metadata on multiple filesystems and using
			the secondary namenode to create checkpoints protects against data loss, but it does
			not provide high availability of the filesystem. 		
				
		-The new namenode is not able to serve requests
		 until it has 
		 (i) loaded its namespace image into memory, 
		 (ii) replayed its edit log, and
		 (iii) received enough block reports from the datanodes to leave safe mode. On large
		 		clusters with many files and blocks, the time it takes for a namenode to start from cold
		 		can be 30 minutes or more.		
	
	-Datanode reads blocks from disk, but for frequently accessed files the blocks
	 may  be  explicitly  cached  in  the  datanode’s  memory,  in  an  off-heap  block  cache
	
	-Users or applications instruct the namenode which files to cache (and for how long) by
	 adding a cache directive to a cache pool. Cache pools are an administrative grouping for
     managing cache permissions and resource usage.
	
	- DataNode: worker
	- Namenode: Failover and fencing
		-Failover: the first implementation  uses  ZooKeeper  to  ensure  that  only  one  namenode  is  active.  
					Each namenode runs a lightweight failover controller process whose job it is to monitor its
					namenode for failures 
		-Fencing: In the case of an ungraceful failover, however, it is impossible to be sure that the failed
					namenode has stopped running. For example, a slow network or a network partition
					can trigger a failover transition. 
			-revoking access of namenode from shred storage
			-killing Namenode process
			-disabling n/w port
			-STONITH: Shoot the other node in the head. forcibly shurtt down machine
	- We’ll be running HDFS on localhost(pseudo-distributed mode), on the default HDFS port, 8020
		- Namenode’s embedded web server (which runs on port 50070): caters webRequests to client HDFS
	- DataNode's embeded webServer (running on port 50075)
	- hdfs dfs -ls .
		-The first column shows the file mode
		-The second column is the replication factor of the file. The entry in this column is 
			empty for directories since the concept of replication does not apply to them. 
			-directories are treated as metadata and stored by the namenode, not the datanodes.
		-.... usual meaning

	-Hadoop  provides  many  interfaces  to  its  filesystems,  and  it  generally  uses  the  URI
		scheme to pick the correct filesystem instance to communicate with	
			-% hdfs dfs -ls file:///
	
	-HDFS security enable dfs.permissions.enabled  property
	
	- Hadoop Filesystems
		-file
		-hdfs
		-hftp
		-hsftp
		-webhdfs
		-...

	-HTTP: accessing HDFS via HTTP
		-There are two ways of accessing HDFS over HTTP: directly, where the HDFS daemons serve 
			HTTP requests to clients; and via a proxy (or proxies), which accesses HDFS on the 
			client’s behalf using the usual DistributedFileSystem API.
		- Directory listings are served by the namenode’s embedded web server (which runs on port 50070) 
			formatted in XML or JSON, while file data is streamed from datanodes by their web servers 
			(running on port 50075)
		- The second way of accessing HDFS over HTTP relies on one or more standalone proxy 
			servers. (The proxies are stateless so they can run behind a standard load balancer.) All
			traffic  to  the  cluster  passes  through  the  proxy	
		- The original  HTTP interface (HFTP and HSFTP) was read-only
		- The new WebHDFS implementation supports all filesystem operations, including Kerberos 
			authentication. WebHDFS must be enabled by setting dfs.webhdfs.enabled to true

	- Hadoop provides a C library called libhdfs that mirrors the Java FileSystem interface.

	- FUSE: Filesystem in Userspace (FUSE) 

	- The Java Interface
		Example   3-1.   Displaying   files   from   a   Hadoop   filesystem   on   standard   output   using   a
		URLStreamHandler

		public class URLCat {
		  static {
			URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory());
		  }

		  public static void main(String[] args) throws Exception {
			InputStream in = null;
			try {
			  in = new URL(args[0]).openStream();
			  IOUtils.copyBytes(in, System.out, 4096, false);
			} finally {
			  IOUtils.closeStream(in);
			}
		  }
		}

		setting up URLStreamHandler is not always possible hence below example

		- A  file  in  a  Hadoop  filesystem  is  represented  by  a  Hadoop  Path  object  (and  not a java.io.File object)
			-Path as a Hadoop filesystem URI, such as hdfs://localhost/user/tom/quangle.txt
		-Rewriting above program

		Example 3-2. Displaying files from a Hadoop filesystem on standard output by using the FileSystem
		directly
		public class FileSystemCat {
		  public static void main(String[] args) throws Exception {
			String uri = args[0];
			Configuration conf = new Configuration();
			FileSystem fs = FileSystem.get(URI.create(uri), conf);
			InputStream in = null;
			try {
			  in = fs.open(new Path(uri));
			  IOUtils.copyBytes(in, System.out, 4096, false);
			} finally {
			  IOUtils.closeStream(in);
			}
		  }
		}

	- HDFS’s coherency model:
		-A coherency model for a filesystem describes the data visibility of reads and writes for
		 a file
		-Once more than a block’s worth of data has been written, the first block will be visible
		 to new readers. This is true of subsequent blocks, too: it is always the current block being
		 written that is not visible to other readers.
		-it’s in the datanodes’ memory
		-With no calls to hflush() or hsync(), you should be prepared to lose up to a block of data in
		 the event of client or system failure
		- 
	-

--------------------------------------------------------------------------------------------------------------			

	- Security(Kerberos) pg 348 definitive guide
		-IN hadoop  secure authentication mechanism was missing
		-HDFS file permissions provide only a mechanism for authorization,
			which  controls  what  a  particular  user  can  do  to  a  particular  file.
		-Kerberos says that a user is who they say they are; it’s Hadoop’s job to
			determine whether that user has permission to perform a given action
		-Kerberos and Hadoop
				At a high level, there are three steps that a client must take to access a service when
				using Kerberos, each of which involves a message exchange with a server:
				1. Authentication.  The  client  authenticates  itself  to  the  Authentication  Server  and
					receives a timestamped Ticket-Granting Ticket (TGT).
				2. Authorization. The client uses the TGT to request a service ticket from the Ticket
					Granting Server.
				3. Service Request. The client uses the service ticket to authenticate itself to the server
					that is providing the service the client is using. In the case of Hadoop, this might
					be the namenode or the jobtracker. 	
				Together, the Authentication Server and the Ticket Granting Server form the Key 
				Distribution Center (KDC). T

			- TGTs last for 10 hours by default (and can be
				renewed for up to a week). It’s common to automate authentication at operating system
				login time, thereby providing single sign-on to Hadoop

			- In  cases  where  you  don’t  want  to  be  prompted  for  a  password  (for  running  an
				unattended MapReduce job, for example), you can create a Kerberos keytab file using
				the ktutil command. A keytab is a file that stores passwords and may be supplied to
				kinit with the -t option.

			- Security Example
				- Enable hadoop.security.authentication  property  in  core-site.xml  to  kerberos
					- To use Kerberos authentication with Hadoop, you need to install, configure, and run a KDC (Hadoop
					  does not come with one)
				- Enable service-level authorization by setting hadoop.security.authorization to true in the same file.
				- Property is enabled by setting dfs.block.access.token.enable to true.
				- [[ Configure Access Control Lists (ACLs) in the
					hadoop-policy.xml configuration file to control which users and groups have permission
					to connect to each Hadoop service
					By default, all ACLs are set to *, which means that all users have permission to access each
					service, but on a real cluster you should lock the ACLs down to only those users and
					groups that should have access.
				  ]]
				- We only needed to call kinit once, since the Kerberos ticket is valid for 10 hours 
					(use the klist command to see the expiry time of your tickets and kdestroy to 
					invalidate your tickets)\

			-Delegation Tokens
				- HDFS read operation will involve multiple calls to the namenode and calls to one 
					or more datanodes. Instead of using the three-step Kerberos ticket exchange 
					protocol to authenticate each call, which would present a high load on the 
					KDC on a busy cluster
				- Hadoop uses delegation tokens to allow later authenticated access without having to 
				  contact the KDC again	
				- On the first RPC call to the namenode, the client has no delegation token, 
					so it uses Kerberos to authenticate, and as a part of the response it gets a 
					delegation token from the namenode. In subsequent calls, it presents the 
					delegation token, which the namenode can verify (since it generated it using a secret key)
				- block access token: 
					- The client uses a special kind of delegation token, called a block access token, 
						that the namenode passes to the client in response to a metadata request. 
						The client uses the block access token to authenticate itself to datanodes. 
						This is possible only because the namenode shares its secret key used to generate 
						the block access token with datanodes
				- This property is enabled by setting dfs.block.access.token.enable to true.
				- Delegation tokens are used by the jobtracker and tasktrackers to access HDFS 
					during the course of the job. When the job has finished, the delegation tokens 
					are invalidated.

			- Other Security Enhancements
				- Tasks can be run using the operating system account for the user who submitted 
				  the job, rather than the user running the tasktracker.
					- mapred.task.tracker.task-controller  to org.apache.hadoop.mapred.LinuxTaskController.  
				- Shared cache and Private cache	
					- When  tasks  are  run  as  the  user  who  submitted  the  job,  the  distributed  cache
						(Distributed Cache on page 288) is secure: files that are world-readable are put
						in a shared cache (the insecure default), otherwise they go in a private cache, only
						readable by the owner.
				- Users can view and modify only their own jobs, not others. This is enabled by
				  setting mapred.acls.enabled to true.
				-Configure Hadoop to use a keytab  
					- For a datanode, 
						-Set the dfs.datanode.keytab.file property to the keytab filename
						- dfs.datanode.kerberos.principal to the username to use for the datanode
					-

				-kinit
				-kutil
				-klist
				-kdestroy



	-*** Distributed Cache (on page 288)
	-*** Anatomy of a MapReduce Job Run
	-*** How much memory does a namenode need?
	-*** Benchmarking a Hadoop Cluster

	*** As described, the edits file would grow without bound. Though this state of affairs would have no impact on 
	   the system while the namenode is running, if the namenode were restarted, it would take a long time to apply 
	   each of the operations in its (very long) edit log. The solution is the checkPoint process.

	***The filesystem image and edit log	
	- When a filesystem client performs a write operation (such as creating or moving a file), it is first recorded in the edit log
	- The namenode also has an in-memory representation of the filesystem metadata, which it updates 
	  after the edit log has been modified. (may be this is concurrentSkipList)
	- The in-memory metadata is used to serve read requests.
	- The edit log is flushed and synced after every write before a success code is returned to the client.
	- For namenodes that write to multiple directories, the write must be flushed and synced to every copy 
	  before returning successfully. This ensures that no operation is lost due to machine failure.
	- The ***fsimage file is a persistent checkpoint of the filesystem metadata.
		- However, it is not  updated  for  every  filesystem  write  operation,  since  writing  out  the  
		  fsimage  file, which can grow to be gigabytes in size, would be very slow
		- ***This does not compromise resilience, however, because if the namenode fails, then the latest 
		  state of its metadata can be reconstructed by loading the fsimage from disk into memory, 
		  then applying each of the operations in the edit log.
		- The fsimage file contains a serialized form of all the directory and file inodes in the filesystem.  
		- Each inode is an internal representation of a file or directory’s metadata and contains such information 
		  as the file’s replication  level,  modification  and  access  times,  access  permissions, block size, 
		  and the blocks a file is made up of.
		- The fsimage file does not record the datanodes on which the blocks are stored.  
		- Instead the namenode keeps this mapping in memory, which it constructs by asking the datanodes for their
		  block lists when they join the cluster and periodically afterward to ensure the namenode’s block mapping 
		  is up-to-date.
	- The checkpointing process: The solution is to run the secondary namenode, whose purpose is to produce check-points
								 of the primary’s in-memory filesystem metadata.
		1 The secondary asks the primary to roll its edits file, so new edits go to a new file
		2 The secondary retrieves fsimage and edits from the primary (using HTTP GET).
		3 The secondary loads fsimage into memory, applies each operation from edits, then creates a new consolidated fsimage file.
		4 The secondary sends the new fsimage back to the primary (using HTTP POST).
		5 The primary replaces the old fsimage with the new one from the secondary, and the old edits file with the new 
		  one it started in step 1. It also updates the fstime file to record the time that the checkpoint was taken.

		- At the end of the process, the primary has an up-to-date  fsimage file and a shorter edits  file
		- It is possible for an administrator to run this process manually   while   the   namenode   is   
		  in   safe   mode,   using   the   hadoop  dfsadmin -saveNamespace command.
	- This procedure makes it clear why the secondary has similar memory requirements to the primary 
	  (since it loads the fsimage into memory), which is the reason that the secondary needs a dedicated 
	  machine on large clusters.
	- Schedule of checkPoints: fs.checkpoint.period  in  seconds  
	- The schedule for checkpointing is controlled by two configuration parameters. The secondary  namenode  
	  checkpoints  every  hour  (fs.checkpoint.period  in  seconds)  or sooner if the edit log has reached 64 MB 
	  (fs.checkpoint.size in bytes), which it checks every five minutes.

*********************************************************
R script
--------

In real life scenarios, you would train the model using one “tagged” dataset, verify the quality, 
save the trained model and use it to predict with different datasets serving as input.



.RDA 
-----
load("filename.RDA")

(.RDA (or .rda) is short for .RData (or .rdata :-).  It is the usual
file format for saving R objects to file (with save() or
save.image()).)
----

> > vars<-load('~/523/Data.rda') 
> > vars 
> [1] "seg_vegNew" 
> >print(seg_vegNew) 

Try to "ls(seg_vegNew)" 
R> my.x <- get('x', seg_vegNew) 

You should see a list of variables in it ... say "x" was one of them, one 
way that you can then get access to it is like so: 

R> my.x <- get('x', seg_vegNew) 


RP_MODEL.rda
------------
library(RODBC)
library(randomForest)
library(nza)
library(nzr)
library(tree)
RP_Model <- "/home/pipe/ajit/rdemo/RP_MODEL.rda"
vars <- load (RP_Model)
vars
	
> print(RP_Model)

Call:
 randomForest(formula = as.factor(training_data$TARGET) ~ ., data = training_data[,      !names(training_data) %in% c("SDP_WID", "ANALYSIS_END_DATE",          "ANALYSIS_START_DATE", "SDP_SEGMENT_CONFIG_WID")], ntree = 50) 
               Type of random forest: classification
                     Number of trees: 50
No. of variables tried at each split: 3

        OOB estimate of  error rate: 33.11%
Confusion matrix:
      Clean Theft class.error
Clean 13411  5483   0.2901979
Theft  6616 11030   0.3749292



-ntree	  sets	  how	  large	  your	  Random	  Forest	  is	  going	  to	  be.	  Or	  in	  other	
  words,	how	  many	  trees	  should	  be	  contained	  in	  your	   ensemble.

-There are two main types of decision trees:
	-Classification trees : Predicted outcome is the class the data belongs.
	-Regression trees : Predicted outcome is continuous variable e.g. a real number such as the price of a commodity.
	
	
	
---
> ls(RP_Model)
 [1] "call"            "classes"         "confusion"       "err.rate"       
 [5] "forest"          "importance"      "importanceSD"    "inbag"          
 [9] "localImportance" "mtry"            "ntree"           "oob.times"      
[13] "predicted"       "proximity"       "terms"           "test"           
[17] "type"            "votes"           "y"     
-----

Below is an example of building a decision tree for predicting income, based on age, sex 
and numbers of hours per week.
> # build a tree using built-in analytics
> adultTree = nzDecTree(income~age+sex+HOURSPERWEEK, data=nzadult)

The fitted model can be applied to another dataset. If the dataset is stored in a database table, massive data 
transfer can be avoided by using the overloaded function predict() to [ Refer to http://archive.ics.uci.edu/ml/datasets/Adult for more information]
Wrappers for Built-in Analytics perform classification inside the NPS system.
> # use the previously created tree for prediction 
> adultPred = predict(adultTree, nzadult)

> # bring the results of the prediction to R using as.data.frame 
> head(as.data.frame(adultPred)) 



nz.data.frame
--------------

The most important and frequent construct is the object of the class nz.data.frame. The 
function nz.data.frame() creates a pointer to a table located on the NPS system. This point-
er can later be used to run data transformations with nzApply or nzRun or data mining al-
gorithms. It does not store any data in local memory but rather provides metadata that can 
be used to determine the correct table subset (columns and/or rows) where user code 
should run. It is the standard output of the majority of data manipulation functions in the 
nzLibrary for R.
> nzConnect("admin", "password", "TT4-R040", "mm") 
# get the data 
> nzadult = nz.data.frame("adult") 
> nzadult 
SELECT  AGE,WORKCLASS,FNLWGT,EDUCATION,EDUCATIONNUM,MARITALSTATUS,OCCUPATION, 
RELATIONSHIP,RACE,SEX,CAPITALGAIN,CAPITALLOSS,HOURSPERWEEK,NATIVECOUNTRY, 
INCOME,ID  FROM  adult
The nz.data.frame class implements a number of methods for extracting a subset of its 
data, gathering meta-info similar to data.frame, and working with parallel data-processing 
algorithms. The table schema is read from the NPS system and all necessary metadata is 
available to the user.


[,] and $
---------

A subset of columns and/or rows may be specified using the [,] operator.
A limitation is that rows cannot be referenced by their numbers since there is no continuous 
row numbering on the NPS system. Instead, you must specify value-based conditions, for 

example, 
d [,1]>10 which means "all rows where the value of the first column is greater than 10." 
The $ operator may also be used to select an nz.data.frame column.

head, tail
----------

In order to get a sample of the data, the head() and tail() functions may be useful. The 
functions pull the specified data from the start or end of the dataset, respectively.

as.nz.data.frame
----------------

Another useful data manipulation function is as.nz.data.frame. It creates an 
nz.data.frame object from a different R object. Then, an NPS system table is created and 
the passed data is inserted into this table. The created object points to the newly created 
NPS system table.
This example shows how an nz.data.frame object can be created from another R object, in 
this case from a data.frame iris. 




?? How a model is created 
?? How model evaluation is done i.e. how the the best model is selected
?? If a model is hardCoded, then how will it learn over data set 
?? What are the properties that were considered in creating RandomForest like 
	-depth of tree
	-no of nodes
	-......
?? training data


URLs

R 
http://www.toptal.com/machine-learning/machine-learning-theory-an-introductory-primer
https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm

Quick-R good for quick ref
http://www.statmethods.net/index.html

training Tutorial
https://www.kaggle.com/c/bike-sharing-demand/forums/t/11525/tutorial-0-433-score-with-randomforest-in-r/80074

*********************************************************
Serialization:
http://docs.oracle.com/javase/8/docs/platform/serialization/spec/serialTOC.html

*********************************************************



*********************************************************
BigData

*********************************************************
 Hive
------

Hive is not designed for online transaction processing and does not offer real-time queries and row level updates. 
It is best used for batch jobs over large sets of immutable data (like web logs).

hive-site.xml  file  to  $SPARK_HOME/conf, 

imports		
// Import Spark SQL
import org.apache.spark.sql.hive.HiveContext;
// Or if you can't have the hive dependencies
import org.apache.spark.sql.SQLContext;
// Import the JavaSchemaRDD
import org.apache.spark.sql.SchemaRDD;
import org.apache.spark.sql.Row;

Notes:
-Cache table as
	hiveCtx.cacheTable("tableName")
	In Spark 1.2, the regular  cache()  method on RDDs also results in a cacheTable()



1)
JavaSparkContext ctx = new JavaSparkContext(...);
SQLContext sqlCtx = new HiveContext(ctx);
OR 
HiveContext hiveCtx = new HiveContext(sc);


2)
//Read from Hive table
HiveContext hiveCtx = new HiveContext(sc);
SchemaRDD rows = hiveCtx.sql("SELECT key, value FROM mytable");
JavaRDD<Integer> keys = rows.toJavaRDD().map(new Function<Row, Integer>() {
  public Integer call(Row row) { 
  	return row.getInt(0); 
  	}
});


3) Jason example
SchemaRDD input = hiveCtx.jsonFile(inputFile);
// Register the input schema RDD
input.registerTempTable("tweets");
// Select tweets based on the retweetCount
SchemaRDD topTweets = hiveCtx.sql("SELECT text, retweetCount FROM tweets ORDER BY retweetCount LIMIT 10");

4) ParquetFile
saveAsParquetFile()

rest almost same as above load from Parquet file and create TempTable of it
and use it to display data

class HappyPerson implements Serializable {
	  private String name;
	  private String favouriteBeverage;
	  public HappyPerson() {}
	  public HappyPerson(String n, String b) {
	    name = n; favouriteBeverage = b;
	  }
	  public String getName() { return name; }
	174      |      Chapter 9: Spark SQL  public void setName(String n) { name = n; }
	  public String getFavouriteBeverage() { return favouriteBeverage; }
	  public void setFavouriteBeverage(String b) { favouriteBeverage = b; }
	};
	...
	ArrayList<HappyPerson> peopleList = new ArrayList<HappyPerson>();
	peopleList.add(new HappyPerson("holden", "coffee"));
	JavaRDD<HappyPerson> happyPeopleRDD = sc.parallelize(peopleList);
	SchemaRDD happyPeopleSchemaRDD = hiveCtx.applySchema(happyPeopleRDD,
	  HappyPerson.class);
	happyPeopleSchemaRDD.registerTempTable("happy_people");

-Pseudodistributed mode is nearly identical; it’s effectively a one-node cluster.

Hive properties & commands:

-hive.metastore.warehouse.dir : The default value for this property is /user/hive/warehouse

-.hiverc file: Best to put commands like this in the $HOME/.hiverc file, which will be processed when Hive starts.

-If you omit the EXTERNAL keyword and the original table is external, the
	new table will also be external. If you omit EXTERNAL and the original
	table is managed, the new table will also be managed. However, if you
	include the EXTERNAL keyword and the original table is managed, the new
	table will be external. Even in this scenario, the  LOCATION clause will
	still be optional.

-hive> SHOW PARTITIONS employees;

-hive> set hive.mapred.mode=strict;

-hadoop distcp /data/log_messages/2011/12/02 s3n://ourbucket/logs/2011/12/02
	DistCp Version 2 (distributed copy) is a tool used for large inter/intra-cluster copying. 
	It uses MapReduce to effect its distribution, error handling and recovery, and reporting. 
	It expands a list of files and directories into input to map tasks, each of which will 
	copy a partition of the files specified in the source list.

-partition problem:
	When we executes select * from user; nothing appears.
	Why?
	I spent a long time searching for an answer.
	Finally, solved.
	Because when external table is declared, default table path is changed to specified 
	location in hive metadata which contains in metastore, but about partition, nothing 
	is changed, so, we must manually add those metadata.
	
	
	ALTER TABLE user ADD PARTITION(date='2010-02-22');
	Every time a new data=... folder (partition) is created, we must manually alter the 
	table to add partition information.

-we can get a partition’s location as follows:
	hive> DESCRIBE EXTENDED log_messages PARTITION (year=2012, month=1, day=2);
	...
	location:s3n://ourbucket/logs/2011/01/02,

-DESCRIBE EXTENDED table command lists the input and output formats,
	the SerDe, and any SerDe properties in the DETAILED TABLE INFORMATION.
	
-The CLUSTERED BY … INTO … BUCKETS clause, with an optional SORTED BY … clause is used
	to optimize certain kinds of queries.
	CREATE EXTERNAL TABLE IF NOT EXISTS stocks (
	  exchange        STRING,
	  symbol          STRING,
	  ymd             STRING,
	  price_open      FLOAT,
	  price_high      FLOAT,
	  price_low       FLOAT,
	  price_close     FLOAT,
	  volume          INT,
	  price_adj_close FLOAT)
	CLUSTERED BY (exchange, symbol)
	SORTED BY (ymd ASC)
	INTO 96 BUCKETS
	ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
	LOCATION '/data/stocks';
	
-IF EXISTS
	DROP TABLE IF EXISTS employees;

-Drop table: For managed tables, the table metadata and data are deleted.
	For external tables, the metadata is deleted but the data is not.

- Hadoop Trash feature [fs.trash.interval]:  which is not on by
	default,  the  data  is  moved  to  the  .Trash  directory  in  the  distributed
	filesystem for the user, which in HDFS is /user/$USER/.Trash. To enable
	this feature, set the property fs.trash.interval to a reasonable positive
	number. It’s the number of minutes between “trash checkpoints”; 1,440
	would be 24 hours. While it’s not guaranteed to work for all versions of
	all distributed filesystems,

-ALTER TABLE  modifies  table  metadata  only.  The  data  for  the  table  is untouched. 

- change a partition location
	ALTER TABLE log_messages PARTITION(year = 2011, month = 12, day = 2)
	SET LOCATION 's3n://ourbucket/logs/2011/01/02';
	
	This command does not move the data from the old location, nor does it delete the old data.

-Changing Columns
	You can rename a column, change its position, type, or comment:
	ALTER TABLE log_messages
	CHANGE COLUMN hms hours_minutes_seconds INT
	COMMENT 'The hours, minutes, and seconds part of the timestamp'
	AFTER severity;

-Alter table properties
	ALTER TABLE log_messages SET TBLPROPERTIES (
	 'notes' = 'The process id is no longer captured; this column is always NULL');

-The ALTER TABLE … TOUCH

- hive.files.umask.value that defines  a  umask  value  used  to  set  the  default  
	permissions  of  newly  created  files,  by masking bits

- hive.metastore.authorization.storage.checks: is true, Hive prevents a user from dropping a table 
	when the user does not have permission to delete the underlying files that back the table

- hive.metastore.execute.setugi: true, When running in secure mode.

- hive.security.authorization.enabled: Enable or disable the hive client authorization

-

-

-

-

-

-

 Data Types and File Formats
 HiveQL: Data Definition
 HiveQL: Queries (optional)
 Schema Design
 Other File Formats and Compression
 Developing 
 Security
 
*********************************************************
Hadoop Training: Mohit 

hadoop 1= only read
hadoop 2= append

hadoop adviced to work in same data centre (e.g over internet)
cassandra is adviced to work in diff data centre

skiplist
Namenode is (1)editLog- only added ()
 			(2)concurrentSkipList: sortedMap CSLM
			
			FsImage: serialized concurrntSkipList as FsImage
			BlockPoolMap stores which block coming from which node. BlockId is the key of the map

scheduler			
			
File write is 2 step 
File read is 1 step process

In production secondary node is started, goes to nameNode and download editLog




costOfLock
HashTable is used by reducer
sequenceFile:
map file: sorted seq file with index file (index key)
AVRO: allows avro schema 

?How last 1 hr editLog is downloaded is it hourly based File
:: rolled in 20 min.. so 1 hr can be downloaded
?why desirealize fsImage we already have updatedConcrntSkipListMap
::can be done
?after crash recovery who becomes secondayNameNode
::
?
::
?
::
?
::
?
::
?
::
?
::
?
::
?
::
?
::
?

*********************************************************
Kerberos

Kerberos uses symmetric cryptography to authenticate clients to services and vice versa. 


*********************************************************

*********************************************************

*********************************************************

*********************************************************

*********************************************************

*********************************************************

*********************************************************
classLoader
https://www.youtube.com/watch?v=t8sQw3pGJzM&list=PLXvUQtFPPOS-y9yuG0KiAb5pORGBKXjz9

Vijay saraswat java is not type safe 1997

sheng liang and gilad bracha. Dynamic class loading in JVM

NoClassDefFound
find *.jar -exec jar -tf '{}'\; | grep myClass
A.class.getClassLoader().getResourece().

NoSuchMethodError
javap -private myclass


***********************************************************


find . -exec jar -tf '{}'\; | grep "sparkProperties"



??????????
-linear-probe hash maps
****************************************************************************************************
Algorithms:

decide between big O = f(n) = O(g(n)) i.e. fn is < or takes less time
omega => f(n) = omega(g(n)) i.e. fn is > or g(n) takes less time 
theta => f(n) = theta(g(n)) i.e. fn is = or equals


Rules are defined
- Multiplicative constants can be omitted: 14n2 becomes n2
-n^a dominates n^b if a > b : for instance, n^2 dominates n
- polynomial dominates any logarithm: n dominates (log n)^ 3
-Any exponential dominates any polynomial: 3^n dominates n^5 (it even dominates 2^n )
-n! & 2^n are theta

-The moral: in big- T terms, the sum of a geometric series is simply the ?rst term if the series is
strictly decreasing, the last term if the series is strictly increasing, or the number of terms if the
series is unchanging.

more rules:
http://www.chegg.com/homework-help/algorithms-1st-edition-solutions-9780073523408


-	It is a basic property of decimal numbers that
	The sum of any three single-digit numbers is at most two digits long.
	The rule is correct(not only for decimal) for any base b = 2
	
-	How many digits are needed to represent the number N = 0 in base b ?
	With k digits in base b we can express numbers up to bk - 1 
	999 = 10^3 - 1

-	The rule for converting logarithms from base a to base b : logb N = (loga N)/(loga b)
	
-	Modular arithmetic is a system for dealing with restricted ranges of integers. 

-	Euclid’s algorithm for greatest common divisor:
	Given two integers a and b , it ?nds the largest integer that divides both of
	them, known as their greatest common divisor (gcd).
	The most obvious approach is to ?rst factor a and b , and then multiply together their
	common factors. For instance, 1035 = 32 · 5 · 23 and 759 = 3 · 11 · 23 , so their gcd is 3 · 23 = 69 .
	
	Rule: If x and y are positive integers with x = y , then gcd(x, y) = gcd(x mod y, y) .
	
-	Divide-and-conquer algorithms 
	recursion: Divide-and-conquer algorithms often follow a generic pattern:  they tackle a problem of size
	n by recursively solving, say, a subproblems of size n/b and then combining these answers in
	O(nd) time, for some a, b, d > 0
	
	- BinarySearch: O(n log n)
	- MergeSort: O(n log n)
	
	
-Recursive call are expensive than iterative calls

Divide & conquer Algo
Greedy Algo
Dynamic programming 
Linear programming and reduction
NP complete solutions
Quantum algo

	
	
	
-Binary search work on Arrays but not on Linked list	

BinarySearch- log n

Selection sort: find min no and swap with sorted list first element. Now move +1 in list. O(n^2)
avg case:
worst case:

Insertion sort: eg arrange  cards in deck.  O(n^2)
avg case: average-case complexity for insertion sort is (N-1)*N/4
worst case: O(n^2)

Bubble sort: O(n^2)

avg case:
worst case:

Insertion sort better than Selection sort is better than bubble sort
Insertion sort<selection sort<bubble sort
Because in sorted list insertion sort is linear

MergeSort:
avg case: 
worst case: O(nlogn)

Quick sort:
avg case: O(nlogn)
worst case: O(n^2) 

O(n^2) is infeasible for n > 10000				

log n < n < n log n < n^2 < n^3 < 2^n < n!

stable sorting : spreadSheet where 1 column sorting does not affect sorting of other column
use Merge sort with careful at merging arrays: only move A[i] <= A[j]
	
TODO: bubble sort, Heapsort

Graphs:
traversals:
	-BFS:visited + parent info. gives shortest path
		 -Level by level exploration
		 -edges explored from BFS forms a tree
		 -acyclic graph = connected with n-1 edges, it is also known as tree	
	
	-DFS: Explore each vertex as soon it is visited, DFS numbering 
		path discovered by DFS are not shortest unlike BFS  so why use DFS as it gives:
		-DFS numbering: get no when we started in node and when we get out of node. pre[] & post[]
		pre and post can be used to find:
		-cycle in graph
		-cut vertex i.e. removal of node disconnects a graph
		-Undirected:Non tree edge generates a cycle
		-directed graph: Non tree edge
			-Forward edge: using pre post [u     v]
											[v v]
			-backward edge: only backward edge forms cycle
								[u v]
							  [v 	v ]	
			-cross edge
					disjoint set 
						[a 	b] 
						[c 	d]
		-DFS numbering can be used to compute strongly connected graph(if edge b/w i to j then j to i is strongly conected)
		 all pairs of nodes in SSC are strongly connected
		-DFS helps identifying articulating point i.e. removing such vertex disconnects graph OR
		 bridges i.e. removing such edge disconnects graph
		
		-In this graph, an even number of vertices (the four vertices numbered 2, 4, 5, and 6) 
		 have odd degrees. The sum of the degrees of the vertices is 2 + 3 + 2 + 3 + 3 + 1 = 14, twice the number of edges.
	-Directed acyclic graph: no cycles	
		
degree: no of nodes connected to vertex
	-inDegree: no of edges moving in to vertex
	-outDegree: no of edges moving out of vertex
	
DAG: directed acyclec graph
topological ordering: use indegree to sort. start with 0 and enumerate it, then delete. Repeat and get the topological sorting


******************************************************************************************************************
Micro services: http://martinfowler.com/articles/microservices.html
Monolithic: a monolithic application built as a single unit.
-Client
-DB
-Server

Monolithic applications can be successful, but increasingly people are feeling frustrations with them - 
especially as more applications are being deployed to the cloud . Change cycles are tied together - 
a change made to a small part of the application, requires the entire monolith to be rebuilt and deployed. 
Over time it's often hard to keep a good modular structure, making it harder to keep changes that ought to 
only affect one module within that module. Scaling requires scaling of the entire application rather than parts 
of it that require greater resource

-code change / deploy cycle
-hard to keep a good modular structure
-scaling problem

MicroServices:
	architecture puts each element of functionality into a seperate service and scales by 
	distributing these services across servers as needed ie scale only those MS which require more resources.
-building applications as suites of services
-Services are independently deployable and scalable
-each service also provides a firm module boundary
-even allowing for different services to be written in different programming languages

-definition is that a component is a unit of software that is independently replaceable and upgradeable.
-aim of a good microservice architecture is to minimize these through cohesive service boundaries and 
 evolution mechanisms in the service contracts.

-Applications built from microservices aim to be as decoupled and as cohesive as possible - they own their 
 own domain logic and act more as filters in the classical Unix sense

Conway's Law:
Any organization that designs a system (defined broadly) will produce a design whose structure is a 
copy of the organization's communication structure.

-- Melvyn Conway, 1967


-battle-tested code 

??Patterns like Tolerant Reader and Consumer-Driven Contracts are often applied to microservices. 
??The second approach in common use is messaging over a lightweight message bus. The infrastructure 
  chosen is typically dumb (dumb as in acts as a message router only) - simple implementations 
  such as RabbitMQ or ZeroMQ don't do much more than provide a reliable asynchronous fabric


******************************************************************************************************************

Problems:

Graph:
	1)	consider the task of coloring a political map.  What is the minimum number of colors needed, 
		with the obvious restriction that neighboring countries should have different colors?
		
		A graph with one vertex for each country (1 is Brazil, 11 is Argentina) and edges
		between neighbors. It contains exactly the information needed for coloring, and nothing more.
		The precise goal is now to assign a color to each vertex so that no edge has endpoints of the
		same color.	
		
	2) Examination scheduling with min no of slots: 
		Use one vertex for each exam and put an edge between two vertices if there is a con?ict, 
		that is, if there is somebody taking both endpoint exams
		
	3) Cities and flights: Nodes are cities and edges are flights
	
	
***********************************************************************
Programming hive - 59 


***********************************************************************
11/4/16  TDD training 	
	

--------------------DS questions----------------
Q2May: Print Concatenation of Zig-Zag String in ‘n’ Rows

Given a string and number of rows ‘n’. Print the string formed by concatenating n rows when input string is written in row-wise Zig-Zag fashion.

Examples:

Input: str = "ABCDEFGH"
       n = 2
Output: "ACEGBDFH"
Explanation: Let us write input string in Zig-Zag fashion
             in 2 rows.
A   C   E   G   
  B   D   F   H
Now concatenate the two rows and ignore spaces 
in every row. We get "ACEGBDFH"

Input: str = "GEEKSFORGEEKS"
       n = 3
Output: GSGSEKFREKEOE
Explanation: Let us write input string in Zig-Zag fashion
             in 3 rows.
G       S       G       S
  E   K   F   R   E   K
    E       O       E
Now concatenate the two rows and ignore spaces 
in every row. We get "GSGSEKFREKEOE"

Ans: Use 2D array 
		row[no of rows][length of string]
		Loop till length of string i.e i=0 till i< str.length
		  char letter = processingLetter
		  int rowToWrite = getRowToWrite(i)
		  Loop till no of rows i.e n times j=0 to j<no of rows
			if (i == j)
			  row[j][i] = letter
			else
			  row[j][i] = " "  
		  End Loop
		End Loop
		
	getRowToWrite (){
		//Todo
	}




Q2May Add 1 to a number represented as linked list
	Number is represented in linked list such that each digit corresponds to a node in linked list. 
	Add 1 to it. For example 1999 is represented as (1-> 9-> 9 -> 9) and adding 1 to it should change 
	it to (2->0->0->0)

Ans: Use doubly link lst & stack.

Create doubly LL with values
Push nos to stack
Traverse to last link list node
Loop till stack is not empty
  pop value add to LL value
  add popped value to LL node value
  If result > 9 
      push carry to stack
  if LL prev node is Null i.e. first node
      pop all elements and add to LL first node  
Loop end

Ans 2: 
-reverse the LL
-carry - noToAdd
-Loop whuke carry = 0
-	add no to node
-	If sum > 10 
-		carry 10 - sum
-   Node = next node
-End Loop


Q1 3 May
------
Reverse words in a given string
Example: Let the input string be “i like this program very much”. The function should change the string to “much very program this like i”

Solution:
String s = "i like this program very much";

printReverse(String s){

	String[] split = s.split(" ");
	int length = split.length - 1;
	while(length >= 0){
		System.out.print(split[length] + " ");
		length--;
	}
		
}
o/p - much very program this like i 		

alternate:
reverse(char[], startIndex, endIndex){

}


Q2 3May
-------
Rearrange a linked list such that all even and odd positioned nodes are together

Rearrange a linked list in such a way that all odd position nodes are together and all even positions node are together,

Examples:

Input:   1->2->3->4
Output:  1->3->2->4

Input:   10->22->30->43->56->70
Output:  10->30->56->22->43->70

Solution2:
nodeEven = root; nodeRootOdd = root.next;
while(nodeEven.next != null){
  nodeOdd = nodeEven.next; tmpNode = nodeOdd.next
  nodeEven .next = tmpNode;
  nodeOdd.next = tmpNode.next;
  evenNode = tmpNode.next;
  if (null != evenNode ) tmpNode.next = evenNode.next.next;
}
odd.next = nodeRootOdd;

---------------------------------------

If (node size >= 3) {
  nodeEven = root; nodeRootOdd = root.next;
  while(nodeEven.next != null){
    nodeOdd = nodeEven.next; 
    nodeEven.next = nodeOdd.next;
    nodeEven = nodeOdd.next;
    nodeOdd.next = nodeEven.next
  }
nodeEven.next = nodeRootOdd;
}
print LL;



***********************************************
Java Security:  
-------------

Google book Inside Security: https://books.google.co.in/books?id=XfWlYWVzo20C&printsec=frontcover&source=gbs_ge_summary_r&cad=0#v=onepage&q&f=true

Once a class has been loaded into the virtual machine , its checked by following:

1) Class loaders:

2) ByteCode Verifier: Bytecode Verification
	When a class loader presents the bytecodes of a newly loaded Java platform class to the virtual machine,
	these bytecodes are first inspected by a verifier. The verifier checks that the instructions cannot 
	perform actions that are obviously damaging. All classes except for system classes are verified. 
	You can, however, deactivate verification with the undocumented -noverify option.

	For example,  -noverify (or -Xverify:none) option
	
	java -noverify Hello
	
	Here are some of the checks that the verifier carries out:

	Variables are initialized before they are used.
	Method calls match the types of object references.
	Rules for accessing private data and methods are not violated.
	Local variable accesses fall within the runtime stack.
	The runtime stack does not overflow.
	If any of these checks fails, then the class is considered corrupted and will not be loaded

	You might wonder, however, why a special verifier checks all these features. After all, the compiler 
	would never allow you to generate a class file in which an uninitialized variable is used or in which 
	a private data field is accessed from another class. Indeed, a class file generated by a compiler for 
	the Java programming language always passes verification. 

	However, the bytecode format used in the class files is well documented, and it is an easy matter for 
	someone with some experience in assembly programming and a hex editor to manually produce a class file 
	that contains valid but unsafe instructions for the Java virtual machine. Once again, keep in mind that 
	the verifier is always guarding against maliciously altered class files, not just checking the class 
	files produced by a compiler.
	
3) Security Manager
	Once a class has been loaded into the virtual machine and checked by the verifier, the second security 
	mechanism of the Java platform springs into action: the security manager
	The security manager is a class that controls whether a specific operation is permitted. Operations 
	checked by the security manager include the following:
	
			Creating a new class loader
			Exiting the virtual machine
			Accessing a field of another class by using reflection
			Accessing a file
			Opening a socket connection
			Starting a print job
			Accessing the system clipboard
			Accessing the AWT event queue
			Bringing up a top-level window
			
	The default behavior when running Java applications is that no security manager is installed, so all 
	these operations are permitted. The applet viewer, on the other hand, enforces a security policy 
	that is quite restrictive.		
	
-Anatomy of java program	
	-The bytecode verifier
	 The bytecode verifier ensures that Java class files follow the rules of the Java language. In terms of
	 resources, the bytecode verifier helps enforce memory protections for all Java programs. As the figure
	 implies, not all classes are subject to bytecode verification.	
	 
	-The class loader
	 One or more class loaders load all Java classes. Programatically, the class loader can set permissions
	 for each class it loads. 
	 
	-The access controller
	 The access controller allows (or prevents) most access from the core API to the operating system,
	 based upon policies set by the end user or system administrator.
	
	-The security manager
	 The security manager is the primary interface between the core API and the operating system; it has
	 the ultimate responsibility for allowing or preventing access to all system resources. However, it
	 exists mostly for historical reasons; it defers its actions to the access controller.

	-The security package 
	 The security package (that is, classes in the java.security package as well as those in the
 	 security extensions) allows you to add security features to your own application as well as providing
     the basis by which Java classes may be signed

	-The key database
	 The key database is a set of keys used by the security infrastructure to create or verify digital
	 signatures. In the Java architecture, it is part of the security package, though it may be manifested as
	 an external file or database.
	
	
- Security Debugging : java.security.debug : all / access / jar / policy / scl
	For example, to see the permissions granted by the secure class loader and see a stack trace when a permission
	check fails, you would specify -Djava.security.debug=scl,access,failure


-JSSE: java secure socket extension, provides SSL encryption facilities
	-SSL: secure socket layer
-JCE: java cryprography extension
-JAAS: java authentication and authorization service 


	
audit log
concurrentSkipList
BigTable


-Djava.library.path=/path will be handled in java bytecode level.
 LD_LIBRARY_PATH is Linux specific and is an environment variable pointing to directories where the dynamic 
 loader should look for shared libraries.

-If a library is not explcitely loaded from java, i.e. a dependent library has to be used, then LD_LIBRARY_PATH 
 has to be used to have that library available in the jvm
-There are three options for including the native library: 

1. Add to java.library.path JVM argument 
2. Add to LD_LIBRARY_PATH environment variable 
3. Copy library file to within a path that already exists within the 
library path 


The java.library.path is defined in the jvm.config file and contains 
paths delimited by the "," character. 

The LD_LIBRARY_PATH is an environment variable which can be set one- 
time by using a command such as "export LD_LIBRARY_PATH=/opt/ 
fusionreactor/etc/lib:${LD_LIBRARY_PATH}". Alternatively, on RedHat 
based distributions (which CentOS is) you can also configure 
permanently for all users in the "/etc/ld.so.conf.d/local.conf" file. 

Finally, an alternative is to copy the appropriate library file for 
your system (libFusionReactor.so / libFusionReactor-linux-x86_64.so) 
to a folder already within the library path (eg /opt/jrun4/bin)  

-PATH, CLASSPATH and LD_LIBRARY_PATH
The CLASSPATH environment variable tells the Java virtual machine where to find the class libraries, such as the jdmktk.jar file.

The PATH environment variable specifies the location of executable files, for example, the proxygen tool and the mibgen compiler.

If needed, the LD_LIBRARY_PATH environment variable specifies the location of native libraries, for example, the libstat.so kernel statistics library


--------------------------------JVM perf training----------------------------------

Microbenchmarking -JMH
----------------------
cmpxchg

jdk 
native -> private lib -> public java.util....
JMH
asm

asm lang lib
put library at - --- /jre/lib/amd64/server/hsdis-amd64.so

Perf killers
-memory access
-locks

OS-running and blocking Q
LOCKS ARE BAD compare and exchange are better than lock
-(cost of retrial)
-starvation

New CPU instruction added to 8: witness XAdd(memLocation, increment)  [wait free ]


incrAndGet used compAndXchange in jdk 7 but uses XAdd in JDK8

Macro profilers
---------------

savePoint: time when all info are taken for JVM. All threads are stopped .. for GC etc. Glopal flag is checked by all threads. 
-s/w prof overhead in profiling time. No applicable for production
-no idea of h/w, filesystem... what is slow disk , filsystem
-sampling profiler: b/w save point so no idea when problem occured.


-perf is good profiler
-Need java and system profiler together
-jStack 

System profiler:
-perf_events aka perf: no s/w instrumentation, only H/w ins. Should be used in linux. No of event > no of PMC

CPU with spcl types of register
normal register
PMC: perf monitor Register

-events like cpu, pagefault cache-miss.. etc 

-JDK 8 added -fno-omit-frame-pointer -> preserve frame pointer. RBP register used as the base pointer of the stack
-overhead is < 1%. So use it freely

-Start java program with this option and then use perf. -X:+PerfFramePointer
-show perf the debugSymbol file to convert address Vs methodName at the address

rxNetty better than tomCat.

-perf & analyser

-front end cycle - instruction eneded up in RS
-back end cycle

RS: reservationStation
ROB: reorderingBuffer

-pushing into diff cache line
	-paddedAtomic class
	
-memory and obj layout: 	

likwid - tool to measure CPU reg
likwid -topology

java -XX:+PrintFlagsFinal
java -XX:+PrintFlagsFinal -XX:+UnlockDiagnosticVMOptions
java -XX:+PrintFlagsFinal -XX:+UnlockDiagnosticVMOptions -XX:+UnlockDiagnosticVMOptions 

PrintAssembly

Need Hsdis.dll / .so in location


Tools: 
-JCStress: concurrency stress: best concrr stress 
-perf: best profiling 
-perf report
-analyser
-jmh
-likwid (need to knw the kernel version)
-mpstat: (microprocessor )mpstat -P ALL
-vmstat: thread block status (park Vs unpark)
-ganglia + perf [hadoop / spark]
-solaris analyser (best): get some xtra from perf map the h/w counter with code
-JOL(openJDK):memory and obj layout 
-JOC: java only profiler
-Tracers
		-sa(jdk):tools to look at methodtable: part of JDK sa. Best debugger to look at heap and non heap portion. Need HSDB...
		-stap: system tap. Best traces. Bit complex(written in script version of C). Use /Good for heap profile. 
			-stap -L 'kernel.function("vfs.read")'
			-goto source page on net documentation... alll ex
			-pageCache
			-GC
				-stap -L 
				
		-lttng:linux tracing toolkit new gen. Tracer. 
		-ebps: extented berkley packet filters
-jcmd: collect all info  related to pid. with prop flagsAll gives all flag of running app
	-{manageble} - can be on / off remotely ie. can be changed on running application
-jmap: heapDump
-jvm tools: jcmd, jps
-jinfo <pid>
-GC: printGCDetails + printGCtimestamp / dateStamp + GCLogs
-jstat: jstat -gcutil <PID> <refreshNoOfMilliSec> 
-jhat
-read heapDump:
	-sa
-visualvm - load heapDump -> classes , sort on memory -> instances, show nearest GC root. attach source code to Visualvm
-gdb: coredump: need OS tool
-for window windbg
-jcore: create 
Java only profilers:
--------------------
-jmc: java only profiling(no OS etc) - paid:  Sampling profile. does not instrumenation so can be used at production
	- on/off no xtra cost
	- only on oracle jdk and not openJdk
-honestProfiler: java only, good. Open source
-JFR: dont profile at savepoint Java flight recorder. 
	-start dump and stop. 
****-vmstat: snapshot of machine.
	-1st thng for profiling
	-si so > 0 not performing well 

-iostat

-all my write directly go to pageCache and not to buffer
-debug symbol: linuxDebugSymbols


uname -r : linux ver. 
right debug symbols are not installed 

VMSTAT
Perf
analyzer / jmc


r run
b block
si swap in
so swap out



context swtch high
	- too many threads and not match to th eh/w
	- too many locks
Formula for no of threads:

Nthreads = Ncpu * Ucpu * (1 + w/c) wait to compute
iterations .. bind no of threads to CPU
start with noThread = noCpu
otherwise lot of contextSwitching

JMX - threadMxBean  	use JConsole


onErrorHeapDump 
jcmd (online, overhead)



hadoop etc 
negio
perf + ganglia

-bind the thread to CPU
-taskset -c 1,5: stick app to cpu core 1 7 5

volatile:
- variables cannot be re ordered
- var cannot be cached
- var follow happens before relationship
  lock addl $0x0


-sp stack pointer
rsp - 64 bit stack pointer


Atomic varaibles / classes:
1) reordering, caching, happens before semantics of atomic class are identical of a volatile class
2) this guarantee does not apply to certain case method like lazy set, weak comparator 
3) these spcl functn provide the guarantee of ordering without visibility



mod is slow as div is expensive operation in assmbly .. as CPU 1 port for DIV . Right shift ()
lock

MESI: modfd .. shared ..

false sharing problem??

TO explore:

?? Tungsten is for Spark (CPU opt for spark)
?? RDMA(remote direct .....)




Q
--
?how to read JMH output- 
?How to get to know for wait free alternative, in jdk 
?disable hyperThrd in server ?
?How to use perf for distributed applications: yes
?There is concept of internal and external in JOL

Spark:
?See logs statements in map(funct)
?How to optimize spark application, why slow, not all executors are getting resources



Day 2

JVM
----
-Vtables/methodTable maintained by JVM, every class has acorrsponding methodTable. Building methodTable at time of classLoading
-class has memory structure
-class loading: method structure and memory structure
-stap(impl of solaris dtrace): system tap. Best traces. Bit complex(written in script version of C).  
-sa: tools to look at methodtable: part of JDK sa. Best debugger to look at heap and non heap portion.
-lttng:linux tracing toolkit new gen. Tracer. 
-ebps: extented berkley packet filters
-lttng:linux tracing toolkit new gen 
-threadlocal <...> , to -XX:-UseTLab to switch off default memory allocation. + to on , - to off
-profiler(sampling.. ) vs tracer(deeper dive, like GC called with what param kind of use cases.)
-heap area: yg, tg, pg
-nonHeap area: JVM code, IO buffer
-JVM : 
	-single port for jump, so for loop takes time
	-GC, interpreter,profiler,dynamicCompiler
		-dynmcComp:[-client, -server, -XX:+TC (TieredCompositon)]. 
			-NetBeans uses -client (quickly optimize)
			- -Server
			- -XX:+TC (TieredCompilation): mix of both above. Optional in java 7 i.e false. In java 8 its true. at startup use client and then....
	-cold cachce: dynamicCompiled code kept at cold cache..
	-branch prediction
	-CPU uses spcl interpretation for branch prediction 
	-Jump: all loops(taken care by dynamic compiler) & methods(inline)
	-all method in java are virtual
	-inline virtual functions are difficult, java does that.
	-scala runs on JVM
	-java -XX:+PrintFinalFlags [java -XX:+PrintFlagsFinal -XX:+UnlockDiagnosticVMOptions]
	-JVM escape analysis
	-inlining: static,final,private  always. virtual: often, reflection: sometime
		-javac adds null check ..  that we see when npe is thrown. 
		-JVM rermoves unneccry null check (JVM escape analysis)
		-
	-OSR: on stack replacement technique
	-never executed, zombie, %, !, too big, not enterant(made some assumption but now it does n ot hold)
		- !: exception handling is happening in code. Its expensive
		- n: native method
		- %: OSR - for top level methods like main.. main with loop 
		- zombie: no longer used and candidate for GC
		- made non reentrant:
		- tierCompilation
		- too big
	-JumpTable: if the TypeProfileWidth > 2 , jvm uses it
-HeapDumpAnalysis
	-heapDump(heapdump) vs coreDump(heapdump osdump and non heapdump):

-top	
-JNA: java native API (jna.jar and jna.so)
	-java taskset: bind thrds to cpu
cat /proc/cpuinfo	
-openhft: high frequency trading
	-JTA: java thread affinity: can bind thrds to CPU. 
	Executor.newFixwdThreadPool(4, new AffinityThreadFActory("",SAME_CORE, DIFFERENT_SOCKET,ANY))... binds threads to cpu
	to check 
	Affinity.dumpLocks
	
-nonblockinghashmap better than concurrnt hashMap
-RandomClass uses the lock - use threadLocalRandom does not uses lock
-false sharing: in java counting not with false sharing(if core > 8) LongAdder, LongAccumulator / DoubleAdder, DoubleAccumulator

Some blogs / forums + reference book info + emailId

mohit.riverstone@gmail.com

Q
---
? Some blogs/forums + reference book info
- googleGroups: (mechanical-Sympathy), 
- jeremymanson.blogspot.com
- Cliff click (cliffc.org\blog)
- Dave dice oracle blog(David dice)
- NitSan's: 
- Alexsey shipiney
books
- Java perf tunning: scott oaks... 
- java perf by charlie han	
- openJdk source

?
?
*********************************************************

Sizes of primitive types
boolean & byte -- 1
char & short -- 2
int & float -- 4rfg
long & double -- 8

Each object has a certain overhead for its associated monitor and type information, 
as well as the fields themselves. Beyond that, fields can be laid out pretty much 
however the JVM sees fit 

public class SingleByte
{
    private byte b;
}
vs

public class OneHundredBytes
{
    private byte b00, b01, ..., b99;
}

100 instance of 1 byte VS 1 instance of 100 byte
-On a 32-bit JVM, 
	-I'd expect 100 instances of SingleByte to take 1200 bytes (8 bytes of overhead + 4 bytes for the field due to padding/alignment).
	-1 instance with 100 byte leading to it taking 408 bytes (= 8 bytes overhead + 4 * 100 aligned/padded bytes).

-On a 64-bit JVM, data may differ

**For modern JDK and 64bit architecture object has 12-bytes header and padding by 8 bytes - so minimum object size is 16 bytes. 

*********************************************************
english:
https://www.youtube.com/watch?v=rjWd8U-6jbA&list=PL6BDo90oiwpS4_AM1c0s0ozpROeE2A9ff

change happens from 1st person to 3rd person

present simple
	-take infinitive form of verb: I work, it works, do I work, does it work, I do not work, It does not work
	-facts and permanent situations: cows eat grass, 
	-thngs that happen regularly or often
	-demonstration, or descrbng series of events
present continous
	-temp actions happeneing now or in progress: I am making a sandwich
	-progressive change / actions that are evolving: I am getting fatter, Climate is getting warmer 
	-future plans and arrangements: what r u doing tonight? I am going to Itraly next week, what bar are we going for drinks tomorrow 
	-in case of feeling present simple and present contnous zre correct: my leg hurts / My leg is hurting



*****
http://www.talkenglish.com/Speaking/Basics/Speaking_Basics_III.http://www.talkenglish.com/speaking/basics/speaking_basics_i.aspxaspx
Basics I
--------

Basics II
---------
"I'm calling to tell you about my day."
"I'm calling to accept your invitation."
"I'm calling to answer your question."
"I'm calling to book a reservation at your restaurant."
"I'm calling to complain about something."
"I'm calling to thank you."
"I'm calling to support your decision."
"I'm calling to remind you of our dinner plans."
"I'm calling to report a lost wallet."
"I'm calling to receive my prize."

"I'm working on a big project."
"I'm working on training my dog."
"I'm working on making new friends."
"I'm working on educating myself."
"I'm working on my homework."
"I am working on painting a house."
"I am working on a new idea."
"I am working on my computer."
"I'm working on my website."

"I'm sorry to be so late."
"I'm sorry to hear about your sick mother."
"I'm sorry to waste your time."
"I'm sorry to make you feel so sad."
"I'm sorry to frighten you."
"I'm sorry to disagree with your decision."
"I'm sorry to call so late."
"I'm sorry to admit what I did."
"I'm sorry to end this relationship.

"I'm thinking of checking out the new movie."
"I'm thinking of filming my vacation."
"I'm thinking of following a healthy diet."
"I'm thinking of handing out flyers describing our business."
"I'm thinking of increasing my work load."
"I am thinking of introducing myself to him."
"I am thinking of launching a new website."
"I am thinking of moving to a new city."
"I am thinking of offering her the position."
"I am thinking of opening up a store."

"I'll help you cook dinner tonight."
"I'll help you raise money for your charity."
"I'll help you register for your class online."
"I'll help you move to your new house."
"I'll help you prevent that from happening again."
"I will help you park your car."
"I will help you provide all the information you need."
"I will help you realize your potential."
"I will help you stop smoking."
"I will help you shop for groceries."

"I'm dying to relax on the beach."
"I'm dying to pick some fresh fruit."
"I'm dying to order some desserts."
"I'm dying to find out if I got the job."
"I'm dying to move to a bigger house."
"I'm dying to look at all the work you've done."
"I'm dying to learn more about you."
"I'm dying to introduce you to my parents."
"I'm dying to expand my business."
"I'm dying to check my score on the test."

"It's my turn to walk you home."
"It's my turn to do laundry."
"It's my turn to work late."
"It's my turn to take out the trash."
"It's my turn to choose where we eat."
"It is my turn to pay for dinner."
"It is my turn to roll the dice."
"It is my turn to provide an answer."
"It is my turn to try and play the game."
"It is my turn to attempt solving the problem."


It's hard for me to + (verb)

"It's hard for me to accept what you are telling me."
"It's hard for me to argue your point."
"It's hard for me to balance my check book."
"It's hard for me to concentrate on the task."
"It's hard for me to consider your other options."
"It's hard for me to depend on you."
"It is hard for me to decide where to go tonight."
"It is hard for me to explain my actions."
"It is hard for me to guarantee your success."
"It is hard for me to handle so much pressure."


I'm having a hard time + (verb-ing)

"I'm having a hard time writing."
"I'm having a hard time understanding you."
"I'm having a hard time answering your question."
"I'm having a hard time downloading songs to my iPod."
"I'm having a hard time agreeing to the terms."

"I'm having an extremely hard time trusting you."
"I'm having an extremely hard time with my wife."
"I'm having a very hard time finding a job."
"I'm having a very hard time finding parts for my car."




Basics III
----------
"How often do you exercise?"
"How often do you change your password?"
"How often do you help out at school?"
"How often do you listen to your MP3 player?"
"How often do you need to go to the dentist?"
"How often do you receive your magazine in the mail?"
"How often do you report to your supervisor?"
"How often do you stretch before working out?"
"How often do you talk to your parents?"
"How often do you travel?"

"Do you want me to pick up the kids?"
"Do you want me to fix your flat tire?"
"Do you want me to help you read that book?"
"Do you want me to remind you?"
"Do you want me to remove my shoes?"


"I want you to come over."
"I want you to make a decision."
"I want you to water the flowers."
"I want to understand what you are trying to say."
"I want to be better at swimming."
"I want to be more involved at church."


"What do you think about having a cup of tea with me?"
"What do you think about working overtime next week?"
"What do you think about waiting in line for tickets?"
"What do you think about sailing?"
"What do you think about staying here another night?"
"What do you think about retiring from your job?"
"What do you think about planting new trees in the backyard?"
"What do you think about offering to babysit?"
"What do you think about living in a new city?"
"What do you think about filming our vacation?"


Why don't we + (verb)

"Why don't we go bowling tonight?"
"Why don't we pick some fresh flowers?"
"Why don't we play a game of chess?"
"Why don't we save more money?"
"Why don't we remember this place?"
"Why don't we test this before using it?"
"Why don't we try and do it again?"
"Why don't we post our results online?"
"Why don't we gather more firewood?"
"Why don't we earn more money?"

-It's too bad that

"It's too bad that she lost her job."
"It's too bad that you have to go."
"It's too bad that I found out about it."
"It's too bad we will not be there on time."
"It's too bad that tickets are all gone to that concert."
"It's too bad that it is supposed to rain."
"It's too bad that she got hurt."
"It's too bad that my work has to lay off people."
"It's too bad that you do not understand."


-You could have + (past participle)

Using 'could have' you are speaking about something that was, should be or would be. You are stating that they had other options 
that could have been chosen.

"You could have completed it sooner."
"You could have blown your chance."
"You could have done better on your exam."
"You could have given me more time to get ready."
"You could have heard that from someone else."
"You could have sent that package first class."
"You could have slept a little longer."
"You could have written him a letter."
"You could have thought of something to do."
"You could have upset her by saying that."

-If I were you, I would + (verb)
Here you are giving an example of what decision YOU would do given the circumstances. 
This can be in past tense or in a conditional present.

"If I were you, I would enjoy my vacation."
"If I were you, I would explain what happened."
"If I were you, I would continue working until it is done."
"If I were you, I would book my reservations now."
"If I were you, I would answer the question."

By adding 'have' after the word 'would' you are talking about something in the past tense
"If I were you, I would have enjoyed my vacation."
"If I were you, I would have explained what happened."
"If I were you, I would have continued working until it was done."
"If I were you, I would have booked my reservations now."
"If I were you, I would have answered the question."

-It's gonna be + (adjective)

		You're informing someone what something is going to be like. 
		This could be something you are going to do, see or feel.

		"It's going to be delicious."
		"It's gonna be easy."
		"It's gonna be depressing."
		"It's going to be exciting."
		"It's going to be disgusting."

		You can also add 'he or she' or a person's name to describe how they might react to something.
		
		"He is going to be tough to deal with."
		"He is going to be terrific at that."
		"She is going to be relieved to hear that."
		"She is going to be scared after watching that movie."
		"Sally is going to be successful."
		"Mike is going to be grumpy after I tell him."



-It looks like + (noun)

	You could be describing how something is similar or appears to be by the way it looks.	
	"It looks like a balloon."
	"It looks like a jellyfish."
	"It looks like a banana."
	"It looks like a fish."
	You can also use 'it looks like' to describe something that might be in the future.
	
	Here are some examples:
	
	"It looks like it's going to rain."
	"It looks like it's going to be fun."
	"It looks like it's going to be a long day."

-That's why + (subject + verb)

		That's' is short for 'that is.' Here you are telling someone 'because of this' or 'therefore.'

		Here are some examples:

		"That's why people admire you."
		"That's why she appears so happy."
		"That's why babies crawl before they can walk."
		"That's why Pam cries at sad movies."
		"That's why you fail to understand."
		"That is why you help out people in need."
		"That is why you try and include everyone."
		"That is why you lock your doors when you leave home."
		"That is why she smiles when you walk by."
		"That is why you use it for emergencies."		

*********************************************************

Urls:
HDFS
		
Hadoop operations:
-
Hadoop yarn internals
-
SparkInternals
-
HBase
-
Oozie
-
Hadoop security
-Kerbros
Hadoop serialization issues:

-Kryo
?Why kryo and not Java serialization
?Where to use it


*****************************************************************************************************************


JavaDocs

http://docs.oracle.com/javase/7/docs/technotes/tools/windows/javadoc.html




Java Collections:
-HashMap | ConcurrentHashMap | nonblockingHashmap 
-Generics

Threads:
-ThreadLocal
-Locks
-ConsumerProducer 

-Java security(last)
-DesignPatterns

JDK - 8
-lambda + its internals 
-
 
Hadoop 
-HDFS
-HBase
-Oozie
-spark
	-anatomy of a program/lifeCycle
	-why faster than Mapreduce
-zooKeeper
-Security in all
-serialization/compression
	-protocolBuffer
	-parquet
-UseCase
	-trending etc
-kafka
	-https://www.infoq.com/articles/apache-kafka

-Distributed data structures like locks, queues, barriers, and latches	

To Read
https://www.javacodegeeks.com/2016/08/functional-approach-logging-apache-spark.html



Q&A
1.What is Hadoop framework?
2.On What concept the Hadoop framework works?
3.what is HDFS?
4.what is the patition tolerance in hadoop?
5.what is MAP REDUCE?
6.What Mapper does?
7.what is meaning Replication factor?
8.what is the default replication factor in HDFS?
9.what is the typical block size of an HDFS block?
10.what is a datanode?	
11.what is namenode?	
12.NameNode is single point of failure . what are remedies to overcome  this problem explain?
13.How does master slave architecture in the Hadoop?
14.How many maps are there in a particular Job?
15.What is the Reducer used for?
16.How many instances of JobTracker can run on a Hadoop Cluser?
17.how does job tracker knows whether tasktracker is running or not?
18.What alternate way does HDFS provides to recover data in case a Namenode, 	without backup, fails and cannot be recovered?
19.what is the scalability hadoop explain?
20.where exactly job tracker runs?
21.how does namenode knows whether datanode is running or not?
22.where exactly task tracker runs?
23.what is the size of each split"
24.explain what is FSImage and EditLogs files
25.what is bigdata?


		Paper-2(Hadoop)(10 to 11 am)
1)What is a commodity hardware? Does commodity hardware include RAM?
2)Is Namenode also a commodity?
3)What is a heartbeat in HDFS?
4)Are Namenode and job tracker on the same host?
5)If a particular file is 50 mb, will the HDFS block still consume 64 mb as the default size?
6)A user is like you or me, who has some query or who needs some kind of data.Is client the end user in HDFS?
7)On what basis data will be stored on a rack?
8)What is a Secondary Namenode? Is it a substitute to the Namenode?
9)Why ‘Reading‘ is done in parallel and ‘Writing‘ is not in HDFS?
10)What is a JobTracker in Hadoop? How many instances of JobTracker run on a Hadoop Cluster?
11)What is a Task Tracker in Hadoop? How many instances of TaskTracker run on a Hadoop Cluster
12)What is configuration of a typical slave node on Hadoop cluster? How many JVMs run on a slave node?
13)Does MapReduce programming model provide a way for reducers to communicate with each other? In a MapReduce job can a reducer communicate )with another reducer?
15)What is the meaning of speculative execution in Hadoop? Why is it important?
16)If reducers do not start before all mappers finish then why does the progress on MapReduce job shows something like Map(50%) Reduce(10%)? Why reducers progress percentage is displayed when mapper is not finished yet?

17)What is the difference between a Hadoop database and Relational Database?
What is Hadoop framework?
18)On What concept the Hadoop framework works?
19)what is HDFS?
20)what is MAP REDUCE?
21)What Mapper does?
22)What is the InputSplit in map reduce software?
23)what is meaning Replication factor?
24)what is the default replication factor in HDFS?
25)Explain the WordCount implementation via Hadoop framework ?
26)Which interface needs to be implemented to create Mapper and Reducer for the Hadoop?
27)What is the InputFormat ?
28)How does Mappers run() method works?
29)what is the typical block size of an HDFS block?
34)How many maps are there in a particular Job?
36)What are the primary phases of the Reducer?
37)Explain the shuffle?	
40)It can be possible that a Job has 0 reducers?
41)What happens if number of reducers are 0?
42)How many instances of JobTracker can run on a Hadoop Cluser?
43)Explain the Reducer?s Sort phase?
44)Explain the core methods of the Reducer?
47)What is the use of Context object?
52)What are the key features of HDFS?
53)What is Fault Tolerance?

		Paper-3(Hadoop)(10 to 11 am)
1)What is a commodity hardware? Does commodity hardware include RAM?
2)Is Namenode also a commodity?
3)What is a heartbeat in HDFS?
4)Are Namenode and job tracker on the same host?
5)If a particular file is 50 mb, will the HDFS block still consume 64 mb as the default size?
6)A user is like you or me, who has some query or who needs some kind of data.Is client the end user in HDFS?
7)On what basis data will be stored on a rack?
8)What is a Secondary Namenode? Is it a substitute to the Namenode?
9)Why ‘Reading‘ is done in parallel and ‘Writing‘ is not in HDFS?
10)What is a JobTracker in Hadoop? How many instances of JobTracker run on a Hadoop Cluster?
11)What is a Task Tracker in Hadoop? How many instances of TaskTracker run on a Hadoop Cluster
12)What is configuration of a typical slave node on Hadoop cluster? How many JVMs run on a slave node?
13)Does MapReduce programming model provide a way for reducers to communicate with each other? In a MapReduce job can a reducer communicate )with another reducer?
15)What is the meaning of speculative execution in Hadoop? Why is it important?
16)If reducers do not start before all mappers finish then why does the progress on MapReduce job shows something like Map(50%) Reduce(10%)? Why reducers progress percentage is displayed when mapper is not finished yet?
17)What is the difference between a Hadoop database and Relational Database?
What is Hadoop framework?
18)On What concept the Hadoop framework works?
19)what is HDFS?
20)what is MAP REDUCE?
21)What Mapper does?
22)What is the InputSplit in map reduce software?
23)what is meaning Replication factor?
24)what is the default replication factor in HDFS?
25)Explain the WordCount implementation via Hadoop framework ?
26)Which interface needs to be implemented to create Mapper and Reducer for the Hadoop?
27)What is the InputFormat ?
28)How does Mappers run() method works?
29)what is the typical block size of an HDFS block?
34)How many maps are there in a particular Job?
36)What are the primary phases of the Reducer?
37)Explain the shuffle?	
40)It can be possible that a Job has 0 reducers?
41)What happens if number of reducers are 0?
42)How many instances of JobTracker can run on a Hadoop Cluser?
43)Explain the Reducer?s Sort phase?
44)Explain the core methods of the Reducer?
47)What is the use of Context object?
52)What are the key features of HDFS?
53)What is Fault Tolerance?

HDFS Questions

1. Filesystems that manage the storage across a network of machines are called 
A. Distributed Filesystems 
B. Clusters
C. Partitions
D.Server


2. Hadoop comes with a distributed filesystem called
A.NDFS
B.GFS
C.HDFS
D.HFS


3.HDFS stands for
A. Hadoop Dynamic Filsystem
B. Hadoop Debug Filesystem
C. Hadoop Dangerous Filesystem
D. None of the above


4. HDFS is a filesystem designed for
A. Storing large files
B. Commodity Hardware
C. To Support Nutch
D. All Correct


5. Hadoop clusters running today that stores
A. bytes of data
B. gigabytes of data
C. petabytes of data
D. megabytes of data


6. HDFS is built around the idea that the most efficient data processing pattern is a
A. write-once, read-many-times
B. write-many-times, read-many-times
C. write-once, read-once
D. write-many-times, read-once


7. Commodity Hardware means more expensive
A. True
B. False


8. Which is not in HDFS?
A. Namenode
B. Secondary Datanode
C. Datanode
D. Secondary Namenode


9. Namenode holds ................. in memeory
A. Text files
B. Images
C. Metadata
D. Rawdata


10. As a rule of thumb, each file, directory, and block takes about 
A. 150 bytes
B. 4 kb
C. 64 mb
D. 1 byte
E. None of the above


11. What is the  purpose of  Block in HDFS?
A. We can modify data
B. We can't see data
C. We can see, but cannot modify data
D. There is no block in HDFS


12. What is Block?
A. A logical storage
B. A physical storage
C. It is not storage
D. None of the above


13. The typical Block size is?
A. 128 mb
B. 256 mb
C. 64 mb
D. 512 mb


14. The default block size is?
A. 128 mb
B. 512 mb
C. 256 mb
D. 64 mb



15. Blocks are a fixed size?
A. True
B. False


16. Will list the blocks that make up each file in the filesystem?
A. hadoop fsck / -files -blocks
B. hadoop fs -ls
C. hadoop blocks / -ls
D. hadoop fs -mkdir


17. An HDFS cluster has two types of nodes operating in a master-worker pattern:
A. Namenode- Master, Datanode-Worker
B. Namenode- Worker, Datanode- Master
C. Namenode- Master, Secondary Namenode- worker
D. None of the above


18. The namenode manages the filesystem
A. Blocks
B. Namespace
C. Raw data
D. Secondary Namenode


19. Without the namenode,
A. The filesystem can run
B. The filesystem cannot run
C. The filesystem can run, but cannot access
D. A & C


20. What is Secondary Namenode?
A. It is backup node for Namenode
B. It is optional node in HDFS
C. Without Secondary Namenode, Filesystem cannot work
D. We can replace Namenode with Secondary Namenode


21. Namenode is Single point of failure
A. True
B. False


22. Minimum replication factor in HDFS is
A. 1
B. 0
C. -1
D. 3


23. Default replication factor in HDFS is
A. 2
B. 3
C. 1
D. None


24. Maximum replication factor in HDFS is
A. 64
B. 512
C. 256
D. 128
E. None


25. Hadoop is written in
A. C++
B. Erlang
C. Bigdata
D. Java


26. Whats the default  port where the dfs namenode web ui will listen on
A. 8080
B. 5070
C. 50070
D.8085


27. HDFS is block structured file system" means 
A. In HDFS individual files are broken into blocks of a fixed size
B. in HDFS blocks are partitioned into individual files
C. None


28. You can choose replication factor per directory ?
A. True
B. False


29. You can choose replication factor per file in a directory?
A. True
B. False




30. You can choose replication factor per block of a file 
A. True
B. False


31. To list the the number of files in a HDFS directory?
A. hadoop fs ls
B. hadoop ls -fs
C. hadoop fs -ls
D. ls


32. To create a directory in HDFS?
A. hadoop fs mkdir
B. hadoop fs -mkdir
C. mkdir
D. hadoop fs -cat


33. To copy file from your local directory to HDFS?
A. hadoop fs -put <hdfsfile> <localfile>
B. hadoop fs -copy <localfile> <hdfsfile>
C. hadoop fs -put <localfile> <hdfsfile>
D. hadoop fs -copyFromLocal <hdfsfile> <localfile>


34. Can we do online transactions(OLTP) with HDFS?
A. True
B. False


35. chmod command for
A. to change directory to file
B. to change file permissions only
C. to change directory permissions only
D. B & C


36. can we create a file in HDFS?
A. Yes
B. No


37. Can we create a directory in HDFS?
A. Yes
B. No



38. How to create a file in HDFS?
A. hadoop fs -mkfile
B. hadoop fs mkfile
C. cannot create file
D. None


39. How to create a directory in HDFS?
A. hadoop fs -mkdir
B. hadoop fs mkdir
C. hadoop fs -ls
D.cannot create directory


40. What will get by this command 'hadoop dfsadmin -safemode get'.
A. Namenode will ON
B. Datanode will ON
C. Namenode will OFF
D. display status Namenode


41. Which command to switch Namenode safemode status as ON
A. hadoop dfsadmin -safemode on
B. hadoop dfsadmin -safemode get
C. hadoop dfsadmin -safemode enter
D. None


42. Which command to switch Namenode safemode status as OFF
A. hadoop dfsadmin -safemode off
B. hadoop dfsadmin -safemode leave
C. hadoop dfsadmin -safemode get
D. None


43. Which command to get Namenode safemode status?
A. hadoop dfsadmin -safemode off
B. hadoop dfsadmin -safemode leave
C. hadoop dfsadmin -safemode get
D. None


44. hadoop fs -rm command for
A. to delete a directory from HDFS
B. to delete multiple files from HDFS
C. to delete a file from HDFS
D. All



45. hadoop fs -rmr command for
A. to delete a directory from HDFS
B. to delete multiple files from HDFS
C. to delete a file from HDFS
D. A & C


46. How to get report for HDFS?
A. hadoop dfsadmin -info
B. hadoop dfsadmin -report
C. hadoop dfsadmin -get report
D. All


47. What is Heartbeat time for Datanode?
A. 5 seconds
B. 6 seconds
C. 3 seconds
D. 10 seconds


48. Can we change Heartbeat time for Datanode?
A. Yes
B. No, It is not possible


49. In what time Secondary Namenode will communicate with Namenode?
A. 60 Minutes
B. 30 Minutes
C. 10 Minutes
D. 5 Minutes


50. Apache Hadoop is Open Source project to support Lucene and Nutch web search engines.
A. True
B.. False


51. In which file we will configure replication factor?
A. conf/core-site.xml
B. conf/mapred-site.xml
C. conf/hdfs-site.xml
D. conf/hadoop-env.xml


52. In which file we will configure hdfs?
A. conf/core-site.xml
B. conf/mapred-site.xml
C. conf/hdfs-site.xml
D. conf/hadoop-env.xml

53. In which file we will configure mapreduce?
A. conf/core-site.xml
B. conf/mapred-site.xml
C. conf/hdfs-site.xml
D. conf/hadoop-env.xml


54. In which file we will configure block size?
A. conf/core-site.xml
B. conf/mapred-site.xml
C. conf/hdfs-site.xml
D. conf/hadoop-env.xml


55. In which file we will configure JAVA_HOME path?
A. conf/core-site.xml
B. conf/mapred-site.xml
C. conf/hdfs-site.xml
D. conf/hadoop-env.xml


56. In which directory, default configuration files available?
A. bin
B. lib
C. conf
D. src


57. What is the command to format Namenode?
A. hadoop fs namenode -format
B. hadoop namenode -format
C. hadoop fs -format
D. hadoop namenode -clean


58. What is the command to start all daemon services?
A. start-dfs.sh
B. start-mapred.sh
C. start-all.sh
D. None


59. What is the command to start  HDFS services?
A. start-dfs.sh
B. start-mapred.sh
C. start-all.sh
D. None



60. What is the command to start mapreduce services?
A. start-dfs.sh
B. start-mapred.sh
C. start-all.sh
D. None


61. What is the command to stop all daemon services?
A. stop-dfs.sh
B. stop-mapred.sh
C. stop-all.sh
D. None


62. What is the command to stop HDFS services?
A. stop-dfs.sh
B. stop-mapred.sh
C. stop-all.sh
D. None


63. What is the command to stop mapreduce services?
A. stop-dfs.sh
B. stop-mapred.sh
C. stop-all.sh
D. None


64. By which command we will test, which services are running?
A. cps
B. mps
C. jps
D. run service


65. To install hadoop cluster, we need to have java in machine?
A. Yes
B. No


66. Abbreviation for IPV6 is?
A. Intranet protocol variety 6
B. Internet protocol vision 6
C. Internet protocol version 6
D. None




67. What are pre-requisites to avoid Namenode failures?
A. High Configured Machine
B. Take backup
C. Limit Scaling
D. All
E. A&C


68. In what conditions Namenode  will fail?
A. Software/Hardware failures
B. Unlimited Scaling
C. Running more number jobs
D. All


69. What will happen if Namenode fail?
A. Nothing will happen
B. All processes will stop
C. Datanode will take responsibility
D. B & C


70. Is there any backup node for Namenode in HDFS?
A. Yes
B. No


71. How to display file data in terminal?
A. hadoop fs -display 
B. hadoop fs -view
C. hadoop fs -cat
D. hadoop fs -gedit


72. How to Copy HDFS files to Local Disk?
A. hadoop fs -put <hdfsfile> <localfile>
B. hadoop fs -copy <hdfsfile> <localfile>
C. hadoop fs -copyToLocal <localfile> <hdfsfile>
D. hadoop fs -get <hdfsfile> <localfile>


73. How to move HDFS files to Local Disk?
A. hadoop fs -put <hdfsfile> <localfile>
B. hadoop fs -move <hdfsfile> <localfile>
C. hadoop fs -moveToLocal <hdfsfile> <localfile>
D. hadoop fs -get <hdfsfile> <localfile>



74. How many datanodes we need for typical hadoop cluster?
A. 3
B. 4
C. 5
D. 6


75. HDFS is like a
A. Database
B. Operating System
C. Datawarehouse
D. None


76.HDFS stands for
A. Hadoop Dynamic Filsystem
B. Hadoop Debug Filesystem
C. Hadoop Distributed Filesystem
D. None of the above


77. HDFS is a filesystem designed for
A. Storing large files
B. Commodity Hardware
C. To Support Nutch
D. All Correct


78. Hadoop clusters running today that stores
A. bytes of data
B. gigabytes of data
C. petabytes of data
D. megabytes of data


79. Default replication factor in HDFS is
A. 1
B. 2
C. 3
D. None


80. Maximum replication factor in HDFS is
A. 64
B. 128
C. 256
D. 512
E. None



81. Hadoop is written in
A. C++
B. Java
C. Bigdata
D. C Language


82. What's the default  port where the dfs namenode web ui will listen on
A. 8080
B. 5070
C. 8085
D. 50070


83. You can choose replication factor per block of a file 
A. True
B. False


84. To list the the number of files in a HDFS directory?
A. hadoop fs ls
B. hadoop ls -fs
C. hadoop fs -ls
D. ls


85. To create a directory in HDFS?
A. hadoop fs mkdir
B. hadoop fs -mkdir
C. mkdir
D. hadoop fs -cat


86. To copy file from your local directory to HDFS?
A. hadoop fs -put <hdfsfile> <localfile>
B. hadoop fs -copy <localfile> <hdfsfile>
C. hadoop fs -put <localfile> <hdfsfile>
D. hadoop fs -copyFromLocal <hdfsfile> <localfile>


87. What is Heartbeat time for Datanode?
A. 5 seconds
B. 6 seconds
C. 3 seconds
D. 10 seconds


88. Can we change Heartbeat time for Datanode?
A. Yes
B. No, It is not possible

89. In what time Secondary Namenode will communicate with Namenode?
A. 60 Minutes
B. 30 Minutes
C. 10 Minutes
D. 5 Minutes


90. Apache Hadoop is Open Source project to support Lucene and Nutch web search engines.
A. True
B.. False


---
HDFS Questions

1. Filesystems that manage the storage across a network of machines are called 
A. Distributed Filesystems 
B. Clusters
C. Partitions
D.Server
Answer: A

2. Hadoop comes with a distributed filesystem called
A.NDFS
B.GFS
C.HDFS
D.HFS
Answer: C

3.HDFS stands for
A. Hadoop Dynamic Filsystem
B. Hadoop Debug Filesystem
C. Hadoop Dangerous Filesystem
D. None of the above
Answer: D

4. HDFS is a filesystem designed for
A. Storing large files
B. Commodity Hardware
C. To Support Nutch
D. All Correct
Answer: D

5. Hadoop clusters running today that stores
A. bytes of data
B. gigabytes of data
C. petabytes of data
D. megabytes of data
Answer: C

6. HDFS is built around the idea that the most efficient data processing pattern is a
A. write-once, read-many-times
B. write-many-times, read-many-times
C. write-once, read-once
D. write-many-times, read-once
Answer: A

7. Commodity Hardware means more expensive
A. True
B. False
Answer: B

8. Which is not in HDFS?
A. Namenode
B. Secondary Datanode
C. Datanode
D. Secondary Namenode
Answer: B

9. Namenode holds ................. in memeory
A. Text files
B. Images
C. Metadata
D. Rawdata
Answer: C

10. As a rule of thumb, each file, directory, and block takes about 
A. 150 bytes
B. 4 kb
C. 64 mb
D. 1 byte
E. None of the above
Answer: A

11. What is the  purpose of  Block in HDFS?
A. We can modify data
B. We can't see data
C. We can see, but cannot modify data
D. There is no block in HDFS
Answer: C

12. What is Block?
A. A logical storage
B. A physical storage
C. It is not storage
D. None of the above
Answer: A

13. The typical Block size is?
A. 128 mb
B. 256 mb
C. 64 mb
D. 512 mb
Answer: A

14. The default block size is?
A. 128 mb
B. 512 mb
C. 256 mb
D. 64 mb
Answer: D 


15. Blocks are a fixed size?
A. True
B. False
Answer: A

16. Will list the blocks that make up each file in the filesystem?
A. hadoop fsck / -files -blocks
B. hadoop fs -ls
C. hadoop blocks / -ls
D. hadoop fs -mkdir
Answer: A

17. An HDFS cluster has two types of nodes operating in a master-worker pattern:
A. Namenode- Master, Datanode-Worker
B. Namenode- Worker, Datanode- Master
C. Namenode- Master, Secondary Namenode- worker
D. None of the above
Answer: A

18. The namenode manages the filesystem
A. Blocks
B. Namespace
C. Raw data
D. Secondary Namenode
Answer: B

19. Without the namenode,
A. The filesystem can run
B. The filesystem cannot run
C. The filesystem can run, but cannot access
D. A & C
Answer: B

20. What is Secondary Namenode?
A. It is backup node for Namenode
B. It is optional node in HDFS
C. Without Secondary Namenode, Filesystem cannot work
D. We can replace Namenode with Secondary Namenode
Answer: B

21. Namenode is Single point of failure
A. True
B. False
Answer: A

22. Minimum replication factor in HDFS is
A. 1
B. 0
C. -1
D. 3
Answer: A

23. Default replication factor in HDFS is
A. 2
B. 3
C. 1
D. None
Answer: B

24. Maximum replication factor in HDFS is
A. 64
B. 512
C. 256
D. 128
E. None
Answer: B

25. Hadoop is written in
A. C++
B. Erlang
C. Bigdata
D. Java
Answer: D

26. Whats the default  port where the dfs namenode web ui will listen on
A. 8080
B. 5070
C. 50070
D.8085
Answer: C

27. HDFS is block structured file system" means 
A. In HDFS individual files are broken into blocks of a fixed size
B. in HDFS blocks are partitioned into individual files
C. None
Answer: A

28. You can choose replication factor per directory ?
A. True
B. False
Answer: A

29. You can choose replication factor per file in a directory?
A. True
B. False
Answer: A



30. You can choose replication factor per block of a file 
A. True
B. False
Answer: B

31. To list the the number of files in a HDFS directory?
A. hadoop fs ls
B. hadoop ls -fs
C. hadoop fs -ls
D. ls
Answer: C

32. To create a directory in HDFS?
A. hadoop fs mkdir
B. hadoop fs -mkdir
C. mkdir
D. hadoop fs -cat
Answer: B

33. To copy file from your local directory to HDFS?
A. hadoop fs -put <hdfsfile> <localfile>
B. hadoop fs -copy <localfile> <hdfsfile>
C. hadoop fs -put <localfile> <hdfsfile>
D. hadoop fs -copyFromLocal <hdfsfile> <localfile>
Answer: A

34. Can we do online transactions(OLTP) with HDFS?
A. True
B. False
Answer: B

35. chmod command for
A. to change directory to file
B. to change file permissions only
C. to change directory permissions only
D. B & C
Answer: D

36. can we create a file in HDFS?
A. Yes
B. No
Answer: B

37. Can we create a directory in HDFS?
A. Yes
B. No
Answer: A


38. How to create a file in HDFS?
A. hadoop fs -mkfile
B. hadoop fs mkfile
C. cannot create file
D. None
Answer: C

39. How to create a directory in HDFS?
A. hadoop fs -mkdir
B. hadoop fs mkdir
C. hadoop fs -ls
D.cannot create directory
Answer: A

40. What will get by this command 'hadoop dfsadmin -safemode get'.
A. Namenode will ON
B. Datanode will ON
C. Namenode will OFF
D. display status Namenode
Answer: D 

41. Which command to switch Namenode safemode status as ON
A. hadoop dfsadmin -safemode on
B. hadoop dfsadmin -safemode get
C. hadoop dfsadmin -safemode enter
D. None
Answer: C

42. Which command to switch Namenode safemode status as OFF
A. hadoop dfsadmin -safemode off
B. hadoop dfsadmin -safemode leave
C. hadoop dfsadmin -safemode get
D. None
Answer: B

43. Which command to get Namenode safemode status?
A. hadoop dfsadmin -safemode off
B. hadoop dfsadmin -safemode leave
C. hadoop dfsadmin -safemode get
D. None
Answer: C

44. hadoop fs -rm command for
A. to delete a directory from HDFS
B. to delete multiple files from HDFS
C. to delete a file from HDFS
D. All
Answer: C


45. hadoop fs -rmr command for
A. to delete a directory from HDFS
B. to delete multiple files from HDFS
C. to delete a file from HDFS
D. A & C
Answer: D

46. How to get report for HDFS?
A. hadoop dfsadmin -info
B. hadoop dfsadmin -report
C. hadoop dfsadmin -get report
D. All
Answer: B

47. What is Heartbeat time for Datanode?
A. 5 seconds
B. 6 seconds
C. 3 seconds
D. 10 seconds
Answer: C

48. Can we change Heartbeat time for Datanode?
A. Yes
B. No, It is not possible
Answer: B

49. In what time Secondary Namenode will communicate with Namenode?
A. 60 Minutes
B. 30 Minutes
C. 10 Minutes
D. 5 Minutes
Answer: A

50. Apache Hadoop is Open Source project to support Lucene and Nutch web search engines.
A. True
B.. False
Answer: A

51. In which file we will configure replication factor?
A. conf/core-site.xml
B. conf/mapred-site.xml
C. conf/hdfs-site.xml
D. conf/hadoop-env.xml
Answer: C

52. In which file we will configure hdfs?
A. conf/core-site.xml
B. conf/mapred-site.xml
C. conf/hdfs-site.xml
D. conf/hadoop-env.xml
Answer: A
53. In which file we will configure mapreduce?
A. conf/core-site.xml
B. conf/mapred-site.xml
C. conf/hdfs-site.xml
D. conf/hadoop-env.xml
Answer: B

54. In which file we will configure block size?
A. conf/core-site.xml
B. conf/mapred-site.xml
C. conf/hdfs-site.xml
D. conf/hadoop-env.xml
Answer: C

55. In which file we will configure JAVA_HOME path?
A. conf/core-site.xml
B. conf/mapred-site.xml
C. conf/hdfs-site.xml
D. conf/hadoop-env.xml
Answer: D

56. In which directory, default configuration files available?
A. bin
B. lib
C. conf
D. src
Answer: D

57. What is the command to format Namenode?
A. hadoop fs namenode -format
B. hadoop namenode -format
C. hadoop fs -format
D. hadoop namenode -clean
Answer: B

58. What is the command to start all daemon services?
A. start-dfs.sh
B. start-mapred.sh
C. start-all.sh
D. None
Answer: C

59. What is the command to start  HDFS services?
A. start-dfs.sh
B. start-mapred.sh
C. start-all.sh
D. None
Answer: A


60. What is the command to start mapreduce services?
A. start-dfs.sh
B. start-mapred.sh
C. start-all.sh
D. None
Answer: B

61. What is the command to stop all daemon services?
A. stop-dfs.sh
B. stop-mapred.sh
C. stop-all.sh
D. None
Answer: C

62. What is the command to stop HDFS services?
A. stop-dfs.sh
B. stop-mapred.sh
C. stop-all.sh
D. None
Answer: A

63. What is the command to stop mapreduce services?
A. stop-dfs.sh
B. stop-mapred.sh
C. stop-all.sh
D. None
Answer: B

64. By which command we will test, which services are running?
A. cps
B. mps
C. jps
D. run service
Answer: C

65. To install hadoop cluster, we need to have java in machine?
A. Yes
B. No
Answer: A

66. Abbreviation for IPV6 is?
A. Intranet protocol variety 6
B. Internet protocol vision 6
C. Internet protocol version 6
D. None
Answer: C



67. What are pre-requisites to avoid Namenode failures?
A. High Configured Machine
B. Take backup
C. Limit Scaling
D. All
E. A&C
Answer: D

68. In what conditions Namenode  will fail?
A. Software/Hardware failures
B. Unlimited Scaling
C. Running more number jobs
D. All
Answer: D

69. What will happen if Namenode fail?
A. Nothing will happen
B. All processes will stop
C. Datanode will take responsibility
D. B & C
Answer: B

70. Is there any backup node for Namenode in HDFS?
A. Yes
B. No
Answer: B

71. How to display file data in terminal?
A. hadoop fs -display 
B. hadoop fs -view
C. hadoop fs -cat
D. hadoop fs -gedit
Answer: C

72. How to Copy HDFS files to Local Disk?
A. hadoop fs -put <hdfsfile> <localfile>
B. hadoop fs -copy <hdfsfile> <localfile>
C. hadoop fs -copyToLocal <localfile> <hdfsfile>
D. hadoop fs -get <hdfsfile> <localfile>
Answer: D

73. How to move HDFS files to Local Disk?
A. hadoop fs -put <hdfsfile> <localfile>
B. hadoop fs -move <hdfsfile> <localfile>
C. hadoop fs -moveToLocal <hdfsfile> <localfile>
D. hadoop fs -get <hdfsfile> <localfile>
Answer: C


74. How many datanodes we need for typical hadoop cluster?
A. 3
B. 4
C. 5
D. 6
Answer: B

75. HDFS is like a
A. Database
B. Operating System
C. Datawarehouse
D. None
Answer: B

76.HDFS stands for
A. Hadoop Dynamic Filsystem
B. Hadoop Debug Filesystem
C. Hadoop Distributed Filesystem
D. None of the above
Answer: C

77. HDFS is a filesystem designed for
A. Storing large files
B. Commodity Hardware
C. To Support Nutch
D. All Correct
Answer: D

78. Hadoop clusters running today that stores
A. bytes of data
B. gigabytes of data
C. petabytes of data
D. megabytes of data
Answer: C

79. Default replication factor in HDFS is
A. 1
B. 2
C. 3
D. None
Answer: C

80. Maximum replication factor in HDFS is
A. 64
B. 128
C. 256
D. 512
E. None
Answer: D


81. Hadoop is written in
A. C++
B. Java
C. Bigdata
D. C Language
Answer: B

82. What's the default  port where the dfs namenode web ui will listen on
A. 8080
B. 5070
C. 8085
D. 50070
Answer: D

83. You can choose replication factor per block of a file 
A. True
B. False
Answer: B

84. To list the the number of files in a HDFS directory?
A. hadoop fs ls
B. hadoop ls -fs
C. hadoop fs -ls
D. ls
Answer: C

85. To create a directory in HDFS?
A. hadoop fs mkdir
B. hadoop fs -mkdir
C. mkdir
D. hadoop fs -cat
Answer: B

86. To copy file from your local directory to HDFS?
A. hadoop fs -put <hdfsfile> <localfile>
B. hadoop fs -copy <localfile> <hdfsfile>
C. hadoop fs -put <localfile> <hdfsfile>
D. hadoop fs -copyFromLocal <hdfsfile> <localfile>
Answer: A

87. What is Heartbeat time for Datanode?
A. 5 seconds
B. 6 seconds
C. 3 seconds
D. 10 seconds
Answer: C

88. Can we change Heartbeat time for Datanode?
A. Yes
B. No, It is not possible
Answer: B
89. In what time Secondary Namenode will communicate with Namenode?
A. 60 Minutes
B. 30 Minutes
C. 10 Minutes
D. 5 Minutes
Answer: A

90. Apache Hadoop is Open Source project to support Lucene and Nutch web search engines.
A. True
B.. False
Answer: A

----


•	What is Hadoop? Brief about the components of  Hadoop.
•	What are the Hadoop daemon processes tell the components of Hadoop and functionality?
•	Tell steps for configuring Hadoop?
•	What is architecture of HDFS and flow?
•	Can we have more than one configuration setting for Hadoop cluster how can you switch between these configurations?
•	What will be your troubleshooting approach in Hadoop?
•	What are the exceptions you have come through while working on Hadoop, what was your approach for getting rid of those exceptions or errors?

•	How will you proceed if Namenode is down?
•	What will be your approach if Datanode is down?
•	How can you start the cluster?
•	How can you stop the cluster?
•	What is dfs.name.dir and dfs.data.dir is used for?
•	What is SSH?
•	What is password less SSH?
•	Why do we need password less SSH in Hadoop?
•	How can you transfer configuration files of Hadoop from one system to another system
•	Have you ever come across bind exception while configuring cluster? How you solved it? Why it comes
•	When do you get connection refused error? How can you solve this problem
•	Have you ever come across "no route to host" error? If yes how you solved?
•	What is socket timeout error? And where it effect in Hadoop cluster?
•	How unknown host errors make sense to you?
•	Could not replicate data have you ever come across this error if yes what is the probable reason behind it?
•	What is heap memory? How we use it in Hadoop cluster?
•	What is a zombie process in Linux?
•	What zero size file problem in Hadoop and what is the reason behind it?
•	What is over replicated, under replicated blocks give some scenarios
•	Have you ever come across " too many file open error"
•	How communication between clients-->Namenode->Datanode happens? Explain the steps by which file is send from client the HDFS.
•	what configuration settings will you implement for HBase and hive with Hadoop
•	Have you ever come across HMaster not running ?? If yes what was the reason and how you solved it?
•	What are the daemon processes for HBase?
•	What may be the problem behind region servers being not started.
•	How many way you can execute queries in hive?
•	Have you ever come across error when master initializes, but region servers do not. What solution did you synthesized from that?
•	What is the role of JVM in Hadoop
•	Have you heard about JPS command? How can you use it and what you need to install before using that command?
•	 How can you configure a client for HBase? If yes what were the settings you used?
•	What is zookeeper?
•	How we can load table in hive?
•	How can we start hive server?
•	What is thrift server?
•	Can you tell me basic syntax for connecting hive through JDBC ?
•	What are no SQL databases how they differ from relational database, what are the NSQL databases you know?
•	Have you ever come across java.lang.outofmemoryerror?
•	How will you check if Hadoop Daemons(Namenode, Datanode, Jobtracker, Tasktracker, and Secondrynamenode) are running?


•	Q1. What are the default configuration files that are used in Hadoop
As of 0.20 release, Hadoop supported the following read-only default configurations
- src/core/core-default.xml
- src/hdfs/hdfs-default.xml
- src/mapred/mapred-default.xml
•	Q2. How will you make changes to the default configuration files
Hadoop does not recommends changing the default configuration files, instead it recommends making all site specific changes in the following files
- conf/core-site.xml
- conf/hdfs-site.xml
- conf/mapred-site.xml
•	Unless explicitly turned off, Hadoop by default specifies two resources, loaded in-order from the classpath:
- core-default.xml : Read-only defaults for hadoop.
- core-site.xml: Site-specific configuration for a given hadoop installation.
•	Hence if same configuration is defined in file core-default.xml and src/core/core-default.xml then the values in file core-default.xml (same is true for other 2 file pairs) is used.
•	Q3. Consider case scenario where you have set property mapred.output.compress to true to ensure that all output files are compressed for efficient space usage on the cluster. If a cluster user does not want to compress data for a specific job then what will you recommend him to do ?
Ask him to create his own configuration file and specify configuration mapred.output.compress to false and load this file as a resource in his job.
•	Q4. In the above case scenario, how can ensure that user cannot override the configuration mapred.output.compress to false in any of his jobs
This can be done by setting the property final to true in the core-site.xml file
•	Q5. What of the following is the only required variable that needs to be set in file conf/hadoop-env.sh for hadoop to work
-	HADOOP_LOG_DIR
-	JAVA_HOME
- HADOOP_CLASSPATH
The only required variable to set is JAVA_HOME that needs to point to directory
•	Q6. List all the daemons required to run the Hadoop cluster
- NameNode
- DataNode
- JobTracker
- TaskTracker
•	Q7. Whats the default port that jobtrackers listens to
50030
•	Q8. Whats the default port where the dfs namenode web ui will listen on
50070
Posted by Aman at 9:04 AM 0 comments
Email ThisBlogThis!Share to TwitterShare to Facebook
Sunday, November 28, 2010
Java interview questions for Hadoop developer Part 3
Q21. Explain difference of Class Variable and Instance Variable and how are they declared in Java
Class Variable is a variable which is declared with static modifier.
Instance variable is a variable in a class without static modifier.
The main difference between the class variable and Instance variable is, that first time, when class is loaded in to memory, then only memory is allocated for all class variables. That means, class variables do not depend on the Objets of that classes. What ever number of objects are there, only one copy is created at the time of class loding.
•	Q22. Since an Abstract class in Java cannot be instantiated then how can you use its non static methods
By extending it
•	Q23. How would you make a copy of an entire Java object with its state?
Have this class implement Cloneable interface and call its method clone().
•	Q24. Explain Encapsulation,Inheritance and Polymorphism
Encapsulation is a process of binding or wrapping the data and the codes that operates on the data into a single entity. This keeps the data safe from outside interface and misuse. One way to think about encapsulation is as a protective wrapper that prevents code and data from being arbitrarily accessed by other code defined outside the wrapper.
Inheritance is the process by which one object acquires the properties of another object.
The meaning of Polymorphism is something like one name many forms. Polymorphism enables one entity to be used as as general category for different types of actions. The specific action is determined by the exact nature of the situation. The concept of polymorphism can be explained as “one interface, multiple methods”.
•	Q25. Explain garbage collection?
Garbage collection is one of the most important feature of Java.
Garbage collection is also called automatic memory management as JVM automatically removes the unused variables/objects (value is null) from the memory. User program cann’t directly free the object from memory, instead it is the job of the garbage collector to automatically free the objects that are no longer referenced by a program. Every class inherits finalize() method from java.lang.Object, the finalize() method is called by garbage collector when it determines no more references to the object exists. In Java, it is good idea to explicitly assign null into a variable when no more in us
•	Q26. What is similarities/difference between an Abstract class and Interface?
Differences- Interfaces provide a form of multiple inheritance. A class can extend only one other class.
- Interfaces are limited to public methods and constants with no implementation. Abstract classes can have a partial implementation, protected parts, static methods, etc.
- A Class may implement several interfaces. But in case of abstract class, a class may extend only one abstract class.
- Interfaces are slow as it requires extra indirection to find corresponding method in in the actual class. Abstract classes are fast.
Similarities
- Neither Abstract classes or Interface can be instantiated
•	Q27. What are different ways to make your class multithreaded in Java
There are two ways to create new kinds of threads:
- Define a new class that extends the Thread class
- Define a new class that implements the Runnable interface, and pass an object of that class to a Thread’s constructor.
•	Q28. What do you understand by Synchronization? How do synchronize a method call in Java? How do you synchonize a block of code in java ?
Synchronization is a process of controlling the access of shared resources by the multiple threads in such a manner that only one thread can access one resource at a time. In non synchronized multithreaded application, it is possible for one thread to modify a shared object while another thread is in the process of using or updating the object’s value. Synchronization prevents such type of data corruption.
- Synchronizing a method: Put keyword synchronized as part of the method declaration
- Synchronizing a block of code inside a method: Put block of code in synchronized (this) { Some Code }
•	Q29. What is transient variable?
Transient variable can’t be serialize. For example if a variable is declared as transient in a Serializable class and the class is written to an ObjectStream, the value of the variable can’t be written to the stream instead when the class is retrieved from the ObjectStreamthe value of the variable becomes null.
•	Q30. What is Properties class in Java. Which class does it extends?
The Properties class represents a persistent set of properties. The Properties can be saved to a stream or loaded from a stream. Each key and its corresponding value in the property list is a string
•	Q31. Explain the concept of shallow copy vs deep copy in Java
In case of shallow copy, the cloned object also refers to the same object to which the original object refers as only the object references gets copied and not the referred objects themselves.
In case deep copy, a clone of the class and all all objects referred by that class is made.
•	Q32. How can you make a shallow copy of an object in Java
Use clone() method inherited by Object class
•	Q33. How would you make a copy of an entire Java object (deep copy) with its state?
Have this class implement Cloneable interface and call its method clone().

•	What is a JobTracker in Hadoop? How many instances of JobTracker run on a Hadoop Cluster?
•	JobTracker is the daemon service for submitting and tracking MapReduce jobs in Hadoop. There is only One Job Tracker process run on any hadoop cluster. Job Tracker runs on its own JVM process. In a typical production cluster its run on a separate machine. Each slave node is configured with job tracker node location. The JobTracker is single point of failure for the Hadoop MapReduce service. If it goes down, all running jobs are halted. JobTracker in Hadoop performs following actions(from Hadoop Wiki:)
•	Client applications submit jobs to the Job tracker.
•	The JobTracker talks to the NameNode to determine the location of the data
•	The JobTracker locates TaskTracker nodes with available slots at or near the data
•	The JobTracker submits the work to the chosen TaskTracker nodes.
•	The TaskTracker nodes are monitored. If they do not submit heartbeat signals often enough, they are deemed to have failed and the work is scheduled on a different TaskTracker.
•	A TaskTracker will notify the JobTracker when a task fails. The JobTracker decides what to do then: it may resubmit the job elsewhere, it may mark that specific record as something to avoid, and it may may even blacklist the TaskTracker as unreliable.
•	When the work is completed, the JobTracker updates its status.
•	

•	Client applications can poll the JobTracker for information.

•	How JobTracker schedules a task?
•	The TaskTrackers send out heartbeat messages to the JobTracker, usually every few minutes, to reassure the JobTracker that it is still alive. These message also inform the JobTracker of the number of available slots, so the JobTracker can stay up to date with where in the cluster work can be delegated. When the JobTracker tries to find somewhere to schedule a task within the MapReduce operations, it first looks for an empty slot on the same server that hosts the DataNode containing the data, and if not, it looks for an empty slot on a machine in the same rack.
•	What is a Task Tracker in Hadoop? How many instances of TaskTracker run on a Hadoop Cluster
•	A TaskTracker is a slave node daemon in the cluster that accepts tasks (Map, Reduce and Shuffle operations) from a JobTracker. There is only One Task Tracker process run on any hadoop slave node. Task Tracker runs on its own JVM process. Every TaskTracker is configured with a set of slots, these indicate the number of tasks that it can accept. The TaskTracker starts a separate JVM processes to do the actual work (called as Task Instance) this is to ensure that process failure does not take down the task tracker. The TaskTracker monitors these task instances, capturing the output and exit codes. When the Task instances finish, successfully or not, the task tracker notifies the JobTracker. The TaskTrackers also send out heartbeat messages to the JobTracker, usually every few minutes, to reassure the JobTracker that it is still alive. These message also inform the JobTracker of the number of available slots, so the JobTracker can stay up to date with where in the cluster work can be delegated.
•	What is a Task instance in Hadoop? Where does it run?
•	Task instances are the actual MapReduce jobs which are run on each slave node. The TaskTracker starts a separate JVM processes to do the actual work (called as Task Instance) this is to ensure that process failure does not take down the task tracker. Each Task Instance runs on its own JVM process. There can be multiple processes of task instance running on a slave node. This is based on the number of slots configured on task tracker. By default a new task instance JVM process is spawned for a task.
•	How many Daemon processes run on a Hadoop system?
•	Hadoop is comprised of five separate daemons. Each of these daemon run in its own JVM. Following 3 Daemons run on Master nodes NameNode - This daemon stores and maintains the metadata for HDFS. Secondary NameNode - Performs housekeeping functions for the NameNode. JobTracker - Manages MapReduce jobs, distributes individual tasks to machines running the Task Tracker. Following 2 Daemons run on each Slave nodes DataNode – Stores actual HDFS data blocks. TaskTracker - Responsible for instantiating and monitoring individual Map and Reduce tasks.
•	What is configuration of a typical slave node on Hadoop cluster? How many JVMs run on a slave node?
•	Single instance of a Task Tracker is run on each Slave node. Task tracker is run as a separate JVM process.
•	Single instance of a DataNode daemon is run on each Slave node. DataNode daemon is run as a separate JVM process.
•	One or Multiple instances of Task Instance is run on each slave node. Each task instance is run as a separate JVM process. The number of Task instances can be controlled by configuration. Typically a high end machine is configured to run more task instances.
•	What is the difference between HDFS and NAS ?
•	The Hadoop Distributed File System (HDFS) is a distributed file system designed to run on commodity hardware. It has many similarities with existing distributed file systems. However, the differences from other distributed file systems are significant. Following are differences between HDFS and NAS
•	In HDFS Data Blocks are distributed across local drives of all machines in a cluster. Whereas in NAS data is stored on dedicated hardware.
•	HDFS is designed to work with MapReduce System, since computation are moved to data. NAS is not suitable for MapReduce since data is stored seperately from the computations.
•	HDFS runs on a cluster of machines and provides redundancy usinga replication protocal. Whereas NAS is provided by a single machine therefore does not provide data redundancy.
•	How NameNode Handles data node failures?
•	NameNode periodically receives a Heartbeat and a Blockreport from each of the DataNodes in the cluster. Receipt of a Heartbeat implies that the DataNode is functioning properly. A Blockreport contains a list of all blocks on a DataNode. When NameNode notices that it has not recieved a hearbeat message from a data node after a certain amount of time, the data node is marked as dead. Since blocks will be under replicated the system begins replicating the blocks that were stored on the dead datanode. The NameNode Orchestrates the replication of data blocks from one datanode to another. The replication data transfer happens directly between datanodes and the data never passes through the namenode.
•	Does MapReduce programming model provide a way for reducers to communicate with each other? In a MapReduce job can a reducer communicate with another reducer?
•	Nope, MapReduce programming model does not allow reducers to communicate with each other. Reducers run in isolation.
•	Can I set the number of reducers to zero?
•	Yes, Setting the number of reducers to zero is a valid configuration in Hadoop. When you set the reducers to zero no reducers will be executed, and the output of each mapper will be stored to a separate file on HDFS. [This is different from the condition when reducers are set to a number greater than zero and the Mappers output (intermediate data) is written to the Local file system(NOT HDFS) of each mappter slave node.]
•	Where is the Mapper Output (intermediate kay-value data) stored ?
•	The mapper output (intermediate data) is stored on the Local file system (NOT HDFS) of each individual mapper nodes. This is typically a temporary directory location which can be setup in config by the hadoop administrator. The intermediate data is cleaned up after the Hadoop Job completes.
•	What are combiners? When should I use a combiner in my MapReduce Job?
•	Combiners are used to increase the efficiency of a MapReduce program. They are used to aggregate intermediate map output locally on individual mapper outputs. Combiners can help you reduce the amount of data that needs to be transferred across to the reducers. You can use your reducer code as a combiner if the operation performed is commutative and associative. The execution of combiner is not guaranteed, Hadoop may or may not execute a combiner. Also, if required it may execute it more then 1 times. Therefore your MapReduce jobs should not depend on the combiners execution.
•	What is Writable & WritableComparable interface?
•	org.apache.hadoop.io.Writable is a Java interface. Any key or value type in the Hadoop Map-Reduce framework implements this interface. Implementations typically implement a static read(DataInput) method which constructs a new instance, calls readFields(DataInput) and returns the instance.
•	org.apache.hadoop.io.WritableComparable is a Java interface. Any type which is to be used as a key in the Hadoop Map-Reduce framework should implement this interface. WritableComparable objects can be compared to each other using Comparators.
•	What is the Hadoop MapReduce API contract for a key and value Class?
•	The Key must implement the org.apache.hadoop.io.WritableComparable interface.
•	The value must implement the org.apache.hadoop.io.Writable interface.
•	What is a IdentityMapper and IdentityReducer in MapReduce ?
•	org.apache.hadoop.mapred.lib.IdentityMapper Implements the identity function, mapping inputs directly to outputs. If MapReduce programmer do not set the Mapper Class using JobConf.setMapperClass then IdentityMapper.class is used as a default value.
•	org.apache.hadoop.mapred.lib.IdentityReducer Performs no reduction, writing all input values directly to the output. If MapReduce programmer do not set the Reducer Class using JobConf.setReducerClass then IdentityReducer.class is used as a default value.

•	What is the meaning of speculative execution in Hadoop? Why is it important?
•	Speculative execution is a way of coping with individual Machine performance. In large clusters where hundreds or thousands of machines are involved there may be machines which are not performing as fast as others. This may result in delays in a full job due to only one machine not performaing well. To avoid this, speculative execution in hadoop can run multiple copies of same map or reduce task on different slave nodes. The results from first node to finish are used.
•	When is the reducers are started in a MapReduce job?
•	In a MapReduce job reducers do not start executing the reduce method until the all Map jobs have completed. Reducers start copying intermediate key-value pairs from the mappers as soon as they are available. The programmer defined reduce method is called only after all the mappers have finished.
•	If reducers do not start before all mappers finish then why does the progress on MapReduce job shows something like Map(50%) Reduce(10%)? Why reducers progress percentage is displayed when mapper is not finished yet?
•	Reducers start copying intermediate key-value pairs from the mappers as soon as they are available. The progress calculation also takes in account the processing of data transfer which is done by reduce process, therefore the reduce progress starts showing up as soon as any intermediate key-value pair for a mapper is available to be transferred to reducer. Though the reducer progress is updated still the programmer defined reduce method is called only after all the mappers have finished.
•	What is HDFS ? How it is different from traditional file systems?
•	HDFS, the Hadoop Distributed File System, is responsible for storing huge data on the cluster. This is a distributed file system designed to run on commodity hardware. It has many similarities with existing distributed file systems. However, the differences from other distributed file systems are significant.
•	HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware.
•	HDFS provides high throughput access to application data and is suitable for applications that have large data sets.
•	HDFS is designed to support very large files. Applications that are compatible with HDFS are those that deal with large data sets. These applications write their data only once but they read it one or more times and require these reads to be satisfied at streaming speeds. HDFS supports write-once-read-many semantics on files.
•	What is HDFS Block size? How is it different from traditional file system block size?
•	In HDFS data is split into blocks and distributed across multiple nodes in the cluster. Each block is typically 64Mb or 128Mb in size. Each block is replicated multiple times. Default is to replicate each block three times. Replicas are stored on different nodes. HDFS utilizes the local file system to store each HDFS block as a separate file. HDFS Block size can not be compared with the traditional file system block size.
•	What is a NameNode? How many instances of NameNode run on a Hadoop Cluster?
•	The NameNode is the centerpiece of an HDFS file system. It keeps the directory tree of all files in the file system, and tracks where across the cluster the file data is kept. It does not store the data of these files itself. There is only One NameNode process run on any hadoop cluster. NameNode runs on its own JVM process. In a typical production cluster its run on a separate machine. The NameNode is a Single Point of Failure for the HDFS Cluster. When the NameNode goes down, the file system goes offline. Client applications talk to the NameNode whenever they wish to locate a file, or when they want to add/copy/move/delete a file. The NameNode responds the successful requests by returning a list of relevant DataNode servers where the data lives.
•	What is a DataNode? How many instances of DataNode run on a Hadoop Cluster?
•	A DataNode stores data in the Hadoop File System HDFS. There is only One DataNode process run on any hadoop slave node. DataNode runs on its own JVM process. On startup, a DataNode connects to the NameNode. DataNode instances can talk to each other, this is mostly during replicating data.
•	How the Client communicates with HDFS?
•	The Client communication to HDFS happens using Hadoop HDFS API. Client applications talk to the NameNode whenever they wish to locate a file, or when they want to add/copy/move/delete a file on HDFS. The NameNode responds the successful requests by returning a list of relevant DataNode servers where the data lives. Client applications can talk directly to a DataNode, once the NameNode has provided the location of the data.
•	How the HDFS Blocks are replicated?
•	HDFS is designed to reliably store very large files across machines in a large cluster. It stores each file as a sequence of blocks; all blocks in a file except the last block are the same size. The blocks of a file are replicated for fault tolerance. The block size and replication factor are configurable per file. An application can specify the number of replicas of a file. The replication factor can be specified at file creation time and can be changed later. Files in HDFS are write-once and have strictly one writer at any time. The NameNode makes all decisions regarding replication of blocks. HDFS uses rack-aware replica placement policy. In default configuration there are total 3 copies of a datablock on HDFS, 2 copies are stored on datanodes on same rack and 3rd copy on a different rack.
•	
Can you think of a questions which is not part of this post? Please don't forget to share it with me in comments section & I will try to include it in the list. 






•	NoSQL Interview Questions

•	Describe CAP theorem (Brewer's theorem)
•	What is CAP theorem? Describe how weakening consistency constraints can yield highly available distributed systems (e.g. databases). Give an example.
•	Consistent Hashing
•	Describe consistent hashing and the advantages that it has over traditional hashing techniques. How can this technique help while scaling distributed systems, for example, distributed hash tables?
•	Dynamo Vs BigTable
•	Contrast and compare the features of Google's BigTable and Amazon's Dynamo databases
•	Different types of NoSQL databases
•	Explain the properties, advantages and drawbacks of these different types of NoSQL databases. Give examples of each type of database e.g. CouchDB is an example of a document Oriented Database.
•	Document Oriented Database
•	Ordered Key/Value Store
•	Eventually consistent Key/Value Store
•	Graph Database
•	Object Database



-----------

Spark

Big Data enthusiasts have certified Apache Spark as the hottest data compute engine for Big Data in the world. It is fast ejecting MapReduce and Java from their positions, and job trends are reflecting this change. According to a survey by TypeSafe, 71% of global Java developers are currently evaluating or researching Spark, and 35% of them have already started using it. Spark experts are currently in demand, and in the days to follow, the number of Spark-related job opportunities is only expected to go through the roof.

The Edureka course on Apache Spark and Scala helps you master large-scale data processing though Spark Streaming, Spark SQL, MLlib, GraphX, among others. The course aims to enhance your career path as a Big Data developer through live projects and interactive tutorials by industry experts.



This is the perfect time to prepare for a Spark interview. We have curated a list Spark interview questions and answers to help you breeze through your interview and have a successful career around Spark. If you have any specific questions, we encourage you to write them in the comments section. Our experts will be happy to answer them for you.

All the best!

1. What is Apache Spark?
Wikipedia defines Apache Spark “an open source cluster computing framework originally developed in the AMPLab at University of California, Berkeley but was later donated to the Apache Software Foundation where it remains today. In contrast to Hadoop’s two-stage disk-based MapReduce paradigm, Spark’s multi-stage in-memory primitives provides performance up to 100 times faster for certain applications. By allowing user programs to load data into a cluster’s memory and query it repeatedly, Spark is well-suited to machine learning algorithms.”

Spark is essentially a fast and flexible data processing framework. It has an advanced execution engine supporting cyclic data flow with in-memory computing functionalities. Apache Spark can run on Hadoop, as a standalone system or on the cloud. Spark is capable of accessing diverse data sources including HDFS, HBase, Cassandra among others

2. Explain the key features of Spark.
• Spark allows Integration with Hadoop and files included in HDFS.

• It has an independent language (Scala) interpreter and hence comes with an interactive language shell.

• It consists of RDD’s (Resilient Distributed Datasets), that can be cached across computing nodes in a cluster.

• It supports multiple analytic tools that are used for interactive query analysis, real-time analysis and graph processing. Additionally, some of the salient features of Spark include:

Lighting fast processing: When it comes to Big Data processing, speed always matters, and Spark runs Hadoop clusters way faster than others. Spark makes this possible by reducing the number of read/write operations to the disc. It stores this intermediate processing data in memory.

Support for sophisticated analytics: In addition to simple “map” and “reduce” operations, Spark supports SQL queries, streaming data, and complex analytics such as machine learning and graph algorithms. This allows users to combine all these capabilities in a single workflow.

Real-time stream processing: Spark can handle real-time streaming. MapReduce primarily handles and processes previously stored data even though there are other frameworks to obtain real-time streaming.  Spark does this in the best way possible.

3. What is “RDD”?
RDD stands for Resilient Distribution Datasets: a collection of fault-tolerant operational elements that run in parallel. The partitioned data in RDD is immutable and is distributed in nature.

4. How does one create RDDs in Spark?
In Spark, parallelized collections are created by calling the SparkContext “parallelize” method on an existing collection in your driver program.

                val data = Array(4,6,7,8)

                val distData = sc.parallelize(data)

Text file RDDs can be created using SparkContext’s “textFile” method. Spark has the ability to create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, among others. Spark supports text files, “SequenceFiles”, and any other Hadoop “InputFormat” components.

                 val inputfile = sc.textFile(“input.txt”)

5. What does the Spark Engine do?
Spark Engine is responsible for scheduling, distributing and monitoring the data application across the cluster.

6. Define “Partitions”.
A “Partition” is a smaller and logical division of data, that is similar to the “split” in Map Reduce. Partitioning is the process that helps derive logical units of data in order to speed up data processing.

Here’s an example:  val someRDD = sc.parallelize( 1 to 100, 4)

Here an RDD of 100 elements is created in four partitions, which then distributes a dummy map task before collecting the elements back to the driver program.

7. What operations does the “RDD” support?
Transformations
Actions
8. Define “Transformations” in Spark.
“Transformations” are functions applied on RDD, resulting in a new RDD. It does not execute until an action occurs. map() and filer() are examples of “transformations”, where the former applies the function assigned to it on each element of the RDD and results in another RDD. The filter() creates a new RDD by selecting elements from the current RDD.

9. Define “Action” in Spark.
An “action” helps in bringing back the data from the RDD to the local machine. Execution of “action” is the result of all transformations created previously. reduce() is an action that implements the function passed again and again until only one value is left. On the other hand, the take() action takes all the values from the RDD to the local node.

10. What are the functions of “Spark Core”?
The “SparkCore” performs an array of critical functions like memory management, monitoring jobs, fault tolerance, job scheduling and interaction with storage systems.

It is the foundation of the overall project. It provides distributed task dispatching, scheduling, and basic input and output functionalities. RDD in Spark Core makes it fault tolerance. RDD is a collection of items distributed across many nodes that can be manipulated in parallel. Spark Core provides many APIs for building and manipulating these collections.

11. What is an “RDD Lineage”?
Spark does not support data replication in the memory. In the event of any data loss, it is rebuilt using the “RDD Lineage”. It is a process that reconstructs lost data partitions.

12. What is a “Spark Driver”?
“Spark Driver” is the program that runs on the master node of the machine and declares transformations and actions on data RDDs. The driver also delivers RDD graphs to the “Master”, where the standalone cluster manager runs.

13. What is SparkContext?
“SparkContext” is the main entry point for Spark functionality. A “SparkContext” represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster.

14. What is Hive on Spark?
Hive is a component of Hortonworks’ Data Platform (HDP). Hive provides an SQL-like interface to data stored in the HDP. Spark users will automatically get the complete set of Hive’s rich features, including any new features that Hive might introduce in the future.

The main task around implementing the Spark execution engine for Hive lies in query planning, where Hive operator plans from the semantic analyzer which is translated to a task plan that Spark can execute. It also includes query execution, where the generated Spark plan gets actually executed in the Spark cluster.

15. Name a few commonly used Spark Ecosystems.
Spark SQL (Shark)
Spark Streaming
GraphX
MLlib
SparkR
16. What is “Spark Streaming”?
Spark supports stream processing, essentially an extension to the Spark API. This allows stream processing of live data streams. The data from different sources like Flume and HDFS is streamed and processed to file systems, live dashboards and databases. It is similar to batch processing as the input data is divided into streams like batches.

Business use cases for Spark streaming: Each Spark component has its own use case. Whenever you want to analyze data with the latency of less than 15 minutes and greater than 2 minutes i.e. near real time is when you use Spark streaming

17. What is “GraphX” in Spark?
“GraphX” is a component in Spark which is used for graph processing. It helps to build and transform interactive graphs.

18. What is the function of “MLlib”?
“MLlib” is Spark’s machine learning library. It aims at making machine learning easy and scalable with common learning algorithms and real-life use cases including clustering, regression filtering, and dimensional reduction among others.

19. What is “Spark SQL”?
Spark SQL is a Spark interface to work with structured as well as semi-structured data. It has the capability to load data from multiple structured sources like “textfiles”, JSON files, Parquet files, among others. Spark SQL provides a special type of RDD called SchemaRDD. These are row objects, where each object represents a record.

Here’s how you can create an SQL context in Spark SQL:

        SQL context: scala> var sqlContext=new SqlContext

        HiveContext: scala> var hc = new HIVEContext(sc)

20. What is a “Parquet” in Spark?
“Parquet” is a columnar format file supported by many data processing systems. Spark SQL performs both read and write operations with the “Parquet” file.

21. What is an “Accumulator”?
“Accumulators” are Spark’s offline debuggers. Similar to “Hadoop Counters”, “Accumulators” provide the number of “events” in a program.

Accumulators are the variables that can be added through associative operations. Spark natively supports accumulators of numeric value types and standard mutable collections. “AggregrateByKey()” and “combineByKey()” uses accumulators.

22. Which file systems does Spark support?
Hadoop Distributed File System (HDFS)
Local File system
S3
23. What is “YARN”?
“YARN” is a large-scale, distributed operating system for big data applications. It is one of the key features of Spark, providing a central and resource management platform to deliver scalable operations across the cluster.

24. List the benefits of Spark over MapReduce.
Due to the availability of in-memory processing, Spark implements the processing around 10-100x faster than Hadoop MapReduce.
Unlike MapReduce, Spark provides in-built libraries to perform multiple tasks form the same core; like batch processing, steaming, machine learning, interactive SQL queries among others.
MapReduce is highly disk-dependent whereas Spark promotes caching and in-memory data storage
Spark is capable of iterative computation while MapReduce is not.
Additionally, Spark stores data in-memory whereas Hadoop stores data on the disk. Hadoop uses replication to achieve fault tolerance while Spark uses a different data storage model, resilient distributed datasets (RDD). It also uses a clever way of guaranteeing fault tolerance that minimizes network input and output.

25. What is a “Spark Executor”?
When “SparkContext” connects to a cluster manager, it acquires an “Executor” on the cluster nodes. “Executors” are Spark processes that run computations and store the data on the worker node. The final tasks by “SparkContext” are transferred to executors.

26. List the various types of “Cluster Managers” in Spark.
The Spark framework supports three kinds of Cluster Managers:

Standalone
Apache Mesos
YARN
27. What is a “worker node”?
“Worker node” refers to any node that can run the application code in a cluster.

28. Define “PageRank”.
“PageRank” is the measure of each vertex in a graph.

29. Can we do real-time processing using Spark SQL?
Not directly but we can register an existing RDD as a SQL table and trigger SQL queries on top of that.

30. What is the biggest shortcoming of Spark?
Spark utilizes more storage space compared to Hadoop and MapReduce.

Also, Spark streaming is not actually streaming, in the sense that some of the window functions cannot properly work on top of micro batching.

Got a question for us? Please mention it in the comments section and we will get back to you.

-------

https://0x0fff.com/spark-misconceptions/

-----

HBase

Interview Question for – HBase

Q1 What are the different types of tombstone markers in HBase for deletion?

Answer: There are 3 different types of tombstone markers in HBase for deletion-

1)Family Delete Marker- This markers marks all columns for a column family.

2)Version Delete Marker-This marker marks a single version of a column.

3)Column Delete Marker-This markers marks all the versions of a column.

Q2 When should you use HBase and what are the key components of HBase?

Answer: HBase should be used when the big data application has –

1)A variable schema

2)When data is stored in the form of collections

3)If the application demands key based access to data while retrieving.

Key components of HBase are –

Region- This component contains memory data store and Hfile.

Region Server-This monitors the Region.

HBase Master-It is responsible for monitoring the region server.

Zookeeper- It takes care of the coordination between the HBase Master component and the client.

Catalog Tables-The two important catalog tables are ROOT and META.ROOT table tracks where the META table is and META table stores all the regions in the system.

Q3 Explain the difference between HBase and Hive.

Answer: HBase and Hive both are completely different hadoop based technologies-Hive is a data warehouse infrastructure on top of Hadoop whereas HBase is a NoSQL key value store that runs on top of Hadoop. Hive helps SQL savvy people to run MapReduce jobs whereas HBase supports 4 primary operations-put, get, scan and delete. HBase is ideal for real time querying of big data where Hive is an ideal choice for analytical querying of data collected over period of time.

Q4 What is Row Key?

Answer: Every row in an HBase table has a unique identifier known as RowKey. It is used for grouping cells logically and it ensures that all cells that have the same RowKeys are co-located on the same server. RowKey is internally regarded as a byte array.

Q5 Explain the difference between RDBMS data model and HBase data model.

Answer: RDBMS is a schema based database whereas HBase is schema less data model.

RDBMS does not have support for in-built partitioning whereas in HBase there is automated partitioning.

RDBMS stores normalized data whereas HBase stores de-normalized data.

Q6 What are the different operational commands in HBase at record level and table level?

Answer: Record Level Operational Commands in HBase are –put, get, increment, scan and delete.

Table Level Operational Commands in HBase are-describe, list, drop, disable and scan.

Q7 Explain about the different catalog tables in HBase?

Answer: The two important catalog tables in HBase, are ROOT and META. ROOT table tracks where the META table is and META table stores all the regions in the system.

Q8 Explain the process of row deletion in HBase.

Answer: On issuing a delete command in HBase through the HBase client, data is not actually deleted from the cells but rather the cells are made invisible by setting a tombstone marker. The deleted cells are removed at regular intervals during compaction.

Q9 What is column families? What happens if you alter the block size of ColumnFamily on an already populated database?

Answer: The logical deviation of data is represented through a key known as column Family. Column families consist of the basic unit of physical storage on which compression features can be applied. In an already populated database, when the block size of column family is altered, the old data will remain within the old block size whereas the new data that comes in will take the new block size. When compaction takes place, the old data will take the new block size so that the existing data is read correctly.

Q10 Explain about HLog and WAL in HBase.

Answer: All edits in the HStore are stored in the HLog. Every region server has one HLog. HLog contains entries for edits of all regions performed by a particular Region Server.WAL abbreviates to Write Ahead Log (WAL) in which all the HLog edits are written immediately.WAL edits remain in the memory till the flush period in case of deferred log flush.

Q11 what is NoSql?

Answer: Apache HBase is a type of “NoSQL” database. “NoSQL” is a general term meaning that the database isn’t an RDBMS which supports SQL as its primary access language, but there are many types of NoSQL databases: BerkeleyDB is an example of a local NoSQL database, whereas HBase is very much a distributed database. Technically speaking, HBase is really more a “Data Store” than “Data Base” because it lacks many of the features you find in an RDBMS, such as typed columns, secondary indexes, triggers, and advanced query languages, etc.

Q12 What is regionserver?

Answer: It is a file which lists the known region server names.

Q13 Give the name of the key components of Hbase

Answer: The key components of Hbase are Zookeeper, RegionServer, Region, Catalog Tables and Hbase Master.

Q14 What is the reason of using Hbase?

Answer: Hbase is used because it provides random read and write operations and it can perform number of operation per second on a large data sets.

Q15 Define standalone mode in Hbase?

Answer: It is a default mode of Hbase .In standalone mode, HBase does not use HDFS—it uses the local filesystem instead—and it runs all HBase daemons and a local ZooKeeper in the same JVM process.

Q16 Which operating system is supported by Hbase?

Answer: Hbase supports those OS which supports java like windows, Linux.

Q17 What are the main features of Apache HBase?

Answer: Apache HBase has many features which supports both linear and modular scaling,HBase tables are distributed on the cluster via regions, and regions are automatically split and re-distributed as your data grows(Automatic sharding).HBase supports a Block Cache and Bloom Filters for high volume query optimization(Block Cache and Bloom Filters).

Q18 What is the difference between HDFS/Hadoop and HBase?

Answer: HDFS doesn’t provides fast lookup records in a file,IN Hbase provides fast lookup records for large table.

Q19 What are datamodel operations in HBase?

Answer: 1)Get(returns attributes for a specified row,Gets are executed via HTable.get)

2)put(Put either adds new rows to a table (if the key is new) or can update existing rows (if the key already exists). Puts are executed via HTable.put (writeBuffer) or HTable.batch (non-writeBuffer))

3)scan(Scan allow iteration over multiple rows for specified attributes)

4)Delete(Delete removes a row from a table. Deletes are executed via HTable.delete)

HBase does not modify data in place, and so deletes are handled by creating new markers called tombstones. These tombstones, along with the dead values, are cleaned up on major compaction.

Q20 How many filters are available in Apache HBase?

Answer: Total we have 18 filters are support to hbase.They are:

ColumnPrefixFilter

TimestampsFilter

PageFilter

MultipleColumnPrefixFilter

FamilyFilter

ColumnPaginationFilter

SingleColumnValueFilter

RowFilter

QualifierFilter

ColumnRangeFilter

ValueFilter

PrefixFilter

SingleColumnValueExcludeFilter

ColumnCountGetFilter

InclusiveStopFilter

DependentColumnFilter

FirstKeyOnlyFilter

KeyOnlyFilter

Q21 Does HBase support SQL?

Answer: Not really. SQL-ish support for HBase via Hive is in development, however Hive is based on MapReduce which is not generally suitable for low-latency requests.By using Apache Phoenix  can retrieve data from hbase by using sql queries.

Q22 Is there any difference between HBase datamodel and RDBMS datamodel?

Answer: In Hbase,data is stored as a table(have rows and columns) similar to RDBMS but this is not a helpful analogy. Instead, it can be helpful to think of an HBase table as a multi-dimensional map.

Q23 What is Apache HBase?

Answer: Apache Hbase is one the sub-project of  Apache Hadoop,which was designed for NoSql database(Hadoop Database),bigdata store and a distributed, scalable.Use Apache HBase when you need random, realtime read/write access to your Big Data.A table which contain billions of rows X millions of columns -atop clusters of commodity hardware. Apache HBase is an open-source, distributed, versioned, non-relational database modeled after Google’s Bigtable. Apache HBase provides Bigtable-like capabilities  run on top of Hadoop and HDFS.

Q24 What is the use of shutdown command?

Answer: It is used to shut down the cluster.

Q25  How to delete the table with the shell?

Answer: To delete table first disable it then delete it.

Q26 What is the full form of MSLAB?

Answer: MSLAB stands for Memstore-Local Allocation Buffer.

Q27 What is REST?

Answer: Rest stands for Representational State Transfer which defines the semantics so that the protocol can be used in a generic way to address remote resources. It also provides support for different message formats, offering many choices for a client application to communicate with the server.

Q28 Mention what are the key components of Hbase?

Answer:Zookeeper: It does the co-ordination work between client and Hbase Maser

Hbase Master: Hbase Master monitors the Region Server

RegionServer: RegionServer monitors the Region

Region: It contains in memory data store(MemStore) and Hfile.

Catalog Tables: Catalog tables consist of ROOT and META

Q29 In Hbase what is column families?

Answer: Column families comprise the basic unit of physical storage in Hbase to which features like compressions are applied.

Q30 Explain how does Hbase actually delete a row?

Answer: In Hbase, whatever you write will be stored from RAM to disk, these disk writes are immutable barring compaction. During deletion process in Hbase, major compaction process delete marker while minor compactions don’t. In normal deletes, it results in a delete tombstone marker- these delete data they represent are removed during compaction.

Also, if you delete data and add more data, but with an earlier timestamp than the tombstone timestamp, further Gets may be masked by the delete/tombstone marker and hence you will not receive the inserted value until after the major compaction.

Q31 What Is The Difference Between HBase and Hadoop/HDFS?

Answer: HDFS : is a distributed file system that is well suited for the storage of large files. It\’s documentation states that it is not, however, a general purpose file system, and does not provide fast individual record lookups in files.

HBase: on the other hand, is built on top of HDFS and provides fast record lookups (and updates) for large tables. This can sometimes be a point of conceptual confusion. HBase internally puts your data in indexed “StoreFiles” that exist on HDFS for high-speed lookups.

Q32 How many Operational command in Hbase?

Answer: There are five main command in HBase.

Get
Put
Delete
Scan
Increment
Q33 Why cant I iterate through the rows of a table in reverse order?

Answer: Because of the way HFile works: for efficiency, column values are put on disk with the length of the value written first and then the bytes of the actual value written second. To navigate through these values in reverse order, these length values would need to be stored twice (at the end as well) or in a side file. A robust secondary index implementation is the likely solution here to ensure the primary use case remains fast.

Q34 Explain what is Hbase?

Answer: Hbase is a column-oriented database management system which runs on top of HDFS (Hadoop Distribute File System). Hbase is not a relational data store, and it does not support structured query language like SQL.

In Hbase, a master node regulates the cluster and region servers to store portions of the tables and operates the work on the data.

Q35 How to connect to Hbase?

Answer: A connection to Hbase is established through Hbase Shell which is a Java API.

Q36 Why we describe HBase Schema less?

Answer: Other than the column family name, HBase doesn’t require you to tell it anything about your data ahead of time. That’s why HBase is often described as a schema-less database.

Q37 What is Hfile?

Answer: All columns in a column family are stored together in the same low level storage file, called an Hfile.

Q38 How data is written into HBase?

Answer: When data is updated it is first written to a commit log, called a write-ahead log (WAL) in HBase, and then stored in the in-memory memstore. Once the data in memory has exceeded a given maximum value, it is flushed as an HFile to disk. After the flush, the commit logs can be discarded up to the last unflushed modification.

Q39 How data is read back from HBase?

Answer: Reading data back involves a merge of what is stored in the memstores, that is, the data that has not been written to disk, and the on-disk store files. Note that the WAL is never used during data retrieval, but solely for recovery purposes when a server has crashed before writing the in-memory data to disk.

Q40 What is the role of Zookeeper in Hbase?

Answer: The zookeeper maintains configuration information, provides distributed synchronization, 
and also maintains the communication between clients and region servers.

Q41 What are the different types of filters used in Hbase?

Answer: Filters are used to get specific data form a Hbase table rather than all the records.

They are of the following types.

Column Value Filter

Column Value comparators

KeyValue Metadata filters.

RowKey filters.

Q42 How does Hbase provide high availability?

Answer: Hbase uses a feature called region replication. In this feature for each region of a table, there will be multiple replicas that are opened in different RegionServers. The Load Balancer ensures that the region replicas are not co-hosted in the same region servers.

Q43 Explain what is the row key?

Answer: Row key is defined by the application. As the combined key is pre-fixed by the rowkey, it enables the application to define the desired sort order. It also allows logical grouping of cells and make sure that all cells with the same rowkey are co-located on the same server.

Q44 What are the different compaction types in Hbase?

Answer: There are two types of compaction. Major and Minor compaction. In minor compaction, the adjacent small HFiles are merged to create a single HFile without removing the deleted HFiles. Files to be merged are chosen randomly.

In Major compaction, all the HFiles of a column are emerged and a single HFiles is created. The delted HFiles are discarded and it is generally triggered manually.

Q45 What is TTL (Time to live) in Hbase?

Answer: TTL is a data retention technique using which the version of a cell can be preserved till a specific time period.Once that timestamp is reached the specific version will be removed

Q46 In Hbase what is log splitting?

Answer: When a region is edited, the edits in the WAL file which belong to that region need to be replayed. Therefore, edits in the WAL file must be grouped by region so that particular sets can be replayed to regenerate the data in a particular region. The process of grouping the WAL edits by region is called log splitting.

Q47 Why MultiWAL is needed?

Answer: With a single WAL per RegionServer, the RegionServer must write to the WAL serially, because HDFS files must be sequential. This causes the WAL to be a performance bottleneck.

Q48 What are the different Block Caches in Hbase?

Answer: HBase provides two different BlockCache implementations: the default on-heap LruBlockCache and the BucketCache, which is (usually) off-heap.

Q49 Can you create HBase table without assigning column family.

Answer:  No, Column family also impact how the data should be stored physically in the HDFS file system, hence there is a mandate that you should always have at least one column family. We can also alter the column families once the table is created.

Q50 What is HFile ?

Answer: The HFile is the underlying storage format for HBase.

HFiles belong to a column family and a column family can have multiple HFiles.

But a single HFile can’t have data for multiple column families


-------------
Spring guides
https://spring.io/guides

Spring boot
http://docs.spring.io/spring-boot/docs/2.0.0.BUILD-SNAPSHOT/reference/htmlsingle/
Getting started: https://spring.io/guides/gs/spring-boot/
Samples: https://github.com/spring-projects/spring-boot/tree/master/spring-boot-samples

Cloud foundry:



ML

Machine  learning  algorithms  attempt  to  make  predictions  or  decisions  based  on
training  data.

There  are  multiple  types  of  learning  problems,  including  
classification- There  are  multiple  types  of  learning  problems,  including  classification, regression, or clustering,
regression-
clustering,


classification, which involves identifying which of several categories an
item  belongs  to  (e.g.,  whether  an  email  is  spam  or  non-spam),  based  on  labeled
examples of other items (e.g., emails known to be spam or not).

Classification  and  regression  are  two  common  forms of supervised learning, where
algorithms attempt to predict a variable from features of objects using labeled train-
ing data

Clustering
Clustering is the unsupervised learning task that involves grouping objects into clus-
ters of high similarity. Unlike the supervised tasks seen before, where data is labeled,
clustering can be used to make sense of unlabeled data. It is commonly used in data
exploration (to find what a new dataset looks like) and in anomaly detection (to iden-
tify points that are far from any cluster).

RandomForestModel??
MLLibRegressionModel??

*****************************************
Classification  and  regression  are  two  common  forms  of  supervised  learning,  where
algorithms attempt to predict a variable from features of objects using labeled train-
ing data (i.e., examples where we know the answer). 

 The difference between them is
the type of variable predicted: in classification, the variable is discrete (i.e., it takes on
a finite set of values called classes); for example, classes might be  spam  or  nonspam  for
emails, or the language in which the text is written. In regression, the variable predic-
ted is continuous (e.g., the height of a person given her age and weight).

Decision trees and random forests
Decision trees are a flexible model that can be used for both classification and regres-
sion. They represent a tree of nodes, each of which makes a binary decision based on
a feature of the data (e.g., is a person’s age greater than 20?),


***************************************

All learning algorithms require defining a set of features for each item, which will be
fed  into  the  learning  function

At the end, the
algorithm  will  return  a  model  representing  the  learning  decision

This  model  can  now  be  used  to  make  predictions  on  new  points



dataTypes
Vector
A mathematical vector. MLlib supports both dense vectors, where every entry is
stored,  and  sparse  vectors,  where  only  the  nonzero  entries  are  stored  to  save
space.

LabeledPoint
A labeled data point for supervised learning algorithms such as classification and
regression. Includes a feature vector and a label 


OUPCTN NeighborHood 
Feature
Compares SDP usage pattern with its neighbours.
 * Neighbours are identified by zip + 3 codes, If not found then zip + 2 considered 
 Finally the SDPs with correlation coefficient of less than -0.75 compared to the neighbors and 
 usage of at least 20% of the neighborhood usage (zip+3 or zip+2) are inserted into RP_FEATURE table. 
 The actual_value is the correlation coefficient.

UI
The SDP usage comparison to neighbors' chart displays the average interval usage (in kWh) of the SDP and 
compares it to three metrics of interval usage for the neighborhood during the investigation period. 
The three interval usage metrics of the neighborhood that are computed are: 
average interval usage, 
average interval usage for 75% of the neighbors, 
average interval usage for 95% of the neighbors.


NTB

Run the Night Time Bypass algorithm.  
Number of days in the past X days which
the total usage at night (for example 7pm and 5am ) is less than 0.32 
and during the day (for example 8am to 4pm) is greater than 3.2
and account is active and MAX usage during day window / AVG usage during day window >= 1.5
Z_RP_SP_ALGORITHM_NTB
count(distinct night_usage_date) as ACTUAL_VALUE


UWNAA
SDP with no active accounts 
inactive account window should meet or exceed parameter THRESHOLD_SUM_INACTIVE_DAYS
During this inactive window period is usage is > than threshold
Sum of duration of all intervals

--------------

wiki***OozieJobExecutor is the gateway to any Hadoop job in EIP. Oozie job executor is a 24x7 EIP application 
so it's spring application context contains all the EIP common property sets like DB Datasources, logger, ebo etc.
These environment scope properties can be picked from the ApplicationContext and propagated to Oozie jobs in Hadoop

Oozie is a  workflow scheduler system, manage the timely execution of thousands of workflows of dependent jobs in
a Hadoop cluster.

OozieJob executor while launching the job access the ${EIP_HOME}/.keytabs/<org>.keytab to authenticate and the 
obtain the Hbase Auth Token.

This Hbase Auth Token is then Base 64 encoded and sent as job properties to Spark/MR executors where is can be 
decoded and used for Hbase authentication. Auth tokens for HDFS and YARN are implicitly passed by Qozie

<action name="LoadIntervalReads">
		<shell xmlns="uri:oozie:shell-action:0.1">
			<job-tracker>${jobTracker}</job-tracker>
			<name-node>${nameNode}</name-node>
			<configuration>
				<property>
					<name>mapred.job.queue.name</name>
					<value>${queueName}</value>
				</property>
			</configuration>
			<exec>spark-submit</exec>
			<argument>--master</argument>
			<argument>yarn</argument>
			<argument>--deploy-mode</argument>
			<argument>cluster</argument>
			<argument>--principal</argument>
			<argument>${orgKerberosPrincipalWithRealm}</argument>
			<argument>--keytab</argument>
			<argument>${orgKerberosPrincipal}.keytab</argument>
			<argument>--class</argument>
			<argument>com.emeter.loader.acloader.lp.LpIntervalLoader</argument>
			<argument>--num-executors</argument>
			<argument>${numOfExecutors}</argument>
			<argument>${sparkAppJar}</argument>
			<argument>${sparkProperties}</argument>
			<file>${orgKerberosPrincipal}.keytab#${orgKerberosPrincipal}.keytab</file>
		</shell>
		<ok to="MoveProcessed"/>
		<error to="kill"/>
	</action>

oozie.wf.application.path

Web
Apache Oozie is a system for running workflows of dependent jobs

Oozie has been
designed to scale, and it can manage the timely execution of thousands of workflows in
a Hadoop cluster.

Oozie runs
as a service in the cluster, and clients submit workflow definitions for immediate or later
execution.

We have taken advantage of JSP Expression Language (EL) syntax in several places in
the  workflow  definition. 

manually running oozie
we export the  OOZIE_URL environment variable to tell the  oozie
command which Oozie server to use 






MTM wiki
Oozie
Oozie is a  workflow scheduler system to manage Apache Hadoop jobs.
Oozie is integrated with the rest of the Hadoop stack supporting several types of Hadoop jobs out of the box
This involves creating the Oozie workflow files and submitting them to the scheduling engine through the OozieExecutor

Oozie Job executor is a Jobserver that is responsible to launch all the Hadoop Jobs from eip

OozieJob executor while launching the job access the ${EIP_HOME}/.keytabs/<org>.keytab to authenticate and the 
obtain the Hbase Auth Token.

This Hbase Auth Token is then Base 64 encoded and sent as job properties to Spark/MR executors where is can be 
decoded and used for Hbase authentication. Auth tokens for HDFS and YARN are implicitly passed by Qozie

HbaseDAO has the capability to authenticate using keytab or HBASE_AUTH_TOKEN
either keep keytab file <user>.keytab in ${EIP_HOME}/.keytabs
or
Make sure UseeGroupInformation object available to HbaseDAO has hbase_auth_token before it connects to hbase.

----

Kerberos

Cloudera uses Kerberos to secure the hadoop cluster. After we enable Kerberos security, 
access to all the Hadoop services is authenticated using Kerberos.

During the installation process installer creates keytabs for all the necessary principals inside ${EIP_HOME}/.keytabs folder
Hence all the eip processes can access these keytabs to authenticate from Kerberos Authentication server.

Generating a Kerberos Keytab File
run ktutil tool from shell:
ktutil

1.  Authentication.  The  client  authenticates  itself  to  the  Authentication  Server  and
receives a timestamped Ticket-Granting Ticket (TGT).
2.  Authorization. The client uses the TGT to request a service ticket from the Ticket-
Granting Server.
3.  Service request. The client uses the service ticket to authenticate itself to the server
that is providing the service the client is using. In the case of Hadoop, this might
be the namenode or the resource manager.

Together, the Authentication Server and the Ticket Granting Server form the Key Dis-
tribution Center (KDC). 

To use Kerberos authentication with Hadoop, you need to install, configure, and run a KDC (Hadoop does
not come with one)

A keytab is a file that stores passwords 

Instead of  using  the  three-step  Kerberos  ticket  exchange  protocol  to  authenticate  each  call,
which would present a high load on the KDC on a busy cluster, Hadoop uses delegation
tokens to allow later authenticated access without having to contact the KDC again.

A delegation token is generated by the server (the namenode, in this case) and can be
thought of as a shared secret between the client and the server. On the first RPC call to
the namenode, the client has no delegation token, so it uses Kerberos to authenticate.
As a part of the response, it gets a delegation token from the namenode. In subsequent
calls it presents the delegation token, which the namenode can verify (since it generated
it using a secret key), and hence the client is authenticated to the server.

Kerberos builds on symmetric key cryptography and requires a trusted third party, and optionally may use 
public-key cryptography during certain phases of authentication.[1] Kerberos uses UDP port 88 by default.



CDH
Cloudera’s  Distribution  Including  Apache  Hadoop  (hereafter  CDH)  is  an  integrated
Apache  Hadoop–based  stack  containing  all  the  components  needed  for  production,
tested and packaged to work together.

-Lifecycle of spark program
	-create RDD(parallelize / external data) in driver program
	-Lazy transform them to new RDDs using transformations like filter() or map()
	-Cache any intermediate RDDs that can be reused
	-Launch actions like count() and collect() to kick of parallel computation. Optimized and executed by spark
	

Fast:
1) keeps intermediate data in memory unlike Hadoop MR it does not write to HDFS 10-100x speed
2) MapRed has hardcoded map and reduce slots so CPU usage is not 100%. In spark has generic slots that can be used either by map/reduce
3) Empty slots for map or reduce are not filled aggressively in Map-reduce (Face book noticed that and used corona to 
   agressively start next map/reduce job). In spark it does
4) parallelism in MapRed - is by process Id on a node (???). Slots in MapRed is called processId
   parallelism in spark - is by threads on a executor. So better. Spark calls them cores. So eg. 6 cores are started in a Executor 	
   
HBase: The Hadoop Database   
HBase is a distributed, persistent, strictly consistent storage system
with near-optimal write—

MapReduce works by breaking the processing into two phases: the map phase and the
reduce phase. Each phase has key-value pairs as input and output, the types of which
may be chosen by the programmer. 
Input is split in input-spilts for each task to be used by its mapping function....(usually a block size 128 MB default)

Map task have advntage of data locality
Reduce task doesnot have it


The term sharding describes the logical separation of records into horizontal partitions.
The idea is to spread data across multiple storage files—or servers—as opposed to
having each stored contiguously.
The separation of values into those partitions is performed on fixed boundaries: you
have to set fixed rules ahead of time to route values to their appropriate store. With it
comes the inherent difficulty of having to reshard the data when one of the horizontal
partitions exceeds its capacity.
Resharding is a very costly operation, since the storage layout has to be rewritten. This
entails defining new boundaries and then horizontally splitting the rows across them.
Massive copy operations can take a huge toll on I/O performance as well as temporarily
elevated storage requirements. And you may still take on updates from the client ap-
plications and need to negotiate updates during the resharding process.


https://www.snapdeal.com/product/casio-pace-black-steel-watch/20479

-----------------------------------------------------------------------------
computation 

Michael Armbrust

Structuring Apache Spark 2.0: SQL, DataFrames, Datasets And Streaming - by Michael Armbrust

Aspects of Big Data
-three Vs of big data: data velocity, variety, and complexity, in addition to volume.

How Big Data Differs from Traditional BI
-BI:Traditional BI methodology works on the principle of assembling all the enterprise data in a central server. The data 
 	is generally analyzed in an offline mode. The online transaction processing (OLTP) transactional data is transferred to 
 	a denormalized environment called as a data warehouse. The data is usually structured in an RDBMS with very little 
 	unstructured data.
-BigData:Data is retained in a distributed file system instead of on a central server.
		The processing functions are taken to the data rather than data being taking to the functions.
		Data is of different formats, both structured as well as unstructured.
		Data is both real-time data as well as offline data.
		Technology relies on massively parallel processing (MPP) concepts. 
 
How Big Is the Opportunity?
-	The amount of data is growing all around us every day, coming from various channels (see Figure 1-1).  
	As 70 percent of all data is created by individuals who are customers of some enterprise or the other, organizations 
	cannot ignore this important source of feedback from the customer as well as insight into customer behavior.
-	
----------------------------------------------------------------------------------


parquet-column-1.5.0-cdh5.4.1.jar
parquet-hadoop-1.5.0-cdh5.4.1.jar
parquet-avro-1.5.0-cdh5.4.1.jar
parquet-cascading-1.5.0-cdh5.4.1.jar
parquet-common-1.5.0-cdh5.4.1.jar
parquet-encoding-1.5.0-cdh5.4.1.jar
parquet-format-2.1.0-cdh5.4.1-javadoc.jar
parquet-format-2.1.0-cdh5.4.1-sources.jar
parquet-format-2.1.0-cdh5.4.1.jar
parquet-generator-1.5.0-cdh5.4.1.jar
parquet-hadoop-bundle-1.5.0-cdh5.4.1.jar
parquet-jackson-1.5.0-cdh5.4.1.jar
parquet-pig-1.5.0-cdh5.4.1.jar
parquet-pig-bundle-1.5.0-cdh5.4.1.jar
parquet-protobuf-1.5.0-cdh5.4.1.jar
parquet-scala_2.10-1.5.0-cdh5.4.1.jar
parquet-scrooge_2.10-1.5.0-cdh5.4.1.jar
parquet-test-hadoop2-1.5.0-cdh5.4.1.jar
parquet-thrift-1.5.0-cdh5.4.1.jar
parquet-tools-1.5.0-cdh5.4.1.jar
		
		
		hadoop.version=2.6.0
		hbase.version=1.0.0
		cdh.version=cdh5.4.1
		hive.version=1.1.0
		cdh.protobuf.version=2.5.0
		cdh.jets3t.version=0.9.0
		cdh.avro.version=1.7.6-cdh5.4.1
		cdh.hadoop.version=2.6.0-cdh5.4.1
		cdh.slf4j.version=1.7.5
		servlet-version=3.0.20100224
		zookeeper.version=3.4.5
		spark.version=1.3.0
		oozie.version=4.1.0


Cluster capacity planning:
-Need toknow what services u need for the cluster as spark, oozie etc
-Then comes capacity planning
	-Two things drive the capacity planning for bigData projects
		-How much data u keep in hadoop
		-How much data u want to process in daily basis
12 Master and 18 slave	
	
Services				Master				Share Slave?				Workers/Slaves				Utility/Edge/Gateway
HDFS					2(NameNode			Yes(DataNode)				18
YARN + MR2				2(ResouceManager)	Yes(NodeManager)			18
Hive					1 or 2				No							0
Spark					withYarn:			Yes(Executors but will be under yarn)0	
HBase					3(HMaster, as HA is more imp)Yes(RegionServer)	18
Impala					1 or 2				Yes(ImpalaD)				18
Kafka					3					No							0

						~12 masters
						run on 12 physical
						servers except 
						zookeepers

Master
-----
Storage for master is not very imp except for the Gateway. 
Master configuration is straight forward
Lets say each master has 
-64 GB of RAM | eg NameNode give 8 GB to OS and rest to Master (NameNode)
-16 cores CPU



Slaves
------
Let Each slave has 64 GB RAM and 16 core and 16TB of storage
overall = 16*18=288 TB total storage

With HDFS relication default factor of 3, i.e. 288/3 = ~90 TB of data can be saved
	
With 16 cores CPU	

Slave resource distribution is not straight forward as it has to look
Each slave has 

Service					RAM(64)									core	
-OS						6 GB is good starting point for slave	1
-HDFS | dataNode		1 GB(hrtBt to NameNode + writing to otherDataNode in case of replicFac) 		1 core
-Yarn+MR2 | NodeManager	1 GB(hrtbt to res Manager, does not process data, facilitate Mapper/reducer tasks) 1 core 	
-Impala | ImpalaD 		8GB(is memory intensive)	2 or 4 cores
-HBase | RegionServer	4GB		2 or 4 cores
-other services like mail, encryption and decryption

Have allocated 20 GB of RAM and 7 cores for each slave
44 GB RAM & 9 cores left on worker Node

-unix lscpu command to get CPU cores 

Resource manager run on 8088 port | <ip>:8088/cluster

-vcore multiplier: for more I/O value sld be ~4 | for more CPU intensive value = 1
-44 GB RAM & 9 cores left on worker Node
	-44*18=792 GB of total RAM for worker nodes
	-9*18*2(vcoreMultiplier for moderate load)=324 of total cores for all nodes
	
	***So 44 GB of memory & 9 cores are required as overall memory and oveerall cores for Yarn(NodeManager)
	depicted by 
	-Java heap size of NodeManager: 1 GB(initially decided)
	-****containerMemory(yarn.nodeManager.resource.memory): 44 GB(as calculated)
	-yarn.nodeManager.resource.memory = 44 GB of memory
	-yarn.nodeManager.resource.cpu-vcores: 27 for each node as 9*3 (2 i changed to 3 for moderate node)
		-This tells how many cores to give by containers managed by Yarn
	min/max 
	-yarn.scheduler.minimum-allocation-mb: min memory of a contatiner  e.g. 1 GB
	-yarn.scheduler.increment-allocation-mb: incremental memory of a contatiner	1 GB 
	-yarn.scheduler.maximum-allocation-mb: max memory of a contatiner  4 GB
	
	-yarn.scheduler.minimum-allocation-vcores: min vcore	1
	-yarn.scheduler.increment-allocation-vcores: incremental vcore	1
	-yarn.scheduler.maximum-allocation-vcores: max vcore	3
	
-So if all configurations done properly, on ResourceManager you can see
 Total memory as 792 and VCore as 324 then all configrations are correct 
 else some issue with the configuration
 
-I/O is data copy from HardDisk to memory, when I/O happens CPU issues I/O request
  but I/O handler does the I/O. Once data is loaded then CPU is used.
 
 -When u want ur machine to be used at full power, the context switch b/w processes 
  should not happen. In such case where CPU intensive processes are running context 
  switch will be costly and through put will be down
 -In I/O context switch will happen.
	

-mapredure configuration properties:
	-mapreduce.map.memory-mb
	-xx... vcore
	
-*** you have to satisfy the restrictions enforced by YARN for jobs like Mapper/Reducer eg given 5 GB instead of 4
	without calculation then we run into issues like out of mem 
	
-refer yarn tunning on cloudera. cdh_ig_yarn_tunning
+ https://www.youtube.com/watch?v=arDBjDO2QcA 
UTube