on any eip user dir 

confXML/EIPCore/DataTypes_Core.xml


UOM
//EnergyIP/eip-apps/release/8.3/shared/eip-seed-core/resources/org/Sample/refData/eip/core/meas/EM.ELECTRIC.VOLTS.xml

guava-15.0.jar

eip@lnxapp94.emeter.com:/home/eip->kinit rporg1@HADOOP.EMETER.COM
Password for rporg1@HADOOP.EMETER.COM:

2641713022
04404

https://docs.emeter.com/pages/viewpage.action?spaceKey=RP20&title=Revenue+Protection+Jobs

1) SDP loader - load Sdp only using sdp store and no channel lookup

2) Register load: 	* Register read extractor (low priority)
					No channel look up as extracted data will have 
					* channel udcId
					* channel Id
					* sdpUdcId 
					* sdpId
					* Persist using Meter data Store
					
3) Interval load: 	* LP read extractor (low priority)
					* channel udcId
					* channel Id
					* sdpUdcId 
					* sdpId
					* Persist using Meter data Store
					
					
4) Device Event load:
					* Integration testing with device event extracted CSV 
					
					

Register read loader
====================
? Channel lookup, which should not be required in 8x-8x
? Flow of any loader. Should each loader perform SDP load and respective load 
	OR it will be a 2 step process 1st sdp load job 2nd respective loader
? load sdp actually loads sdp and channel look up(not required now)






@D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/platform/trunk/db/DatabaseBuild/eip/full/ddl/TempViews.080300.sql
@D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/platform/trunk/db/DatabaseBuild/eip/full/ddl/ins_ts_tab_map.080300.sql
@D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/platform/trunk/db/DatabaseBuild/eip/full/ddl/Org.080300.sql
@D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/platform/trunk/db/DatabaseBuild/eip/full/ddl/ProcessCommon.080300.sql
@D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/platform/trunk/db/DatabaseBuild/eip/full/ddl/EntityConfig.080300.sql
@D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/platform/trunk/db/DatabaseBuild/eip/full/ddl/JobFramework.080300.sql
@D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/platform/trunk/db/DatabaseBuild/eip/full/ddl/StructuralData.080300.sql
@D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/platform/trunk/db/DatabaseBuild/eip/full/ddl/Security.080300.sql
@D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/platform/trunk/db/DatabaseBuild/eip/full/ddl/SystemConsole.080300.sql
@D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/platform/trunk/db/DatabaseBuild/eip/full/ddl/Ami.080300.sql
@D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/platform/trunk/db/DatabaseBuild/eip/full/ddl/Audit.080300.sql
@D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/platform/trunk/db/DatabaseBuild/eip/full/ddl/Billing.080300.sql
@D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/platform/trunk/db/DatabaseBuild/eip/full/ddl/DMS.080300.sql
@D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/platform/trunk/db/DatabaseBuild/eip/full/ddl/DTS.080300.sql
@D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/platform/trunk/db/DatabaseBuild/eip/full/ddl/Framer.080300.sql
@D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/platform/trunk/db/DatabaseBuild/eip/full/ddl/ServiceRequest.080300.sql
@D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/platform/trunk/db/DatabaseBuild/eip/full/ddl/StatsExceptions.080300.sql
@D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/platform/trunk/db/DatabaseBuild/eip/full/ddl/VEE.080300.sql
@D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/platform/trunk/db/DatabaseBuild/eip/full/ddl/MissingReads.080300.sql
@D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/platform/trunk/db/DatabaseBuild/eip/full/ddl/Misc.080300.sql
@D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/platform/trunk/db/DatabaseBuild/eip/full/ddl/StatsCollector.080300.sql
@D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/platform/trunk/db/DatabaseBuild/eip/full/ddl/UtilTS.080300.sql
@D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/platform/trunk/db/DatabaseBuild/eip/full/ddl/UI_count_cache.080300.sql
@D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/platform/trunk/db/DatabaseBuild/eip/full/ddl/StructuralData_FK.080300.sql
@D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/platform/trunk/db/DatabaseBuild/eip/full/ddl/ForeignKeys.080300.sql
@D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/platform/trunk/db/DatabaseBuild/eip/full/ddl/ACTIVITI.080300.sql
@D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/platform/trunk/db/DatabaseBuild/eip/full/ddl/Forcast.080300.sql
@D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/platform/trunk/db/DatabaseBuild/eip/full/ddl/EipComments.080300.sql
@D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/platform/trunk/db/DatabaseBuild/eip/full/ddl/ForcastComm.080300.sql




select * from device_event;
select * from svc_pt@mudr2eip;
ind-lnxapp18
emdb36.emeter.com:1521/poltp06

 

TODO list:
1) sample seed :: measTypeIntUdcId = "EM.ELECTRIC.KWH.1.4"; 239l  "EM.ELECTRIC.KVARH.2.10"; 	Long measTypeRRId = 223l;
2) statsLogging
3) Neighbours pattern Algo
4) RankAlgorithm
5) Device event impl and integration
6) LP int impl and integration
7) RR impl and integration
8) CodeCoverage



disable 'rporg:sdp'
drop 'rporg:sdp'
create 'rporg:sdp','s','m'
create 'u007169584:sdp','s','m'

create 'testram:abc','s','m'

create 'u007169584:sdpBkp', 
{NAME => 'm', BLOOMFILTER => 'ROWCOL', VERSIONS => '10', DATA_BLOCK_ENCODING => 'FAST_DIFF', COMPRESSION => 'SNAPPY' , BLOCKCACHE => 'true', BLOCKSIZE => '65536'}, 
{NAME => 's', BLOOMFILTER => 'ROW', VERSIONS => '1', DATA_BLOCK_ENCODING => 'FAST_DIFF', COMPRESSION => 'SNAPPY',  BLOCKCACHE => 'true', BLOCKSIZE => '65536'}

create 'rporg:rpresult','r'

disable 'rporg:rpresult'
drop 'rporg:rpresult'
create 'rporg:rpresult', {NAME => 'r', BLOOMFILTER => 'ROWCOL', VERSIONS => '1000000', DATA_BLOCK_ENCODING => 'FAST_DIFF', COMPRESSION => 'SNAPPY' , BLOCKCACHE => 'true', BLOCKSIZE => '65536'}


disable 'rporg1:rpresult2'
drop 'rporg1:rpresult2'
create 'rporg1:rpresult2',{NAME => 'r', BLOOMFILTER => 'ROWCOL', VERSIONS => '1000000', DATA_BLOCK_ENCODING => 'FAST_DIFF', COMPRESSION => 'SNAPPY' , BLOCKCACHE => 'true', BLOCKSIZE => '65536'}


create 'rporg1:rpresult2','r'
create 'rporg2:rpresult2','r'
REGISTER_READS

36998


libsvm***********************************************************************************************************************************************
Scrum meetings
---------------------


 ---- ELM3.1
16/6/15 

- Which value to store
- which prefix is used ?

-Need to pull Rest service from core to ELM3.1 
-(optional)EIP spring context
-Fetch sdp from Hbase rather than EIP
getLatest ??


Done with 8x extractor
weightage algo approach with Vikas


------------------------------------------------------------------------------

TODO:
	
---------------------------------------------------------------------------------------------------------------	

LP_SEP_2013	1891937244
LP_Jan_2016	6519407072 
LP_FEB_2016	 227858370
LP_MAR_2016	  64652562

30/6
-Discuss about re-partitioning LP data for Jan-2016 & LP_SEP_2013 - No need now
-Mean time we are loading RR data 
-Design page review

-Comment s_asset_rel 
-Design & impl in one epic
-7x -incremental - ADFTransactionMonitor, update  
	-initial - PoC need to be created as formal project
date by when to deliver ?


Feb, March2015
Full March2015

27/6 - did not attend

4/5/16
-coding done for RR & LP still need to code for DeviceEvent
-Still start testing all transactional
-LP data volume:  ~8.7 billion (8703855248) 122K 29feb16 - 1Mar2016
-RR data volume: ~1.2 billion

INFO  - APPLICATION SUCCESSFULLY STARTED
INFO  - StructuralExtractApplication Mode:: initial
INFO  - Total Records for cycleCd extraction:29
INFO  - Total Records for Seed data extraction:676
INFO  - AssetDataDevice Total Records:414310
INFO  - AssetDataRoute Total Records:486
INFO  - AssetDataAccount Total Records:101352
INFO  - AssetDataPremise Total Records:73998
INFO  - AssetDataDistNode Total Records:0
 - SdpStackData stat : 
ServiceDeliveryPoint,241744
TotalChannel,2506108,Channel,2506108,VirtualChannel,0
servicePointServiceAgreementAssociation,182367
servicePointDataServiceAssociation,1578077
servicePointDeviceAssociation,240892
deviceChannelAssociation,3510551
deviceFunctionAssociation,193302
servicePointServicePointGroupAssociation,294162
accountServicePointAssociation,383132
servicePointServicePointAssociation SDP-DN,0
servicePointServicePointAssociation DN-DN,0

-Get info on any app parallel to ADFMonitor for incremental transactional data
-RP2.3 -> Engnrng home page



28/4/16
-Identify org name in KCBPU extraction
-Mail to Apratim for orgCreation
-checkin code for HDC-74 in HC1.1
-Create mapping file for structural extraction
	-services
	-measurement
-Get DN extraction working

29/4/16
-Install HC1.1 on lnxapp342 
-Create trasactional extraction scripts for 
	-Interval
	-DeviceEvent
	-RegisterRead


5/2/16
KCBPU extraction
-Dependency: hdfs org creation for -007169584 on lnxapp342 (lnxcdhmg02)
[]
-The product mapping file that has 7x product mapping to 8x products
	-We need RR channel mapping to respective splitted measTypes 
[]
-The RR & LP we need device_data_src & device_data_src_details to be prepopulated in 8x as Loaders does not support loading
-Similarly expect device_event types should be prepopulated

private Map<String, Long> svcPtMap = new HashMap<>();
private Map<String, MeasTypeBO> measTypeMap = new HashMap<>();
private Map<String, Long> userMap = new HashMap<>();
private Map<String, Long> deviceMap = new HashMap<>();
private Map<String, Long> deviceDataSrcMap = new HashMap<>();




7/4/16
-New Wiki home page for extractor / loader
-


7/3/16

-RP: hive integration done, QA is testing it
-Extraction:
	-Transactional extraction is done and QA ie testing
	-Structural extract: Reference data extraction is done
						 Now we r working on asset data extraction
						 



11/2
-Vikas suggested 
	-to include few headers and updated few mappings.
	-Merge logic: save latest record. 

	We are still not handling historical data
Finish for Inverval by EOD



15/1/16
- Working on customization of custom prefetch and custom filters, doc is in review state
	and working on sample code
-


28/1/16
-field name same as protoBuff
-group by keys now part of job arg

14/1/16
-feature and score: document review
-Filter and prefetch: working on doc & sample code
-Any update on RP2.1 requirements ?
	-Extraction Ref + structural 
	-Mariam provided scorer


11/1/16
-Working on customization
	feature and score: document review
	prefetch & filter: Analysis
-Any update on EDP requirements ?

Ling: Epic date is not updated
JV: Extraction is not covered in the epic
	Ref and asset data in diff epic

7/1/16
-EDP: new set or req
-Epic and story to be updated in Jira
-Seperate Dev env for Alyt, raise req with khirod


17/12/15
--------
-Continue working on Loader documentation..
-

14/12/15
--------
-Done with LP loader.
-Need to work on loader documentation
-Hadoop common 1.0 is out, so to incorporate Loaders with release branch, do we need any approval

Ling/Sunitha comments:


10/12/15
--------
-Working on LP loader
?? There is duplicate int check in 7x loader, are we expecting priorver reads ?
If yes, then 7x loader is just discarding 2nd record and adding it to dupList for logging. 
How to select which record to keep.

Ling/Sunitha comments: No need to check duplicate rec as in priorver


7/12/15
-------
-Completed RegisterRead loader
-Started working on LP reads loader
-

Ling/Sunitha comments:







19/10
Algo junit: TWUDZ (tampered with usage drop to zero) & RDLSVZU (Load Side Voltage and Zero Usage Within a Week of Remote Disconnect )
In progress: NUFXD & Opp usage pattern compared to neighbr  

Feature - TAMPER WITH USAGE DROP TO ZERO (TWUDZ) Shambhu  Complete In Progress 
Feature - LOAD SIDE VOLTAGE AND ZERO USAGE ... PAST 30 DAYS Shambhu  Complete In Progress 
Feature - NO USAGE FOR X DAYS (NUFXD) Ravi  In Progress Open 
Feature - OPPOSITE USAGE PATTERN COMPARED TO NEIGHBORS  30-Oct-15 In Progress Open 
Scoring - Implementation using weightage of each feature (Open Open) 

feature status


2/11
-Walkthru with MAriam on weightage Algo
-Discuss Neighbour Algo

12/10
-Have Deployed RP on CDH cluster (mg02), able to run RP jobs. Now need to create test data for features 
-

17/9
-we r incorporating review commnts from Vikas
-Model will now be created with every analysis run
-Data storage structure for HDFS & HBASE
-RP tables restructuring from 7x to 8x
-Code restructuring 


3/9
-From last demo meeting, we have some next steps:
1) Refactor data_svc_gen_conf configuration paramters and clearify the contract b/w executor services
	:Shambhu will be back by tomorrow so we'll take up this task in next sprint.
2) Data storage design & architecture 
	: Ajit and me will be working on it. 
3) Sprint tasks
	: Ajit is working on 7x RP UI deployment and understanding the case management
	: I am working on test case integration and other test case tasks.
	


1/9
Demo II


31/8
-Have incorporated the review comments
-Working on sprint tasks: Test case integration, HDFS data save, seqment query data fetch,
Ajit has started looking into RP UI





24/8 Demo 
comment
-diagram arrows
-job properties not relevant
	-move preFetch related to dataSvc
	-segment Q in oozie.xml
	-start date ... not required instead last 3 minths kind of info is req.

-DataSvc name - RP analysis
	-type - should have prefix - rp.RPfeature

-Define algoDef attributes in entity_flex_attr
-Move RPContext out of driver.....
-remove line propogation from driver  
-Executor
		-Abstract EIDG .. not to be embeded so deep ...
		-move dataFetch sperate from executor and embed it to EIDG...
		-Instantiate new RPContext and execute fetch logic
		-Rename algoDef to DataSvcConfiguration
		-Criteria creation should be encapsulated in some diff function ....
		-Dao is not serializable so encapsulated out and not put in context


-DataStore
	-Move ... data from dataFetch function as overTime they can change
	
	-seperate package -> data ... which will contain all dataStructures


27/7
-Working on POC sparkMlib for model creation
-code integration with ALgorithms start up

6/7
-NTB Algo 
-Algo testing start

7/6
-NTB Algo testing complete
-Code Other Algo

8/6
-Test other Algo
-Integrate with Shambhu code


9/6
-Test deployment complete app
-Start Spark ML POC

10/6 
-Spark ML continues

-sparkML
-spark HDFS data store
-spark hive
-spark job trigger

-spark basics
-spark streaming
-spark cluster tuning
-Lambda exp
-data scientist
-
----------------------------------------------------------------------------

Revenue protection cloud:
-------------------------
17/8
-started deploying RP application on CDH cluster. 
	-resolving configuration exceptions.
	
	
	
	

10/8
-updated status,
-Ling had a question


9/7
-we picked 2 algo , for 1 we'll start testing today and for other implementation is in progress
-
-use /home/tmp to create org folder. HDFS users.
-ravigu org user name password
-login as our user and use deployment. 
-Ask IT ask create users for us

ETP - ready to pick RP.




6/7
-Me and Ajit are Working on Algos
-Shambhu is working on framework

-Coming thursday morning we'll need discussion with Mariam regarding like 
	-Machine learning Model used to generate scores. How it was generated
	-and some more understanding of R
	

2/7
-Completed Wiki doc & Phoenix POC page
-Started working on
	-Code setup with spark and Algo initialization 
	-Algorithms
			-NTB
			-Usage with no Active a/c
-Need info on Nz server and how to access it like aginity client etc



29/6

-Working on implementing design review changes and it will be updated by next meeting for review
-Also we have started working on 
	-RP Configuration Frmwrk
	-Understanding algorithms

19/6

Design review with vikas


22/6

- Working on design changes that were discussed with vikas
	- Phoenix POC: table and data prep
	- Design changes like 
		- Algo configuration step i.e. segment_code_config setup with Algo configuration and initialization.
		- Data prep stage.
		-
		
		
- We will need reference test data and their results of RP Algos i.e what algo value was generated on what test data
  so that we can be sure of implemenation details
	



















2/3

Understand RP existing flow						Complete
Transactional Environment setup					In Progress (5/3)
Understand Hadoop/Spark/Cloudera/HDFS/….		In Progress (13/3)
RP Cloud environment setup						Open
Requirement Analysis							Open


-Completed the understanding of exisiting RP flow, used code to understand the flow.
-In process of setting up the transactional env. To get the insight of data flow in and out of HBASE  
-Now its time to start the design discussions around the Transactional load / RP  design.
-
                           
- Nikhil team UI KT


9/3
-Understanding the hadoop training guide. 
	-hadoop mapreduce
	-hbase
	-spark
We may need the next session by Thursday. I'll confirm as I am done

http://www.immihelp.com/business-visa



16/3
- I am working on cloudera trainings (Using Clodera provided VM setup)
	Now have some understanding with
	-MapReduce
	-HBase
	-Need some more time to understand Spark and then we'll be able to discuss the design
		(On our Thursday / Friday)


Questions:
1) Dev env setup
	- Build projects (ivy resolve)
	- create new org on hbase cluster


30/3
 -Got the basic flow working - spark and HBase
 	
 	-create RDD using HBase
 	-HBase with multi table join
 
 -Need some more documentation on better understanding spark
 -Ozie WF
 
 -What are the next steps 
 
 	
 	
no resolver found called eip-snapshot-with-cloudera-chain: 	
	
2) What is the use of SDP_SEGEMNT table in RP1.0
3) Does the team has any experience on working with HBase Coprocessors like endPoint which are like stored procs in Hbase
	The other approach will be to use filters while scanning the data 
4) Why we want to keep RP 7x tables in Oracle like RP_Feature, RP_algo etc. Reason: In above approach we need to interact with both Oracle and Hbase

load fore cast
 

tasks
-----
1) sdp_segment_cd issue
2) schema design for tables in hbase
3) POC for 1st job to insert data in RP tables and run any 1 algorithm






****************************************************************************************************************************************************




\\192.168.99.20\PIP_Consulting

CNP zone: cnp-int-ws01.emeter.com

./ApplicationLauncher.sh start CloudAlytStructuralExtraction 1 -Da108-structuralExtractorDao.listOfOrgs=FCU
./ApplicationLauncher.sh start CloudAlytStructuralExtraction 1 -Da108-structuralExtractorDao.listOfOrgs=957877905


./ApplicationLauncher.sh start CloudAlytStructuralExtraction 1 -Da108-structuralExtractorDao.listOfOrgs=007169584


0-R9NH,0-5200,0-55RNV,1-338,1-2VEJ,1-J6I,1-1AEB,1-1AFM,1-79N4,1-4O8B

EIP-21470
EIP-21230
EIP-21469
EIP-21468

https://docs.emeter.com/display/EIP77/Dynamic+Throttling+Publisher+Settings


http://jira.emeter.com/browse/EIP-28451 comment-156766
https://docs.emeter.com/display/ALYT14/Analytics+Applications

CloudAnalytics: Structural extract framework setup
CLDALYT-174: Structural extract framework setup
***********************************
//QA/test s/rel_8.2/Automation/flex_sync/testdata/input/run_once_data/flexsync_seeddata_rdu_files/resources/org/LDC1/refData/eip/CycleReferenceData_flexsync_001.xml
//QA/testcases/rel_8.2/Automation/flex_sync/testdata/input/run_once_data/flexsync_seeddata_rdu_files/resources/org/LDC1/refData/eip/

Disconnect collor & Distribution Node

Vikas comment in last meeting

-write down content of file from top down...
-query in batches like 100 sdps then process it.
-suffix register channel like r1.  
-expose dataSrc / use pathName for storing refId - done added in assumption 


9/6/14
* updated the design page with Extrator details
Questions:
  No extract for channel-channel rel 
ANS: We may need in future.
  Service split based on overlapping param
ANS: Split service based on params. Only few has .. only split them 

* Ling - get update for approach in incremental change i.e Trigger Vs Query


comments:
seperate table to capture updated 
poc on sdp chunks in memory 
-get the code up in running soon and will be in incremental mode .. may add cases ongoing
-identify priority objects
	-services, (split logic later) 
	-

16/06
- Document updates
- RP : A new job needs to be written in 7x to identify the records for a batch id
- RP invoke R issues


26/06

capure data as per logical grp,,
7x opt app deployment
dataSrc- system or user... ignore org.. OR keep this configuration
	global vs 
	org Mode - dataSrc is properly popultaed
	
loader : 1 flexSync instance will cater multi Org data load

meas type 7x: carry the information like this 7x meas type is for 
Channel name logic

start with
seed data + svcpt param, meter, DistNode and channel

include sample data for channel and other Assset data_src value in pathName

30/06

Started working on structural data load... 
-framework and code base setup

7/7
- Seed data framework: extraction framework framework with Junits.
- svcptclass and param
- scrum board & add tasks accordingly

15/7

Q1: 7x has Route seed data extraction but 8x RDU does not have support for Route.
Ans: 

Q2: product_mapping table currently planned to be part of siebel scema, but configurtion can also be loaded in memory at start up...
Suggestion required

21/7

Extraction 17/7
-unit test cases framework + build and deploy
-Need to add Meas type values discussed in last meeting
-Design updates done
-Will start on Loader framework

projectName: cloudAlytStructuralExtraction  
cloudAlytAmiExtraction

24/7
-Started with initial extraction of assets
-

28/7
-Working on initial extraction of assets
Route & DistNode are left. Done with account, premise, devices 
+ JUnits are left for Assets 


******************************************************************************************************************************************
Extraction 11/8
----------------
**	
Done with Junits for Asset data 
Will focus on deployment & integration of extractor & Loader to chk for any gap.
For this we need any testdb env on CNP zone , IT WILL be only select queries

sanjay QA env .... 

Extraction 14/8
---------------
Started working on SDP stack extraction for initial mode.
In parallel we'll be doing the integration testing.

..FlexSync changes update on wiki 

25/8

SDPStack creation:
sdp done
in process for channel
distNode extraction discussions

28/8
-Done channel ext.. VC extraction is left
-Spent time in writing queries for rels

distNode: 3 cases
implement only 1: 

follow with Nikhil on namespace - JV


Rakesh:
1) Update query change status to 'Y'
2) incr query add chk for is_processed = 'Y'
3) purging of all processed records
5) Exception handling - revert status to N


 
******************************************************************************************************************************************
2/9

Done Initial SDPStack creation + build given to QA
- DistNode is left 
- Junits for sdpStack

- SdpDataSvcRel splitting is left
- SdpDataSvcRel Channel Type in case of Register splitting - which channel type to use 
- Integration test

- Incremental load query 


4/9
-Working on DistNode extraction
-**Junit for sdpStack

- Rakesh transition: 
	Incr asset load is done need to test 
	Distnode extraction Query handed over for usecase 1
	
** DistNode hieracy : 	
    for each DN corresponding to sdp-dn rel, DN can have parent and could be possible that parent DN can also have another parent DN. 
-- Go for one level

-- FlexSync - Rules test code covrg .. dropped to 65

9/9
-Done - distNode ext + multiMessage structure in message
-wrking on Junit for sdpStack
------
11/9

-Code refactor
-Junit

QA - cross verify from Migration pages
-------
15/9
- Junits, done but have some Q related to DN and DataSvcRel
- Have put on some questions on wiki page known issue 6-13
- continue with Integration test for SdpStack

---------
https://www.youtube.com/watch v=vl9er5-8tpo


Demo test data for 
-----------------
Control flow & Data flow

Prepare
-VC 
-distNode 
-distNode Rel
-Seq diagrams
-


Questions:
-Validate wiki page open issue and known issue
-Validate Measurement Param unused
-validate header
-validateFileName
-validateFolderName
-Once loader processes the zip file should the zip file be moved/deleted
-Discuss jar compatibility as extraction needs to be compatible with 7.2 versions


17/09
- implementing vikas changes
- Added code for cycleCd seed data, DN hirerchy 
- Discuss cycleCd query + DN premise issue

- selecting latest attrib + logging

22/09
-Done with Vikas changes.
-Need to discuss one use case of svcptDataSvc with Jitendra
One integration issue for svcptDataSvcRel: for channel based svcs we have wrapped channel information in attribute so now 
consider the case 1 sdp with 2 channels but same svc on diff channel, since we did hide the channel information so both the
entities become same for eBO. 
-Will start on incr changes verification. 

25/09
-Working on incr changes
-Junit

29/9
-Done with incremental dev and Junits + In process of doing the integration testing 
-Testing changes that Vikas suggested for svcptdataSvcRel.
-Jira story CLDALYT-20 : 7.x Incremental Extraction - De-dup process

acDataLoader/acDataExtractor
-Will start with loader tmrw

6/10

- In process of creating loader build
- 
Code coverage 
Perf data - initial, 1 day and 1 week data 

9/10
-Today we'll move loader code to OPT 
-Working on bugs


13/10
-Please chk the email regarding build details, 
provide ur input for Loader
-Also discuss with Joel to configure the same on buildConsole 
and verify.. code coverage etc


3/11
-ask Khirod for perf env.
-do the perf testing 
-source1 mapping to xxx org

5/11 - regd triggers in cloud
-
-
-


7/11
lnxapp301 - 8x
75dev15lnx - fc
lnxapp285 - 7x
75DEV15LNX-FC 

17/11
-10k data 20 min .. Now ~3 mins but still application gets slow during long run
working on application profiling

20/11

CLDALYT-338 - Connection pool and out of Memory errors during Cloud data extraction
[Have fixed the issue with performance changes. The performace code will be checked in today and we'll do the regression testing tmrw. 
After that I'll close the bug]

CLDALYT-385 - Sample product mapping file does not extract Dist Nodes with Sub type Transformers
[Configuration issue: The product_mapping file does not have entry related to transformers. Please add required entry in file]

CLDALYT-384 - Looks like a valid bug. We'll fix it


15/12
- Regression on updated code -- done
- FCU data load is in progress have installed the application
- Code merge for device & Route: in CNP there are ~22 route and this doesn not take much time even in FCU it is ~300: Shall we do this change  
  Can we expect more route  	
- DN-DN query (while fetching DN) - need to work on it

- QA to AIX execute test case  + DB deployment 


5/1

- CLDALYT439: p1 - Incorrect number of Distribution Nodes getting extracted in Wesstar environment (extractor and loader).
create 
- CLDALYT440: p3 - Wrong message displayed on the console while loader is getting executed (loader)


----------------------------------------------------------------------------------------------

select 'Premise::' || count(*)  from premise;
select 'Accnt::' || count(*) from accnt;
select 'Device::' || count(*) from device;
select 'Route::' || count(*) from svc_pt_group;
select 'SDP::' || count(*) from svc_pt;

select count(*) from svc_pt;

---------------------------------------------------------------------------------------------


4-12
------
FC: Without DN 7 mins for 100K SDPs
2014-12-03 06:14:09,148 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - APPLICATION SUCCESSFULLY STARTED
2014-12-03 06:14:09,148 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - StructuralExtractApplication Mode:: initial
2014-12-03 06:14:10,895 com.emeter.extractor.runner.seeddata.SeedDataCycleCdExtractRunner [pool-2-thread-2] INFO  - Total Records for cycleCd extraction:16
2014-12-03 06:14:11,469 com.emeter.extractor.runner.seeddata.SeedDataExtractRunner [pool-2-thread-1] INFO  - Total Records for Seed data extraction:346
2014-12-03 06:15:16,928 com.emeter.extractor.runner.assetdata.account.AssetDataAccount [main] INFO  - AssetDataAccount Total Records:106778
2014-12-03 06:15:16,928 com.emeter.extractor.runner.assetdata.premise.AssetDataPremise [main] INFO  - AssetDataPremise Total Records:71916
2014-12-03 06:15:16,928 com.emeter.extractor.runner.assetdata.device.AssetDataDevice [main] INFO  - AssetDataDevice Total Records:204374
2014-12-03 06:15:16,929 com.emeter.extractor.runner.assetdata.route.AssetDataRoute [main] INFO  - AssetDataRoute Total Records:398
2014-12-03 06:20:39,954 com.emeter.extractor.runner.sdpstackdata.SdpStackData [main] INFO  - SdpStackData stat : 
ServiceDeliveryPoint,100334
DistributionNode,0
TotalChannel,409768,Channel,409020,VirtualChannel,748
servicePointServiceAgreementAssociation,0
servicePointDataServiceAssociation,181949
servicePointDeviceAssociation,111679
deviceChannelAssociation,466465
deviceFunctionAssociation,128985
servicePointServicePointGroupAssociation,100466
accountServicePointAssociation,147691
servicePointServicePointAssociation SDP-DN,206883
servicePointServicePointAssociation DN-DN,0
2014-12-03 06:21:11,344 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - Done Structural execution in 422.195498 sec

1) Query optimization for DN 
2) Initial mode for CNP takes ~15 min to start as s_asset data is large and has outer join with paramater table 
(same will be with asset data like device)
Solution: for initial mode 
	(i) fetch only sdpSrcIds from s_asset and populate in new o/p table (sno & src_id)
	(ii) Query s_asset using above o/p table in chunks (say 300 at one go)


Task pending: 
1) Documentation

******************************************************************************************************************************************

----Todo


-Loader code checkin 
-Mail to Joel for setting up the build for Loader
-Loader test cases
-Performance: extract
-Performance: loader
-Wiki update 
	queries
	triggers
	services update

****************************************************************************************************************************************
***********************************************************************
h3. Extractor components
{gliffy:name=ExtractorProcess|align=left|size=L|version=1}

Control flow & Data flow: -RunOnceApplication [StructuralExtract7xApplication] - 
				**TASKS: creates execution context.
			  -Delegates control to ExtractManager [StructuralExtract7xManager] - Has list of extractors (SeedData, AssetData & SDPStack). 
			  	**TASKS: (1) Executes extractorList in sequence. 
			  		 (2) Upon successful execution of extractorList, Creates Org based Zip file
			  		 
			  -----AbstractRunner (callable): Each extrator is of type AbstractRunner and executes workflow 
			  	-preProcess()
			  	-processInitalExtraction() / processIncrementalExtraction()
			  	-writeAllDocsToFile();
				-writeAllValidationFailedRecordStat();
				-writeAllSuccessRecordStat();
				-postProcess();
			  -SeedDataExtractor: Perform extraction data for all seed data (SvcPtClass, DeviceClass, srvc etc)
			  	**TASKS: (1) Has a list of extractorRunnerList (a901-seedDataAssetClass). In seedData its one but more in other extraction
			  		 (2) (Generic logic of having runnerList) Executes them in parallel
			  	
			  	-a108-seedDataAssetClass: runnerTask for seedDataExtractor (SeedDataExtractRunner)
			  		**TASKS: (1) preProcess: ExtractorDir creation, loads the mapping table content (truncate and deletes the data)
			  					, loadFilterData .. has configurable file to configure filter data SQLs
			  			 (2) Has respective writer for each type
			  			 (3) Fetches seed data as list and based on handlerType respective writer is invoked:
			  
			  
			  -AssetDataExtractor: Perform extraction for Asset data (Account, Route, Device etc)
			  	**TASKS: (1) Has a list of extractorRunnerList (a108-assetDataExtractor) like assetDataPremise, assetDataDevice, assetDataAccount, assetDataRoute
			  		 (2) (Generic logic of having runnerList) Executes them in parallel
			  			  	
			  	-a108-assetDataPremise: runnerTask for assetDataPremise (AssetDataPremise)
			  		**TASKS: (1) preProcess() -> perform some preprocess like ExtractorDir creation, loadFilterData
						 (2) Mode: initial -> processInitalExtraction
						 			a. Fetch list of Premise
						 			b. Iterate over list and keeps adding to sdpSyncMessage to template (AbstractAssetDataXMLWriter).
						 			   When counter reached close old file and create new file 	
						 			 
							 : incremental 	-> processIncrementalExtraction
						 (3) writeAllDocsToFile : Close all open files
						 (4) writeAllValidationFailedRecordStat: Write failed records stat
						 (5) writeAllSuccessRecordStat: Write successful records stat
			  		  	 (6) postProcess : nothing

			  
			   -SdpStackDataExtractor (SdpStackData): Perform extraction for sdpStack (sdp, channel, DN, rels)
			  	**TASKS: (1) preProcess() -> perform some preprocess like ExtractorDir creation, loadFilterData, Thread pool for worker threads
			  		 (2) Mode: initial -> processInitalExtraction
			  						a. Fetch list of sdps
			  						b. Iterate over list and keep spawning a thread to process sdpStackCreation	
			  						 			 
			  			 : incremental 	-> processIncrementalExtraction
			  		 (6) postProcess : handleCloseFile
							   writeAllValidationFailedRecordStat
							   writeAllSuccessRecordStat
				

			  	SdpStackRunner(callable): receives a sdp an creates sdpStack
			  		**TASKS: (1) sdp: append Sdp data to sdpSyncMessage template
			  			 (2) channel: Fetch channel list for this sdp and append data to sdpSyncMessage template
			  			 (3) vcChannel: Fetch vcChannel list for this sdp and append data to sdpSyncMessage template
			  			 (4) relSdpSvcAgree: Fetch relSdpSvcAgree list for this sdp and append data to sdpSyncMessage template
 			  			 (5) relSdpDataSvc: Fetch relSdpDataSvc list for this sdp and append data to sdpSyncMessage template
			  			 (6) relSdpMeter: Fetch relSdpMeter list for this sdp, filter records and append data to sdpSyncMessage template
			  			 (7) relMeterCommFn: Use above fetched meter list to fetch related relMeterCommFn and append data to sdpSyncMessage template
			  			 (8) relMeterChannel: Fetch relMeterChannel list for this sdp and append data to sdpSyncMessage template
			  			 (9) relsdpRoute: Fetch relsdpRoute list for this sdp and append data to sdpSyncMessage template
			  			 (10) relSdpAccount: Fetch relSdpAccount list for this sdp and append data to sdpSyncMessage template
			  			 (11) relSdpDN: Use above fetched relDN list and append data to sdpSyncMessage template
			  			 (12) DN:  Use above fetched DN list and append data to sdpSyncMessage template
			  			 (13) Merge template across threads


*************************************************************************************************************************************
	

Loader 
-Framework
-unit test cases
-propsets
-build and deploy


12/8 -> With Rakesh
Add last upd time of source to output tables : may need if trigges fail. Now we caan actually have information regarding last successful run.
Make purging configurable:


***Training






javac -cp /home/pipe/opt/revenueprotection/tools/r/Rinstaller/POC/src/rdemo/commons-lang-2.1.jar:/home/pipe/opt/revenueprotection/tools/r/Rinstaller/POC/src/rdemo/JRIEngine.jar:/home/pipe/opt/revenueprotection/tools/r/Rinstaller/POC/src/rdemo/REngine.jar:/home/pipe/opt/revenueprotection/tools/r/Rinstaller/POC/src/rdemo/JRI.jar rdemo/*.java
java -cp /home/pipe/opt/revenueprotection/tools/r/Rinstaller/POC/src/rdemo/commons-lang-2.1.jar:/home/pipe/opt/revenueprotection/tools/r/Rinstaller/POC/src/rdemo/JRIEngine.jar:/home/pipe/opt/revenueprotection/tools/r/Rinstaller/POC/src/rdemo/REngine.jar:/home/pipe/opt/revenueprotection/tools/r/Rinstaller/POC/src/rdemo/JRI.jar:. rdemo/RTestJRI


<beans xmlns= http://www.springframework.org/schema/beans  
		xmlns:xsi= http://www.w3.org/2001/XMLSchema-instance  
		xmlns:aop= http://www.springframework.org/schema/aop  
		xmlns:jaxws= http://cxf.apache.org/jaxws  
		xsi:schemaLocation= http://www.springframework.org/schema/beans 
		http://www.springframework.org/schema/beans/spring-beans-3.0.xsd 
		http://www.springframework.org/schema/aop 
		http://www.springframework.org/schema/aop/spring-aop-3.0.xsd 
		http://cxf.apache.org/jaxws http://cxf.apache.org/schemas/jaxws.xsd >
    
    <aop:config proxy-target-class= true  /> 
	
	<bean class= com.emeter.revenueprotection.util.PropertyPlaceholderConfigurer  />
	
	***************************************


select DISTINCT
   sdp.src_id, sdp.premise_wid, sdp.udc_id, sdp.universal_id, sdp.gps_lat, sdp.gps_long,
   prms.addr_line_1 || prms.addr_line_2 sdp_address,
   op.wid model_output_wid, op.sdp_wid, op.score, op.model_version, op.org_id, op.batch_id, 
        /* op.insert_time, op.last_upd_time, op.analysis_end_date,*/
   ftr.algorithm_wid, ftr.ACTUAL_VALUE, ftr.description
   from RP_FEATURE ftr, RP_MODEL_OUTPUT op, sdp_d sdp, premise_d prms
   where op.sdp_wid = ftr.sdp_wid
   AND prms.wid = sdp.premise_wid
   AND op.sdp_wid = sdp.wid
   AND op.score >= 0.5
   AND op.sdp_wid NOT IN (select sdp_wid from rp_investigation_request);
      
insert into rp_investigation_request values (3,'3',3,1,SYSDATE,SYSDATE);

select DISTINCT ex.sdp_wid, d.src_id from RP_SDP_EXEMPT ex, sdp_d d where ex.sdp_wid = d.wid;
select DISTINCT sr.sdp_wid, d.src_id from RP_SR sr, sdp_d d where sr.sdp_wid = d.wid;

select *
from rp_investigation_request;

--1
insert into premise_d(WID,POSTAL_CODE_WID,SRC_ID,ADDR_LINE_1,ADDR_LINE_2)
values(1,1,1,'Addr11','Addr12');
insert into sdp_d (WID,SRC_ID,ORG_ID,UDC_ID,premise_wid)
values(1, 'srcId-1',1,'udcId-1',1);
insert into rp_feature(BATCH_ID,ALGORITHM_WID,ANALYSIS_END_DATE,SDP_WID,ACTUAL_VALUE,DESCRIPTION,ORG_ID,INSERT_TIME,LAST_UPD_TIME)
values(1,1,sysdate,1,0.1,'Desc 1',1,sysdate,sysdate);
insert into RP_MODEL_OUTPUT(WID,BATCH_ID,ANALYSIS_END_DATE,SDP_WID,SCORE,MODEL_VERSION,ORG_ID,INSERT_TIME,LAST_UPD_TIME)
values(1,1,sysdate,1,0.4,'modelVersion1',1,sysdate,sysdate);
--2
insert into premise_d(WID,POSTAL_CODE_WID,SRC_ID,ADDR_LINE_1,ADDR_LINE_2)
values(2,2,2,'Addr21','Addr22');
insert into sdp_d (WID,SRC_ID,ORG_ID,UDC_ID,premise_wid)
values(2, 'srcId-2',1,'udcId-2',2);
insert into rp_feature(BATCH_ID,ALGORITHM_WID,ANALYSIS_END_DATE,SDP_WID,ACTUAL_VALUE,DESCRIPTION,ORG_ID,INSERT_TIME,LAST_UPD_TIME)
values(2,2,sysdate,2,0.2,'Desc 2',2,sysdate,sysdate);
insert into RP_MODEL_OUTPUT(WID,BATCH_ID,ANALYSIS_END_DATE,SDP_WID,SCORE,MODEL_VERSION,ORG_ID,INSERT_TIME,LAST_UPD_TIME)
values(2,2,sysdate,2,0.5,'modelVersion2',1,sysdate,sysdate);
insert into rp_sdp_exempt(SDP_WID,EXEMPT_START_TIME,EXEMPT_END_TIME,EXEMPT_TYPE,SERVICE_REQUEST_ID,MODEL_OUTPUT_WID,ORG_ID,INSERT_TIME,INSERT_BY,LAST_UPD_BY,LAST_UPD_TIME,REC_VERSION_NUM)
values(2,sysdate,sysdate,'y',22,2,2,sysdate,2,2,sysdate,1);
--3
insert into premise_d(WID,POSTAL_CODE_WID,SRC_ID,ADDR_LINE_1,ADDR_LINE_2)
values(3,3,3,'Addr31','Addr32');
insert into sdp_d (WID,SRC_ID,ORG_ID,UDC_ID,premise_wid)
values(3, 'srcId-3',1,'udcId-3',3);
insert into rp_feature(BATCH_ID,ALGORITHM_WID,ANALYSIS_END_DATE,SDP_WID,ACTUAL_VALUE,DESCRIPTION,ORG_ID,INSERT_TIME,LAST_UPD_TIME)
values(3,3,sysdate,3,0.3,'Desc 3',3,sysdate,sysdate);
insert into RP_MODEL_OUTPUT(WID,BATCH_ID,ANALYSIS_END_DATE,SDP_WID,SCORE,MODEL_VERSION,ORG_ID,INSERT_TIME,LAST_UPD_TIME)
values(3,3,sysdate,3,0.6,'modelVersion3',1,sysdate,sysdate);
insert into rp_investigation_request values (3,'3',3,3,SYSDATE,SYSDATE);
--4
insert into premise_d(WID,POSTAL_CODE_WID,SRC_ID,ADDR_LINE_1,ADDR_LINE_2)
values(4,4,4,'Addr41','Addr42');
insert into sdp_d (WID,SRC_ID,ORG_ID,UDC_ID,premise_wid)
values(4, 'srcId-4',1,'udcId-4',4);
insert into rp_feature(BATCH_ID,ALGORITHM_WID,ANALYSIS_END_DATE,SDP_WID,ACTUAL_VALUE,DESCRIPTION,ORG_ID,INSERT_TIME,LAST_UPD_TIME)
values(4,4,sysdate,4,0.4,'Desc 4',4,sysdate,sysdate);
insert into RP_MODEL_OUTPUT(WID,BATCH_ID,ANALYSIS_END_DATE,SDP_WID,SCORE,MODEL_VERSION,ORG_ID,INSERT_TIME,LAST_UPD_TIME)
values(4,4,sysdate,4,0.7,'modelVersion4',4,sysdate,sysdate);

commit;

-----------------------

select * from rp_model_output;
select * from rp_investigation_request;
select * from rp_feature;
select * from rp_sdp_exempt;
select * from sdp_d;

delete from sdp_d;
delete from rp_investigation_request;
insert into rp_investigation_request values (3,'3',3,3,SYSDATE,SYSDATE);
commit;

drop table rp_feature;
drop table rp_model_output;
drop table rp_investigation_request;
drop table sdp_d;
drop table RP_SDP_EXEMPT;


CREATE TABLE RP_SDP_EXEMPT(
    SDP_WID               NUMBER,
    exempt_start_time     TIMESTAMP(0),
    exempt_end_time       TIMESTAMP(0),
    exempt_type           CHAR(10),
    service_request_id    NUMBER,
    model_output_wid      NUMBER,
    ORG_ID                       NUMBER          NOT NULL,
    INSERT_TIME                  TIMESTAMP(0)    NOT NULL,
    INSERT_BY                    NUMBER          NOT NULL,
    LAST_UPD_BY                  NUMBER          NOT NULL,
    LAST_UPD_TIME                TIMESTAMP(0)    NOT NULL,
    REC_VERSION_NUM              NUMBER          NOT NULL,
    constraint PK_RP_SDP_EXEMPT primary key (SDP_WID, exempt_start_time) using index   
)
;

create table sdp_d(wid INTEGER, src_id varchar2(20));
CREATE TABLE RP_FEATURE(
    BATCH_ID                 INTEGER not null,
    ALGORITHM_WID         	INTEGER    NOT NULL,
    ANALYSIS_END_DATE              date not null,
    SDP_WID                INTEGER not null,
    ACTUAL_VALUE          	NUMBER(21,6) not null,
    DESCRIPTION  		varchar2(4000),
    ORG_ID             INTEGER,
    INSERT_TIME       TIMESTAMP    DEFAULT sysdate NOT NULL,
    LAST_UPD_TIME     TIMESTAMP    DEFAULT sysdate NOT NULL,
    CONSTRAINT PK_RP_FEATURE PRIMARY KEY (BATCH_ID, ALGORITHM_WID, ANALYSIS_END_DATE)
);

/*ALTER TABLE RP_FEATURE ADD CONSTRAINT FK_RP_FEATURE_1
    FOREIGN KEY (ALGORITHM_WID)
    REFERENCES RP_ALGORITHM(WID)
;*/


CREATE TABLE RP_MODEL_OUTPUT(
    WID                  	INTEGER   NOT NULL,
    BATCH_ID                 INTEGER not null,
    ANALYSIS_END_DATE             date not null,
    SDP_WID                INTEGER not null,
    --SEQ_NUM                INTEGER not null,
    SCORE              		NUMBER(10,3)not null,--INTEGER not null,
    MODEL_VERSION     		 VARCHAR2(100),
  ORG_ID             INTEGER,
    INSERT_TIME       TIMESTAMP    DEFAULT sysdate NOT NULL,
    LAST_UPD_TIME     TIMESTAMP    DEFAULT sysdate NOT NULL,
    CONSTRAINT PK_RP_MODEL_OUTPUT PRIMARY KEY (WID),
    CONSTRAINT UK_RP_MODEL_OUTPUT unique (BATCH_ID,  ANALYSIS_END_DATE, SDP_WID, MODEL_VERSION)
);

CREATE TABLE RP_INVESTIGATION_REQUEST(
    model_output_wid    INTEGER           NOT NULL,
    sdp_wid             INTEGER           NOT NULL,
    batch_id            INTEGER    NOT NULL,
    ORG_ID          	INTEGER,
    INSERT_TIME       TIMESTAMP    DEFAULT sysdate NOT NULL,
    LAST_UPD_TIME     TIMESTAMP    DEFAULT sysdate NOT NULL,
    CONSTRAINT PK_RP_INVESTIGATION_REQUEST PRIMARY KEY (model_output_wid)
);

/*ALTER TABLE RP_INVESTIGATION_REQUEST ADD CONSTRAINT FK_RP_INVESTIGATION_REQUEST_1
    FOREIGN KEY (model_output_wid)
    REFERENCES RP_MODEL_OUTPUT(WID)
;*/




CREATE TABLE PREMISE_D
   (WID NUMBER NOT NULL, 
	BATCH_ID NUMBER, 
	POSTAL_CODE_WID NUMBER NOT NULL, 
	SRC_ID VARCHAR2(15 BYTE) NOT NULL, 
	ORG_ID NUMBER, 
	UDC_ID VARCHAR2(50 BYTE), 
	ADDR_LINE_1 VARCHAR2(200 BYTE), 
	ADDR_LINE_2 VARCHAR2(200 BYTE), 
	GPS_LAT NUMBER(22,8), 
	GPS_LONG NUMBER(22,8), 
	TIMEZONE VARCHAR2(80 BYTE), 
	DISTRICT VARCHAR2(100 BYTE), 
	BLDG_NAME VARCHAR2(50 BYTE), 
	BLDG_TYPE VARCHAR2(50 BYTE), 
	FLOOR VARCHAR2(50 BYTE), 
	FLOOR_TYPE VARCHAR2(50 BYTE), 
	STREET_NUM VARCHAR2(50 BYTE), 
	STREET_NUM_SUFFIX VARCHAR2(50 BYTE), 
	POSTAL_DELIVERY_ID VARCHAR2(50 BYTE), 
	POSTAL_DELIVERY_TYPE VARCHAR2(50 BYTE), 
	STREET_NAME VARCHAR2(100 BYTE), 
	STREET_PREFIX VARCHAR2(50 BYTE), 
	STREET_SUFFIX VARCHAR2(50 BYTE), 
	STREET_TYPE VARCHAR2(50 BYTE), 
	UNIT_NUM VARCHAR2(50 BYTE), 
	UNIT_TYPE VARCHAR2(50 BYTE), 
	REGION VARCHAR2(50 BYTE), 
	TERRITORY VARCHAR2(50 BYTE), 
	MAP_INFO VARCHAR2(50 BYTE), 
	DESC_TEXT VARCHAR2(255 BYTE), 
	INSERT_TIME TIMESTAMP (0) DEFAULT systimestamp NOT NULL, 
	LAST_UPD_TIME TIMESTAMP (0) DEFAULT systimestamp NOT NULL);
  
  
  CREATE TABLE SDP_D 
   (	WID NUMBER NOT NULL , 
	BATCH_ID NUMBER, 
	SRC_ID VARCHAR2(15 BYTE) NOT NULL , 
	ORG_ID NUMBER NOT NULL , 
	CLASS_WID NUMBER, 
	DESC_TEXT VARCHAR2(1000 BYTE), 
	PREMISE_WID NUMBER, 
	FEED_LOC VARCHAR2(80 BYTE), 
	GPS_LAT NUMBER(22,8), 
	GPS_LONG NUMBER(22,8), 
	PULSE_OUTPUT_BLOCK VARCHAR2(80 BYTE), 
	UDC_ID VARCHAR2(200 BYTE) NOT NULL , 
	UNIVERSAL_ID VARCHAR2(200 BYTE), 
	IS_VIRTUAL_FLG CHAR(1 BYTE), 
	SEAL_INFO VARCHAR2(50 BYTE), 
	ACCESS_INFO VARCHAR2(200 BYTE), 
	ALT_ACCESS_INFO VARCHAR2(200 BYTE), 
	LOC_INFO VARCHAR2(200 BYTE), 
	ALT_LOC_INFO VARCHAR2(200 BYTE), 
	TYPE VARCHAR2(50 BYTE), 
	SUB_TYPE VARCHAR2(50 BYTE), 
	TIMEZONE_ID NUMBER, 
	GIS_ID VARCHAR2(50 BYTE), 
	BILLED_UPTO_TIME TIMESTAMP (0), 
	POWER_STATUS CHAR(1 BYTE), 
	LOAD_STATUS CHAR(1 BYTE), 
	BILLING_HOLD_STATUS CHAR(1 BYTE), 
	INSERT_TIME TIMESTAMP (0) DEFAULT systimestamp NOT NULL , 
	LAST_UPD_TIME TIMESTAMP (0) DEFAULT systimestamp NOT NULL , 
	X_CUSTTEST10_WID NUMBER, 
	X_CUSTTEST11_WID NUMBER, 
	X_CUSTTEST12_WID NUMBER, 
	X_CUSTTEST13_WID NUMBER, 
	X_CUSTTEST14_WID NUMBER, 
	X_CUSTTEST15_WID NUMBER, 
	X_CUSTTEST16_WID NUMBER, 
	X_CUSTTEST17_WID NUMBER, 
	X_CUSTTEST18_WID NUMBER, 
	X_CUSTTEST19_WID NUMBER, 
	X_CUSTTEST1_WID NUMBER, 
	X_CUSTTEST2_WID NUMBER, 
	X_CUSTTEST3_WID NUMBER, 
	X_CUSTTEST4_WID NUMBER, 
	X_CUSTTEST5_WID NUMBER, 
	X_CUSTTEST6_WID NUMBER, 
	X_CUSTTEST7_WID NUMBER, 
	X_CUSTTEST8_WID NUMBER, 
	X_CUSTTEST9_WID NUMBER, 
	X_CUSTTEST20_WID NUMBER,
  CONSTRAINT PK_SDP_D PRIMARY KEY (WID));
  
  


Have done the 8.x ebo related code changes and have done the unit testing for it.
Will create the cmd application for same
Q
1) 8.x opt has 2 proj em-RP & RP where to chek in
2) these projects have eBO implemented are they working  


lnxapp
76 & 77, 88, 


cat ~/.bash_profile
********************************************************************************************
  .bash_profile

  Get the aliases and functions
if [ -f ~/.bashrc ]; then
        . ~/.bashrc
fi

  User specific environment and startup programs

. /home/pipe/bin/EnvSetup.sh
 export R_HOME=/home/pipe/khirod/R_HOME
export TZ=CST6DCT
export ODBCINI=/etc
export NZ_ODBC_INI_PATH=/etc
export R_HOME=/home/pipe/khirod/R_HOME
export LD_LIBRARY_PATH= R_HOME/lib/R/lib: R_HOME/lib/R/library: R_HOME/lib/R/library/rJava/jri: JAVA_HOME/jre/lib/i386/server: JAVA_HOME/jre/lib/i386: LD_LIBRARY_PATH
export PATH= R_HOME/bin: R_HOME/lib/R/library/rJava/jri: R_HOME/lib/R/lib/libR.so: PATH
********************************************************************************************

javac  -cp :/home/pipe/khirod/R_HOME/lib/R/library/rJava/jri/* rdemo/RTestJRI.java
java  -cp :/home/pipe/khirod/R_HOME/lib/R/library/rJava/jri/* rdemo/RTestJRI

********************************************************************************************


–no-save”,”–no-environ



drwxr-x--- 3 pipe pipe 4096 Mar 10 08:48 share
drwxr-x--- 4 pipe pipe 4096 Mar 10 08:48 lib
drwxr-x--- 2 pipe pipe 4096 Mar 10 08:48 bin
drwxr-x--- 2 pipe pipe 4096 Mar 11 23:48 etc 

nzDebug(TRUE)

R -q --vanilla --args eip_ra_nz_01_usr 3m3t3r_cnp_nzo1 vnetezza.emeter.com eip_ra_nz_01 81 RP_MODEL_INPUT_V RP_MODEL_OUTPUT /home/pipe/RGA/RevenueProtection/trunk/netezza/bin/RP_MODEL_Version1.rda </home/pipe/RGA/RevenueProtection/trunk/netezza/bin/scoring_file_wrapper.R> /home/pipe/RGA/R/testROUT.out

R -q --vanilla --args eip_ra_nz_01_usr 3m3t3r_cnp_nzo1 vnetezza.emeter.com eip_ra_nz_01 2012-07-01 2012-07-04 RP_MODEL_INPUT_V RP_MODEL_OUTPUT /home/pipe/opt/revenueprotection/bin/RP_MODEL_Version1.rda /home/pipe/opt/revenueprotection/bin/ </home/pipe/opt/revenueprotection/bin/scoring_file_wrapper.R> /home/pipe/RGA/Rinstaller/POC/src/testROUT.out
R -q --vanilla --args eip_ra_nz_01_usr 3m3t3r_cnp_nzo1 vnetezza.emeter.com eip_ra_nz_01 2012-07-10 2012-07-13 RP_MODEL_INPUT_V RP_MODEL_OUTPUT /home/pipe/Mariam/R_Model_1_0/RP_MODEL.rda /home/pipe/Mariam/R_Model_1_0/ </home/pipe/Mariam/R_Model_1_0/scoring_file_wrapper.R> /home/pipe/Mariam/R_Model_1_0/testROUT.out

R -q --vanilla --args eip_ra_nz_01_usr 3m3t3r_cnp_nzo1 vnetezza.emeter.com eip_ra_nz_01 2012-07-01 2012-07-13 RP_MODEL_INPUT_V RP_MODEL_OUTPUT /home/pipe/opt/revenueprotection/bin/RP_MODEL.rda /home/pipe/opt/revenueprotection/bin/ </home/pipe/opt/revenueprotection/bin/scoring_file_wrapper.R> /home/pipe/logs/testROUT.out


R -q --vanilla --args eip_ra_nz_30_usr eip_ra_nz_30_usr vnetezza.emeter.com eip_ra_nz_30 1 RP_MODEL_INPUT_V RP_MODEL_OUTPUT /home/pipe/opt/revenueprotection/bin/RP_MODEL.rda /home/pipe/opt/revenueprotection/bin/ </home/pipe/opt/revenueprotection/bin/scoring_file_wrapper.R> /home/pipe/logs/testROUT.out


************************************************
To disable telnet/rexec services on Aix servers [ http://booosystemadmin.blogspot.in/2010/07/to-disable-telnetrexecrsh-on-aix.html ]

1. Take a backup of /etc/inetd.conf(before editing the file better to to a backup)
cp -p /etc/inetd.conf /etc/inetd.conf_backup

2. Edit /etc/inetd.conf file and comment out telnet,rexec, services
vi /etc/inetd.conf
comment out (enter   ) the telnet, rexec services

3. Restart the inetd service
refresh -s inetd

........Linux......... [ http://www.linuxquestions.org/questions/linux-server-73/disabling-rexec-rsh-rlogin-and-rcp-on-debian-4175425605/ ]

ps -ef |grep inetd







***************************************************


Test

R
>library(RODBC)
>library(randomForest)
>library(nza)
>library(nzr)
>library(tree)
>nzConnect( eip_ra_nz_01_usr ,  3m3t3r_cnp_nzo1 ,  vnetezza.emeter.com ,  eip_ra_nz_01 )

With JRI 


********************************************************************************************
.bash_profile

. /home/pipe/bin/EnvSetup.sh
 export R_HOME=/home/pipe/khirod/R_HOME
export TZ=CST6DCT
export ODBCINI=/etc
export NZ_ODBC_INI_PATH=/etc
export R_HOME=/home/pipe/khirod/R_HOME
export LD_LIBRARY_PATH= R_HOME/lib/R/lib: R_HOME/lib/R/library: R_HOME/lib/R/library/rJava/jri: JAVA_HOME/jre/lib/i386/server: JAVA_HOME/jre/lib/i386: LD_LIBRARY_PATH
export PATH= R_HOME/bin: R_HOME/lib/R/library/rJava/jri: R_HOME/lib/R/lib/libR.so: R_HOME/lib/R/lib/libRlapack.so: PATH



- RDU for SR type & subType for RP
- nzjdbc jar ivy dependency
- 





 export R_ENVIRON= R_HOME/lib/R/etc/Renviron.site
export R_LIBS= R_HOME/lib/R/library



********************************************************************************************
.bash_profile

  .bash_profile

  Get the aliases and functions
if [ -f ~/.bashrc ]; then
        . ~/.bashrc
fi

  User specific environment and startup programs

. /home/pipe/bin/EnvSetup.sh
 export R_HOME=/home/pipe/khirod/R_HOME
export TZ=CST6DCT
export ODBCINI=/etc
export NZ_ODBC_INI_PATH=/etc
export R_HOME=/home/pipe/R_HOME/R-2.15.3
export LD_LIBRARY_PATH= R_HOME/lib/R/lib: R_HOME/lib/R/library: R_HOME/lib/R/library/rJava/jri: JAVA_HOME/jre/lib/i386/server: JAVA_HOME/jre/lib/i386: LD_LIBRARY_PATH
export PATH= R_HOME/bin: R_HOME/lib/R/library/rJava/jri: R_HOME/lib/R/lib/libR.so: R_HOME/lib/R/lib/libRlapack.so: PATH
 export R_ENVIRON= R_HOME/lib/R/etc/Renviron.site
export R_LIBS= R_HOME/lib/R/library






********************************************************************************************
set

[pipe@lnxapp41 rdemo]  set
APP_PROPS=/home/pipe/conf/appProperties
BASH=/bin/bash
BASH_ARGC=()
BASH_ARGV=()
BASH_LINENO=()
BASH_SOURCE=()
BASH_VERSINFO=([0]= 3  [1]= 2  [2]= 25  [3]= 1  [4]= release  [5]= i686-redhat-linux-gnu )
BASH_VERSION='3.2.25(1)-release'
CATALINA_OPTS='-Xms256m -Xmx1024m -XX:MaxPermSize=512m'
COLORS=/etc/DIR_COLORS
COLUMNS=178
CVS_RSH=ssh
DIRSTACK=()
EUID=504
GROUPS=()
G_BROKEN_FILENAMES=1
HISTFILE=/home/pipe/.bash_history
HISTFILESIZE=1000
HISTSIZE=1000
HOME=/home/pipe
HOSTNAME=lnxapp41
HOSTTYPE=i686
IFS= ' \t\n'
INPUTRC=/etc/inputrc
JAVA_HOME=/home/java
JAVA_OPTS=' -server -Dfile.encoding=utf8 '
KDEDIR=/usr
KDE_IS_PRELINKED=1
KDE_NO_IPV6=1
LANG=en_US.UTF-8
LD_LIBRARY_PATH=/home/pipe/khirod/R_HOME/lib/R/lib:/home/pipe/khirod/R_HOME/lib/R/library:/home/pipe/khirod/R_HOME/lib/R/library/rJava/jri:/home/java/jre/lib/i386/server:/home/java/jre/lib/i386:/home/pipe/khirod/R_HOME/lib/R/etc:/home/tibco/tibrv/8.1/lib:/home/oracle/10g/lib32:/home/oracle/10g/lib:/home/pipe/nz/lib:/home/pipe/nz/lib64
LESSOPEN='|/usr/bin/lesspipe.sh  s'
LIBPATH=/home/tibco/tibrv/8.1/lib:/home/oracle/10g/lib32:/home/oracle/10g/lib:/home/pipe/nz/lib
LIB_PATH=/home/tibco/tibrv/8.1/lib:/home/oracle/10g/lib32:/home/oracle/10g/lib
LINES=58
LOGNAME=pipe
LS_COLORS='no=00:fi=00:di=01;34:ln=01;36:pi=40;33:so=01;35:bd=40;33;01:cd=40;33;01:or=01;05;37;41:mi=01;05;37;41:ex=01;32:*.cmd=01;32:*.exe=01;32:*.com=01;32:*.btm=01;32:*.bat=01;32:*.sh=01;32:*.csh=01;32:*.tar=01;31:*.tgz=01;31:*.arj=01;31:*.taz=01;31:*.lzh=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.gz=01;31:*.bz2=01;31:*.bz=01;31:*.tz=01;31:*.rpm=01;31:*.cpio=01;31:*.jpg=01;35:*.gif=01;35:*.bmp=01;35:*.xbm=01;35:*.xpm=01;35:*.png=01;35:*.tif=01;35:'
MACHTYPE=i686-redhat-linux-gnu
MAIL=/var/spool/mail/pipe
MAILCHECK=60
NZ_HOST=vnetezza
NZ_ODBC_INI_PATH=/etc
ODBCINI=/etc
OLDPWD=/home/pipe/RGA/POC/src
OPTERR=1
OPTIND=1
ORACLE_BASE=/home/oracle
ORACLE_HOME=/home/oracle/10g
OSTYPE=linux-gnu
PATH=/home/pipe/khirod/R_HOME/bin:/home/pipe/khirod/R_HOME/lib/R/library/rJava/jri:/home/pipe/khirod/R_HOME/lib/R/lib/libR.so:/home/pipe/khirod/R_HOME/lib/R/etc/Renviron:/home/java/bin:/home/java/jre/bin:/usr/lib/qt-3.3/bin:/usr/kerberos/bin:/usr/local/bin:/bin:/usr/bin:/home/pipe/bin:/home/pipe/bin:/home/tibco/tibrv/8.1/bin:/home/oracle/10g/bin:/home/pipe/nz/bin:/home/pipe/nz/support/bin:/home/pipe/opt/analytics/bin
PIPESTATUS=([0]= 0 )
PIPE_BIN=/home/pipe/bin
PIPE_CONFIG=/home/pipe/conf
PIPE_CONFIG_APP=/home/pipe/conf/appProperties
PIPE_CONFIG_SYS=/home/pipe/conf/systemProperties
PIPE_HOME=/home/pipe
PIPE_LIB=/home/pipe/lib
PIPE_LOGS=/home/pipe/logs
PIPE_OUT=/home/pipe/logs/out
PIPE_REPORTS=/home/pipe/Reports
PPID=6729
PRELINKING=yes
PRELINK_FULL_TIME_INTERVAL=14
PRELINK_NONRPM_CHECK_INTERVAL=7
PRELINK_OPTS=-mR
PS1='[\u@\h \W]\  '
PS2='> '
PS4='+ '
PWD=/home/pipe/RGA/POC/src/rdemo
QTDIR=/usr/lib/qt-3.3
QTINC=/usr/lib/qt-3.3/include
QTLIB=/usr/lib/qt-3.3/lib
R_ENVIRON=/home/pipe/khirod/R_HOME/lib/R/etc/Renviron.site
R_HOME=/home/pipe/khirod/R_HOME
SHELL=/bin/bash
SHELLOPTS=braceexpand:emacs:hashall:histexpand:history:interactive-comments:monitor
SHLVL=1
SSH_ASKPASS=/usr/libexec/openssh/gnome-ssh-askpass
SSH_CLIENT='192.168.195.100 65142 22'
SSH_CONNECTION='192.168.195.100 65142 192.168.195.181 22'
SSH_TTY=/dev/pts/4
TERM=vt100
TIBCO_HOME=/home/tibco
TZ=CST6DCT
UID=504
USER=pipe
_=/home/pipe/.bash_profile
consoletype=pty
mpi_selection=
mpi_selector_dir=/var/lib/mpi-selector/data
mpi_selector_homefile=/home/pipe/.mpi-selector
mpi_selector_sysfile=/etc/sysconfig/mpi-selector
qt_prefix=/usr/lib/qt-3.3

********************************************************************************************

+ rJava package installation. 


R install with 

8.x india env
-------------
DB: ind-syncdb01.emeter.com
user/pswd: eip02/eip02
SID:db01


RP UI tables:

RP_SDP_EXEMPT
RP_SR
RP_SR_ALGORITHM
RP_SR_INVESTIGATION
RP_FEEDBACK_INFO
service_request
sr_pending


CreateInvestigationTicket Job uses following tables:

1) rp_sdp_exempt: used as input table i.e queries it for further processing.
2) rp_sr: used as input table i.e queries it for further processing.
3) service_request: inserts data into the table
4) sr_pending: inserts data into the table
5) rp_sr: inserts data into the table
6) rp_sr_algorithm: inserts data into the table

Few useful insert statement:

insert into rp_sdp_exempt(SDP_WID,EXEMPT_START_TIME,EXEMPT_END_TIME,EXEMPT_TYPE,SERVICE_REQUEST_ID,MODEL_OUTPUT_WID,ORG_ID,INSERT_TIME,INSERT_BY,LAST_UPD_BY,LAST_UPD_TIME,REC_VERSION_NUM)
values(2,sysdate,sysdate,'y',22,2,1,sysdate,1,1,sysdate,1);

insert into rp_sr(service_request_id,service_request_open_time,model_output_wid,rp_model_score,sdp_wid,sdp_udc_id,
sdp_universal_id,sdp_gps_lat,sdp_gps_long,sdp_address,premise_wid,data_ref_start_time,data_ref_end_time,
recommendation_cd,ORG_ID,INSERT_TIME,INSERT_BY,LAST_UPD_BY,LAST_UPD_TIME,REC_VERSION_NUM )
values(4,sysdate,4,0.7,4,'sdpudc-4','sdpuniv-4',0.23,0.45,'address',4,sysdate,sysdate,'Y',1,sysdate,1,1,sysdate,1);








http://cran.fhcrc.org/src/contrib/
[http://cran.r-project.org/src/contrib/Archive/randomForest/]

---------------------------------------------------------------------------------------------------------------------

2014-03-18 21:57:20,812 [pool-2-thread-1] INFO  - Start R
2014-03-18 21:57:20,826 [pool-2-thread-1] INFO  -  [1]
2014-03-18 21:57:20,826 [pool-2-thread-1] INFO  -   eip_ra_nz_01_usr                                           
2014-03-18 21:57:20,826 [pool-2-thread-1] INFO  - 

2014-03-18 21:57:20,826 [pool-2-thread-1] INFO  -  [2]
2014-03-18 21:57:20,826 [pool-2-thread-1] INFO  -   3m3t3r_cnp_nzo1                                            
2014-03-18 21:57:20,826 [pool-2-thread-1] INFO  - 

2014-03-18 21:57:20,826 [pool-2-thread-1] INFO  -  [3]
2014-03-18 21:57:20,826 [pool-2-thread-1] INFO  -   vnetezza                                                   
2014-03-18 21:57:20,826 [pool-2-thread-1] INFO  - 

2014-03-18 21:57:20,826 [pool-2-thread-1] INFO  -  [4]
2014-03-18 21:57:20,826 [pool-2-thread-1] INFO  -   eip_ra_nz_01                                               
2014-03-18 21:57:20,826 [pool-2-thread-1] INFO  - 

2014-03-18 21:57:20,826 [pool-2-thread-1] INFO  -  [5]
2014-03-18 21:57:20,826 [pool-2-thread-1] INFO  -   2014-03-17                                                 
2014-03-18 21:57:20,826 [pool-2-thread-1] INFO  - 

2014-03-18 21:57:20,826 [pool-2-thread-1] INFO  -  [6]
2014-03-18 21:57:20,827 [pool-2-thread-1] INFO  -   2014-03-18                                                 
2014-03-18 21:57:20,827 [pool-2-thread-1] INFO  - 

2014-03-18 21:57:20,827 [pool-2-thread-1] INFO  -  [7]
2014-03-18 21:57:20,827 [pool-2-thread-1] INFO  -   RP_MODEL_INPUT_V                                           
2014-03-18 21:57:20,827 [pool-2-thread-1] INFO  - 

2014-03-18 21:57:20,827 [pool-2-thread-1] INFO  -  [8]
2014-03-18 21:57:20,827 [pool-2-thread-1] INFO  -   RP_MODEL_OUTPUT                                            
2014-03-18 21:57:20,827 [pool-2-thread-1] INFO  - 

2014-03-18 21:57:20,827 [pool-2-thread-1] INFO  -  [9]
2014-03-18 21:57:20,827 [pool-2-thread-1] INFO  -   /home/pipe/opt/revenueprotection/bin/RP_MODEL_Version1.rda 
2014-03-18 21:57:20,827 [pool-2-thread-1] INFO  - 

2014-03-18 21:57:20,827 [pool-2-thread-1] INFO  - [10]
2014-03-18 21:57:20,827 [pool-2-thread-1] INFO  -   /home/pipe/opt/revenueprotection/bin/                      
2014-03-18 21:57:20,827 [pool-2-thread-1] INFO  - 

2014-03-18 21:57:20,896 [pool-2-thread-1] INFO  - randomForest 4.5-34

2014-03-18 21:57:20,896 [pool-2-thread-1] INFO  - Type rfNews() to see new features/changes/bug fixes.

2014-03-18 21:57:20,913 [pool-2-thread-1] INFO  - Loading required package: nzr

2014-03-18 21:57:20,921 [pool-2-thread-1] INFO  - Loading required package: bitops

2014-03-18 21:57:21,292 [pool-2-thread-1] INFO  - Loading required package: tree

2014-03-18 21:57:21,327 [pool-2-thread-1] INFO  - Loading required package: e1071

2014-03-18 21:57:21,332 [pool-2-thread-1] INFO  - Loading required package: class

2014-03-18 21:57:21,443 [pool-2-thread-1] INFO  - Loading required package: MASS

2014-03-18 21:57:21,494 [pool-2-thread-1] INFO  - Loading required package: ca

2014-03-18 21:57:21,523 [pool-2-thread-1] INFO  - Loading required package: XML

2014-03-18 21:57:25,374 [pool-2-thread-1] INFO  - Installed version of '  r_ae  ' cartridge:  3.0.0.33762

On the spus: R 2.15.3
On the host: R 2.15.3
2014-03-18 21:57:28,993 [pool-2-thread-1] INFO  - [1]
2014-03-18 21:57:28,994 [pool-2-thread-1] INFO  -   Scored records:  
2014-03-18 21:57:28,994 [pool-2-thread-1] INFO  -   24617            
2014-03-18 21:57:28,994 [pool-2-thread-1] INFO  - 

2014-03-18 21:57:28,998 [pool-2-thread-1] INFO  -   
2014-03-18 21:57:28,998 [pool-2-thread-1] INFO  -  SDP_WID
2014-03-18 21:57:28,998 [pool-2-thread-1] INFO  -  ANALYSIS_END_DATE
2014-03-18 21:57:28,998 [pool-2-thread-1] INFO  -  SCORE
2014-03-18 21:57:28,998 [pool-2-thread-1] INFO  - 
1 
2014-03-18 21:57:28,998 [pool-2-thread-1] INFO  -   609841
2014-03-18 21:57:28,998 [pool-2-thread-1] INFO  -         2012-07-01
2014-03-18 21:57:28,998 [pool-2-thread-1] INFO  -      0
2014-03-18 21:57:28,998 [pool-2-thread-1] INFO  - 
2 
2014-03-18 21:57:28,998 [pool-2-thread-1] INFO  -  1137965
2014-03-18 21:57:28,998 [pool-2-thread-1] INFO  -         2012-07-01
2014-03-18 21:57:28,998 [pool-2-thread-1] INFO  -      2
2014-03-18 21:57:28,998 [pool-2-thread-1] INFO  - 
3 
2014-03-18 21:57:28,998 [pool-2-thread-1] INFO  -  1464190
2014-03-18 21:57:28,999 [pool-2-thread-1] INFO  -         2012-07-01
2014-03-18 21:57:28,999 [pool-2-thread-1] INFO  -      0
2014-03-18 21:57:28,999 [pool-2-thread-1] INFO  - 
4 
2014-03-18 21:57:28,999 [pool-2-thread-1] INFO  -  1839284
2014-03-18 21:57:28,999 [pool-2-thread-1] INFO  -         2012-07-01
2014-03-18 21:57:28,999 [pool-2-thread-1] INFO  -      0
2014-03-18 21:57:28,999 [pool-2-thread-1] INFO  - 
5 
2014-03-18 21:57:28,999 [pool-2-thread-1] INFO  -  2034191
2014-03-18 21:57:28,999 [pool-2-thread-1] INFO  -         2012-07-01
2014-03-18 21:57:28,999 [pool-2-thread-1] INFO  -      0
2014-03-18 21:57:28,999 [pool-2-thread-1] INFO  - 
6 
2014-03-18 21:57:28,999 [pool-2-thread-1] INFO  -   491403
2014-03-18 21:57:28,999 [pool-2-thread-1] INFO  -         2012-07-01
2014-03-18 21:57:28,999 [pool-2-thread-1] INFO  -     10
2014-03-18 21:57:28,999 [pool-2-thread-1] INFO  - 
7 
2014-03-18 21:57:28,999 [pool-2-thread-1] INFO  -  1878476
2014-03-18 21:57:28,999 [pool-2-thread-1] INFO  -         2012-07-01
2014-03-18 21:57:28,999 [pool-2-thread-1] INFO  -      0
2014-03-18 21:57:28,999 [pool-2-thread-1] INFO  - 
8 
2014-03-18 21:57:28,999 [pool-2-thread-1] INFO  -  1854096
2014-03-18 21:57:28,999 [pool-2-thread-1] INFO  -         2012-07-01
2014-03-18 21:57:28,999 [pool-2-thread-1] INFO  -      0
2014-03-18 21:57:28,999 [pool-2-thread-1] INFO  - 
9 
2014-03-18 21:57:29,000 [pool-2-thread-1] INFO  -   222327
2014-03-18 21:57:29,000 [pool-2-thread-1] INFO  -         2012-07-01
2014-03-18 21:57:29,000 [pool-2-thread-1] INFO  -      0
2014-03-18 21:57:29,000 [pool-2-thread-1] INFO  - 
10
2014-03-18 21:57:29,000 [pool-2-thread-1] INFO  -   361925
2014-03-18 21:57:29,000 [pool-2-thread-1] INFO  -         2012-07-01
2014-03-18 21:57:29,000 [pool-2-thread-1] INFO  -      0
2014-03-18 21:57:29,000 [pool-2-thread-1] INFO  - 




---------------------------------------------------------------------------------------------------

pipe@lnxapp30:/home/pipe/RGA/Rinstaller/POC/src->java -cp :/home/pipe/RGA/Rinstaller/POC/src/rdemo/* rdemo/RTestJRI
Args:[-q, --vanilla, --args, eip_ra_nz_01_usr, 3m3t3r_cnp_nzo1, vnetezza.emeter.com, eip_ra_nz_01, 2012-07-01, 2012-07-04, RP_MODEL_INPUT_V, RP_MODEL_OUTPUT, /home/pipe/opt/revenueprotection/bin/RP_MODEL_Version1.rda, /home/pipe/opt/revenueprotection/bin/]
Initailize R:true
Initialize done R
Start R
 [1]  eip_ra_nz_01_usr                                           
 [2]  3m3t3r_cnp_nzo1                                            
 [3]  vnetezza.emeter.com                                        
 [4]  eip_ra_nz_01                                               
 [5]  2012-07-01                                                 
 [6]  2012-07-04                                                 
 [7]  RP_MODEL_INPUT_V                                           
 [8]  RP_MODEL_OUTPUT                                            
 [9]  /home/pipe/opt/revenueprotection/bin/RP_MODEL_Version1.rda 
[10]  /home/pipe/opt/revenueprotection/bin/                      
randomForest 4.5-34
Type rfNews() to see new features/changes/bug fixes.
Loading required package: nzr
Loading required package: bitops
Loading required package: tree
Loading required package: e1071
Loading required package: class
Loading required package: MASS
Loading required package: ca
Loading required package: XML
Installed version of '  r_ae  ' cartridge:  3.0.0.33762

On the spus: R 2.15.3
On the host: R 2.15.3[1]  Scored records:    24617            
   SDP_WID ANALYSIS_END_DATE SCORE
1   835539        2012-07-01     0
2  1290414        2012-07-01     0
3  1394868        2012-07-01     0
4  1797470        2012-07-01     0
5  1817434        2012-07-01     0
6  1922532        2012-07-01     0
7  2195656        2012-07-01     0
8  4301804        2012-07-01     0
9  4407730        2012-07-01     0
10   52875        2012-07-01     0
------------------------------------------------------------------------------------------------------------

2014-03-19 00:00:39,945 [pool-2-thread-2] INFO  - [10]
2014-03-19 00:00:39,945 [pool-2-thread-2] INFO  -   /home/pipe/opt/revenueprotection/bin/                      
2014-03-19 00:00:39,945 [pool-2-thread-2] INFO  - 

2014-03-19 00:00:39,950 [pool-2-thread-2] INFO  - Error in nz.connection.sanity.check(force) : 
  already connected - close the previous connection with `nzDisconnect` before opening a new one

2014-03-19 00:00:39,950 [pool-2-thread-2] INFO  - R invocation process failed
2014-03-19 00:00:39,950 [pool-2-thread-2] INFO  - R invocation process failed.   Done R execution in 0.010515701 sec


------------------------------------------------------------------------------------------------------------
20/03

Will try with updated files
Need 8.x server to test RP create ticket job (Netezza shld be accessible from  this machine)

a1dev..15 205
lnxapp205
lnxapp206 - QA
****************************************************************************************************************************************

java -cp :/home/eip/lib/*:/home/eip/opt/revenueprotection/lib/em-revenueprotection.jar:/home/eip/opt/revenueprotection/lib/nzjdbc.jar org.junit.runner.JUnitCore com.emeter.revenueprotection.job.createTicket.test.CreateInvestigationTicketJobTest

****************************************************************************************************************************************
./orgAdmin.sh -o 957877905 -u 957877905 -f 957877905 -l 957877905 -p 957877905 -c create
correct
./orgAdmin.sh -o 957877905 -u Admin957877905 -f 957877905 -l LastName -p pipepass -c create

./bin/orgAdmin.sh -c create -d /home/eip -o DATAMIG1 -u DATAMIGORG1USER1 -p DATAMIGORG1USER1 -f fname_DATAMIGORG1USER1 -l lname_DATAMIGORG1USER1
./bin/orgAdmin.sh -c create -d /home/eip -o <orgName> -u <userName> -p <userPassword> -f <user_firstName> -l <user_lastName>


./orgAdmin.sh -o ravigu -u ravigu_user -f ravigu -l ravigu -p ravigu_user -c create
./orgAdmin.sh -c importRefData -o ravigu 

./orgAdmin.sh -o ajitsh -u ajitsh_user -f ajitsh -l ajitsh -p ajitsh_user -c create
./orgAdmin.sh -c importRefData -o ajitsh 


./orgAdmin.sh -o 007169584 -u 007169584 -f 007169584 -l 007169584 -p 007169584 -c create

****************************************************************************************************************************************

Step 1: Get list of faulty SDPs, with details, which has sdpScore >= thresholdSDPScore
	Query (Analytics DB) RP_MODEL_OUTPUT & RP_FEATURE to get SDPs and their details having score > thresholdSDPScore


Step 2: Get list of all SDPs that need to be exempted

i.	No open ticket exist for identified SDP.
	Query (Analytics DB) RP_INVESTIGATION_REQUEST(This table will have all the open investigation request)] 
	
ii.	SDP is not an exempt SDP.
	Query (Eip schema) RP_SDP_EXEMPT to get the list of SDPs that need to be exempted
	
iii	SDP is not in RP_SR
	Query (Eip schema) RP_SR to get the list of SDPs that need to be exempted


Step 3: Filter the list i.e. faulty - exempted.

Step 4: Insert the filtered list from step3 into RP_INVESTIGATION_REQUEST.

Step 5: Create entry into SERVICE_REQUEST & SR_PENDING for each SDP.  

Step 6: Insert record in RP_SR  and RP_SR_ALGORITHM(using SERVICE_REQUEST_ID from step 5) for each SDP.


****************************************************************************************************************************************

>java -cp :/home/eip/lib/*:/home/eip/opt/revenueprotection/lib/em-revenueprotection.jar:/home/eip/opt/revenueprotection/lib/nzjdbc.jar org.junit.runner.JUnitCore com.emeter.revenueprotection.job.createTicket.test.CreateInvestigationTicketJobTest

****************************************************************************************************************************************

WIKI link 
http://wiki.emeter.com/display/PRDMAN/Revenue+Protection
http://wiki.emeter.com/display/ENG/REVPRO1+Design+Docs

****************************************************************************************************************************************


documentation
Junit

/data/jenkins/workspace/em-revenueprotection/build.xml:27: /data/jenkins/workspace/em-revenueprotection/ {publish.stl.dir}/db/DatabaseBuild does not exist.



* execute line by line from R console
* active-sessions
* Trouble-shooting Nz / R 
* Session browser
* extraction scripts

lnxapp30
R -q --vanilla --args eip_ra_nz_01_usr 3m3t3r_cnp_nzo1 vnetezza.emeter.com eip_ra_nz_01 2012-07-01 2012-07-04 RP_MODEL_INPUT_V RP_MODEL_OUTPUT /home/pipe/opt/revenueprotection/bin/RP_MODEL_Version1.rda /home/pipe/opt/revenueprotection/bin/ </home/pipe/opt/revenueprotection/bin/scoring_file_wrapper.R> /home/pipe/logs/testROUT.out

lnxapp41
/home/pipe/khirod/R_HOME/bin/R -q --vanilla --args eip_ra_nz_01_usr 3m3t3r_cnp_nzo1 vnetezza.emeter.com eip_ra_nz_01 2012-07-01 2012-07-04 RP_MODEL_INPUT_V RP_MODEL_OUTPUT /home/pipe/RGA/RevenueProtection/trunk/netezza/bin/RP_Model_11_Month.rda </home/pipe/RGA/RevenueProtection/trunk/netezza/bin/main_wrapper.R> /home/pipe/RGA/POC/src/testROUT.out

new on lnxapp41
/home/pipe/khirod/R_HOME/bin/R -q --vanilla --args eip_ra_nz_01_usr 3m3t3r_cnp_nzo1 vnetezza.emeter.com eip_ra_nz_01 2012-07-01 2012-07-04 RP_MODEL_INPUT_V RP_MODEL_OUTPUT /home/pipe/opt/revenueprotection/bin/RP_MODEL_Version1.rda /home/pipe/opt/revenueprotection/bin/ </home/pipe/opt/revenueprotection/bin/scoring_file_wrapper.R> /home/pipe/logs/testROUT.out

library(RODBC)
library(randomForest)
library(nza)
library(nzr)
library(tree)
nzConnect('eip_ra_nz_01_usr' , '3m3t3r_cnp_nzo1', 'vnetezza.emeter.com', 'eip_ra_nz_01')
input_cmd <-  paste( select * from   ,  'RP_MODEL_INPUT_V'  ,   where to_char(ANALYSIS_END_DATE ,'yyyy-mm-dd') between   ,  '2012-07-01'  ,  and   ,  '2012-07-04' , sep=  )
input_cmd 
input_data <- nzQuery(input_cmd)
row_count <- nrow(input_data)

input_data[is.na(input_data)] = 0
scored_data <- input_data[, !names(input_data)  in  c( SDP_SEGMENT_CONFIG_WID )]

load('/home/pipe/opt/revenueprotection/bin/RP_MODEL_Version1.rda')
scored_data SCORE <-  predict( '/home/pipe/opt/revenueprotection/bin/RP_MODEL_Version1.rda', scored_data[, !names(scored_data)  in  c( SDP_WID  ,  ANALYSIS_END_DATE )],type='prob')[,2]
scored_data SCORE <- round((scored_data SCORE*100),0 )
scored_data <- scored_data [, names(scored_data)  in  c( SDP_WID  ,  ANALYSIS_END_DATE ,  SCORE  ) ]
print( c( Scored records:  , nrow(scored_data) ) )
as.nz.data.frame (scored_data, 'x_r_model_output_temp', clear.existing=TRUE)




-netezza developer nw
-

--------------------------------------------------------
./ConfigurationManagement.sh -Dapplication.command=IncrementalImport -DsourceDirectory=/home/pipe/opt/revenueprotection/tools/confXML
./ReferenceDataUtil.sh -Dapplication.command=import -DimportReferenceDataService.referenceDataSourcePath=/home/pipe/opt/revenueprotection/
-------------------------------------------------------

authenticationConfig.runAsOrgUdcId=SOURCE1
a105-createInvTicketDao.thresholdSdpScore=60

nzDataSource.driverClassName=org.netezza.Driver
nzDataSource.url=jdbc:netezza://vnetezza:5480/eip_ra_nz_01
nzDataSource.username=eip_ra_nz_01_usr
nzDataSource.password=3m3t3r_cnp_nzo1
nzDataSource.encrypted=false
nzDataSource.testSQL=select now()


updated 
http://wiki.emeter.com/display/ENG/Revenue+Protection+Tech+Spec
Any update regarding R ticket with IBM support  


docs - 
Analytics 1.4 - data feed -> file format 
structural data extractor - zip of csv file

meet:sip:ling.chien-sha@siemens.com;gruu;opaque=app:conf:focus:id:3b5147f15a844ae1b8d1f3ad65e4994a 3Fconf-key=5605

******************************************************************************************************************************
Structural - extractor 
(24/7) AdfStructuralDataExtract application, run once daily. Additionally, it can also be invoked from the command line for the 
initial or exception handling process by using the AdfExtractCommand process. This process queries for changes since the last run. 
The last run milestone is maintained in the PROCESS_MILESTONE table in MUDR.

<ref bean= extractOrgJob  />
<ref bean= extractAccntParamJob  />
<ref bean= extractAccntJob  /> 	
<ref bean= extractAssetClassJob  />
<ref bean= extractAssetClassParamJob  />
<ref bean= extractAssetRelJob  />
<ref bean= extractAssetParamJob  />
<ref bean= extractAssetJob  />
<ref bean= extractAccntSvcPtRelJob  />
<ref bean= extractCalendarParamJob  />
<ref bean= extractCalendarJob  />
<ref bean= extractChannelJob  />
<ref bean= extractPremiseJob  />
<ref bean= extractSvcAgreeJob  />
<ref bean= extractDistNodeRelJob  />
<ref bean= extractTouBinJob />

<!-- <ref bean= extractServiceParamJob  /> -->
<!-- <ref bean= extractServiceClassParamJob  /> -->
<!-- <ref bean= extractSvcPtSvcAgreeRelJob  /> -->

Structural - loader

Transactional - extractor 
Meter Reads data feed, which is retrieved either by directly listening to the TIBCO bus or by querying the MUDR database
The reads-related data is retrieved in two ways:
-The data could be extracted real-time by listening to the Archiver. Device events, register reads, and interval data can be 
 captured real-time by listening on the TIBCO bus.
-Direct database query. This can be used to retrieve data for a certain date range. This function can be used to extract 
 register reads, interval data, device events, missing reads, and missing read reasons directly from the MUDR database.

Initial loading
---------------
....not in documentation....
ADFDeviceEventExtract.sh


ADFMissingReadsMonitorCmd (run once)
ADFMissingReadsReasonExtractCmd (run once)
ADFDeviceEventExtractCmd (run once)
TeleventMissingReadsImportAdapter

AdfIntervalDataExtract.sh
AdfPriorVerionIntervalDataExtract.sh
AdfRegisterReadExtract.sh
AdfPriorVerionRegisterReadExtract.sh
 --usage
AdfRegisterUsageDataExtract.sh
AdfPriorverRegisterUsageDataExtract.sh
AdfUsageDataExtract.sh
AdfPriorverUsageDataExtract.sh


Incremental loading
-----------------------
ADFTransactionMonitor(24X7)
ADFMissingReadsMonitor(24X7)








Transactional - loader


Note: - https://docs.emeter.com/display/ALYT14/Analytics+Data+Flow
-Those records will be copied to the _ERROR tables. For records with a logical error, for example, FK violation, 
 and that may be reprocessed, those records will be stored in the _BAD tables.
-Error table: The records in the error tables will need to be fixed manually, as most of them should have data-related issues. 
-Bad table: Any records in the relation tables with a record value of -1 will be inserted in the _bad table with the source_key as the identifier. Only one record is inserted in its _bad table even though multiple columns might have the value of -1. 
-process_exception table: The error record and bad record are also inserted in the process_exception table. If three columns have the value of -1 in the relation table, three records are inserted in the process_exception table.


------------------------------------------------------------------------------------------------------------------
--------------Transactional--------------


--------------Structural --------------
<bean id= orgItemReader 
		class= com.emeter.eip.analytics.job.batch.item.database.NamedParameterSQLQueryItemReader 
		parent= masterDataFileAbstractItemReader >
		<property name= dataSource  ref= amiDBDataSource  />
		<property name= sqlQuery >
 <value>
 	<![CDATA[
 	SELECT
 	    id 	  AS id,
 	    udc_id 	 		AS udc_id,
 	    name 	 		AS name,
 	    desc_text 	 	AS desc_text,
 	    master_data_org_id	AS master_data_org_id,
 	    rec_version_num	AS rec_version_num,
 	    insert_by 	 	AS insert_by,
 	    last_upd_by		AS last_upd_by,
 	    NULL		 	AS insert_time,
 	    NULL		 	AS last_upd_time
 	FROM 
 	    org
              		]]>
 
 </value>
		</property>
	</bean>
	
	<bean id= accntParamItemReader 
		class= com.emeter.eip.analytics.job.batch.item.database.NamedParameterSQLQueryItemReader 
		parent= masterDataFileAbstractItemReader >
		<property name= sqlQuery >
 <value>
 	<![CDATA[
 	SELECT
 	    oxm.row_id 	 		AS src_id,
 	    NVL(attrib_05,o.x_data_src)  	AS data_src,
 	    attrib_01 	 	AS name,
 	    attrib_02 	 	AS value,
 	    attrib_12 	 	AS eff_start_time,
 	    attrib_13 	 	AS eff_end_time,
 	    oxm.par_row_id 	 	AS par_src_id,
 	    ou_type_cd 	   	AS type,
 	    oxm.created 	 	AS source_created,
 	    oxm.last_upd 	 	AS source_last_upd,
 	    NULL  	AS insert_time,
 	    NULL  	AS last_upd_time
 	FROM 
 	    s_org_ext_xm oxm, s_org_ext o
 	WHERE 
 	    oxm.par_row_id = o.row_id (+) 
 	    AND 
 	    (
 		oxm.last_upd > = NVL(:lastUpdateTime,TO_DATE('01/01/1970','MM/DD/YYYY'))
 		OR 
 		o.last_upd > = NVL(:lastUpdateTime,TO_DATE('01/01/1970','MM/DD/YYYY'))
 		)
              		]]>
 
 </value>
		</property>
	</bean>
	
	<bean id= accntItemReader 
		class= com.emeter.eip.analytics.job.batch.item.database.NamedParameterSQLQueryItemReader 
		parent= masterDataFileAbstractItemReader >
		<property name= sqlQuery >
 <value>
 	<![CDATA[
 	SELECT
 	    row_id 	   		AS src_id,
 	    x_accnt_class  	AS accnt_class,
 	    ou_type_cd 	   	AS type,
 	    x_name_1 	   	AS name,
 	    x_name_2 	   	AS alt_name,
 	    x_udc_accnt_id 	AS udc_id,
 	    x_esp_accnt_id 	AS alt_udc_id,
 	    x_universal_id 	AS universal_id,
 	    x_data_src 	   	AS data_src,
 	    cust_stat_cd   	AS status_cd,
 	    desc_text 	   	AS desc_text,
 	    created 	   	AS source_created,
 	    last_upd 	   	AS source_last_upd,
 	    NULL   	AS insert_time,
 	    NULL   	AS last_upd_time
 	FROM 
 	    s_org_ext
 	WHERE 
 	    last_upd > = NVL(:lastUpdateTime,TO_DATE('01/01/1970','MM/DD/YYYY'))
               ]]>
	
 </value>
		</property>
	</bean>
	
	<bean id= accntSvcPtRelItemReader 
		class= com.emeter.eip.analytics.job.batch.item.database.NamedParameterSQLQueryItemReader 
		parent= masterDataFileAbstractItemReader >
		<property name= sqlQuery >
 <value>
 	<![CDATA[
		SELECT 
 	  rel.row_id  AS src_id,  
 	  NVL(x_rel_data_src,a.x_data_src)	AS data_src,
 	  rel.accnt_id  AS accnt_src_id,  
 	  rel.asset_id  AS svc_pt_src_id,
 	  rel.x_rel_start_dt 	AS eff_start_time,
 	  rel.x_rel_end_dt 		AS eff_end_time,
 	  rel.created  AS source_created,
 	  rel.last_upd  AS source_last_upd,
 	  NULL  		AS last_upd_time, 
 	  NULL	  	AS insert_time
		FROM cx_asset_accnt rel, s_org_ext a
		WHERE
 	  rel.accnt_id = a.row_id (+)
 	  AND rel.rel_type_cd = 'Account:SDP'
 	  AND
 	  (
 	  rel.last_upd > = NVL(:lastUpdateTime,TO_DATE('01/01/1970','MM/DD/YYYY'))
 	  OR
 	  a.last_upd > = NVL(:lastUpdateTime,TO_DATE('01/01/1970','MM/DD/YYYY'))
 	  )
             ]]>
	
 </value>
		</property>
	</bean>
	
	<bean id= calendarParamItemReader 
		class= com.emeter.eip.analytics.job.batch.item.database.NamedParameterSQLQueryItemReader 
		parent= masterDataFileAbstractItemReader >
		<property name= sqlQuery >
 <value>
 	<![CDATA[
 	SELECT
 	    row_id  AS  src_id,
 	    utility_id 		AS  data_src,
 	    cal_id  AS  calendar_src_id,
 	    cal_day 		AS  cal_day,
 	    day_type_class 	AS category,
 	    day_type 		AS value,
 	    status  AS  status_cd,
 	    remarks 		AS  desc_text,
 	    created 		AS  source_created,
 	    last_upd 		AS  source_last_upd,
 	    NULL AS insert_time,
 		NULL  AS last_upd_time
 	FROM 
 	    cx_cal_param
 	WHERE 
 	    day_type_class IN 
 		(
 		    SELECT name
 		    FROM s_lst_of_val 
 		    WHERE type='CALENDAR_CATEGORY'
 		    AND par_row_id IS NULL
 		    AND name NOT IN ('Data Delivery Cycle','Meter Reading Cycle')
 		)
 	    AND last_upd > = NVL(:lastUpdateTime,TO_DATE('01/01/1970','MM/DD/YYYY'))
           ]]>
		
 </value>
		</property>
	</bean>
	
	<bean id= calendarItemReader 
		class= com.emeter.eip.analytics.job.batch.item.database.NamedParameterSQLQueryItemReader 
		parent= masterDataFileAbstractItemReader >
		<property name= sqlQuery >
 <value>
 	<![CDATA[
 	SELECT
 	    row_id  AS src_id,
 	    name  AS name,
 	    status  AS status,
 	    utility_id 		AS data_src,
 	    created 		AS source_created,
 	    last_upd 	 	AS source_last_upd,
 	    description 	AS desc_text,
 	    NULL AS insert_time,
 	    NULL 	AS last_upd_time
 	FROM 
 	    cx_cal
 	WHERE 
 	    last_upd > = NVL(:lastUpdateTime,TO_DATE('01/01/1970','MM/DD/YYYY'))
           ]]>
	
 </value>
		</property>
	</bean>
		
	<bean id= channelItemReader 
		class= com.emeter.eip.analytics.job.batch.item.database.NamedParameterSQLQueryItemReader 
		parent= masterDataFileAbstractItemReader >
		<property name= sqlQuery >
 <value>
 	<![CDATA[
		SELECT
 	  a.row_id  	AS src_id,          
 	  a.data_src  AS data_src,     
 	  a.service_point_id 	AS svc_pt_src_id,   
 	  a.x_udc_asset_id 		AS udc_id,          
 	  p.name  	AS name,            
 	  p.uom_cd  	AS uom,           
 	  x.attrib_02  AS interval_len,   
 	  a.x_physical_ch_num 	AS channel_num,     
 	  a.x_virtual_asset 	AS is_virtual_flg,  
 	  a.status_cd   AS status_cd,     
 	  a.type_cd  AS type,  
 	  a.cfg_type_cd 		AS sub_type,
 	  NULL  		AS meas_type_id,                                      
 	  a.created  AS source_created,  
 	  a.last_upd  AS source_last_upd, 
 	  NULL  		AS insert_time,     
 	  NULL	  	AS last_upd_time   
		FROM s_asset a, s_prod_int p, s_prod_int_xm x
		WHERE 
 	  a.prod_id = p.row_id(+)
 	  AND p.row_id = x.par_row_id(+)
 	  AND x.attrib_01(+) = 'Interval Length'
 	  AND a.type_cd = 'Channel'
 	  AND 
 	  (
 	  a.last_upd > = NVL(:lastUpdateTime,TO_DATE('01/01/1970','MM/DD/YYYY'))
 	  OR
 	  p.last_upd > = NVL(:lastUpdateTime,TO_DATE('01/01/1970','MM/DD/YYYY'))
 	  OR
 	  x.last_upd > = NVL(:lastUpdateTime,TO_DATE('01/01/1970','MM/DD/YYYY'))
 	  )
         ]]>
	
 </value>
		</property>
	</bean>	
	
	<bean id= assetRelItemReader 
		class= com.emeter.eip.analytics.job.batch.item.database.NamedParameterSQLQueryItemReader 
		parent= masterDataFileAbstractItemReader >
		<property name= sqlQuery >
 <value>
 	<![CDATA[
		  SELECT 
 	    rel.row_id  		AS  src_id,
 	    NVL(x_rel_data_src,s.data_src) 		AS data_src,
 	    rel.par_asset_id  	AS par_src_id,  
 	    rel.asset_id  		AS child_src_id,
 	    rel.meter_loc_start_dt  AS eff_start_time,
 	    rel.meter_loc_end_dt  AS eff_end_time,
 	    rel.x_asset_relation_type_cd                AS rel_type,
 	    CASE WHEN rel.x_asset_relation_type_cd = 'ROUTE-SDP' THEN
 		s.cfg_type_cd || '-SDP'
 	    END AS rel_type2,
 	    rel.created  		AS source_created,
 	    rel.last_upd  		AS source_last_upd,
 	    NULL  	AS last_upd_time, 
 	    NULL		  		AS insert_time
		  FROM s_asset_rel rel, s_asset s
		  WHERE
 	    rel.par_asset_id = s.row_id (+)
 	    AND 
 	    (
 	    rel.last_upd > = NVL(:lastUpdateTime,TO_DATE('01/01/1970','MM/DD/YYYY'))
 	    OR
 	    s.last_upd > = NVL(:lastUpdateTime,TO_DATE('01/01/1970','MM/DD/YYYY'))
 	    )
     	]]>
	
 </value>
		</property>
	</bean>
	
	<bean id= assetClassParamItemReader 
 class= com.emeter.eip.analytics.job.batch.item.database.NamedParameterSQLQueryItemReader 
 parent= masterDataFileAbstractItemReader >
 <property name= sqlQuery >
 	<value>
 	<![CDATA[
 	   SELECT          
 	   pxm.row_id       AS src_id,
 	   NULL             AS data_src,
 	   par_row_id       AS par_src_id,
 	   attrib_01        AS name,
 	   attrib_02        AS value,
 	   attrib_03        AS status_cd,
 	   attrib_26		AS eff_start_time,       
 	   attrib_27		AS eff_end_time,       
 	   pxm.type,
 	   p.sub_type,
 	   DECODE(pxm.type, 'Service', p.sub_type, pxm.type) AS handler_type,
 	   pxm.created      AS source_created,
 	   pxm.last_upd     AS source_last_upd,
 	   NULL      AS insert_time,
 	   NULL     AS last_upd_time
 	   FROM s_prod_int_xm pxm, s_prod_int p
 	   WHERE 
 	   pxm.par_row_id = p.row_id (+)
 	   AND 
 	   (
 	   pxm.last_upd > = NVL(:lastUpdateTime,TO_DATE('01/01/1970','MM/DD/YYYY'))
 	   OR
 	   p.last_upd > = NVL(:lastUpdateTime,TO_DATE('01/01/1970','MM/DD/YYYY'))
 	   )
 	   ]]>    		
 	</value>
 </property>
	</bean>
                       
	<bean id= assetClassItemReader 
		class= com.emeter.eip.analytics.job.batch.item.database.NamedParameterSQLQueryItemReader 
		parent= masterDataFileAbstractItemReader >
		<property name= sqlQuery >
 <value>
 	<![CDATA[
 	  SELECT          
 row_id 		AS src_id,
 NULL  AS data_src,
 type,
 sub_type, 
 DECODE(type, 'Service', sub_type, type) AS handler_type,
 name,
 status_cd,
 sub_type 		AS service_type,
 NULL  AS comm_technology,
 desc_text,
 created 		AS source_created,
 last_upd 		AS source_last_upd,
 NULL		 	AS insert_time,
 NULL		 	AS last_upd_time
 	  FROM s_prod_int
 	  WHERE
 last_upd > = NVL(:lastUpdateTime,TO_DATE('01/01/1970','MM/DD/YYYY'))
       	]]>
	
 </value>
		</property>
	</bean>	
	
	<bean id= assetParamItemReader 
 class= com.emeter.eip.analytics.job.batch.item.database.NamedParameterSQLQueryItemReader 
 parent= masterDataFileAbstractItemReader >
 <property name= sqlQuery >
 	<value>
 	<![CDATA[
 		SELECT 
 		p.row_id             AS src_id,                          
 		NVL(p.attrib_05,s.data_src)        AS data_src, 
 		p.attrib_01         AS name,                  
 		p.attrib_02         AS value,                 
 		p.attrib_03         AS status_cd,  
 		p.par_row_id         AS par_src_id,
 		p.attrib_12         AS eff_start_time,        
 		p.attrib_13         AS eff_end_time,
 		type, 
 		s.cfg_type_cd,      
 		e.x_attribute_02     AS value_alt,  		
 		DECODE(type_cd, 'Service', s.cfg_type_cd, type_cd) AS handler_type,
 		p.created             AS source_created,        
 		p.last_upd             AS source_last_upd,       
 		NULL	   AS insert_time,           
 		NULL	   AS last_upd_time         
                          FROM s_asset_xm p, s_asset s, cx_asset_xm_x e
                          WHERE
                          p.par_row_id = s.row_id (+)
 		AND p.row_id = e.par_row_id (+)
 		AND 
 		(
 		p.last_upd > = NVL(:lastUpdateTime,TO_DATE('01/01/1970','MM/DD/YYYY'))
 		OR
 		s.last_upd > = NVL(:lastUpdateTime,TO_DATE('01/01/1970','MM/DD/YYYY'))
 		OR
 		e.last_upd > = NVL(:lastUpdateTime,TO_DATE('01/01/1970','MM/DD/YYYY'))
 		)
 		]]>

 	</value>
 </property>
	</bean>
	
	<bean id= assetItemReader 
		class= com.emeter.eip.analytics.job.batch.item.database.NamedParameterSQLQueryItemReader 
		parent= masterDataFileAbstractItemReader >
		<property name= sqlQuery >
 <value>
 	<![CDATA[
 	SELECT
 	    s.row_id  AS src_id,
 	    data_src  AS data_src,
 	    x_udc_asset_id 		AS udc_id,
 	    prod_id 	AS class_src_id,  
 	    serial_num  AS mfg_serial_num,
 	    x_lot_num AS mfg_lot_num, 		
 	    x_network_id 		AS network_id,
 	    status_cd  AS status_cd,
 	    s.desc_text 		AS desc_text,
 	    purch_dt  AS purchase_date,
 	    ship_dt  AS ship_date,
 	    NULL  	AS mfg_test_date,
 	    x_retire_dt 		AS retire_date,
 	    x_mfg_date  AS mfg_date,            
 	    make_cd  AS make,
 	    model_cd  AS model,
 	    last_test_dt 		AS last_test_date,
 	    x_virtual_asset		AS is_virtual_flg,
 	    x_universal_id 		AS badge_id,
 	    x_aep_num  AS standard_id,
 	    x_electronic_id		AS electronic_id,
 	    NULL  	AS comm_technology,
 	    NULL  	AS cur_inv_location_id,
 	    NULL  	AS sku,
 	    NULL  	AS part_num,
 	    type_cd  AS type,
 	    cfg_type_cd 		AS sub_type,
 	    DECODE(type_cd, 'Service', cfg_type_cd, type_cd) AS handler_type,
 	    per_addr_id 		AS premise_src_id,
 	    x_ax_feed_loc 		AS feed_loc,
 		x_latitude_new 		AS gps_lat,
 		x_longitude_new 	AS gps_long,
            		x_pulse_output_blk 	AS pulse_output_block,
            		x_seal_info  		AS seal_info,
 		x_lock_info 		AS lock_info,
 		x_util_access_info 	AS access_info,
 		x_emeter_acces_info AS alt_access_info,
 		x_util_premise_loc 	AS loc_info,
            		x_emeter_premise_loc AS alt_loc_info,
 		x_billing_cycle		 AS billing_cycle,
 		x_reading_cycle		 AS reading_cycle,
            		NULL  	AS timezone_id,
 		NULL  	AS gis_id,
 		x_read_dt  AS billed_upto_time,
 		x_power_status 		AS power_status,
 		x_usage_status 		AS load_status,
            		x_billing_hold 		AS billing_hold_status,
            		meter_loc       	AS location_info,
                	service_point_id 	AS svc_pt_src_id, 	  
                	start_dt  AS eff_start_time,        
 	    end_dt  	AS eff_end_time,    
 	    a.svc_provider_id   AS  svc_provider_src_id,
                    s.cur_agree_id      AS  svc_agree_src_id,        
 	    s.created  AS source_created,
 	    s.last_upd  AS source_last_upd,
 	    NULL 	AS insert_time,
 	    NULL 	AS last_upd_time
 	FROM s_asset s, s_doc_agree a
                WHERE
                    s.cur_agree_id = a.row_id (+)
 		AND 
 	    (
 	    s.last_upd > = NVL(:lastUpdateTime,TO_DATE('01/01/1970','MM/DD/YYYY'))
 	    OR 
 	    a.last_upd > = NVL(:lastUpdateTime,TO_DATE('01/01/1970','MM/DD/YYYY'))
 	    )
            		]]>	
 </value>
		</property>
	</bean>	
	
	<bean id= distNodeRelItemReader 
 class= com.emeter.eip.analytics.job.batch.item.database.NamedParameterSQLQueryItemReader 
 parent= masterDataFileAbstractItemReader >
 <property name= sqlQuery >
 	<value>
 	<![CDATA[
 		SELECT
  row_id   		AS src_id,        
  data_src,
  par_asset_id   AS par_src_id,
  row_id   		AS child_src_id,
  TO_DATE('01/01/1970','MM/DD/YYYY') 	AS eff_start_time,
  NULL   		AS eff_end_time,
  'DISTNODE-DISTNODE'  	AS role,
  created   	AS source_created,
  last_upd   	AS source_last_upd,
  NULL		   AS insert_time,    
  NULL  		AS last_upd_time
 		 FROM s_asset
 		 WHERE 
  type_cd = 'Distribution Node'
  AND par_asset_id IS NOT NULL                
                		AND last_upd > = NVL(:lastUpdateTime,TO_DATE('01/01/1970','MM/DD/YYYY'))
 		]]>
		
 	</value>
 </property>
	</bean>	
	
	<bean id= premiseItemReader 
		class= com.emeter.eip.analytics.job.batch.item.database.NamedParameterSQLQueryItemReader 
		parent= masterDataFileAbstractItemReader >
		<property name= sqlQuery >
 <value>
 	<![CDATA[
 	SELECT
 	    row_id  	AS src_id,
 	    x_data_src  AS data_src,
 	    x_client_prmse_id 	AS udc_id,
 	    x_status_cd 		AS status_cd,
 	    addr  	AS addr_line_1,
 	    addr_line_2 		AS addr_line_2,
 	    city  	AS city,
 	    state  	AS state_province,
 	    zipcode   AS postal_code,
 	    country 	AS country,
 	    x_latitude  AS gps_lat,
 	    x_longitude 		AS gps_long,
 	    NULL  	AS desc_text,      
 	    x_time_zn  AS timezone,
 	    district  AS district,
 	    x_bldg_name 		AS bldg_name,
 	    x_bldg_type 		AS bldg_type,
 	    x_floor  AS floor,
 	    x_floor_type 		AS floor_type,
 	    NULL  	AS street_num,
 	    NULL  	AS street_num_suffix,
 	    x_post_del_id 		AS postal_delivery_id,
 	    x_post_del_type 	AS postal_delivery_type,
 	    x_street_name 		AS street_name,
 	    x_street_prfx 		AS street_prefix,
 	    x_street_suff 		AS street_suffix,
 	    x_street_type 		AS street_type,
 	    x_unit_num  AS unit_num,
 	    x_unit_type 		AS unit_type,
 	    NULL  	AS region,
 	    NULL  	AS territory,
 	    NULL  	AS map_info,
 	    created  AS source_created,
 	    last_upd  AS source_last_upd,
 	    NULL 	AS insert_time,
 	    NULL 	AS last_upd_time         
 	FROM
 	    s_addr_per
 	WHERE
 	    last_upd > = NVL(:lastUpdateTime,TO_DATE('01/01/1970','MM/DD/YYYY'))
               ]]>
	
 </value>
		</property>
	</bean>	
	
	<bean id= svcAgreeItemReader 
		class= com.emeter.eip.analytics.job.batch.item.database.NamedParameterSQLQueryItemReader 
		parent= masterDataFileAbstractItemReader >
		<property name= sqlQuery >
 <value>
 	<![CDATA[
 	SELECT
 	  row_id  	AS src_id,
 	  NULL  		AS data_src,
 	  product_id  AS class_src_id,
 	  stat_cd  	AS status_cd,
 	  agree_num  AS udc_id,
 	  eff_start_dt  AS eff_start_time,
 	  eff_end_dt  AS eff_end_time,
 	  svc_provider_id 		AS vend_svc_prov_src_id,
 	  target_ou_id  AS purch_svc_prov_src_id,
 	  x_vendor_agree_id 	AS vend_agree_id,
 	  x_purchaser_agree_id 	AS purch_agree_id,
 	  desc_text  AS desc_text,
 	  created  	AS source_created,
 	  last_upd  	AS source_last_upd,
 	  NULL		  AS insert_time,
 	  NULL  		AS last_upd_time
     FROM s_doc_agree
     WHERE
 	   last_upd > = NVL(:lastUpdateTime,TO_DATE('01/01/1970','MM/DD/YYYY'))
 	]]>	
 </value>
		</property>
	</bean>
	<bean id= touBinItemReader 
		class= com.emeter.eip.analytics.job.batch.item.database.NamedParameterSQLQueryItemReader 
		parent= masterDataFileAbstractItemReader >
		<property name= sqlQuery >
 <value>
 	<![CDATA[
 	SELECT
 	    row_id  AS src_id,
            		bin_number      AS bin_num,
 	    name  AS name,
 	    status  AS status_cd,
            description 	AS desc_text,
 	    utility_id 		AS data_src
 	FROM 
 	    cx_tou_bin
 	]]>
 </value>
		</property>
	</bean>




Total number of request processed	
Total number of successful request
Total number of unsuccessful request

convertInputToTZ(params[ Start Date ].value, params[ serverTimeZone ].value)

displayString = reportContext.getMessage( SummaryFailExcecptionCode , reportContext.getLocale());


SummaryFailExcecptionCode
SummaryFailFileName
SummaryFailMessageId
SummaryFailDescription
SummaryFailInsertTime

SELECT NVL(SUM(1),0) RequestCount
     , NVL(SUM(DECODE (stat_num, 60001, 1)),0) + NVL(SUM(DECODE (stat_num, 60002, 1)),0) Success
     , NVL(SUM(DECODE (stat_num, 60004, 1)),0) Fail
      FROM process_stat
WHERE stat_num in (60001,60002,60004)
and org_id = 52
and TO_DATE(TO_CHAR(start_time, 'DD-MON-YYYY'), 'DD-MON-YYYY') >= to_date( , 'MM/DD/YY') -2
and TO_DATE(TO_CHAR(end_time, 'DD-MON-YYYY'), 'DD-MON-YYYY') <= to_date( , 'MM/DD/YY')

SELECT DECODE(SUM(1), null, 0) RequestCount
     , DECODE(SUM(DECODE (stat_num, 60001, 1)), null, 0) Success
     , DECODE(SUM(DECODE (stat_num, 60004, 1)), null, 0) Fail
FROM process_stat
WHERE stat_num in (60001,60004)  ;


select
(select count(id) RequestCount
from process_stat
where stat_num in (60001,60004)) RequestCount,
(
select count(id) success
from process_stat
where stat_num = 60001) Success,
(
select count(id) fail
from process_stat
where stat_num = 60004) fail
from dual ;

jdbc:oracle:thin:@(DESCRIPTION=(ADDRESS_LIST=(ADDRESS=(PROTOCOL=TCP)(HOST=ind-db03.emeter.com)(PORT=1521)))(CONNECT_DATA=(SERVICE_NAME=ieip_01)))


We also need to replicate dimension tables in HBASE
1	Structural load	Do we have to remove PL/SQl procs for strutural data load	 
Ans -- no need 
2	Structural extract	What will be the preferred CSV format, canonical file or Multiple files (as in 7.x)	 \
Ans -- let 
3	Structural load	 Assumption: There will now be no _d tables i.e. dimension tables as EIP 8 schema (on Oracle) will be refered for Dimension data	 
Ans
4	Transactional data	 how staging look up table will be populated
Ans	-- At time of loading we need to populate the look up tables. 	It will be in Oracle
5 	XML - loading error - file will be moved to error directory. 



*Command-Based Data Extraction*

  ADFMissingReadsMonitorCmd (run once)
  ADFMissingReadsReasonExtractCmd (run once)
  ADFDeviceEventExtractCmd (run once)
  TeleventMissingReadsImportAdapter
  AdfIntervalDataExtract.sh
  AdfPriorVerionIntervalDataExtract.sh
  AdfRegisterReadExtract.sh
  AdfPriorVerionRegisterReadExtract.sh
  AdfRegisterUsageDataExtract.sh
  AdfPriorverRegisterUsageDataExtract.sh
  AdfUsageDataExtract.sh
  AdfPriorverUsageDataExtract.sh

The ADF process can also&nbsp;extracts the reads directly from the database into csv format,, store them in a predetermined location based on certain date criteria.&nbsp; This is the same process as we have in Analytics1.4 release.&nbsp; Once the data is extracted into csv file, it&nbsp;will follow the same process as described in the above real-time data feed.&nbsp;

*The Command-Based Data Extraction will consist of following extraction jobs (same as in 7.x)*



The mechanism to migrate data in various forms via one or more MapReduce jobs that pull the data from CSVs, 
apply multiple transformations (resolving challenIds and other dimension references) and loading the data to HBASE. 



We can extract and load data into Hadoop storage, for staging, 
and then take full advantage of the Hadoop compute infrastructure to transform.

HBase is a distributed column-oriented database built on top of HDFS. HBase is the
Hadoop application to use when you require real-time read/write random-access to
very large datasets.

Hadoop is not an Extract-Transform-Load (ETL) tool.  It is a platform that supports running ETL processes in parallel.

Using tools such as Apache Pig, advanced transformations can be applied in Hadoop with little manual programming effort; 
and since Hadoop is a low cost storage repository, data can be held for months or even years.  
Since Hadoop has been used to clean and transform the data, it is loaded directly into the HBASE

In MapReduce mode, Pig translates queries into MapReduce jobs and runs them on a
Hadoop cluster. 


You can run Pig programs from Java using the PigServer class, much like you can use JDBC to run SQL programs from Java. 
For programmatic access to Grunt, use PigRunner.

HBaseStorage


look up table order:

zip -> order of converting csv to xml
2 step process (i) csv to xml
 (ii) populate / merge csv to DB look up
premise
timezone

Mapping:
SDP
1) premise source id


1) INFO
84.105092 sec. taken in 500 files: 
29.510486 taken in one file:


2) Error
78.876719 sec ERROR mode... 500 files: 
29.510486 taken in one file:

sdpUdcId 
prodName


Following fields are identified that needs resolution for their udc_id in ASSET extract:

||Exisiting field that needs lookup|| Additional data needed to avoid lookup|| What needs to be done||
|prod_id (class_src_id)|class_name|Asset query need to have join with s_prod_int|
|per_addr_id(premise_src_id)|premise_udc_id|Asset query need to have join with s_addr_per|
|service_point_id(svc_pt_src_id)|svc_pt_udc_id|Asset query need to have self join with s_asset|

channel_type_meas_map



*****************************************

select * from device_param;
select * from device;
select * from device_class;
select * from premise_lookup;
select * from sdp_lookup;
select * from channel_lookup;
select * from device_lookup where src_id = '1-3JE6';
select * from device_class_lookup;
select * from sdp_meter_lookup;
select * from meter_commFunction_lookup;
select * from meter_channel_lookup;
select * from register_mapping;
select * from  meas_type;

select * from svc_pt;
select * from device where type != 'Meter';
select count(*) from svc_pt;
select count(*) from device;

drop table device_class_lookup;
create table device_class_lookup(
src_id varchar2(10),
udc_id varchar2(50),
name varchar2(100),
type varchar2(100),
sub_type varchar2(100),
org_name varchar2(50),
last_upd_date Timestamp default sysdate
);

drop table sdp_lookup;
create table sdp_lookup(
src_id varchar2(10),
udc_id varchar2(50),
premise_src_id varchar2(50),
sdp_class_src_id varchar2(100),
org_name varchar2(50),
last_upd_date Timestamp default sysdate
);
create table premise_lookup(
src_id varchar2(10),
udc_id varchar2(50),
org_name varchar2(50),
last_upd_date Timestamp default sysdate
);

drop table channel_lookup;
create table channel_lookup(
src_id varchar2(10),
udc_id varchar2(50),
name varchar2(100),
sdp_src_id varchar2(50),
uom varchar2(50),
interval_len varchar2(50),
channel_len varchar2(50),
type varchar2(100),
sub_type varchar2(100),
org_name varchar2(50),
last_upd_date Timestamp default sysdate
);

drop table device_lookup;
create table device_lookup(
src_id varchar2(10),
udc_id varchar2(50),
device_class_src_id varchar2(100),
type varchar2(100),
sub_type varchar2(100),
org_name varchar2(50),
last_upd_date Timestamp default sysdate
);

drop table register_mapping;
create table register_mapping(
--src_id varchar2(10),
channel_name_7x varchar2(100),
meter_param_dials_value varchar2(100),
channel_name_8x varchar2(100),
org_name varchar2(50),
last_upd_date Timestamp default sysdate
);

drop table device_param_lookup;
create table device_param_lookup(
src_id varchar2(10),
par_src_id varchar2(50),
name varchar2(50),
value varchar2(100),
start_date Timestamp,
end_date Timestamp,
org_name varchar2(50),
last_upd_date Timestamp default sysdate
);

drop table sdp_meter_lookup;
create table sdp_meter_lookup(
src_id varchar2(10),
sdp_src_id varchar2(50),
meter_src_id varchar2(50),
rel_type varchar2(50),
start_date Timestamp,
end_date Timestamp,
org_name varchar2(50),
last_upd_date Timestamp default sysdate
);

drop table meter_commFunction_lookup;
create table meter_commFunction_lookup(
src_id varchar2(10),
COMMFN_SRC_ID varchar2(50),
meter_src_id varchar2(50),
rel_type varchar2(50),
start_date Timestamp,
end_date Timestamp,
org_name varchar2(50),
last_upd_date Timestamp default sysdate
);

drop table meter_channel_lookup;
create table meter_channel_lookup(
src_id varchar2(10),
meter_src_id varchar2(50),
channel_src_id varchar2(50),
start_date Timestamp,
end_date Timestamp,
org_name varchar2(50),
last_upd_date Timestamp default sysdate
);

select * from meas_type;
select * from entity_def;
select * from entity_flexattr_def where entity_def_id = 238;


insert into register_mapping(channel_name_7x, channel_name_8x, meter_param_dials_value)
values('Register, KWH','KWH Total Cumulative Register Read',5);
insert into register_mapping(channel_name_7x, channel_name_8x, meter_param_dials_value)
values('Register, KWH','KWH On Peak Cumulative Register Read',5);
insert into register_mapping(channel_name_7x, channel_name_8x, meter_param_dials_value)
values('Register, KWH','KWH Off Peak Cumulative Register Read',5);

insert into register_mapping(channel_name_7x, channel_name_8x, meter_param_dials_value)
values('Register, KVARH','KVARH Total Cumulative Register Read',6);
insert into register_mapping(channel_name_7x, channel_name_8x, meter_param_dials_value)
values('Register, KVARH','KVARH On Peak Cumulative Register Read',6);
insert into register_mapping(channel_name_7x, channel_name_8x, meter_param_dials_value)
values('Register, KVARH','KVARH Off Peak Cumulative Register Read',6);
commit;






select * from svc_pt;
select count(*) from svc_pt_device_rel;
select * from device where type != 'Meter';
select distinct value from device_param;
select count(*) from device_param;
select count(*) from svc_pt;
select count(*) from device;





DELETE FROM activity_attr WHERE org_id=2;
DELETE FROM accnt_param WHERE org_id=2;
DELETE FROM accnt_accnt_rel WHERE org_id=2;
DELETE FROM accnt_svc_pt_rel WHERE org_id=2;
DELETE FROM svc_pt_consumer_rel WHERE org_id=2;
DELETE FROM svc_pt_data_svc_attr WHERE org_id=2;
DELETE FROM svc_pt_data_svc_rel WHERE org_id=2;
DELETE FROM svc_pt_device_rel WHERE org_id=2;
DELETE FROM svc_pt_group_param WHERE org_id=2;
DELETE FROM svc_pt_group_rel WHERE org_id=2;
DELETE FROM svc_pt_group WHERE org_id=2;
DELETE FROM svc_pt_condition WHERE org_id=2;
DELETE FROM svc_pt_status WHERE org_id=2;
DELETE FROM svc_pt_param WHERE org_id=2;
DELETE FROM svc_pt_rel WHERE org_id=2;
DELETE FROM svc_pt_svc_agree_attr WHERE org_id=2;
DELETE FROM service_request_attr WHERE org_id=2;
DELETE FROM svc_pt_svc_agree_rel WHERE org_id=2;
DELETE FROM svc_agree_param WHERE org_id=2;
DELETE FROM svc_agree WHERE org_id=2;
DELETE FROM device_meas WHERE org_id=2;
DELETE FROM device_channel_rel WHERE org_id=2;
DELETE FROM device_comm_gateway_rel WHERE org_id=2;
DELETE FROM device_function_rel WHERE org_id=2;
DELETE FROM svc_pt_device_rel WHERE org_id=2;
DELETE FROM device_group WHERE org_id=2;
DELETE FROM device_multiplier WHERE org_id=2;
DELETE FROM device_param WHERE org_id=2;
DELETE FROM channel_param WHERE org_id=2;
DELETE FROM consumer_accnt_rel WHERE org_id=2;
DELETE FROM consumer_addr_rel WHERE org_id=2;
DELETE FROM consumer_consumer_rel WHERE org_id=2;
DELETE FROM consumer_contact WHERE org_id=2;
DELETE FROM consumer_param WHERE org_id=2;
DELETE FROM consumer_addr WHERE org_id=2;
DELETE FROM vc_formula WHERE org_id=2;
DELETE FROM activity WHERE org_id=2;
DELETE FROM service_request WHERE org_id=2;
DELETE FROM device WHERE org_id=2;
DELETE FROM channel WHERE org_id=2;
DELETE FROM consumer WHERE org_id=2;
DELETE FROM accnt WHERE org_id=2;
DELETE FROM svc_pt WHERE org_id=2;
-- DELETE FROM premise WHERE org_id=2;
DELETE FROM inv_location WHERE org_id=2;
DELETE FROM inv_movement WHERE org_id=2;
DELETE FROM vc_contributor WHERE org_id=2;
DELETE FROM generic_locking;
COMMIT;



******************************************

rm -f premise/*.xml
rm -f meter-commfn/*.xml
rm -f meter-channel/*.xml
rm -f meter/*.xml
rm -f sdp/*.xml
rm -f sdp-meter/*.xml
rm -f commfn/*.xml
rm -f meterParam/*.xml
rm -f meter/*.xml
rm -f channel/*.xml
rm -f flexsyncincoming/*.xml
rm -f *.zip



mv flexsyncincoming/Sdp_*.xml sdp/
mv flexsyncincoming/SdpMeter_* sdp-meter/
mv flexsyncincoming/Channel_* channel/
mv flexsyncincoming/Meter_* meter/
mv flexsyncincoming/CommFunction_* commfn/
mv flexsyncincoming/MeterParam_* meterParam/
mv flexsyncincoming/MeterCommFn_* meter-commfn/
mv flexsyncincoming/MeterChannel_* meter-channel/


mv flexsyncincoming/Sdp_*.xml sdp/
mv flexsyncincoming/SdpMeter_* sdp-meter/
mv flexsyncincoming/Channel_* channel/
mv flexsyncincoming/Meter_* meter/
mv flexsyncincoming/CommFunction_* commfn/
mv flexsyncincoming/MeterParam_* meterParam/
mv flexsyncincoming/MeterCommFn_* meter-commfn/
mv flexsyncincoming/MeterChannel_* meter-channel/


---- without reg channel----

cp sdp/Sdp_* ../flexsync/in/
cp meter/Meter_* ../flexsync/in/
cp meterParam/MeterParam_* ../flexsync/in/
cp commfn/CommFunction_* ../flexsync/in/
cp sdp-meter/SdpMeter_* ../flexsync/in/
cp meter-commfn/MeterCommFn_* ../flexsync/in/

---with reg channel--------

cp sdp/Sdp_* ../flexsync/in/
cp meter/Meter_* ../flexsync/in/
cp meter-param/MeterParam_* ../flexsync/in/
cp comfn/CommFunction_* ../flexsync/in/
cp channel/Channel_* ../flexsync/in
cp sdp-meter/SdpMeter_* ../flexsync/in/
cp meter-commfn/MeterCommFn_* ../flexsync/in/
cp meter-channel/MeterChannel_* ../flexsync/in/

-----With Reg channel & merged file------------------------
mv flexsyncincoming/Final_Sdp_*.xml sdp/
mv flexsyncincoming/Final_SdpMeter_* sdp-meter/
mv flexsyncincoming/Final_Channel_* channel/
mv flexsyncincoming/Final_Meter_* meter/
mv flexsyncincoming/Final_CommFunction_* comfn/
mv flexsyncincoming/Final_MeterParam_* meter-param/
mv flexsyncincoming/Final_MeterCommFn_* meter-comfn/
mv flexsyncincoming/Final_MeterChannel_* meter-channel/

cp sdp/Final_Sdp_* ../flexsync/in/
cp meter/Final_Meter_* ../flexsync/in/
cp meter-param/Final_MeterParam_* ../flexsync/in/
cp comfn/Final_CommFunction_* ../flexsync/in/
cp channel/Final_Channel_* ../flexsync/in
cp sdp-meter/Final_SdpMeter_* ../flexsync/in/
cp meter-commfn/Final_MeterCommFn_* ../flexsync/in/
cp meter-channel/Final_MeterChannel_* ../flexsync/in/

select count(*) from svc_pt;
select min(insert_time) from svc_pt;
select max(insert_time) from svc_pt;

----------------------------------------------------------

Seed data syncronization

Need following data from customer to 
country
stateProvince
timezone
Meas_type(8x)
RegisterChannel Mapping - will contain channel name mapping from 7x to 8x 
sdp_class_mapping - sdp 7x class name to 8x class name 
device_class_mapping - device 7x class name to 8x class name 
And some services product name mapping from 7x-8x




Structural data syncronization

Register channel split appraoches:
1) Create Register channel while creating XMLs 
condn: based on commMod class & some meter param  

channel & meter channel rel all are created using XMLs

2) Create register channel using rule sheet
condn: based on commMod class & some meter param  

in both above approaches channel dates are taken from CSV.

Transactional data syncronization



Total processing time until current file completed in 2933.963112961 seconds.
~49 minutes 
process 41332 files
select min(last_upd_time) from svc_pt;
select max(last_upd_time) from device_channel_rel;
--1  05-MAY-14 09.52.00.000000000 PM  05-MAY-14 11.15.01.000000000 PM




1) Multi thread: 
./ApplicationLauncher.sh start FlexSync_FileService 1 -DuniversalSyncInterfacePortTypeFilePoller.noOfThreads=32 -DuniversalSyncInterfacePortTypeFilePoller.asynchronous=true -DuniversalSyncInterfacePortTypeFilePoller.asyncProcessorNoOfThreads=32


./ApplicationLauncher.sh start FlexSync_FileService_7x 1 -DmonitoringClient.checkDependency=false
./ApplicationLauncher.sh start FlexSync_FileService_7x 1 -DmonitoringClient.checkDependency=false -DuniversalSyncInterfacePortTypeFilePoller.noOfThreads=4 -DuniversalSyncInterfacePortTypeFilePoller.asynchronous=true -DuniversalSyncInterfacePortTypeFilePoller.asyncProcessorNoOfThreads=16



Removed section from wiki

*Approach 1*
{gliffy:name=StructuralDataApp1|align=left|size=L|version=3}

||New asset extraction query||Old asset extraction query||
|SELECT 
       s.row_id AS src_id,
       s.data_src AS data_src,
       s.x_udc_asset_id AS udc_id,
       s.prod_id AS class_src_id,
       s.serial_num AS mfg_serial_num,
       s.x_lot_num AS mfg_lot_num,
       s.x_network_id AS network_id,
       s.status_cd AS status_cd,
       s.desc_text AS desc_text,
       s.purch_dt AS purchase_date,
       s.ship_dt AS ship_date,
       NULL AS mfg_test_date,
       s.x_retire_dt AS retire_date,
       s.x_mfg_date AS mfg_date,
       s.make_cd AS make,
       s.model_cd AS model,
       s.last_test_dt AS last_test_date,
       s.x_virtual_asset AS is_virtual_flg,
       s.x_universal_id AS badge_id,
       s.x_aep_num AS standard_id,
       s.x_electronic_id AS electronic_id,
       NULL AS comm_technology,
       NULL AS cur_inv_location_id,
       NULL AS sku,
       NULL AS part_num,
       s.type_cd AS TYPE,
       s.cfg_type_cd AS sub_type,
       DECODE (s.type_cd, 'Service', s.cfg_type_cd, s.type_cd)
          AS handler_type,
       s.per_addr_id AS premise_src_id,
       s.x_ax_feed_loc AS feed_loc,
       s.x_latitude_new AS gps_lat,
       s.x_longitude_new AS gps_long,
       s.x_pulse_output_blk AS pulse_output_block,
       s.x_seal_info AS seal_info,
       s.x_lock_info AS lock_info,
       s.x_util_access_info AS access_info,
       s.x_emeter_acces_info AS alt_access_info,
       s.x_util_premise_loc AS loc_info,
       s.x_emeter_premise_loc AS alt_loc_info,
       s.x_billing_cycle AS billing_cycle,
       s.x_reading_cycle AS reading_cycle,
       NULL AS timezone_id,
       NULL AS gis_id,
       s.x_read_dt AS billed_upto_time,
       s.x_power_status AS power_status,
       s.x_usage_status AS load_status,
       s.x_billing_hold AS billing_hold_status,
       s.meter_loc AS location_info,
       s.service_point_id AS svc_pt_src_id,
       s.start_dt AS eff_start_time,
       s.end_dt AS eff_end_time,
       a.svc_provider_id AS svc_provider_src_id,
       s.cur_agree_id AS svc_agree_src_id,
       s.created AS source_created,
       s.last_upd AS source_last_upd,
       NULL AS insert_time,
       NULL AS last_upd_time,
       asset_for_udc.x_udc_asset_id AS sdp_udc_id,
       premise_tab.x_client_prmse_id AS premise_udc_id,
       prod.name
  FROM s_asset s,
       s_doc_agree a,
       *s_asset asset_for_udc*,
       *s_addr_per premise_tab*,
       *s_prod_int prod*
 WHERE s.cur_agree_id = a.row_id(+)
       AND (s.last_upd >=
               NVL (:lastUpdateTime, TO_DATE ('01/01/1970', 'MM/DD/YYYY'))
            OR a.last_upd >=
                  NVL (:lastUpdateTime, TO_DATE ('01/01/1970', 'MM/DD/YYYY')))
       *AND s.service_point_id = asset_for_udc.row_id*(+)
       *AND s.per_addr_id = premise_tab.row_id*(+)
       *AND s.prod_id = prod.row_id*|SELECT
 s.row_id  AS src_id,
 data_src  AS data_src,
 x_udc_asset_id 		AS udc_id,
 prod_id 	AS class_src_id,  
 serial_num  AS mfg_serial_num,
 x_lot_num AS mfg_lot_num, 		
 x_network_id 		AS network_id,
 status_cd  AS status_cd,
 s.desc_text 		AS desc_text,
 purch_dt  AS purchase_date,
 ship_dt  AS ship_date,
 NULL  	AS mfg_test_date,
 x_retire_dt 		AS retire_date,
 x_mfg_date  AS mfg_date,            
 make_cd  AS make,
 model_cd  AS model,
 last_test_dt 		AS last_test_date,
 x_virtual_asset		AS is_virtual_flg,
 x_universal_id 		AS badge_id,
 x_aep_num  AS standard_id,
 x_electronic_id		AS electronic_id,
 NULL  	AS comm_technology,
 NULL  	AS cur_inv_location_id,
 NULL  	AS sku,
 NULL  	AS part_num,
 type_cd  AS type,
 cfg_type_cd 		AS sub_type,
 DECODE(type_cd, 'Service', cfg_type_cd, type_cd) AS handler_type,
 per_addr_id 		AS premise_src_id,
 x_ax_feed_loc 		AS feed_loc,
 x_latitude_new 		AS gps_lat,
 x_longitude_new 	AS gps_long,
 x_pulse_output_blk 	AS pulse_output_block,
 x_seal_info  		AS seal_info,
 x_lock_info 		AS lock_info,
 x_util_access_info 	AS access_info,
 x_emeter_acces_info AS alt_access_info,
 x_util_premise_loc 	AS loc_info,
 x_emeter_premise_loc AS alt_loc_info,
 x_billing_cycle AS billing_cycle,
 x_reading_cycle AS reading_cycle,
 NULL  AS timezone_id,
 NULL  	AS gis_id,
 x_read_dt  AS billed_upto_time,
 x_power_status 		AS power_status,
 x_usage_status 		AS load_status,
 x_billing_hold 		AS billing_hold_status,
 meter_loc       	AS location_info,
 service_point_id 	AS svc_pt_src_id, 	  
 start_dt  AS eff_start_time,        
 end_dt  	AS eff_end_time,    
 a.svc_provider_id   AS  svc_provider_src_id,
 s.cur_agree_id      AS  svc_agree_src_id,        
 s.created  AS source_created,
 s.last_upd  AS source_last_upd,
 NULL 	AS insert_time,
 NULL 	AS last_upd_tim
 FROM s_asset s, s_doc_agree a
 WHERE
 s.cur_agree_id = a.row_id (+)
 AND 
    (
    s.last_upd > = NVL(:lastUpdateTime,TO_DATE('01/01/1970','MM/DD/YYYY'))
    OR 
    a.last_upd > = NVL(:lastUpdateTime,TO_DATE('01/01/1970','MM/DD/YYYY'))
    )|
    
    
    
{composition-setup}
cloak.memory.duration = 100
cloak.toggle.type = default
{composition-setup}

{style}
 fixed-div {
    position: fixed;
    bottom: 5pt;
    right: 15pt;
}
.tocCls {
    max-height: 40em;
    overflow-y: auto;
}
.tocCls ul {
    list-style-type: disc;
    margin-left: 7pt; padding-left: 7pt;
}
{style}

{div:id=fixed-div}
{panel}
{toggle-cloak:id=toc}TOC
{cloak:id=toc}{toc:maxLevel=3|printable=false|class=tocCls}{cloak}
{panel}
{div}

{info:title=Technical Specification Template}* {color: 0000ff}Texts in blue are instructions.{color}
* Remove any unneeded sections from the final doc.{info}

{numberedheadings:number-format=iso-2145}

h1. Introduction

{color: 0000ff}Excerpt for design doc listing page goes here, in the format  (State) One-line description , where State = Draft / Ready for Review / Reviewed.{color}
{info}
{excerpt}(Draft) One-line purpose or description.{excerpt}
{info}

h2. Purpose

{color: 0000ff}Briefly describe the purpose of this application / component / feature / re-factoring.  Describe how it fits into the overall product, if non-obvious.{color}

h2. References

{color: 0000ff}Links to internal or external reference docs, that provide related context, background, or supplemental information.{color}

h2. Additional Requirements

{color: 0000ff}Requirements / features are documented in the Requirements section of the project wiki.  List only additional, internally-driven objectives and requirements here:{color}
* {color: 0000ff}specific design objectives{color}
* {color: 0000ff}internal technical requirements{color}
* {color: 0000ff}performance requirements{color}
* {color: 0000ff}non-requirements{color}

h2. Assumptions

{color: 0000ff}State any important, non-obvious assumptions or limitations.{color}

h2. Dependencies

{color: 0000ff}State any important, non-obvious dependencies on other components.{color}

h2. Usage Scenarios

{color: 0000ff}Use this section only if there are major use cases or logical workflows that drive the design of the component.{color}

h2. Important Terms

{color: 0000ff}List any terms here that are specific to and important in understanding the design.{color}
|| Term || Brief Definition ||
| | |


h1. High-Level Design
[Cloud Analytics 2.0 Data Extraction and Load HLD|ENG:(Draft) Cloud Analytics 2.0 Data Extraction and Load HLD]

h2. Component Architecture and Data Flow

h3. Source System is EnergyIP 

h4. Structural Data

h5. Data Extraction
The structural data in EnergyIP 7 will be extracted into the same CSV files as we have in Analytics1.4 release (see [https://docs.emeter.com/display/ALYT14/Structural+Data+Feed|https://docs.emeter.com/display/ALYT14/Structural+Data+Feed]&nbsp;for reference). The data extraction is controlled by the date of MasterStgDataExtractorJob in process_milestone table and it will be updated after each data extraction.

The Extraction process queries for changes since the last run. The last run milestone is maintained in the PROCESS_MILESTONE table in EIP 8x.
The Structural data extractor will consist of following extraction jobs (same as in 7.x)
||S.No.||Job name||
|1|extractOrgJob
|2|extractAccntParamJob
|3|extractAccntJob
|4|extractAssetClassJob
|5|extractAssetClassParamJob
|6|extractAssetRelJob
|7|extractAssetParamJob
|8|extractAssetJob
|9|extractAccntSvcPtRelJob
|10|extractCalendarParamJob
|11|extractCalendarJob
|12|extractChannelJob
|13|extractPremiseJob
|14|extractSvcAgreeJob
|15|extractDistNodeRelJob
|16|extractTouBinJob


h5. Data Load
Could Analytics utility will convert csv files into EIP7 XML format and will copy the file to flexsync incoming directory. FlexSync will use EIP7 to EIP8 adapter to process the XMLs and load data into EIP8 database.
The  utility will also add data to respective ASSET_LOOKUP tables e.g svc_pt_look_up, device_lookup etc. 
The tables will provide mapping for 7x source Id to 8x udc_id and will resolve src_id refernces for structural and transactional data.
Refer [Cloud Analytics 2.0 Data Extraction and Load - Lookup mapping|(Draft) Cloud Analytics 2.0 Data Extraction and Load - Lookup mapping]

Data load will be performed using following:
* Configuration required from Utility.
* Load and synchronization of seed data
* Load and synchronization of asset data

h6. Configuration required from Utility

To run the data load, following information is expected from +Utility+:
||Product data||Required for||Comments||
|country|Required to save Premise data. 7x-Country codes, 8x-full country name. Not part data extraction|( )|
|stateProvince|Required to save Premise data. Not part data extraction|( )|
|timezone|Required to save Premise data. Not part data extraction|( )|
|register_channel_mapping|It will provide mapping for 7x register channel to 8x channel's meas_type udc_id. Also channel splitting logic will be derived from this information. Refer [Cloud Analytics 2.0 Data Extraction and Load - Lookup mapping|(Draft) Cloud Analytics 2.0 Data Extraction and Load - Lookup mapping]|( )|
|interval_channel_mapping |It will provide mapping for 7x interval channel to 8x channel's meas_type udc_id. *May skip* this table if we have meas_type prepopulated with 7x register channel product information|( )|

*Note* May need more 7x-8x product mapping information as we do actual implementation

h6. Load and synchronization of seed data
The existing structural data extract extract provide information related to product and their parameters in respective CSVs e.g.DeviceClass, SvcPtClass etc. This data is needed to create asset data and is required to be part of 8x product information.
The CSVs are incremental i.e. the data is on and after the milestone date.

Following approaches can be used to insert seed data in EIP 8x schema:

* Using RDU: CSV will be converted to RDU input XMLs and RDU will be used to insert the data:
+Disadvantage+: RDU will be invoked from command line and for any exception / error, loader will not have control over it.
* Using insert statement: CSVs will be inserted in database using insert statement.

*Note:* After discussion with team, the approach +'Using insert statement'+ is decided


h6. Load and synchronization of asset data

{gliffy:name=StructuralDataApp2|align=left|size=L|version=31}


Enhancements required for Structural Data Loader process
* Cloud Analytics Loader process
* FlexSync optimization


h5. Cloud Analytics Structural Data Loader process
The process is divided in following stages:
* 7x file Utility that inserts data in respective staging tables.
* Performs transformations on staging data
* Creating 7x input XMLs from staging table
* Copy 7x input XMLs to flexSync inbound directory

h6. 7x file utility
Following tasks are performed by the Utility:
Data load to staging tables: The structural data, zip that is being created by extraction process, will be processed in *specific order* so that when FlexSync processes file all required assets are processed in advance.
eg. Premise CSV to XML is created first and dumped to FlexSync then svc_pt CSV is treated so that the premise refered in svc_pt is already in database.
Note: conversion of CSVto XML is not time consuming. If it is then we may *create XMLs of independent entities in parallel* as below
* SDP & DEVICES XMLs can process in parallel. 
* REL & PARAMS XMLs can process in parallel

XML creation can have two approaches:
* Creating single XML file for an entity type i.e. All CSV rows(of an entity CSV) are part of single input XMLs e.g. svc_pt CSV may have 1000 rows and all 1000 sdps are used in single XML.
*Disadvantage:* An error for an entity would lead to discard entire file and hence all related and subsequent files(which refer to faulty entity) would also fail. 
* Creating 1 XML file for each row

Test data considers 500 sdps processing time (on dev environment). FlexSync performance results for above approaches:
|Multifile appraoch|78.876719 sec|6.33 sdp per sec|
|Single XML approach|29.510486 sec|16.9 sdp per sec|

Refer [Cloud Analytics 2.0 Data Extraction and Load - XML mapping|ENG:(Draft) Cloud Analytics 2.0 Data Extraction and Load - XML mapping] for XML mapping.

h6. Scheduling of files: This would be the threshold count of files that FlexSync can process optimally i.e file poller performs optimal.
Files created will be dumped in FlexSync inbound directory only if the existing count of file count is below threshold.

h6. Once CSV to XML task is done, the relevant data is also populated to respective LOOKUP tables. 
LOOKUP will be split in respective tables e.d. svc_pt_lookup, channel_lookup, device_lookup etc. LOOKUP tables will provide mapping for 7x source Id to 8x udc_id. It will be used in creating facts for transactional data.

-removed lookups-


h5. FlexSync optimazation strategy
* Following assumptions are taken:
** No mapping rule will be executed
** No Derivation rule will be executed
** 7x adapter has removed usage of Dozer.
** Using multithreaded FlexSync instance
** Using Above mentioned File poller strategy.

h5. Exception cases

Use Case 1:
||Transaction||record||Result|| FlexSync Modification required ||
|1|S1-M1 1/1/2010   1/1/2010|S1-M1 1/1/2010   1/1/2010| No|

Use Case 2:
||Transaction||record||Result|| FlexSync Modification required ||
|1|S1-M1 1/1/2010   1/1/2010\\S1-M2 1/1/2010   03/15/2010|S1-M2 1/1/2010   03/15/2010| No|

Use Case 3:
||Transaction||record||Result|| FlexSync Modification required ||
|1|S1-M1 1/1/2010   1/1/2010\\S1-M2 1/1/2010   1/1/2010\\S1-M3 1/1/2010   03/15/2010|S1-M3 1/1/2010   03/15/2010| No|

Use case 4:
||Transaction||record||Result|| FlexSync Modification required ||
|1|S1-M1 1/1/2010   1/1/2010|S1-M1 1/1/2010   1/1/2010| No|
|2|S1-M2 1/1/2010   1/1/2010|S1-M2 1/1/2010   1/1/2010| No|

Use case 5:
||Transaction||record||Result|| FlexSync Modification required ||
|1|S1-M1 1/1/2010   1/1/2010|S1-M1 1/1/2010   1/1/2010| No|
|2|S1-M2 1/1/2010   03/15/2010|S1-M2 1/1/2010   03/15/2010| No|

Use case 6:
||Transaction||record||Result|| FlexSync Modification required ||
|1|S1-M1 1/1/2010   03/15/2010|S1-M1 1/1/2010   03/15/2010| No|
|2|S1-M2 1/1/2010   1/1/2010|S1-M1 1/1/2010   03/15/2010| *Yes*|

Use case 7:
||Transaction||record||Result|| FlexSync Modification required ||
|1|Asset UDC_Id is null|Create it based on some logic or exception record | Yes|

Use case 8:
||Transaction||record||Result|| FlexSync Modification required ||
|1|Unable to resolve Reference |exception| No|

h4. Transactional Data

The Analytics Data Feed (ADF) process will use the same process as we have in Analytics1.4 release (see [https://docs.emeter.com/display/ALYT14/Data+Feeds+Formats|https://docs.emeter.com/display/ALYT14/Data+Feeds+Formats]&nbsp;for reference) for CSV creation; The data, in csv format, will be transformed, processed and stored in HBASE.

* Use&nbsp;the incoming channel ID and lookup its corresponding udc_sdp_id and measurement_id
* Split the register_reads channel into multiple register_reads channel in EIP8 format&nbsp;based on user defined lookups
The data will then be&nbsp;stored in HBase data store. The following components are supported in real-time feed:
* CleanArchivedEvents
* Interval Reads
* Prior Version Interval Reads
* Register Reads
* Prior Version Register Reads

*The Real-Time Data Feed will be created by following 7x components(same as in 7.x)*
  ADFTransactionMonitor
  ADFMissingReadsMonitor 

*Extraction Jobs for transactional data*
||S.No.||Job name||
|1|ADFDeviceEventExtractJob|
|2|ADFIntervalReadExtractJob|
|3|ADFPriorVerIntervalReadExtractJob|
|4|ADFRegisterReadExtractJob|
|5|ADFPriorVerRegisterReadExtractJob|

*Loader Jobs for transactional data*
||S.No.||Job name||
|1|ADFDeviceEventLoadJob|
|2|ADFIntervalReadLoadJob|
|3|ADFPriorVerIntervalReadLoadJob|
|4|ADFRegisterReadLoadJob|
|5|ADFPriorVerRegisterReadLoadJob|

Using tools such as Apache Pig / Hive, advanced transformations can be applied in Hadoop with little manual programming effort.
Pig facilates more transformations and translates queries into MapReduce jobs and runs them on a Hadoop cluster.

{color:red}The design is a proposal based on information provided in documentation. Pig / Hive can be replaced depending upon the final discussions.{color}

{gliffy:name=TransactionalDataApp1|align=left|size=L|version=10}

h2. Deployment View

{color: 0000ff}Describe the physical deployment architecture, with machine and process boundaries.{color}

h2. Database Schema

{color: 0000ff}Describe any database schema additions or changes, starting with logical ER diagram, that outlines the business configuration necessary for this application. Describe key data model standards of use such as flexible params and specific relationships that are assumed by the application.{color}

{color: 0000ff}Detailed column description may be indicated in early phases of design but the authoritative source will eventually be the overall data model documentation and not here.{color}

h2. Other Considerations

{color: 0000ff}Add sections for any other important design topics, including:{color}
* {color: 0000ff}Security{color}
* {color: 0000ff}Scalability, FT and HA{color}
* {color: 0000ff}Business configuration and/or Customization/extension - high level capabilities and approach only{color}
* {color: 0000ff}L10N considerations{color}
* {color: 0000ff}Multi-org considerations{color}
* {color: 0000ff}Lifetime management{color}
* {color: 0000ff}etc.{color}

h2. Design Trade-Off Analysis

{color: 0000ff}Summarize any alternate designs that were seriously considered, and the rationale for selecting the proposed design.{color}


h1. External Interfaces

{color: 0000ff}In describing interfaces, be sure to fully describe the behavior, including constraints, pre-conditions, transaction boundaries, errors, etc.{color}
{color: 0000ff}This section does not describe UI.  Any UI should be described in an accompanying UI Spec.{color}

h2. Application Integration Interfaces

{color: 0000ff}Specify any interfaces that are published for use by applications outside of the product.  These may be provided via multiple protocols, including:{color}
* {color: 0000ff}SOAP/HTTP Interface{color}
* {color: 0000ff}JMS Interface{color}
* {color: 0000ff}File Interface{color}

{color: 0000ff}Template for interface operations. Interfaces are generally described by WSDL. For each WSDL service/port type describe the operations.{color}
|| Operation Name || Description || Request Data Type || Response Data Type ||
| | | | |

{color: 0000ff}For each operation describe the behavior, request and response messages with samples. In many cases XSD is generic and the same elements may have different usage depending on the operation; hence the description of request/response should be operation centric. The type hierarchy in XSD is flattened out into a table.{color}

{color: 0000ff}This documentation may originate here and ultimately be moved to published docs.{color}
|| Element/Attribute Name || Data Type || Required || Description ||
| +MyRequestMessage/Header+ | | | |
| noun | string | Y | Type of message. |
| +MyRequestMessage/Payload+ | | | |
| myField | string | Y | message content |

{color: 0000ff}Indicate how response and exceptions are handled by the interface.{color}

h2. Customization / Extension Interfaces

{color: 0000ff}Specify any interfaces that are published for use by a system implementer, to customize or extend the product.  For example, custom import or export adapters, custom plug-in methods, or custom rules.{color}
{color: 0000ff}Standard types of configuration - business configuration such as flexible params, and app properties - are not described here.{color}

{color: 0000ff}Use the same templates as above to describe interfaces.{color}

h2. Application Developer Interfaces

{color: 0000ff}Specify any public interfaces that are provided to developers, either to internal developers for use within the product, or to EIP platform developers for developing applications on the EIP platform.  These may be provided via multiple protocols, including:{color}
* {color: 0000ff}Java API{color}
* {color: 0000ff}REST/JSON/HTTP Interface{color}
* {color: 0000ff}JMS Interface{color}
* {color: 0000ff}File Interface{color}

{color: 0000ff}Use the same templates as above to describe interfaces.{color}

h2. Statistics and Exceptions

See [EIP 8.0 - Guidelines for Stats and Exception Type Seed Data|ENG:EIP 8.0 - Guidelines for Stats and Exception Type Seed Data] for guidelines.

{color: 0000ff}Specify the exceptions published to the  PROCESS_EXCP  table.{color}
|| Level1 (Module) || Level2 (Service) || Level3 (Action) || Level 4 (Action Qualifier) ||
| Framer | Real Time Framer | Requests Received | |

{color: 0000ff}Specify the statistics published to the  PROCESS_STAT  table.{color}
|| Level1 (Module) || Level2 (Service) || Level3 (Severity) || Level 4 (Exception Type) || Arg 1 || Arg 2 || Arg11 ||
| Framer | Real Time Framer | Error | Missing Intervals | Service Point Id | Channel Id | Usage Date |


h1. Component-Level Design

h2. Component or Feature <1>

{color: 0000ff}Describe the more detailed design for a specific component or feature. Describe the visible functional behavior, and the internal technical design.{color}

{color: 0000ff}Cover additional, relevant topics, and add sub-sections to this section as needed, including:{color}
* {color: 0000ff}Use cases or workflows, with pre and post-conditions{color}
* {color: 0000ff}Algorithms, with pseudo-code{color}
* {color: 0000ff}Data mapping{color}
* {color: 0000ff}Customization/extension support{color}
* {color: 0000ff}Performance{color}
* {color: 0000ff}Testing strategy{color}
* {color: 0000ff}etc.{color}

h2. Component or Feature <2>

{color: 0000ff}Add a section for each component/feature.{color}


h1. Configuration and Deployment

{color: 0000ff}This section covers reference data, permissions,{color} {color: 0000ff}technical configuration,&nbsp;{color}{color: 0000ff}deployment, monitoring, troubleshooting.{color}
{color: 0000ff}Business configuration data model and flexible params are described in the database schema section.{color}

h2. Reference Data

{color: 0000ff}Specify any reference data that is required by this component.{color}

h3. Reference Data Types

{color: 0000ff}Specify and describe any dynamic types introduced in this application, such as VEE Method, Estimation Method, Service Point Type, etc.{color}

h4. <Reference Data Type Name>

|| Property/Parameter Name || Data Type || Default Value || Description ||
| | | | |

h3. Sample Reference Data

{color: 0000ff}Describe the sample reference data included in the build.{color}
|| Reference Data Type || Object Name || Description ||
| | | |

h2. Security

{color: 0000ff}Specify the permissions (capabilities), permission groups (application roles) used and the behavior that they influence.{color}
|| Permission Name || Application || Resource || Action || Description || PG: myApp.admin || PG: myApp.readOnly ||
| myApp.account.r | myApp | acccount | r | Ability to read account data | Y | Y |

h2. Application Properties

{color: 0000ff}Specify the application properties defined by this component, and any property sets defined, and property sets used.  Be sure to follow the guidelines for defining properties and property sets.  For each property, explain the purpose from the user's point of view.{color}

h3. <Property Set or Application Profile Name>

|| Property Name || Data Type || Default Value || Description ||
| | | | |

h2. Application Startup

{color: 0000ff}Specify the app type, app resource type, and dependent apps.  Specify any other startup dependencies and steps.{color}

h2. Monitoring and Troubleshooting

{color: 0000ff}Describe the technical logging provided by the component, and any other troubleshooting or supportability aids, tools, or tips.  Specify any technical monitoring stats or other monitoring support specific to this component.{color}


h1. Upgrade Impact Analysis

h2. From Release X to Release Y

{color: 0000ff}Describe additions or changes in this component that can impact customers on upgrade.  And describe how they will be handled - automated or manual, upgrade vs. co-existence, etc. Of course as a general rule, database schema and external interfaces should remain backward compatibility. Areas to consider include:{color}
* {color: 0000ff}Database schema changes{color}
* {color: 0000ff}Migration of database data{color}
* {color: 0000ff}Changes to external interfaces{color}
* {color: 0000ff}Changes in functional behavior, or business configuration{color}
* {color: 0000ff}Changes in reference data{color}
* {color: 0000ff}Changes in app properties{color}
* {color: 0000ff}Changes to app deployment{color}


h1. Issues

h2. Open Issues

{color:red}Open questions{color}
||S.No.||Component||Question||Comment ||
|1|Structural extract|What will be the preferred CSV format, canonical file or Multiple files (as in 7.x) |No discussion yet, go with existing multiple file approach|
|2|Structural load| Assumption: There will now be no _d tables i.e. dimension tables as EIP 8 schema (on Oracle) will be refered for Dimension data|Yes, look up will be performed from Oracle EIP 8 schema|
|3|Transactional load| how staging look up table will be populated|At the time of inserting data in EIP 8 (Oracle) schema, populate data for table ASSET_LOOKUP in same schema|
|4|Transactional load|Assumption: The Cloud Analytics will use EIP 7x MDM and EIP 7x will have ADFTransactionmonitor & ADFMissingReadmonitor deployed which will provide the CSV for real time data.|  |
|5|Transactional Extraction & load|There will only be extraction & loader jobs and no CMD i.e. run once, interface for transactional data |  |
|6|Extraction & Load|Following list of jobs are also available in 1.4 Analytics. And we assume that below list will not be used in Cloud Analytics: 
* Compute Data Collection Summary
* Compute Interval Summary By Distribution Node
* Compute Interval Summary By Zip Code
* Load Missing Read Fact
* Load Missing Read Reason
* Execute On Demand Missing Read Monitor
* Compute SDP Power Factor
* Load Telvent AMI Performance Summary
* Load Dist Node Data |  |
|7|Transactional Extraction & load|Do we also want to use Hadoop as the repository for CSV data  |  |


h2. Known Issues

{color: 0000ff}List any known issues, that are not planned to be resolved in this design.{color}
{numberedheadings} 



******************************

Approaches are defined in doc
Design doc update (identified lookups but need to complete XML mapping)

design visit 
channel split design 



2777
47536


12/5 - 16/5

12/5---POC for actual save for channel
12/5---POC for entityDef
13/5---POC for entityDef (spilled) with Threading
13/5---actual code impl for above POC
14/5-15/5---complete for all entities
16/5---Add multithreading

19/5 - 23/5 
Performance testing for 1 million data 
19/5--Eclipse setup with profiler.
19/5--Create test data
19/5-16/5--Performance testing

-create test data
-insert seed data
-load mapping tables in memory
-




understanding existing framework and codebaase set up - 1 day
syncronization of seed data 



deviceClass:  Meter
deviceClassParam:		CT-PT

 
deviceFnClass: Communication Module
deviceFnClassParamItemWriter:


svcPtClass: Service Point
svcPtClassParamItemWriter:	Distribution Node

svcPtGroupClass:		Route	
svcPtGrpClsParamItemWriter:

dataSvcClass:   		Data Collection
dataSvcClassParamItemWriter:	Data Delivery 
	 	VEE
	 	Framing
	 	Deployment Planning
	 	Virtual Channel Persistence
	 	Estimation
	 	Outage
	 	Data Transfer
	 	Web Presentment
	 	CO2 Plan	

svcAgreeClass
svcAgreeClassParam
[Energy Purchase, Generation Balance Provider, Consumption Balance Provider, Billing, AMI Operator, Energy Supplier, Energy Services Agent, Pricing Plan]
                
              
              
              Data Collection, Data Delivery, VEE,Framing, Deployment Planning, Virtual Channel Persistence,Estimation,Outage,Data Transfer,Web Presentment,CO2 Plan	
              
              
              


---41 - old
SELECT CAST(COUNT(*) AS INTEGER) AS field FROM _v_obj_relation WHERE objname = UPPER('x_r_model_output_temp') 
SELECT CAST(COUNT(*) AS INTEGER) AS field FROM _v_obj_relation WHERE objname = UPPER('x_r_model_output_temp') 
DROP TABLE x_r_model_output_temp 
CREATE TABLE X_R_MODEL_OUTPUT_TEMP ( SDP_WID  varchar(7), ANALYSIS_END_DATE  varchar(10), SCORE  double) 
SELECT * FROM TABLE WITH FINAL(nzr..fastdataframe_launch(0)); 
SELECT * FROM TABLE WITH FINAL(nzr..fastdataframe('12345'))

--29--old
SELECT CAST(COUNT(*) AS INTEGER) AS field FROM _v_obj_relation WHERE objname = UPPER('x_r_model_output_temp') 
CREATE TABLE X_R_MODEL_OUTPUT_TEMP ( SDP_WID  varchar(7), ANALYSIS_START_DATE  varchar(10), ANALYSIS_END_DATE  varchar(10), SCORE  double) 
SELECT * FROM TABLE WITH FINAL(nzr..fastdataframe_launch(0)); 
SELECT * FROM TABLE WITH FINAL(nzr..fastdataframe('12345'))   


--41 new-------------------------------------------------------------------------------------- 
On the spus: R 2.15.3
On the host: R 2.15.3select * from RP_MODEL_INPUT_V where to_char(ANALYSIS_END_DATE ,'yyyy-mm-dd') between '2012-07-01' and '2012-07-13' 
[1]  Scored records:    134195           
SELECT CAST(COUNT(*) AS INTEGER) AS field FROM _v_obj_relation WHERE objname = UPPER('x_r_model_output_temp') 
SELECT CAST(COUNT(*) AS INTEGER) AS field FROM _v_obj_relation WHERE objname = UPPER('x_r_model_output_temp') 
DROP TABLE x_r_model_output_temp 
CREATE TABLE X_R_MODEL_OUTPUT_TEMP ( SDP_WID  varchar(7), ANALYSIS_START_DATE  varchar(10), ANALYSIS_END_DATE  varchar(10), SCORE  double) 
SELECT * FROM TABLE WITH FINAL(nzr..fastdataframe_launch(0)); 
SELECT * FROM TABLE WITH FINAL(nzr..fastdataframe('12345')) 

On the spus: R 2.15.3
On the host: R 2.15.3select * from RP_MODEL_INPUT_V where to_char(ANALYSIS_END_DATE ,'yyyy-mm-dd') between '2012-07-01' and '2012-07-13' 
[1]  Scored records:    134195           
SELECT CAST(COUNT(*) AS INTEGER) AS field FROM _v_obj_relation WHERE objname = UPPER('x_r_model_output_temp') 
SELECT CAST(COUNT(*) AS INTEGER) AS field FROM _v_obj_relation WHERE objname = UPPER('x_r_model_output_temp') 
DROP TABLE x_r_model_output_temp 
CREATE TABLE X_R_MODEL_OUTPUT_TEMP ( SDP_WID  varchar(7), ANALYSIS_START_DATE  varchar(10), ANALYSIS_END_DATE  varchar(10), SCORE  double) 
SELECT * FROM TABLE WITH FINAL(nzr..fastdataframe_launch(0)); 
SELECT * FROM TABLE WITH FINAL(nzr..fastdataframe('12345'))


insert into X_R_MODEL_OUTPUT_TEMP select * from external '/tmp/tmp.heiSxRq331'  
                                                                        using (
                                                                        delim ','
                                                                        encoding 'internal'
                                                                        ctrlchars true
                                                                        logdir '/tmp'
                                                                        maxerrors 1
                                                                        ) 
SELECT attname AS field FROM _V_RELATION_COLUMN WHERE name = UPPER('X_R_MODEL_OUTPUT_TEMP') ORDER BY ATTNUM 
SELECT * FROM TABLE WITH final(inza..nzaejobcontrol('stop', 0, 'remote_rae', false, NULL, 0)) 
SELECT aerc FROM TABLE WITH final(inza..nzaejobcontrol('ps', 0, 'remote_rae', false, NULL, 0)) 
INSERT INTO RP_MODEL_OUTPUT (WID , BATCH_ID, ANALYSIS_END_DATE , SDP_WID , SCORE , MODEL_VERSION , ORG_ID , INSERT_TIME , LAST_UPD_TIME, ANALYSIS_START_DATE)  SELECT NEXT VALUE FOR SEQ_RP_MODEL_OUTPUT,
                            BATCH_ID,
                            ANALYSIS_END_DATE,
                            SDP_WID,
                            SCORE,
                            1,
                            ORG_ID,
                            CURRENT_TIMESTAMP(6),
                            CURRENT_TIMESTAMP(6),
                            ANALYSIS_START_DATE  FROM  (
  
                          SELECT x.* , rp.BATCH_ID, rp.ORG_ID
                          FROM x_r_model_output_temp x , rp_feature rp
                          WHERE x.ANALYSIS_END_DATE = rp.ANALYSIS_END_DATE
                          and x.sdp_wid = rp.sdp_wid
                          
                        ) t  
[1]  134195 records added to table RP_MODEL_OUTPUT 
--------------------------------------------------------------------------------------
On the spus: R 2.15.3
On the host: R 2.15.3select * from RP_MODEL_INPUT_V where to_char(ANALYSIS_END_DATE ,'yyyy-mm-dd') between '2012-07-01' and '2012-07-13' 
[1]  Scored records:    134195           
SELECT CAST(COUNT(*) AS INTEGER) AS field FROM _v_obj_relation WHERE objname = UPPER('x_r_model_output_temp') 
SELECT CAST(COUNT(*) AS INTEGER) AS field FROM _v_obj_relation WHERE objname = UPPER('x_r_model_output_temp') 
DROP TABLE x_r_model_output_temp 
CREATE TABLE X_R_MODEL_OUTPUT_TEMP ( SDP_WID  varchar(7), ANALYSIS_START_DATE  varchar(10), ANALYSIS_END_DATE  varchar(10), SCORE  double) 
SELECT * FROM TABLE WITH FINAL(nzr..fastdataframe_launch(0)); 
SELECT * FROM TABLE WITH FINAL(nzr..fastdataframe('12345'))


---29 - NEW--------------------------------------------------------------------------------------



--------------------------------------------------------------------------------------

   ./ConfigurationManagement.sh -Dapplication.command=IncrementalImport -DsourceDirectory=/home/pipe/opt/revenueprotection/tools/confXML
   ./ReferenceDataUtil.sh -Dapplication.command=import -  DimportReferenceDataService.referenceDataSourcePath=/home/pipe/opt/revenueprotection/



add deployment diagram
add component diagram

TODO: Add link for for each file.. upload
categorize as per 
products
assets....
rels etc

move data from CSV to EIP8x schema

move to top 
Structural data loader will need: Configuration data from Utility

2.1.1.1.2 Structural Data Loader -> make bullet for each point


     

1) Do we need to migrate Users with org 
If yes then from where we get User information.
-We need to create all users from RDU / Use procedures to migrate users.
-Need to create proxy for PIPE user. 

2) How to populate entity rel def(used by FlexSync to save rels),  entity_def(accnt, etc)
e.g svc_pt-device(meter) 
	
3)  Need info in populating entity_flexattr_def as we do not allowed list of params.
Assumption: Since param were saved in 7x so if param is not in flexattr then insert it.



Refer [Cloud Analytics 2.0 Data Extraction and Load - Lookup mapping|(Draft) Cloud Analytics 2.0 Data Extraction and Load - Lookup mapping]


flexSync up by disabling validation
seed data load 
premise, Svcpt, device,channel loaded in DB



**************************************************************************************************
Seed data merge

{gliffy:name=taskData|align=left|size=L|version=2}
{gliffy:name=TransactionalDataApp1|align=left|size=L|version=12}

Org merge
---------
comments:
1) No look-up table is required. 
2) 7x id from CSV is used as 8x id for inserting org 
3) Master_data_org_id will always be org_id of primary org SOURCE1 i.e. 1 
4) insert_by and last_upd_by will be predefined set of users for insertion

Questions:


svcptClass merge
---------
comments:
id wil be maxid++;

Questions:

xxx merge
---------
comments:

Questions:


xxx merge
---------     
comments:

Questions:


xxx merge
---------
comments:

Questions:


xxx merge
---------
comments:

Questions:


xxx merge
---------
comments:

Questions:


xxx merge
---------
comments:

Questions:


xxx merge
---------
comments:

Questions:




**************************************************************************************************


Transactional Extraction & load	 Do we also want to use Hadoop as the repository for CSV data  

******************************************************************************************************************
Back up Open questions


h2. Open Issues

{color:red}Open questions{color}
|| S.No. || Component || Concerns / Question || Comment ||
| 1 | Structural extract | What will be the preferred CSV format, canonical file or Multiple files (as in 7.x) | No discussion yet, go with existing multiple file approach |
| 2 | Structural load | Assumption: There will now be no \_d tables i.e. dimension tables as EIP 8 schema (on Oracle) will be refered for Dimension data | Yes, look up will be performed from Oracle EIP 8 schema |
| 3 | Transactional load | how staging look up table will be populated | At the time of inserting data in EIP 8 (Oracle) schema, populate data for table ASSET_LOOKUP in same schema |
| 4 | Transactional load | Assumption: The Cloud Analytics will use EIP 7x MDM and EIP 7x will have ADFTransactionmonitor & ADFMissingReadmonitor deployed which will provide the CSV for real time data. | ( )\\ |
| 5 | Transactional Extraction & load | There will only be extraction & loader jobs and no CMD i.e. run once, interface for transactional data | ( )\\ |
| 6 | Extraction & Load | Following list of jobs are also available in 1.4 Analytics. And we assume that below list will not be used in Cloud Analytics:
* Compute Data Collection Summary
* Compute Interval Summary By Distribution Node
* Compute Interval Summary By Zip Code
* Load Missing Read Fact
* Load Missing Read Reason
* Execute On Demand Missing Read Monitor
* Compute SDP Power Factor
* Load Telvent AMI Performance Summary
* Load Dist Node Data | ( )\\ |
| 7 | Existing sdp fact table has WID related to billing etc | As part of 8x cloud implementation there is no such table that has coulumns as sdp fact. The implementation will only populate data to svc_pt table in 8x | ( )\\ |
| 8 | Loading of *Org / User&nbsp;*data | *Question:*&nbsp;\\
   Do we need to migrate 7x Users  &nbsp;If yes then from where we get 7x User information.
  Which of the following approaches can be used to populate Org data:
   Insert statements for org table
   Use script orgAdmin.sh{*}Org CSV&nbsp;headers:* \\
id,udc_id,name,desc_text,master_data_org_id,rec_version_num,insert_by,last_upd_by \\
*Org table columns:* \\
id,udc_id,name,desc_text,master_data_org_id,insert_time,insert_by,last_upd_by,last_upd_time,rec_version_numid,udc_id, name, desc_text, master_data_org_id, insert_time, insert_by,   +   last_upd_by, last_upd_time, rec_version_num \\
\\
Column last_upd_by & insert_by has values related to users. The Structural data extract does not provide the Users data. \\
\\ | ( ) |
| 9 | Loading of seed data | Entity_Rel_Def \\
\\
The Structural extract provide the product class data like svc_pt_class, device_class etc but does not provide any information related to relationship definitions. \\
\\
*Question&nbsp;*
  How to populate data in entity_rel_def (used by FlexSync to save rels)  
*PossibleSolution*: &nbsp;We know possible Relationships that can exist in 8x schema like ACCNT_SDP_REL,&nbsp;SVC_PT_CTPT_REL&nbsp;etc,&nbsp;so during loading data to entity_def from CSVs (like&nbsp;SvcPtClass_xx.csv etc), we will also load entiry_rel_def table. | ( ) |
| 10 | Loading of param seed data | Parameters (data insertion in Entity_Flexattr_Def) \\
\\
The Structural data extract provides&nbsp;data related to:
* Product class parameters like DeviceClassParam_xx.csv, SvcPtClassParam_xx.csv etc.&nbsp;The data is intented to be inserted in respective class level 8x attribute table. e.g.&nbsp;SvcPtClassParam CSV data will be inserted to&nbsp;svc_pt_class_attr. It is part of seed data merge and wil be handled using insert statements
* Asset / Service params like&nbsp;DeviceParam_xx.csv etc. The data is intended to be inserted in respective asset / service param table like device_param table. It is part of Structural data merge and will be handled by creating XMLs and processing them through FlexSync.&nbsp;*FlexSync (eBO) uses* *entity_flexattr_def table to validate allowed paramters*.&nbsp;&nbsp;*Question*\  How to populate data in entity_flexattr_def  &nbsp; \\
*Possible solution*: Since param were saved in 7x so we can assume that extrated data is correct. If param is not in&nbsp;entity_flexattr_def&nbsp;then insert it. | ( ) |

******************************************************************************************************************

OLD:

|| S.No. || CSV name ||
| 1 | [ADF_AccntParam_xxx.csv|^ADF_AccntParam_20140429230948_001_0000.csv] |
| 2 | [ADF_AccntSvcPtRel_xxx.csv|^ADF_AccntSvcPtRel_20140429230948_001_0000.csv] |
| 3 | [ADF_Accnt_xxx.csv|^ADF_Accnt_20140429230948_001_0000.csv]|
| 4 | [ADF_CalendarParam_xxx.csv|^ADF_CalendarParam_20140429230948_001_0000.csv] |
| 5 | [ADF_Calendar_xxx.csv|^ADF_Calendar_20140429230948_001_0000.csv]|
| 6 | [ADF_Channel_xxx.csv|^ADF_Channel_20140429230948_001_0000.csv]|
| 7 | [ADF_DataSvcClassParam_xxx.csv|^ADF_DataSvcClassParam_20140429230948_001_0000.csv]|
| 8 | [ADF_DataSvcClass_xxx.csv|^ADF_DataSvcClass_20140429230948_001_0000.csv]|
| 9 | [ADF_DataSvcParam_xxx.csv|^ADF_DataSvcParam_20140429230948_001_0000.csv] |
| 10 | [ADF_DeviceChannelRel_xxx.csv|^ADF_DeviceChannelRel_20140429230948_001_0000.csv]|
| 12 | [ADF_DeviceClassParam_xxx.csv|^ADF_DeviceClassParam_20140429230948_001_0000.csv]|
| 13 | [ADF_DeviceClass_xxx.csv|^ADF_DeviceClass_20140429230948_001_0000.csv]|
| 14 | [ADF_DeviceFnClassParam_xxx.csv|^ADF_DeviceFnClassParam_20140429230948_001_0000.csv]|
| 15 | [ADF_DeviceFnClass_xxx.csv|^ADF_DeviceFnClass_20140429230948_001_0000.csv]|
| 16 | [ADF_DeviceFnParam_xxx.csv|^ADF_DeviceFnParam_20140429230948_001_0000.csv]|
| 17 | [ADF_DeviceFnRel_xxx.csv|^ADF_DeviceFnRel_20140429230948_001_0000.csv]|
| 18 | [ADF_DeviceFn_xxx.csv|^ADF_DeviceFn_20140429230948_001_0000.csv]|
| 19 | [ADF_DeviceParam_xxx.csv|^ADF_DeviceParam_20140429230948_001_0000.csv]|
| 20 | [ADF_Device_xxx.csv|^ADF_Device_20140429230948_001_0000.csv]|
| 21 | [ADF_DistNodeRel_xxx.csv|^ADF_DistNodeRel_20140429230948_001_0000.csv]|
| 22 | [ADF_Org_xxx.csv|^ADF_Org_20140429230948_001_0000.csv]|
| 23 | [ADF_Premise_xxx.csv|^ADF_Premise_20140429230948_001_0000.csv]|
| 24 | [ADF_ProcessStats_Snapshot_xxx.csv|^ADF_ProcessStats_Snapshot_20140429230948_001_0000.csv]|
| 25 | [ADF_SvcAgreeClassParam_xxx.csv|^ADF_SvcAgreeClassParam_20140429230948_001_0000.csv]|
| 26 | [ADF_SvcAgreeClass_xxx.csv|^ADF_SvcAgreeClass_20140429230948_001_0000.csv]|
| 27 | [ADF_SvcAgree_xxx.csv|^ADF_SvcAgree_20140429230948_001_0000.csv]|
| 28 | [ADF_SvcProviderParam_xxx.csv|^ADF_SvcProviderParam_20140429230948_001_0000.csv]|
| 29 | [ADF_SvcProvider_xxx.csv|^ADF_SvcProvider_20140429230948_001_0000.csv]|
| 30 | [ADF_SvcPtClassParam_xxx.csv|^ADF_SvcPtClassParam_20140429230948_001_0000.csv]|
| 31 | [ADF_SvcPtClass_xxx.csv|^ADF_SvcPtClass_20140429230948_001_0000.csv]|
| 32 | [ADF_SvcPtDataSvcRel_xxx.csv|^ADF_SvcPtDataSvcRel_20140429230948_001_0000.csv]|
| 33 | [ADF_SvcPtDeviceRel_xxx.csv|^ADF_SvcPtDeviceRel_20140429230948_001_0000.csv]|
| 34 | [ADF_SvcPtDistNodeRel_xxx.csv|^ADF_SvcPtDistNodeRel_20140429230948_001_0000.csv]|
| 35 | [ADF_SvcPtGroupClass_xxx.csv|^ADF_SvcPtGroupClass_20140429230948_001_0000.csv]|
| 36 | [ADF_SvcPtGroupParam_xxx.csv|^ADF_SvcPtGroupParam_20140429230948_001_0000.csv]|
| 37 | [ADF_SvcPtGroupRel_xxx.csv|^ADF_SvcPtGroupRel_20140429230948_001_0000.csv]|
| 38 | [ADF_SvcPtGroup_xxx.csv|^ADF_SvcPtGroup_20140429230948_001_0000.csv]|
| 39 | [ADF_SvcPtMultiplierRel_xxx.csv|^ADF_SvcPtMultiplierRel_20140429230948_001_0000.csv]|
| 40 | [ADF_SvcPtParam_xxx.csv|^ADF_SvcPtParam_20140429230948_001_0000.csv]|
| 41 | [ADF_SvcPtSvcAgreeRel_xxx.csv|^ADF_SvcPtSvcAgreeRel_20140429230948_001_0000.csv]|
| 42 | [ADF_SvcPtSvcAgrParam_xxx.csv|^ADF_SvcPtSvcAgrParam_20140429230948_001_0000.csv]|
| 43 | [ADF_SvcPt_xxx.csv|^ADF_SvcPt_20140429230948_001_0000.csv]|
| 44 | [ADF_TouBin_xxx.csv|^ADF_TouBin_20140429230948_001_0000.csv]|
| 45 | [^File_RowCount_Checksum.csv]|


New:

|| S.No. || CSV name ||
| 1 | [ADF_Org_xxx.csv|^ADF_Org_20140429230948_001_0000.csv]|
| 2 | [ADF_CalendarParam_xxx.csv|^ADF_CalendarParam_20140429230948_001_0000.csv] |
| 3 | [ADF_Calendar_xxx.csv|^ADF_Calendar_20140429230948_001_0000.csv]|
| 4 | [ADF_DataSvcClass_xxx.csv|^ADF_DataSvcClass_20140429230948_001_0000.csv]|
| 5 | [ADF_DeviceClass_xxx.csv|^ADF_DeviceClass_20140429230948_001_0000.csv]|
| 6 | [ADF_DeviceFnClass_xxx.csv|^ADF_DeviceFnClass_20140429230948_001_0000.csv]|
| 7 | [ADF_SvcAgreeClass_xxx.csv|^ADF_SvcAgreeClass_20140429230948_001_0000.csv]|
| 8 | [ADF_SvcPtClass_xxx.csv|^ADF_SvcPtClass_20140429230948_001_0000.csv]|
| 9 | [ADF_SvcPtGroupClass_xxx.csv|^ADF_SvcPtGroupClass_20140429230948_001_0000.csv]|
| 10 | [ADF_DataSvcClassParam_xxx.csv|^ADF_DataSvcClassParam_20140429230948_001_0000.csv]|
| 11 | [ADF_DeviceClassParam_xxx.csv|^ADF_DeviceClassParam_20140429230948_001_0000.csv]|
| 12 | [ADF_DeviceFnClassParam_xxx.csv|^ADF_DeviceFnClassParam_20140429230948_001_0000.csv]|
| 13 | [ADF_SvcAgreeClassParam_xxx.csv|^ADF_SvcAgreeClassParam_20140429230948_001_0000.csv]|
| 14 | [ADF_SvcPtClassParam_xxx.csv|^ADF_SvcPtClassParam_20140429230948_001_0000.csv]|
|| || ||
| 15 | [ADF_Premise_xxx.csv|^ADF_Premise_20140429230948_001_0000.csv]|
| 16 | [ADF_Accnt_xxx.csv|^ADF_Accnt_20140429230948_001_0000.csv]|
| 17 | [ADF_SvcPt_xxx.csv|^ADF_SvcPt_20140429230948_001_0000.csv]|
| 18 | [ADF_Device_xxx.csv|^ADF_Device_20140429230948_001_0000.csv]|
| 19 | [ADF_DeviceFn_xxx.csv|^ADF_DeviceFn_20140429230948_001_0000.csv]|
| 20 | [ADF_SvcAgree_xxx.csv|^ADF_SvcAgree_20140429230948_001_0000.csv]|
| 21 | [ADF_SvcProvider_xxx.csv|^ADF_SvcProvider_20140429230948_001_0000.csv]|
| 22 | [ADF_Channel_xxx.csv|^ADF_Channel_20140429230948_001_0000.csv]|
| 23 | [ADF_SvcPtGroup_xxx.csv|^ADF_SvcPtGroup_20140429230948_001_0000.csv]|
| 24 | [ADF_TouBin_xxx.csv|^ADF_TouBin_20140429230948_001_0000.csv]|
|| || ||
| 25 | [ADF_AccntParam_xxx.csv|^ADF_AccntParam_20140429230948_001_0000.csv] |
| 26 | [ADF_SvcPtParam_xxx.csv|^ADF_SvcPtParam_20140429230948_001_0000.csv]|
| 27 | [ADF_DeviceParam_xxx.csv|^ADF_DeviceParam_20140429230948_001_0000.csv]|
| 28 | [ADF_DataSvcParam_xxx.csv|^ADF_DataSvcParam_20140429230948_001_0000.csv] |
| 29 | [ADF_DeviceFnParam_xxx.csv|^ADF_DeviceFnParam_20140429230948_001_0000.csv]|
| 30 | [ADF_SvcProviderParam_xxx.csv|^ADF_SvcProviderParam_20140429230948_001_0000.csv]|
| 31 | [ADF_SvcPtGroupParam_xxx.csv|^ADF_SvcPtGroupParam_20140429230948_001_0000.csv]|
| 32 | [ADF_SvcPtSvcAgrParam_xxx.csv|^ADF_SvcPtSvcAgrParam_20140429230948_001_0000.csv]|
|| || ||
| 33 | [ADF_AccntSvcPtRel_xxx.csv|^ADF_AccntSvcPtRel_20140429230948_001_0000.csv] |
| 34 | [ADF_SvcPtDeviceRel_xxx.csv|^ADF_SvcPtDeviceRel_20140429230948_001_0000.csv]|
| 35 | [ADF_DeviceFnRel_xxx.csv|^ADF_DeviceFnRel_20140429230948_001_0000.csv]|
| 36 | [ADF_DistNodeRel_xxx.csv|^ADF_DistNodeRel_20140429230948_001_0000.csv]|
| 37 | [ADF_SvcPtDistNodeRel_xxx.csv|^ADF_SvcPtDistNodeRel_20140429230948_001_0000.csv]|
| 38 | [ADF_SvcPtGroupRel_xxx.csv|^ADF_SvcPtGroupRel_20140429230948_001_0000.csv]|
| 39 | [ADF_SvcPtMultiplierRel_xxx.csv|^ADF_SvcPtMultiplierRel_20140429230948_001_0000.csv]|
| 40 | [ADF_SvcPtSvcAgreeRel_xxx.csv|^ADF_SvcPtSvcAgreeRel_20140429230948_001_0000.csv]|
| 41 | [ADF_SvcPtDataSvcRel_xxx.csv|^ADF_SvcPtDataSvcRel_20140429230948_001_0000.csv]|
| 42 | [ADF_DeviceChannelRel_xxx.csv|^ADF_DeviceChannelRel_20140429230948_001_0000.csv]|
|| || || 
| 43 | [ADF_ProcessStats_Snapshot_xxx.csv|^ADF_ProcessStats_Snapshot_20140429230948_001_0000.csv]|
| 44 | [^File_RowCount_Checksum.csv]|


*********************************

Group1: Seed data Parallel execution 

Calendar
CalendarParam
DataSvcClass
DataSvcClassParam
DeviceClass
DeviceClassParam
DeviceFnClass
DeviceFnClassParam
SvcAgreeClass
SvcAgreeClassParam
SvcPtClass
SvcPtClassParam
SvcPtGroupClass
SvcPtGroupClassParam
SvcProvider
SvcProviderParam

Once done run 2-3 in parallel


Group2-sequential	
Premise
SvcPt
Channel

Group3-parallel
Accnt
Device
DeviceFn
SvcAgree
SvcPtGroup

Once done run 4-5 in parallel

Group4- parallel
AccntParam
SvcPtParam
DeviceParam
DataSvcParam
DeviceFnParam
SvcPtGroupParam
SvcPtSvcAgrParam

Group5- parallel

AccntSvcPtRel
SvcPtDeviceRel
DeviceFnRel
DistNodeRel
SvcPtDistNodeRel
SvcPtGroupRel
SvcPtMultiplierRel
SvcPtSvcAgreeRel
SvcPtDataSvcRel
DeviceChannelRel



create table premise_s (
src_id varchar2(100),
data_src varchar2(100),
udc_id varchar2(100),
status_cd varchar2(100),
addr_line_1 varchar2(100),
addr_line_2 varchar2(100),
city varchar2(100),
state_province varchar2(100),
postal_code number,
country varchar2(100),
gps_lat number(2),
gps_long number(2),
desc_text varchar2(100),      
timezone varchar2(100),
district varchar2(100),
bldg_name varchar2(100),
bldg_type varchar2(100),
floor varchar2(100),
floor_type varchar2(100),
street_num varchar2(100),
street_num_suffix varchar2(100),
postal_delivery_id varchar2(100),
postal_delivery_type varchar2(100),
street_name varchar2(100),
street_prefix varchar2(100),
street_suffix varchar2(100),
street_type varchar2(100),
unit_num number,
unit_type varchar2(100),
region varchar2(100),
territory varchar2(100),
map_info varchar2(100),
source_created number,
source_lt_upd number,
insert_time timestamp ,
lt_upd_time timestamp ,
);

create table svc_pt_s(
src_id varchar2(100),
data_src varchar2(100),
premise_udc_id varchar2(100),
udc_id varchar2(100),
class_src_id varchar2(100), 			
class_name varchar2(100), 			
mfg_serial_num number,
mfg_lot_num number, 		
network_id number,
status_cd varchar2(100),
desc_text varchar2(100),
purchase_date  timestamp,
ship_date  timestamp,
mfg_test_date timestamp,
retire_date  timestamp,
mfg_date  timestamp,            
make varchar2(100),
model varchar2(100),
last_test_date  timestamp,
is_virtual_flg varchar2(100),
badge_id varchar2(100),
standard_id varchar2(100),
electronic_id varchar2(100),
comm_technology varchar2(100),
cur_inv_location_id varchar2(100),
sku varchar2(100),
part_num number,
type varchar2(100),
sub_type varchar2(100),
handler_type varchar2(100),
premise_src_id varchar2(100),
feed_loc varchar2(100),
gps_lat number(2),
gps_long number(2),
pulse_output_block varchar2(100),
seal_info varchar2(100),
lock_info varchar2(100),
access_info varchar2(100),
alt_access_info varchar2(100),
loc_info varchar2(100),
alt_loc_info varchar2(100),
billing_cycle varchar2(100),
reading_cycle varchar2(100),
timezone_id number,
gis_id number,
billed_upto_time varchar2(100),
power_status varchar2(100),
load_status varchar2(100),
billing_hold_status varchar2(100),
location_info varchar2(100),
svc_pt_src_id varchar2(100), 	  
eff_start_time  timestamp,        
eff_end_time  timestamp,    
svc_provider_src_id varchar2(100),
svc_agree_src_id varchar2(100),        
source_created number,
source_last_upd number,
insert_time  timestamp,
last_upd_time timestamp
);

create table svc_pt_param_s(
src_id varchar2(100),                          
data_src varchar2(100), 
name varchar2(100),                  
value varchar2(100),                 
status_cd varchar2(100),  
par_src_id varchar2(100),
par_udc_id varchar2(100),
eff_start_time varchar2(100),        
eff_end_time varchar2(100),
type varchar2(100), 
sub_type varchar2(100),
value_alt varchar2(100), 					
handler_type varchar2(100),
source_created varchar2(100),        
source_last_upd varchar2(100),       
insert_time timestamp, 
last_upd_time timestamp
);

   where to keep parameter's src_id in input XML
   how to override lastupd & created time in 8x as it is populated by eBO


- Extraction wil be updated 
- during extraction we will fetch the list of dup records and while creating CSV we'll filter out dup
- CSV is required to have consistency when  we have another mdm
- extraction to staging, to perform validation and then create XMLs
- FlexSync will switch off validations for param & rel so that we do not have to sync reldefs & params
- Only channel-lookup is required
- Channel register mapping & product mapping 
- Parallel group processing

- FlexSync bottlenecks:
	File rename takes time
	File lock release takes time
	

Date Format: TO_DATE(date_str, 'MM/DD/YYYY HH:MIS AM')

emdb21
eip204
mudr204
mdmcln





Extraction: 41 days
1) Code base setup Framework - 2 day
2) Test framework - 1 day
3) Extraction on Reference data - 4 day
	-Calendar & CalendarParam
	-DataSvcClass & DataSvcClassParam
	-DeviceClass & DeviceClassParam
	-DeviceFnClass & DeviceFnClassParam
	-SvcAgreeClass & SvcAgreeClassParam
	-SvcPtClass & SvcPtClassParam
	-SvcPtGroupClass & SvcPtGroupClassParam
	-SvcProvider & SvcProviderParam
	
4) Incremental Extraction of Structural data [include filtering of bad data & XML creation] - 
	Assets:  Total 8 days
	Assets-Premise - 1day
	Assets-Account & AccountParam - 1day
	Assets-Device	--Meter & MeterParam - 1day
		--CommMod & CommModParam
		--CTPT & CTPTParam
	Assets-Svcpt & SvcPtParam - 1day 
			-- Service Point
			-- Distribution Node	
	Assets-SvcAgree - 1day
	Assets-SvcptGroup & SvcptGroupParam - 1day
	Assets-Channel & ChannelParam - 2day
	
	-Rels: Total 10 days
		Rels-Meter-Channel - 1day 
		Rels-Communication-Meter
		Rels-Sdp-Meter - 1day
		Rels-Sdp-CTPT
		Rels-Distribution-Sdp - 1day
		Rels-Route-Sdp - 1day
		Rels-svcPtDataSvcRel - 3day 
			---Data Collection
			---Data Delivery
			---VEE
			---Framing
			---Deployment Planning
			---Virtual Channel Persistence
			---Estimation
			---Outage
			---Data Transfer
		Rels-svcPtSvcAgreeRel - 3 day
			---Energy Purchase
			---Generation Balance Provider
			---Consumption Balance Provider
			---Billing
			---AMI Operator
			---Energy Supplier
			---Web Presentment
			---Pricing Plan
			---CO2 Plan

5) Initial Extraction of Structural data [include filtering of bad data & XML creation] - task same as below - 2 days

6) Loader for Structural data - Total 10 day  
	--Reference data loader process - 5 day
		--Using eBO
	--Orchestration - 5 day
		--Processing files in group 

7) Build & deployment - 2 day
8) Integration testing - 2 day
9) Performance tuning


Gliffy backup
{gliffy:name=Flow_Diagram_raj|align=left|size=L|version=2}
{gliffy:name=StructuralDataApp2|align=left|size=L|version=54}


{color: 0000ff}Excerpt for design doc listing page goes here, in the format  (State) One-line description , where State = Draft / Ready for Review / Reviewed.{color}
{info}
{excerpt}(Draft) eMteter Cloud Analytics platform.{excerpt}
{info}



Cloud Analytics Structural data processing consist of extracting data from underlying MDM and use this data to load in EIP 8x schema. It performs following high level tasks:

* Structural data extraction: MDM data will be extracted into input XMLs.
* Structural data loader: It will load data from input XMLs to EIP 8x schema using FlexSync

Cloud Analytics will support data extraction for following MDMs
* EIP7
* EIP8 : Pending..
* Other MDM: Pending...

The loader process is common for all types of extraction. It will use optimized FlexSync for loading extracted data input XMLs in EIP8 schema.

--Note in case of channel: look_up table is populated to resolve references while processing transactional data 
--Edit 4.1.2 3rd point for handler type
--remove comonent diagram .. that is no longer valid



*****************************************************************************
RP -feed back info job 

CREATE TABLE RP_FEEDBACK_INFO(
    SERVICE_REQUEST_ID           NUMBER          NULL,
    SERVICE_REQUEST_OPEN_TIME    TIMESTAMP(0)    NULL,
    MODEL_OUTPUT_WID             NUMBER          NULL,
    INVESTIGATION_OUTCOME_CD	 VARCHAR2(50)	 NOT NULL,
    ORG_ID                       NUMBER          NOT NULL,
    INSERT_TIME                  TIMESTAMP(0)    NOT NULL,
    INSERT_BY                    NUMBER          NOT NULL,
    LAST_UPD_BY                  NUMBER          NOT NULL,
    LAST_UPD_TIME                TIMESTAMP(0)    NOT NULL,
    REC_VERSION_NUM              NUMBER          NOT NULL,
    CONSTRAINT UK_RP_CLOSED_SR UNIQUE (SERVICE_REQUEST_ID, MODEL_OUTPUT_WID)
);



RP_INVESTIGATION
**batch_id
**sdp_wid
sdp_udc_id 
service_request_close_time
****investigation_outcome_cd ....Need mapping
****sr_outcome_cd	....Need mapping
****sr_outcome_reason_cd   ....Need mapping
sr_sub_type_cd



REC_VERSION_NUM
ID
TYPE_CD
SUB_TYPE_CD
DATA_SRC
STATUS_CD
ASSIGNMENT_STATUS_CD
PRIORITY_CD
SEVERITY_CD
DESC_TEXT
EXT_ID
EXTERNAL_SYSTEM
OPEN_TIME
CLOSE_TIME
DUE_TIME
CUR_WORK_QUEUE_ID
ACCNT_ID
SVC_PT_ID
OUTCOME_CD
ALT_OUTCOME_CD
OUTCOME_REASON_CD
ALT_OUTCOME_REASON_CD
REQUEST_REASON_CD
ALT_REQUEST_REASON_CD
DATA_REF_START_TIME
DATA_REF_END_TIME
CONSUMER_ID
INSTALLER_GROUP_ID
DEVICE_ID
WMS_ID
CHANNEL_ID
SVC_AGREE_ID
PARENT_SR_ID
CUR_WORK_FOLDER_ID
CUR_OWNER_ID
ORG_ID
INSERT_TIME
INSERT_BY
LAST_UPD_BY
LAST_UPD_TIME





*****************************************************************************
    The select queries for Assets, relationships and parameters will have the column handler_type, which will be used to process XMLs as:
     handler_type: Communication Module, process XML for deviceFn
     handler_type: Meter, process XML for meter
     handler_type: Route, process XML for svcPtGroup
     handler_type: Service Point, process XML for svcPt
     handler_type: Data Collection, process XML for svcPtDataSvcRel
     handler_type: Data Delivery, process XML for svcPtDataSvcRel


Query page back up

{toc}
h2. Products Extract Query

h2. Output table: 
h3. CX_ALYT_CHANGED_REC_INFO

The output table will be used by triggers to populated updated records.
{code:xml}
create table cx_alyt_changed_rec_info(
    row_id            VARCHAR2(15),
    org_name          VARCHAR2(50),
    record_type       VARCHAR2(30),
    record_subtype    VARCHAR2(50),
    udc_id            VARCHAR2(50),
    last_upd_time     DATE
);
{code}

h3. CX_ALYT_CHANGED_REL_INFO

The output table will be used by triggers to populated updated records.
{code:xml}
create table cx_alyt_changed_rel_info(
    par_asset_id      VARCHAR2(15),
    child_asset_id    VARCHAR2(15),
    rel_type          VARCHAR2(30),
    sdp_row_id        VARCHAR2(15),
    last_upd_time     DATE
);
{code}

h3. CX_TEMP_SDP_REF_ID
{code:xml}
CREATE TABLE CX_TEMP_SDP_REF_ID(
  SDP_ROW_ID  VARCHAR2(15 BYTE)
);
{code}

h3. CX_TEMP_SDP_DETAILS
{code:xml}
{code}
h3. CX_TEMP_REL_DETAILS
{code:xml}
{code}
h3. CX_TEMP_CHANNEL_DETAILS
{code:xml}
{code}

h2. Triggers

|| Type || 7x Table name || Trigger Name || Output table name ||
| Asset | S_ASSET | TRG_ALYT_ASSET_CAPTURE | CX_ALYT_CHANGED_REC_INFO |
| Relationship | S_ASSET_REL | TRG_ALYT_RELATION_CAPTURE | CX_ALYT_CHANGED_REC_INFO |
| Parameter | S_ASSET_XM | TRG_ALYT_ASSET_PARAM_CAPTURE | CX_ALYT_CHANGED_REC_INFO |
| Premise | S_ADDR_PER | TRG_ALYT_PREMISE_CAPTURE | CX_ALYT_CHANGED_REC_INFO |
| Account & Svcprovider | S_ORG_EXT | TRG_ALYT_ACCOUNT_CAPTURE | CX_ALYT_CHANGED_REC_INFO |
| Account param | S_ORG_EXT_XM | TRG_ALYT_ACCOUNT_PARAM_CAPTURE | CX_ALYT_CHANGED_REC_INFO |
| Account SDP rel | CX_ASSET_ACCNT | TRG_ALYT_ACCNT_SVC_PT_CAPTURE | CX_ALYT_CHANGED_REC_INFO |
| ServiceAgreement | S_DOC_AGREE | TRG_ALYT_SVC_AGREE_CAPTURE | CX_ALYT_CHANGED_REC_INFO |


h3. TRG_ALYT_ASSET_CAPTURE
{code:xml}
CREATE OR REPLACE TRIGGER trg_alyt_asset_capture
BEFORE INSERT OR UPDATE ON s_asset
FOR EACH ROW
BEGIN
    INSERT INTO cx_alyt_changed_rec_info
        (
            row_id
          , org_name
          , record_type
          , record_subtype
          , udc_id
          , last_upd_time
        )
    VALUES
        (
            :new.row_id
          , :new.data_src
          , :new.type_cd
          , :new.cfg_type_cd
          , :new.x_udc_asset_id
          , :new.last_upd
        );

EXCEPTION
    WHEN OTHERS
    THEN
        RAISE;
END;
{code}

h3. TRG_ALYT_RELATION_CAPTURE
{code:xml}
CREATE OR REPLACE TRIGGER trg_alyt_relation_capture
BEFORE INSERT OR UPDATE ON s_asset_rel
FOR EACH ROW
BEGIN
    INSERT INTO cx_alyt_changed_rel_info
        (
            par_asset_id
	  , child_asset_id
	  , rel_type
	  , sdp_row_id
          , last_upd_time
        )
    VALUES
        (
            :new.par_asset_id
          , :new.asset_id
          , :new.x_asset_relation_type_cd
          , CASE 
                WHEN :new.x_asset_relation_type_cd IN ('ROUTE-SDP', 'DISTRIBUTION-SDP')
                THEN :new.asset_id
                WHEN :new.x_asset_relation_type_cd IN ('SDP-METER', 'SDP-CT/PT')
                THEN :new.par_asset_id
                ELSE NULL
             END
          , :new.last_upd
        );
 
EXCEPTION
    WHEN OTHERS
    THEN
        RAISE;
END;
{code}


h3. TRG_ALYT_ASSET_PARAM_CAPTURE
{code:xml}
CREATE OR REPLACE TRIGGER trg_alyt_asset_param_capture
BEFORE INSERT OR UPDATE ON s_asset_xm
FOR EACH ROW
BEGIN
    INSERT INTO cx_alyt_changed_rec_info
        (
            row_id
          , org_name
          , record_type
          , last_upd_time
        )
    VALUES
        (
            :new.par_row_id
          , :new.attrib_05
          , :new.type
          , :new.last_upd
        );

EXCEPTION
    WHEN OTHERS
    THEN
        RAISE;
END;
{code}


h3. TRG_ALYT_PREMISE_CAPTURE
{code:xml}
CREATE OR REPLACE TRIGGER trg_alyt_premise_capture
BEFORE INSERT OR UPDATE ON s_addr_per
FOR EACH ROW
BEGIN
    INSERT INTO cx_alyt_changed_rec_info
        (
            row_id
          , org_name
          , record_type
          , last_upd_time
        )
    VALUES
        (
            :new.row_id
          , :new.x_data_src
          , 'PREMISE'
          , :new.last_upd
        );

EXCEPTION
    WHEN OTHERS
    THEN
        RAISE;
END;
{code}


h3. TRG_ALYT_ACCOUNT_CAPTURE
{code:xml}
CREATE OR REPLACE TRIGGER trg_alyt_account_capture
BEFORE INSERT OR UPDATE ON s_org_ext
FOR EACH ROW
BEGIN
    INSERT INTO cx_alyt_changed_rec_info
        (
            row_id
          , org_name
          , record_type
          , record_subtype
          , last_upd_time
        )
    VALUES
        (
            :new.row_id
          , :new.x_data_src
          , 'ACCOUNT'
          , :new.ou_type_cd
          , :new.last_upd
        );

EXCEPTION
    WHEN OTHERS
    THEN
        RAISE;
END;
{code}


h3. TRG_ALYT_ACCOUNT_PARAM_CAPTURE
{code:xml}
CREATE OR REPLACE TRIGGER trg_alyt_account_param_capture
BEFORE INSERT OR UPDATE ON s_org_ext_xm
FOR EACH ROW
BEGIN
    INSERT INTO cx_alyt_changed_rec_info
        (
            row_id
          , org_name
          , record_type
          , last_upd_time
        )
    VALUES
        (
            :new.row_id
          , :new.attrib_05
          , 'ACCOUNT_PARAMETER' --:new.type
          , :new.last_upd
        );

EXCEPTION
    WHEN OTHERS
    THEN
        RAISE;
END;
{code}


h3. TRG_ALYT_ACCNT_SVC_PT_CAPTURE
{code:xml}
CREATE OR REPLACE TRIGGER trg_alyt_accnt_svc_pt_capture
BEFORE INSERT OR UPDATE ON cx_asset_accnt
FOR EACH ROW
BEGIN
    INSERT INTO cx_alyt_changed_rel_info
        (
            par_asset_id
	  , child_asset_id
	  , rel_type
	  , sdp_row_id
          , last_upd_time
        )
    VALUES
        (
            :new.accnt_id
          , :new.asset_id
          , 'ACCOUNT-SDP'
          , :new.asset_id
          , :new.last_upd
        );
 
EXCEPTION
    WHEN OTHERS
    THEN
      RAISE;
END;
{code}

h3. TRG_ALYT_SVC_AGREE_CAPTURE
{code:xml}
CREATE OR REPLACE TRIGGER trg_alyt_svc_agree_capture
BEFORE INSERT OR UPDATE ON s_doc_agree
FOR EACH ROW
BEGIN
    INSERT INTO cx_alyt_changed_rec_info
        (
            row_id
          , record_type
          , last_upd_time
        )
    VALUES
        (
            :new.row_id
          , :new.agree_cd
          , :new.last_upd
        );

EXCEPTION
    WHEN OTHERS
    THEN
        RAISE;
END;
{code}

h2. Query to filter out duplicate records from Assets
{code:xml}
SELECT COUNT(1)
     , data_src
     , type_cd
     , cfg_type_cd
     , x_udc_asset_id
  FROM s_asset
 WHERE type_cd in ('Service Point'
                  ,'Meter'
                  ,'Communication Module'
                  ,'CT-PT'
                  ,'Distribution Node'
                  ,'Disconnect Collar'
                  ,'Equipment Location')
 GROUP BY data_src
     , type_cd
     , cfg_type_cd
     , x_udc_asset_id
HAVING COUNT(1) > 1
{code}

h2. Extraction queries

h3. Initial load

h4. Reference data query
{code:xml}
SELECT          
pxm.row_id       AS param_src_id,
p.row_id         AS src_id,
NULL             AS data_src,
par_row_id       AS par_src_id,
mapping.product_name_8x  AS name,
attrib_02        AS value,
attrib_03        AS status_cd,
attrib_26	 AS eff_start_time,       
attrib_27	 AS eff_end_time,       
mapping.type_8x  AS type,
mapping.sub_type_8x AS sub_type,
DECODE(pxm.type, 'Service', p.sub_type, pxm.type) AS handler_type,
pxm.created      AS source_created,
pxm.last_upd     AS source_last_upd,
NULL			     AS insert_time,
NULL			    AS last_upd_time
FROM s_prod_int_xm pxm, 
s_prod_int p, 
product_name_mapping mapping
WHERE 
pxm.par_row_id = p.row_id (+)
AND p.name     = mapping.product_name_7x
AND p.type     = mapping.type_7x
{code}

h4. Asset extraction query with product_name, svc_pt_udc_id, premise_udc_id resolution
{code:xml}
 SELECT asset_for_udc.x_udc_asset_id,
       s.row_id AS src_id,
       s.data_src AS data_src,
       s.x_udc_asset_id AS udc_id,
       s.prod_id AS class_src_id,
       s.serial_num AS mfg_serial_num,
       s.x_lot_num AS mfg_lot_num,
       s.x_network_id AS network_id,
       s.status_cd AS status_cd,
       s.desc_text AS desc_text,
       s.purch_dt AS purchase_date,
       s.ship_dt AS ship_date,
       NULL AS mfg_test_date,
       s.x_retire_dt AS retire_date,
       s.x_mfg_date AS mfg_date,
       s.make_cd AS make,
       s.model_cd AS model,
       s.last_test_dt AS last_test_date,
       s.x_virtual_asset AS is_virtual_flg,
       s.x_universal_id AS badge_id,
       s.x_aep_num AS standard_id,
       s.x_electronic_id AS electronic_id,
       NULL AS comm_technology,
       NULL AS cur_inv_location_id,
       NULL AS sku,
       NULL AS part_num,
       s.type_cd AS TYPE,
       s.cfg_type_cd AS sub_type,
       DECODE (s.type_cd, 'Service', s.cfg_type_cd, s.type_cd)
          AS handler_type,
       s.per_addr_id AS premise_src_id,
       s.x_ax_feed_loc AS feed_loc,
       s.x_latitude_new AS gps_lat,
       s.x_longitude_new AS gps_long,
       s.x_pulse_output_blk AS pulse_output_block,
       s.x_seal_info AS seal_info,
       s.x_lock_info AS lock_info,
       s.x_util_access_info AS access_info,
       s.x_emeter_acces_info AS alt_access_info,
       s.x_util_premise_loc AS loc_info,
       s.x_emeter_premise_loc AS alt_loc_info,
       s.x_billing_cycle AS billing_cycle,
       s.x_reading_cycle AS reading_cycle,
       NULL AS timezone_id,
       NULL AS gis_id,
       s.x_read_dt AS billed_upto_time,
       s.x_power_status AS power_status,
       s.x_usage_status AS load_status,
       s.x_billing_hold AS billing_hold_status,
       s.meter_loc AS location_info,
       s.service_point_id AS svc_pt_src_id,
       s.start_dt AS eff_start_time,
       s.end_dt AS eff_end_time,
       a.svc_provider_id AS svc_provider_src_id,
       s.cur_agree_id AS svc_agree_src_id,
       s.created AS source_created,
       s.last_upd AS source_last_upd,
       NULL AS insert_time,
       NULL AS last_upd_time,
       asset_for_udc.x_udc_asset_id AS sdp_udc_id,
       premise_tab.x_client_prmse_id AS premise_udc_id,
       prod.name
 FROM s_asset s,
       s_doc_agree a,
       s_asset asset_for_udc,
       s_addr_per premise_tab,
       s_prod_int prod
 WHERE s.cur_agree_id = a.row_id(+)
       AND (s.last_upd >=
               NVL (:lastUpdateTime, TO_DATE ('01/01/1970', 'MM/DD/YYYY'))
            OR a.last_upd >=
                  NVL (:lastUpdateTime, TO_DATE ('01/01/1970', 'MM/DD/YYYY')))
       AND s.service_point_id = asset_for_udc.row_id(+)
       AND s.per_addr_id = premise_tab.row_id(+)
       AND s.prod_id = prod.row_id
{code}

h3. Incremental load

h4.svc_pt incremental extraction query with product_name, premise_udc_id resolution
{code:xml}
 SELECT s.row_id AS src_id,
       s.data_src AS data_src,
       s.x_udc_asset_id AS udc_id,
       s.prod_id AS class_src_id,
       s.serial_num AS mfg_serial_num,
       s.x_lot_num AS mfg_lot_num,
       s.x_network_id AS network_id,
       s.status_cd AS status_cd,
       s.desc_text AS desc_text,
       s.purch_dt AS purchase_date,
       s.ship_dt AS ship_date,
       NULL AS mfg_test_date,
       s.x_retire_dt AS retire_date,
       s.x_mfg_date AS mfg_date,
       s.make_cd AS make,
       s.model_cd AS model,
       s.last_test_dt AS last_test_date,
       s.x_virtual_asset AS is_virtual_flg,
       s.x_universal_id AS badge_id,
       s.x_aep_num AS standard_id,
       s.x_electronic_id AS electronic_id,
       NULL AS comm_technology,
       NULL AS cur_inv_location_id,
       NULL AS sku,
       NULL AS part_num,
       s.type_cd AS TYPE,
       s.cfg_type_cd AS sub_type,
       DECODE (s.type_cd, 'Service', s.cfg_type_cd, s.type_cd) AS handler_type,
       s.per_addr_id AS premise_src_id,
       s.x_ax_feed_loc AS feed_loc,
       s.x_latitude_new AS gps_lat,
       s.x_longitude_new AS gps_long,
       s.x_pulse_output_blk AS pulse_output_block,
       s.x_seal_info AS seal_info,
       s.x_lock_info AS lock_info,
       s.x_util_access_info AS access_info,
       s.x_emeter_acces_info AS alt_access_info,
       s.x_util_premise_loc AS loc_info,
       s.x_emeter_premise_loc AS alt_loc_info,
       s.x_billing_cycle AS billing_cycle,
       s.x_reading_cycle AS reading_cycle,
       NULL AS timezone_id,
       NULL AS gis_id,
       s.x_read_dt AS billed_upto_time,
       s.x_power_status AS power_status,
       s.x_usage_status AS load_status,
       s.x_billing_hold AS billing_hold_status,
       s.meter_loc AS location_info,
       s.service_point_id AS svc_pt_src_id,
       s.start_dt AS eff_start_time,
       s.end_dt AS eff_end_time,
       s.cur_agree_id AS svc_agree_src_id,
       s.created AS source_created,
       s.last_upd AS source_last_upd,
       NULL AS insert_time,
       NULL AS last_upd_time,
       premise_tab.x_client_prmse_id AS premise_udc_id,
       mapping.product_name_8x AS name,
       param.attrib_01 AS param_name,
       param.attrib_02 AS param_value,
       param.attrib_12 AS eff_start_date,
       param.attrib_13 AS eff_end_date
 FROM s_asset s,
       s_addr_per premise_tab,
       s_prod_int prod,
       cx_alyt_changed_rec_info info,
       product_name_mapping mapping,
       s_asset_xm param
 WHERE s.per_addr_id = premise_tab.row_id
       AND s.prod_id = prod.row_id
       AND info.row_id = s.row_id
       AND info.record_type = 'Service Point'
       AND mapping.product_name_7x = prod.name
       AND mapping.type_7x = info.record_type
       AND param.par_row_id (+) = s.row_id;
      
{code}

h4.meter incremental extraction query with product_name resolution
{code:xml}
 SELECT s.row_id AS src_id,
       s.data_src AS data_src,
       s.x_udc_asset_id AS udc_id,
       s.prod_id AS class_src_id,
       s.serial_num AS mfg_serial_num,
       s.x_lot_num AS mfg_lot_num,
       s.x_network_id AS network_id,
       s.status_cd AS status_cd,
       s.desc_text AS desc_text,
       s.purch_dt AS purchase_date,
       s.ship_dt AS ship_date,
       NULL AS mfg_test_date,
       s.x_retire_dt AS retire_date,
       s.x_mfg_date AS mfg_date,
       s.make_cd AS make,
       s.model_cd AS model,
       s.last_test_dt AS last_test_date,
       s.x_virtual_asset AS is_virtual_flg,
       s.x_universal_id AS badge_id,
       s.x_aep_num AS standard_id,
       s.x_electronic_id AS electronic_id,
       NULL AS comm_technology,
       NULL AS cur_inv_location_id,
       NULL AS sku,
       NULL AS part_num,
       s.type_cd AS TYPE,
       s.cfg_type_cd AS sub_type,
       DECODE (s.type_cd, 'Service', s.cfg_type_cd, s.type_cd) AS handler_type,
       s.per_addr_id AS premise_src_id,
       s.x_ax_feed_loc AS feed_loc,
       s.x_latitude_new AS gps_lat,
       s.x_longitude_new AS gps_long,
       s.x_pulse_output_blk AS pulse_output_block,
       s.x_seal_info AS seal_info,
       s.x_lock_info AS lock_info,
       s.x_util_access_info AS access_info,
       s.x_emeter_acces_info AS alt_access_info,
       s.x_util_premise_loc AS loc_info,
       s.x_emeter_premise_loc AS alt_loc_info,
       s.x_billing_cycle AS billing_cycle,
       s.x_reading_cycle AS reading_cycle,
       NULL AS timezone_id,
       NULL AS gis_id,
       s.x_read_dt AS billed_upto_time,
       s.x_power_status AS power_status,
       s.x_usage_status AS load_status,
       s.x_billing_hold AS billing_hold_status,
       s.meter_loc AS location_info,
       s.service_point_id AS svc_pt_src_id,
       s.start_dt AS eff_start_time,
       s.end_dt AS eff_end_time,
       --a.svc_provider_id AS svc_provider_src_id,
       s.cur_agree_id AS svc_agree_src_id,
       s.created AS source_created,
       s.last_upd AS source_last_upd,
       NULL AS insert_time,
       NULL AS last_upd_time,
       mapping.product_name_8x AS name,
       param.attrib_01 AS param_name,
       param.attrib_02 AS param_value,
       param.attrib_12 AS eff_start_date,
       param.attrib_13 AS eff_end_date
 FROM s_asset s,
       s_prod_int prod,
       cx_alyt_changed_rec_info info,
       product_name_mapping mapping,
       s_asset_xm param
 WHERE s.prod_id = prod.row_id
       AND info.row_id = s.row_id
       AND info.record_type = 'Meter'
       AND mapping.product_name_7x = prod.name
       AND mapping.type_7x = info.record_type
       AND param.par_row_id (+) = s.row_id;
       
{code}

h4.channel incremental extraction query with product_name, svc_pt_udc_id and register channel split resolution
{code:xml}
 SELECT s.row_id AS src_id,
       s.data_src AS data_src,
       s.x_udc_asset_id AS udc_id,
       s.prod_id AS class_src_id,
       s.serial_num AS mfg_serial_num,
       s.x_lot_num AS mfg_lot_num,
       s.x_network_id AS network_id,
       s.status_cd AS status_cd,
       s.desc_text AS desc_text,
       s.purch_dt AS purchase_date,
       s.ship_dt AS ship_date,
       NULL AS mfg_test_date,
       s.x_retire_dt AS retire_date,
       s.x_mfg_date AS mfg_date,
       s.make_cd AS make,
       s.model_cd AS model,
       s.last_test_dt AS last_test_date,
       s.x_virtual_asset AS is_virtual_flg,
       s.x_universal_id AS badge_id,
       s.x_aep_num AS standard_id,
       s.x_electronic_id AS electronic_id,
       NULL AS comm_technology,
       NULL AS cur_inv_location_id,
       NULL AS sku,
       NULL AS part_num,
       s.type_cd AS TYPE,
       s.cfg_type_cd AS sub_type,
       DECODE (s.type_cd, 'Service', s.cfg_type_cd, s.type_cd) AS handler_type,
       s.per_addr_id AS premise_src_id,
       s.x_ax_feed_loc AS feed_loc,
       s.x_latitude_new AS gps_lat,
       s.x_longitude_new AS gps_long,
       s.x_pulse_output_blk AS pulse_output_block,
       s.x_seal_info AS seal_info,
       s.x_lock_info AS lock_info,
       s.x_util_access_info AS access_info,
       s.x_emeter_acces_info AS alt_access_info,
       s.x_util_premise_loc AS loc_info,
       s.x_emeter_premise_loc AS alt_loc_info,
       s.x_billing_cycle AS billing_cycle,
       s.x_reading_cycle AS reading_cycle,
       NULL AS timezone_id,
       NULL AS gis_id,
       s.x_read_dt AS billed_upto_time,
       s.x_power_status AS power_status,
       s.x_usage_status AS load_status,
       s.x_billing_hold AS billing_hold_status,
       s.meter_loc AS location_info,
       s.service_point_id AS svc_pt_src_id,
       s.start_dt AS eff_start_time,
       s.end_dt AS eff_end_time,
       s.cur_agree_id AS svc_agree_src_id,
       s.created AS source_created,
       s.last_upd AS source_last_upd,
       NULL AS insert_time,
       NULL AS last_upd_time,
       asset_for_udc.x_udc_asset_id AS sdp_udc_id,
       mapping.meas_type_udc_id_8x AS name,
       param.attrib_01 AS param_name,
       param.attrib_02 AS param_value,
       param.attrib_12 AS eff_start_date,
       param.attrib_13 AS eff_end_date
 FROM s_asset s,
       s_asset asset_for_udc,
       s_prod_int prod,
       cx_alyt_changed_rec_info info,
       channel_product_mapping mapping,
       s_asset_xm param
 WHERE s.service_point_id = asset_for_udc.row_id
       AND s.prod_id = prod.row_id
       AND info.row_id = s.row_id
       AND info.record_type = 'Channel'
       AND mapping.channel_name_7x = prod.name
       AND param.par_row_id (+) = s.row_id
 ORDER BY s.row_id, mapping.meas_type_udc_id_8x;
      
{code}


h2. Mapping and look up table structure

h3. CHANNEL_PRODUCT_MAPPING

The table will be populated using Utility provided information related to register channel.

|| Column name || Type || Description ||
| channel_name_7x | Varchar | channel name from 7x |
| meas_type_udc_id_8x | Varchar | channel's meas_type udc id from 8x |
| meter_param_dials_value | Varchar | meter parameter 'Dials' value from 7x. The value is used to split Register channel from 7x to multiple Register channels in 8x. *The column name may change* |
| insert_time | Timestamp | insert time of asset |

h3. PRODUCT_NAME_MAPPING

|| Column name || Type || Description ||
| product_name_7x | Varchar | product name from 7x |
| type_7x | Varchar | entity type from 7x |
| sub_type_7x | Varchar | entity sub_type from 7x |
| product_name_8x | Varchar | product from 8x |
| type_8x | Varchar | entity type from 8x |
| sub_type_8x | Varchar | entity sub_type from 8x |
| insert_time | Timestamp | insert time of asset |

h3. CHANNEL_LOOKUP mapping

|| Column name || Type || Description ||
| src_id | Varchar | source id of asset from 7x |
| svc_pt_udc_id | Varchar | svc_pt udc_id from 7x |
| meas_type_udc_id_8x | Varchar | channel measType udcId for 8x |
| org_name | Varchar | org name of utility from 7x |
| insert_time | Timestamp | insert time of asset |

{code:xml}
Sample data:
 SRC_ID , SVC_PT_UDC_ID , MEAS_TYPE_UDC_ID_8x , ORG_NAME , INSERT_TIME 
 1-2KFM , ALYTBill000001 , EM.ELECTRIC.KVA.3.3 , ORG1 ,05-MAY-14 03.31.10.000000000 PM
 1-2KHC , ALYTBill000003 , EM.ELECTRIC.KVA.3.3, ORG1 ,05-MAY-14 03.31.10.000000000 PM
 1-2KHF , ALYTBill000005 , EM.ELECTRIC.KVA.3.3 , ORG1 ,05-MAY-14 03.31.10.000000000 PM
{code}

h2. XML templates

h3. SVC_PT template
{code:xml}
<servicePoint>
      <mRID>mRID</mRID>
      <idType>idType</idType>
      <pathName>pathName</pathName>
      <description>description</description>
      <universalId>universalId</universalId>
      <className>className</className>
      <serviceType>serviceType</serviceType>
      <type>type</type>
      <subType>subType</subType>
      <status>status</status>
      <feedLocation>feedLocation</feedLocation>
      <premiseId xsi:type= IdentifiedObject >
        <mRID>mRID</mRID>
        <idType>idType</idType>
        <pathName>pathName</pathName>
        <description>description</description>
      </premiseId>
      <accessInfo>accessInfo</accessInfo>
      <altAccessInfo>altAccessInfo</altAccessInfo>
      <locInfo>locInfo</locInfo>
      <altLocInfo>altLocInfo</altLocInfo>
      <sealInfo>sealInfo</sealInfo>
      <lockInfo>lockInfo</lockInfo>
      <pulseOutputBlock>pulseOutputBlock</pulseOutputBlock>
      <latitude>0.0</latitude>
      <longitude>0.0</longitude>
      <gpsLocAccuracy>0.0</gpsLocAccuracy>
      <virtualInd>virtualInd</virtualInd>
      <criticalCareInd>criticalCareInd</criticalCareInd>
      <criticalLoadInd>criticalLoadInd</criticalLoadInd>
      <timezone>timezone</timezone>
      <gisId>gisId</gisId>
      <parameter>
        <name>name</name>
        <value>value</value>
        <startDate>startDate</startDate>
        <endDate>endDate</endDate>
      </parameter>
</servicePoint>
{code}

h3. Meter (Device) template
{code:xml}
<device>
      <mRID>mRID</mRID>
      <idType>idType</idType>
      <pathName>pathName</pathName>
      <description>description</description>
      <model>model</model>
      <manufacturedDate>manufacturedDate</manufacturedDate>
      <mfgSerialNumber>mfgSerialNumber</mfgSerialNumber>
      <mfgLotNumber>mfgLotNumber</mfgLotNumber>
      <purchaseDate>purchaseDate</purchaseDate>
      <shipDate>shipDate</shipDate>
      <mfgTestDate>mfgTestDate</mfgTestDate>
      <testDate>testDate</testDate>
      <retireDate>retireDate</retireDate>
      <status>status</status>
      <networkId>networkId</networkId>
      <commTechnology>commTechnology</commTechnology>
      <parameter>
        <name>name</name>
        <value>value</value>
        <startDate>startDate</startDate>
        <endDate>endDate</endDate>
      </parameter>
      <badgeId>badgeId</badgeId>
      <standardId>standardId</standardId>
      <electronicId>electronicId</electronicId>
      <make>make</make>
      <type>type</type>
      <virtualInd>virtualInd</virtualInd>
      <className>className</className>
      <location>location</location>
      <deviceFunctionType>deviceFunctionType</deviceFunctionType>
</device>
{code}

h3. Channel template
{code:xml}
<channel xsi:type= Channel >
      <mRID>mRID</mRID>
      <idType>idType</idType>
      <pathName>pathName</pathName>
      <description>description</description>
      <channelType>channelType</channelType>
      <servicePointId>
        <mRID>mRID</mRID>
        <idType>idType</idType>
        <pathName>pathName</pathName>
        <description>description</description>
        <type>type</type>
      </servicePointId>
      <status>status</status>
      <parameter>
        <name>name</name>
        <value>value</value>
        <startDate>startDate</startDate>
        <endDate>endDate</endDate>
      </parameter>
</channel>
{code}


h3. CHANNEL_LOOKUP mapping

|| Column name || Type || Description ||
| src_id | Varchar | source id of asset from 7x |
| svc_pt_udc_id | Varchar | svc_pt udc_id from 7x |
| meas_type_udc_id_8x | Varchar | channel measType udcId for 8x |
| org_name | Varchar | org name of utility from 7x |
| insert_time | Timestamp | insert time of asset |

{code:xml}
Sample data:
 SRC_ID , SVC_PT_UDC_ID , MEAS_TYPE_UDC_ID_8x , ORG_NAME , INSERT_TIME 
 1-2KFM , ALYTBill000001 , EM.ELECTRIC.KVA.3.3 , ORG1 ,05-MAY-14 03.31.10.000000000 PM
 1-2KHC , ALYTBill000003 , EM.ELECTRIC.KVA.3.3, ORG1 ,05-MAY-14 03.31.10.000000000 PM
 1-2KHF , ALYTBill000005 , EM.ELECTRIC.KVA.3.3 , ORG1 ,05-MAY-14 03.31.10.000000000 PM
{code}

*****************************************************************************

createTicket job

sr status - new / open
use sdpExemp b/w analysis start and end date
considerBatchId


use BOTemplate 
check Nz basicDataSource
build & deployment
verify JobStepLogger





delete from job_constraints;
delete from job_def_arg;
delete from job_def_mutex;
delete from job_event_trigger_arg;
delete from job_event_trigger;
delete from job_exec_arg;
delete from job_step_log;
delete from job_execution;
delete from job_fired_trigger;
delete from job_NFN;
delete from job_trigger_arg;
delete from job_trigger;
delete from job_def;
delete from job_group;


--7x
delete from job_constraints where job_def_id=201;
delete from job_def_arg where job_def_id=201;
delete from job_def_mutex_rel where job_def_id=201 or job_mutex_id=101;
delete from job_mutex where name like ' Analyze SDP score ';
--delete from job_event_trigger_arg where job_def_id=201;
delete from job_event_trigger where job_def_id=201;
--delete from job_exec_arg where job_def_id=201;
--delete from job_step_log where job_def_id=201;
--delete from job_execution;
--delete from job_fired_trigger;
delete from job_NFN where job_def_id=201;
--delete from job_trigger_arg where job_def_id=201;
delete from job_trigger where job_def_id=201;
delete from job_def where id=201;


ALYT-3185:

./ConfigurationManagementTool.sh -DconfigDataTransferService.configDataSourcePath=/home/eip/opt/revenueprotection/tools/confXML/
./ReferenceDataUtil.sh importReferenceDataService.referenceDataSourcePath=/home/eip/opt/revenueprotection
./orgAdmin.sh -c deploy -d /home/eip/opt/revenueprotection/ -o FLEXSYNCORG

Edit system console for 
nzDataSource values


SELECT s.row_id AS src_id,
      s.data_src AS data_src,
      s.x_udc_asset_id AS udc_id,
      s.prod_id AS class_src_id,
      s.serial_num AS mfg_serial_num,
      s.x_lot_num AS mfg_lot_num,
      s.x_network_id AS network_id,
      s.status_cd AS status_cd,
      s.desc_text AS desc_text,
      s.purch_dt AS purchase_date,
      s.ship_dt AS ship_date,
      NULL AS mfg_test_date,
      s.x_retire_dt AS retire_date,
      s.x_mfg_date AS mfg_date,
      s.make_cd AS make,
      s.model_cd AS model,
      s.last_test_dt AS last_test_date,
      s.x_virtual_asset AS is_virtual_flg,
      s.x_universal_id AS badge_id,
      s.x_aep_num AS standard_id,
      s.x_electronic_id AS electronic_id,
      NULL AS comm_technology,
      NULL AS cur_inv_location_id,
      NULL AS sku,
      NULL AS part_num,
      s.type_cd AS TYPE,
      s.cfg_type_cd AS sub_type,
      DECODE (s.type_cd, 'Service', s.cfg_type_cd, s.type_cd) AS handler_type,
      s.per_addr_id AS premise_src_id,
      s.x_ax_feed_loc AS feed_loc,
      s.x_latitude_new AS gps_lat,
      s.x_longitude_new AS gps_long,
      s.x_pulse_output_blk AS pulse_output_block,
      s.x_seal_info AS seal_info,
      s.x_lock_info AS lock_info,
      s.x_util_access_info AS access_info,
      s.x_emeter_acces_info AS alt_access_info,
      s.x_util_premise_loc AS loc_info,
      s.x_emeter_premise_loc AS alt_loc_info,
      s.x_billing_cycle AS billing_cycle,
      s.x_reading_cycle AS reading_cycle,
      NULL AS timezone_id,
      NULL AS gis_id,
      s.x_read_dt AS billed_upto_time,
      s.x_power_status AS power_status,
      s.x_usage_status AS load_status,
      s.x_billing_hold AS billing_hold_status,
      s.meter_loc AS location_info,
      s.service_point_id AS svc_pt_src_id,
      s.start_dt AS eff_start_time,
      s.end_dt AS eff_end_time,
      s.cur_agree_id AS svc_agree_src_id,
      s.created AS source_created,
      s.last_upd AS source_last_upd,
      NULL AS insert_time,
      NULL AS last_upd_time,
      premise_tab.x_client_prmse_id AS premise_udc_id,
      mapping.product_name_8x AS name,
      param.attrib_01 AS param_name,
      param.attrib_02 AS param_value,
      param.attrib_12 AS eff_start_date,
      param.attrib_13 AS eff_end_date
 FROM s_asset s,
      s_addr_per premise_tab,
      s_prod_int prod,
      (SELECT sdp_row_id
         FROM temp_sdp_ref_id
       UNION
       SELECT row_id AS sdp_row_id
         FROM cx_alyt_changed_rec_info
        WHERE record_type = 'Service Point'
        UNION
       SELECT service_point_id AS sdp_row_id
         FROM cx_alyt_changed_rec_info
        WHERE record_type = 'Service') ref,
        UNION
       SELECT service_point_id AS sdp_row_id
	         FROM cx_alyt_changed_rec_info
        WHERE record_type = 'Channel'
      product_name_mapping mapping,
      s_asset_xm param
WHERE s.per_addr_id = premise_tab.row_id
      AND s.prod_id = prod.row_id
      AND ref.sdp_row_id = s.row_id
      AND mapping.product_name_7x = prod.name
      AND param.par_row_id (+) = s.row_id
      


create table cx_alyt_changed_rec_info(
    row_id            VARCHAR2(15),
    org_name          VARCHAR2(50),
    record_type       VARCHAR2(30),
    record_subtype    VARCHAR2(50),
    service_point_id  CARCHAR2(15),
    udc_id            VARCHAR2(50),
    last_upd_time     DATE
);





CREATE OR REPLACE TRIGGER trg_alyt_asset_capture
BEFORE INSERT OR UPDATE ON s_asset
FOR EACH ROW
BEGIN
    IF :new.type_cd := 'Service Point'
    THEN 
    INSERT INTO svc_pt_table_name
        (
            row_id
          , org_name
          , record_type
          , record_subtype
          , service_point_id
          , udc_id
          , last_upd_time
        )
    VALUES
        (
            :new.row_id
          , :new.data_src
          , :new.type_cd
          , :new.cfg_type_cd
          , :new.service_point_id
          , :new.x_udc_asset_id
          , :new.last_upd
        );
    ELSIF :new.type_cd := 'Service'
    THEN 
    INSERT INTO service_table_name
        (
            row_id
          , org_name
          , record_type
          , record_subtype
          , service_point_id
          , udc_id
          , last_upd_time
        )
    VALUES
        (
            :new.row_id
          , :new.data_src
          , :new.type_cd
          , :new.cfg_type_cd
          , :new.service_point_id
          , :new.x_udc_asset_id
          , :new.last_upd
        );
 END IF;
 
EXCEPTION
    WHEN OTHERS
    THEN
        RAISE;
END;
/


************************************************************************************************************
{toc}
h2. Output table: 
h3. CX_ALYT_CHANGED_SDP_INFO 

The output table will be used by triggers to populated updated records.
{code:xml}
create table cx_alyt_changed_sdp_info(
    row_id            VARCHAR2(15),
    org_name          VARCHAR2(50),
    record_type       VARCHAR2(30),
    record_subtype    VARCHAR2(50),
    udc_id            VARCHAR2(50),
    last_upd_time     DATE
);
{code}

h3. CX_ALYT_CHANGED_METER_INFO 

The output table will be used by triggers to populated updated records.
{code:xml}
create table cx_alyt_changed_meter_info(
    row_id            VARCHAR2(15),
    org_name          VARCHAR2(50),
    record_type       VARCHAR2(30),
    record_subtype    VARCHAR2(50),
    udc_id            VARCHAR2(50),
    last_upd_time     DATE
);
{code}

h3. CX_ALYT_CHANGED_COMFUN_INFO  

The output table will be used by triggers to populated updated records.
{code:xml}
create table cx_alyt_changed_comfun_info(
    row_id            VARCHAR2(15),
    org_name          VARCHAR2(50),
    record_type       VARCHAR2(30),
    record_subtype    VARCHAR2(50),
    udc_id            VARCHAR2(50),
    last_upd_time     DATE
);
{code}

h3. CX_ALYT_CHANGED_CHANNEL_INFO  

The output table will be used by triggers to populated updated records.
{code:xml}
create table cx_alyt_changed_channel_info(
    row_id            VARCHAR2(15),
    org_name          VARCHAR2(50),
    record_type       VARCHAR2(30),
    record_subtype    VARCHAR2(50),
    udc_id            VARCHAR2(50),
    last_upd_time     DATE
);
{code}

h3. CX_ALYT_CHANGED_DATA_SVC_INFO 

The output table will be used by triggers to populated updated records.
{code:xml}
create table cx_alyt_changed_data_svc_info(
    par_asset_id      VARCHAR2(15),
    child_asset_id    VARCHAR2(15),
    rel_type          VARCHAR2(30),
    sdp_row_id        VARCHAR2(15),
    last_upd_time     DATE
);
{code}

h3. CX_ALYT_CHANGED_SVC_AGREE_INFO 

The output table will be used by triggers to populated updated records.
{code:xml}
create table cx_alyt_changed_svc_agree_info(
    par_asset_id      VARCHAR2(15),
    child_asset_id    VARCHAR2(15),
    rel_type          VARCHAR2(30),
    sdp_row_id        VARCHAR2(15),
    last_upd_time     DATE
);
{code}

h3. CX_ALYT_CHANGED_PREMISE_INFO  

The output table will be used by triggers to populated updated records.
{code:xml}
create table cx_alyt_changed_premise_info(
    row_id            VARCHAR2(15),
    org_name          VARCHAR2(50),
    record_type       VARCHAR2(30),
    last_upd_time     DATE
);
{code}

h3. CX_ALYT_CHANGED_ACCOUNT_INFO  

The output table will be used by triggers to populated updated records.
{code:xml}
create table cx_alyt_changed_account_info(
    row_id            VARCHAR2(15),
    org_name          VARCHAR2(50),
    record_type       VARCHAR2(30),
    record_subtype    VARCHAR2(50),
    last_upd_time     DATE
);
{code}

h3. CX_ALYT_CHANGED_ACCNT_SDP_REL_INFO  

The output table will be used by triggers to populated updated records.
{code:xml}
create table cx_alyt_changed_accnt_sdp_rel_info(
    par_asset_id      VARCHAR2(15),
    child_asset_id    VARCHAR2(15),
    rel_type          VARCHAR2(30),
    sdp_row_id        VARCHAR2(15),
    last_upd_time     DATE
);
{code}


h3. CX_ALYT_CHANGED_SDP_METER_REL_INFO

The output table will be used by triggers to populated updated records.
{code:xml}
create table cx_alyt_changed_sdp_meter_rel_info(
    par_asset_id      VARCHAR2(15),
    child_asset_id    VARCHAR2(15),
    rel_type          VARCHAR2(30),
    sdp_row_id        VARCHAR2(15),
    last_upd_time     DATE
);
{code}

h3. CX_ALYT_CHANGED_METER_COMFUN_REL_INFO

The output table will be used by triggers to populated updated records.
{code:xml}
create table cx_alyt_changed_meter_comfun_rel_info(
    par_asset_id      VARCHAR2(15),
    child_asset_id    VARCHAR2(15),
    rel_type          VARCHAR2(30),
    sdp_row_id        VARCHAR2(15),
    last_upd_time     DATE
);
{code}

h3. CX_ALYT_CHANGED_METER_CHANNEL_REL_INFO

The output table will be used by triggers to populated updated records.
{code:xml}
create table cx_alyt_changed_meter_comfun_rel_info(
    par_asset_id      VARCHAR2(15),
    child_asset_id    VARCHAR2(15),
    rel_type          VARCHAR2(30),
    sdp_row_id        VARCHAR2(15),
    last_upd_time     DATE
);
{code}

h3. CX_TEMP_SDP_REF_ID
{code:xml}
CREATE TABLE CX_TEMP_SDP_REF_ID(
  SDP_ROW_ID  VARCHAR2(15 BYTE)
);
{code}

h3. CX_TEMP_SDP_DETAILS
{code:xml}
{code}
h3. CX_TEMP_REL_DETAILS
{code:xml}
{code}
h3. CX_TEMP_CHANNEL_DETAILS
{code:xml}
{code}
h3. CX_TEMP_SERVICE_DETAILS
{code:xml}
{code}

h2. Triggers

|| Type || 7x Table name || Trigger Name || Output table name ||
| Asset | S_ASSET | TRG_ALYT_ASSET_CAPTURE | CX_ALYT_CHANGED_REC_INFO |
| Relationship | S_ASSET_REL | TRG_ALYT_RELATION_CAPTURE | CX_ALYT_CHANGED_REC_INFO |
| Parameter | S_ASSET_XM | TRG_ALYT_ASSET_PARAM_CAPTURE | CX_ALYT_CHANGED_REC_INFO |
| Premise | S_ADDR_PER | TRG_ALYT_PREMISE_CAPTURE | CX_ALYT_CHANGED_REC_INFO |
| Account & Svcprovider | S_ORG_EXT | TRG_ALYT_ACCOUNT_CAPTURE | CX_ALYT_CHANGED_REC_INFO |
| Account param | S_ORG_EXT_XM | TRG_ALYT_ACCOUNT_PARAM_CAPTURE | CX_ALYT_CHANGED_REC_INFO |
| Account SDP rel | CX_ASSET_ACCNT | TRG_ALYT_ACCNT_SVC_PT_CAPTURE | CX_ALYT_CHANGED_REC_INFO |
| ServiceAgreement | S_DOC_AGREE | TRG_ALYT_SVC_AGREE_CAPTURE | CX_ALYT_CHANGED_REC_INFO |


h3. TRG_ALYT_ASSET_CAPTURE
{code:xml}
CREATE OR REPLACE TRIGGER trg_alyt_asset_capture
BEFORE INSERT OR UPDATE ON s_asset
FOR EACH ROW
BEGIN
   IF :new.type_cd = 'Service Point'
   THEN 
          INSERT INTO cx_alyt_changed_sdp_info 
              (
                  row_id
                , org_name
                , record_type
                , record_subtype
                , udc_id
                , last_upd_time
              )
          VALUES
              (
                  :new.row_id
                , :new.data_src
                , :new.type_cd
                , :new.cfg_type_cd
                , :new.x_udc_asset_id
                , :new.last_upd
           );  
    ELSIF :new.type_cd = 'Meter'
    THEN 
       INSERT INTO cx_alyt_changed_meter_info 
           (
               row_id
             , org_name
             , record_type
             , record_subtype
             , udc_id
             , last_upd_time
           )
       VALUES
           (
               :new.row_id
             , :new.data_src
             , :new.type_cd
             , :new.cfg_type_cd
             , :new.x_udc_asset_id
             , :new.last_upd
           );
    ELSIF :new.type_cd = 'Communication Module'
    THEN 
       INSERT INTO cx_alyt_changed_comfun_info 
           (
               row_id
             , org_name
             , record_type
             , record_subtype
             , udc_id
             , last_upd_time
           )
       VALUES
           (
               :new.row_id
             , :new.data_src
             , :new.type_cd
             , :new.cfg_type_cd
             , :new.x_udc_asset_id
             , :new.last_upd
           );
   ELSIF :new.type_cd = 'Channel'
    THEN 
       INSERT INTO cx_alyt_changed_channel_info 
           (
               row_id
             , org_name
             , record_type
             , record_subtype
             , udc_id
             , last_upd_time
           )
       VALUES
           (
               :new.row_id
             , :new.data_src
             , :new.type_cd
             , :new.cfg_type_cd
             , :new.x_udc_asset_id
             , :new.last_upd
           );           
    ELSIF (:new.type_cd = 'Service'
      AND :new.cfg_type_cd IN ( 'Data Delivery', 'VEE', 'Framing', 'Deployment Planning', 'Virtual Channel Persistence', 'Estimation', 'Outage', 'Data Transfer', 'Web Presentment', 'CO2 Plan'))
    THEN 
       INSERT INTO cx_alyt_changed_data_svc_info 
           (
               row_id
             , org_name
             , record_type
             , record_subtype
             , udc_id
             , last_upd_time
           )
       VALUES
           (
               :new.row_id
             , :new.data_src
             , :new.type_cd
             , :new.cfg_type_cd
             , :new.x_udc_asset_id
             , :new.last_upd
           ); 
     ELSIF (:new.type_cd = 'Service'
      AND :new.cfg_type_cd IN ('Energy Purchase', 'Generation Balance Provider', 'Consumption Balance Provider', 'Billing', 'AMI Operator', 'Energy Supplier', 'Pricing Plan'))
     THEN
       INSERT INTO cx_alyt_changed_agree_svc_info 
           (
               row_id
             , org_name
             , record_type
             , record_subtype
             , udc_id
             , last_upd_time
           )
       VALUES
           (
               :new.row_id
             , :new.data_src
             , :new.type_cd
             , :new.cfg_type_cd
             , :new.x_udc_asset_id
             , :new.last_upd
           );       
 END IF;
EXCEPTION
    WHEN OTHERS
    THEN
        RAISE;
END;
/
{code}

h3. TRG_ALYT_RELATION_CAPTURE
{code:xml}
CREATE OR REPLACE TRIGGER trg_alyt_relation_capture
BEFORE INSERT OR UPDATE ON s_asset_rel
FOR EACH ROW
BEGIN
    INSERT INTO cx_alyt_changed_rel_info
        (
            par_asset_id
	  , child_asset_id
	  , rel_type
	  , sdp_row_id
          , last_upd_time
        )
    VALUES
        (
            :new.par_asset_id
          , :new.asset_id
          , :new.x_asset_relation_type_cd
          , CASE 
                WHEN :new.x_asset_relation_type_cd IN ('ROUTE-SDP', 'DISTRIBUTION-SDP')
                THEN :new.asset_id
                WHEN :new.x_asset_relation_type_cd IN ('SDP-METER', 'SDP-CT/PT')
                THEN :new.par_asset_id
                ELSE NULL
             END
          , :new.last_upd
        );
 
EXCEPTION
    WHEN OTHERS
    THEN
        RAISE;
END;
{code}


h3. TRG_ALYT_ASSET_PARAM_CAPTURE
{code:xml}
CREATE OR REPLACE TRIGGER trg_alyt_asset_param_capture
BEFORE INSERT OR UPDATE ON s_asset_xm
FOR EACH ROW
BEGIN
    INSERT INTO cx_alyt_changed_rec_info
        (
            row_id
          , org_name
          , record_type
          , last_upd_time
        )
    VALUES
        (
            :new.par_row_id
          , :new.attrib_05
          , :new.type
          , :new.last_upd
        );

EXCEPTION
    WHEN OTHERS
    THEN
        RAISE;
END;
{code}


h3. TRG_ALYT_PREMISE_CAPTURE
{code:xml}
CREATE OR REPLACE TRIGGER trg_alyt_premise_capture
BEFORE INSERT OR UPDATE ON s_addr_per
FOR EACH ROW
BEGIN
    INSERT INTO cx_alyt_changed_rec_info
        (
            row_id
          , org_name
          , record_type
          , last_upd_time
        )
    VALUES
        (
            :new.row_id
          , :new.x_data_src
          , 'PREMISE'
          , :new.last_upd
        );

EXCEPTION
    WHEN OTHERS
    THEN
        RAISE;
END;
{code}


h3. TRG_ALYT_ACCOUNT_CAPTURE
{code:xml}
CREATE OR REPLACE TRIGGER trg_alyt_account_capture
BEFORE INSERT OR UPDATE ON s_org_ext
FOR EACH ROW
BEGIN
    INSERT INTO cx_alyt_changed_rec_info
        (
            row_id
          , org_name
          , record_type
          , record_subtype
          , last_upd_time
        )
    VALUES
        (
            :new.row_id
          , :new.x_data_src
          , 'ACCOUNT'
          , :new.ou_type_cd
          , :new.last_upd
        );

EXCEPTION
    WHEN OTHERS
    THEN
        RAISE;
END;
{code}


h3. TRG_ALYT_ACCOUNT_PARAM_CAPTURE
{code:xml}
CREATE OR REPLACE TRIGGER trg_alyt_account_param_capture
BEFORE INSERT OR UPDATE ON s_org_ext_xm
FOR EACH ROW
BEGIN
    INSERT INTO cx_alyt_changed_rec_info
        (
            row_id
          , org_name
          , record_type
          , last_upd_time
        )
    VALUES
        (
            :new.row_id
          , :new.attrib_05
          , 'ACCOUNT_PARAMETER' --:new.type
          , :new.last_upd
        );

EXCEPTION
    WHEN OTHERS
    THEN
        RAISE;
END;
{code}


h3. TRG_ALYT_ACCNT_SVC_PT_CAPTURE
{code:xml}
CREATE OR REPLACE TRIGGER trg_alyt_accnt_svc_pt_capture
BEFORE INSERT OR UPDATE ON cx_asset_accnt
FOR EACH ROW
BEGIN
    INSERT INTO cx_alyt_changed_rel_info
        (
            par_asset_id
	  , child_asset_id
	  , rel_type
	  , sdp_row_id
          , last_upd_time
        )
    VALUES
        (
            :new.accnt_id
          , :new.asset_id
          , 'ACCOUNT-SDP'
          , :new.asset_id
          , :new.last_upd
        );
 
EXCEPTION
    WHEN OTHERS
    THEN
      RAISE;
END;
{code}

h3. TRG_ALYT_SVC_AGREE_CAPTURE
{code:xml}
CREATE OR REPLACE TRIGGER trg_alyt_svc_agree_capture
BEFORE INSERT OR UPDATE ON s_doc_agree
FOR EACH ROW
BEGIN
    INSERT INTO cx_alyt_changed_rec_info
        (
            row_id
          , record_type
          , last_upd_time
        )
    VALUES
        (
            :new.row_id
          , :new.agree_cd
          , :new.last_upd
        );

EXCEPTION
    WHEN OTHERS
    THEN
        RAISE;
END;
{code}

h2. Query to filter out duplicate records from Assets
{code:xml}
SELECT COUNT(1)
     , data_src
     , type_cd
     , cfg_type_cd
     , x_udc_asset_id
  FROM s_asset
 WHERE type_cd in ('Service Point'
                  ,'Meter'
                  ,'Communication Module'
                  ,'CT-PT'
                  ,'Distribution Node'
                  ,'Disconnect Collar'
                  ,'Equipment Location')
 GROUP BY data_src
     , type_cd
     , cfg_type_cd
     , x_udc_asset_id
HAVING COUNT(1) > 1
{code}

h2. Extraction queries

h3. Reference data query
{code:xml}
SELECT          
pxm.row_id       AS param_src_id,
p.row_id         AS src_id,
NULL             AS data_src,
par_row_id       AS par_src_id,
mapping.product_name_8x  AS name,
attrib_02        AS value,
attrib_03        AS status_cd,
attrib_26	 AS eff_start_time,       
attrib_27	 AS eff_end_time,       
mapping.type_8x  AS type,
mapping.sub_type_8x AS sub_type,
DECODE(pxm.type, 'Service', p.sub_type, pxm.type) AS handler_type,
pxm.created      AS source_created,
pxm.last_upd     AS source_last_upd,
NULL			     AS insert_time,
NULL			    AS last_upd_time
FROM s_prod_int_xm pxm, 
s_prod_int p, 
product_name_mapping mapping
WHERE 
pxm.par_row_id = p.row_id (+)
AND p.name     = mapping.product_name_7x
AND p.type     = mapping.type_7x
{code}

h3. Incremental load queries

h4. Extract unique SDP Ref Ids using CX_ALYT_CHANGED_REL_INFO table
{code:xml}
INSERT INTO cx_temp_sdp_ref_id
(SDP_ROW_ID)
SELECT DISTINCT sdp_row_id
  FROM cx_alyt_changed_rel_info
 WHERE sdp_row_id IS NOT NULL
UNION
SELECT DISTINCT par_asset_id
  FROM s_asset_rel rel,
       (SELECT DISTINCT par_asset_id AS asset_id
         FROM cx_alyt_changed_rel_info
        WHERE rel_type = 'METER-CHANNEL'
       UNION
       SELECT DISTINCT child_asset_id AS asset_id
         FROM cx_alyt_changed_rel_info
        WHERE rel_type = 'COMMUNICATION-METER') meter
 WHERE rel.asset_id = meter.asset_id
 AND rel.x_asset_relation_type_cd = 'SDP-METER'
{code}


h4. Asset meter incremental extraction query with product_name resolution
{code:xml}
 SELECT s.row_id AS src_id,
       s.data_src AS data_src,
       s.x_udc_asset_id AS udc_id,
       s.prod_id AS class_src_id,
       s.serial_num AS mfg_serial_num,
       s.x_lot_num AS mfg_lot_num,
       s.x_network_id AS network_id,
       s.status_cd AS status_cd,
       s.desc_text AS desc_text,
       s.purch_dt AS purchase_date,
       s.ship_dt AS ship_date,
       NULL AS mfg_test_date,
       s.x_retire_dt AS retire_date,
       s.x_mfg_date AS mfg_date,
       s.make_cd AS make,
       s.model_cd AS model,
       s.last_test_dt AS last_test_date,
       s.x_virtual_asset AS is_virtual_flg,
       s.x_universal_id AS badge_id,
       s.x_aep_num AS standard_id,
       s.x_electronic_id AS electronic_id,
       NULL AS comm_technology,
       NULL AS cur_inv_location_id,
       NULL AS sku,
       NULL AS part_num,
       s.type_cd AS TYPE,
       s.cfg_type_cd AS sub_type,
       DECODE (s.type_cd, 'Service', s.cfg_type_cd, s.type_cd) AS handler_type,
       s.per_addr_id AS premise_src_id,
       s.x_ax_feed_loc AS feed_loc,
       s.x_latitude_new AS gps_lat,
       s.x_longitude_new AS gps_long,
       s.x_pulse_output_blk AS pulse_output_block,
       s.x_seal_info AS seal_info,
       s.x_lock_info AS lock_info,
       s.x_util_access_info AS access_info,
       s.x_emeter_acces_info AS alt_access_info,
       s.x_util_premise_loc AS loc_info,
       s.x_emeter_premise_loc AS alt_loc_info,
       s.x_billing_cycle AS billing_cycle,
       s.x_reading_cycle AS reading_cycle,
       NULL AS timezone_id,
       NULL AS gis_id,
       s.x_read_dt AS billed_upto_time,
       s.x_power_status AS power_status,
       s.x_usage_status AS load_status,
       s.x_billing_hold AS billing_hold_status,
       s.meter_loc AS location_info,
       s.service_point_id AS svc_pt_src_id,
       s.start_dt AS eff_start_time,
       s.end_dt AS eff_end_time,
       --a.svc_provider_id AS svc_provider_src_id,
       s.cur_agree_id AS svc_agree_src_id,
       s.created AS source_created,
       s.last_upd AS source_last_upd,
       NULL AS insert_time,
       NULL AS last_upd_time,
       mapping.product_name_8x AS name,
       param.attrib_01 AS param_name,
       param.attrib_02 AS param_value,
       param.attrib_12 AS eff_start_date,
       param.attrib_13 AS eff_end_date
 FROM s_asset s,
       s_prod_int prod,
       cx_alyt_changed_rec_info info,
       product_name_mapping mapping,
       s_asset_xm param
 WHERE s.prod_id = prod.row_id
       AND info.row_id = s.row_id
       AND info.record_type = 'Meter'
       AND mapping.product_name_7x = prod.name
       AND mapping.type_7x = info.record_type
       AND param.par_row_id (+) = s.row_id;
       
{code}

h4. Asset svc_pt details incremental extraction query with product_name, premise_udc_id resolution
{code:xml}
 SELECT s.row_id AS src_id,
      s.data_src AS data_src,
      s.x_udc_asset_id AS udc_id,
      s.prod_id AS class_src_id,
      s.serial_num AS mfg_serial_num,
      s.x_lot_num AS mfg_lot_num,
      s.x_network_id AS network_id,
      s.status_cd AS status_cd,
      s.desc_text AS desc_text,
      s.purch_dt AS purchase_date,
      s.ship_dt AS ship_date,
      NULL AS mfg_test_date,
      s.x_retire_dt AS retire_date,
      s.x_mfg_date AS mfg_date,
      s.make_cd AS make,
      s.model_cd AS model,
      s.last_test_dt AS last_test_date,
      s.x_virtual_asset AS is_virtual_flg,
      s.x_universal_id AS badge_id,
      s.x_aep_num AS standard_id,
      s.x_electronic_id AS electronic_id,
      NULL AS comm_technology,
      NULL AS cur_inv_location_id,
      NULL AS sku,
      NULL AS part_num,
      s.type_cd AS TYPE,
      s.cfg_type_cd AS sub_type,
      DECODE (s.type_cd, 'Service', s.cfg_type_cd, s.type_cd) AS handler_type,
      s.per_addr_id AS premise_src_id,
      s.x_ax_feed_loc AS feed_loc,
      s.x_latitude_new AS gps_lat,
      s.x_longitude_new AS gps_long,
      s.x_pulse_output_blk AS pulse_output_block,
      s.x_seal_info AS seal_info,
      s.x_lock_info AS lock_info,
      s.x_util_access_info AS access_info,
      s.x_emeter_acces_info AS alt_access_info,
      s.x_util_premise_loc AS loc_info,
      s.x_emeter_premise_loc AS alt_loc_info,
      s.x_billing_cycle AS billing_cycle,
      s.x_reading_cycle AS reading_cycle,
      NULL AS timezone_id,
      NULL AS gis_id,
      s.x_read_dt AS billed_upto_time,
      s.x_power_status AS power_status,
      s.x_usage_status AS load_status,
      s.x_billing_hold AS billing_hold_status,
      s.meter_loc AS location_info,
      s.service_point_id AS svc_pt_src_id,
      s.start_dt AS eff_start_time,
      s.end_dt AS eff_end_time,
      s.cur_agree_id AS svc_agree_src_id,
      s.created AS source_created,
      s.last_upd AS source_last_upd,
      NULL AS insert_time,
      NULL AS last_upd_time,
      premise_tab.x_client_prmse_id AS premise_udc_id,
      mapping.product_name_8x AS name,
      param.attrib_01 AS param_name,
      param.attrib_02 AS param_value,
      param.attrib_12 AS eff_start_date,
      param.attrib_13 AS eff_end_date
 FROM s_asset s,
      s_addr_per premise_tab,
      s_prod_int prod,
      (SELECT sdp_row_id
         FROM temp_sdp_ref_id
       UNION
       SELECT row_id AS sdp_row_id
         FROM cx_alyt_changed_rec_info
        WHERE record_type = 'Service Point') ref,
      product_name_mapping mapping,
      s_asset_xm param
WHERE s.per_addr_id = premise_tab.row_id
      AND s.prod_id = prod.row_id
      AND ref.sdp_row_id = s.row_id
      AND mapping.product_name_7x = prod.name
      AND param.par_row_id (+) = s.row_id;
{code}

h4. Asset channel incremental extraction query with product_name, svc_pt_udc_id and register channel split resolution
{code:xml}
SELECT s.row_id AS src_id,
      s.data_src AS data_src,
      s.x_udc_asset_id AS udc_id,
      s.prod_id AS class_src_id,
      s.serial_num AS mfg_serial_num,
      s.x_lot_num AS mfg_lot_num,
      s.x_network_id AS network_id,
      s.status_cd AS status_cd,
      s.desc_text AS desc_text,
      s.purch_dt AS purchase_date,
      s.ship_dt AS ship_date,
      NULL AS mfg_test_date,
      s.x_retire_dt AS retire_date,
      s.x_mfg_date AS mfg_date,
      s.make_cd AS make,
      s.model_cd AS model,
      s.last_test_dt AS last_test_date,
      s.x_virtual_asset AS is_virtual_flg,
      s.x_universal_id AS badge_id,
      s.x_aep_num AS standard_id,
      s.x_electronic_id AS electronic_id,
      NULL AS comm_technology,
      NULL AS cur_inv_location_id,
      NULL AS sku,
      NULL AS part_num,
      s.type_cd AS TYPE,
      s.cfg_type_cd AS sub_type,
      DECODE (s.type_cd, 'Service', s.cfg_type_cd, s.type_cd) AS handler_type,
      s.per_addr_id AS premise_src_id,
      s.x_ax_feed_loc AS feed_loc,
      s.x_latitude_new AS gps_lat,
      s.x_longitude_new AS gps_long,
      s.x_pulse_output_blk AS pulse_output_block,
      s.x_seal_info AS seal_info,
      s.x_lock_info AS lock_info,
      s.x_util_access_info AS access_info,
      s.x_emeter_acces_info AS alt_access_info,
      s.x_util_premise_loc AS loc_info,
      s.x_emeter_premise_loc AS alt_loc_info,
      s.x_billing_cycle AS billing_cycle,
      s.x_reading_cycle AS reading_cycle,
      NULL AS timezone_id,
      NULL AS gis_id,
      s.x_read_dt AS billed_upto_time,
      s.x_power_status AS power_status,
      s.x_usage_status AS load_status,
      s.x_billing_hold AS billing_hold_status,
      s.meter_loc AS location_info,
      s.service_point_id AS svc_pt_src_id,
      s.start_dt AS eff_start_time,
      s.end_dt AS eff_end_time,
      s.cur_agree_id AS svc_agree_src_id,
      s.created AS source_created,
      s.last_upd AS source_last_upd,
      NULL AS insert_time,
      NULL AS last_upd_time,
      asset_for_udc.x_udc_asset_id AS sdp_udc_id,
      mapping.meas_type_udc_id_8x AS name,
      param.attrib_01 AS param_name,
      param.attrib_02 AS param_value,
      param.attrib_12 AS eff_start_date,
      param.attrib_13 AS eff_end_date,
      ref.identifier,
      s.service_point_id sdp_row_id
FROM s_asset s,
      s_asset asset_for_udc,
      s_prod_int prod,
      (SELECT row_id AS channel_row_id
           , 'ASSET' AS identifier
         FROM cx_alyt_changed_rec_info
        WHERE record_type = 'Channel'
       UNION
       SELECT child_asset_id AS channel_row_id
            , 'RELATIONSHIP' AS identifier
         FROM cx_temp_rel_details
        WHERE rel_type = 'METER-CHANNEL') ref,
      channel_product_mapping mapping,
      s_asset_xm param
WHERE s.service_point_id = asset_for_udc.row_id
      AND s.prod_id = prod.row_id
      AND ref.channel_row_id = s.row_id
      AND mapping.channel_name_7x = prod.name
      AND param.par_row_id (+) = s.row_id
ORDER BY s.row_id, mapping.meas_type_udc_id_8x;
{code}

h4. Relationship Query for ROUTE-SDP and DISTRIBUTION-SDP
{code:xml}
SELECT rel.asset_id AS sdp_row_id
     , rel.x_asset_relation_type_cd
     , par_asset_id
     , (SELECT x_udc_asset_id
          FROM s_asset
         WHERE row_id = rel.par_asset_id) AS par_udc_id
     , asset_id AS child_asset_id
     , (SELECT x_udc_asset_id
          FROM s_asset
         WHERE row_id = rel.asset_id) AS child_udc_id
FROM s_asset_rel rel
     , temp_sdp_ref_id ref
WHERE ref.sdp_row_id = rel.asset_id;
{code}

h4. Relationship Query for ACCOUNT-SDP
{code:xml}
SELECT rel.asset_id AS sdp_row_id
      , 'ACCOUNT-SDP'
      , accnt_id AS par_asset_id
      , (SELECT x_udc_accnt_id
           FROM s_org_ext
          WHERE row_id = rel.accnt_id) AS par_udc_id
      , asset_id AS child_asset_id
      , (SELECT x_udc_asset_id
           FROM s_asset
          WHERE row_id = rel.asset_id) AS child_udc_id
FROM cx_asset_accnt rel
      , temp_sdp_ref_id ref
WHERE ref.sdp_row_id = rel.asset_id;
{code}

h4. Relationship Query for SDP-METER and SDP-CT/PT
{code:xml}
SELECT rel.par_asset_id AS sdp_row_id
     , rel.x_asset_relation_type_cd
     , par_asset_id
     , (SELECT x_udc_asset_id
          FROM s_asset
         WHERE row_id = rel.par_asset_id) AS par_udc_id
     , asset_id AS child_asset_id
     , (SELECT x_udc_asset_id
          FROM s_asset
         WHERE row_id = rel.asset_id) AS child_udc_id
FROM s_asset_rel rel
     , temp_sdp_ref_id ref
WHERE ref.sdp_row_id = rel.par_asset_id;
{code}

h4. Relationship Query for METER-CHANNEL and COMMUNICATION-METER
{code:xml}
SELECT rel1.par_asset_id AS sdp_row_id
     , rel2.x_asset_relation_type_cd AS rel_type
     , rel2.par_asset_id
     , (SELECT x_udc_asset_id
          FROM s_asset
         WHERE row_id = rel2.par_asset_id) AS par_udc_id
     , rel2.asset_id AS child_asset_id
     , (SELECT x_udc_asset_id
          FROM s_asset
         WHERE row_id = rel2.asset_id) AS child_udc_id
FROM s_asset_rel rel1
     , cx_temp_sdp_ref_id ref
     , s_asset_rel rel2
WHERE ref.sdp_row_id = rel1.par_asset_id
   AND rel1.x_asset_relation_type_cd = 'SDP-METER'
   AND rel2.x_asset_relation_type_cd IN ('METER-CHANNEL','COMMUNICATION-METER')
   AND (rel1.asset_id = rel2.par_asset_id
   OR rel1.asset_id = rel2.asset_id);
{code}

h4. Services Query for SDP-DataService and SDP-ServiceAgree 
{code:xml}
SELECT s.row_id AS src_id,
      s.data_src AS data_src,
      s.x_udc_asset_id AS udc_id,
      s.prod_id AS class_src_id,
      s.serial_num AS mfg_serial_num,
      s.x_lot_num AS mfg_lot_num,
      s.x_network_id AS network_id,
      s.status_cd AS status_cd,
      s.desc_text AS desc_text,
      s.purch_dt AS purchase_date,
      s.ship_dt AS ship_date,
      NULL AS mfg_test_date,
      s.x_retire_dt AS retire_date,
      s.x_mfg_date AS mfg_date,
      s.make_cd AS make,
      s.model_cd AS model,
      s.last_test_dt AS last_test_date,
      s.x_virtual_asset AS is_virtual_flg,
      s.x_universal_id AS badge_id,
      s.x_aep_num AS standard_id,
      s.x_electronic_id AS electronic_id,
      NULL AS comm_technology,
      NULL AS cur_inv_location_id,
      NULL AS sku,
      NULL AS part_num,
      s.type_cd AS TYPE,
      s.cfg_type_cd AS sub_type,
      DECODE (s.type_cd, 'Service', s.cfg_type_cd, s.type_cd)
         AS handler_type,
      s.per_addr_id AS premise_src_id,
      s.x_ax_feed_loc AS feed_loc,
      s.x_latitude_new AS gps_lat,
      s.x_longitude_new AS gps_long,
      s.x_pulse_output_blk AS pulse_output_block,
      s.x_seal_info AS seal_info,
      s.x_lock_info AS lock_info,
      s.x_util_access_info AS access_info,
      s.x_emeter_acces_info AS alt_access_info,
      s.x_util_premise_loc AS loc_info,
      s.x_emeter_premise_loc AS alt_loc_info,
      s.x_billing_cycle AS billing_cycle,
      s.x_reading_cycle AS reading_cycle,
      NULL AS timezone_id,
      NULL AS gis_id,
      s.x_read_dt AS billed_upto_time,
      s.x_power_status AS power_status,
      s.x_usage_status AS load_status,
      s.x_billing_hold AS billing_hold_status,
      s.meter_loc AS location_info,
      s.service_point_id AS svc_pt_src_id,
      s.start_dt AS eff_start_time,
      s.end_dt AS eff_end_time,
      s.cur_agree_id AS svc_agree_src_id,
      s.created AS source_created,
      s.last_upd AS source_last_upd,
      NULL AS insert_time,
      NULL AS last_upd_time,
      asset_for_udc.x_udc_asset_id AS sdp_udc_id,
      prod.name,
      param.attrib_01 AS param_name,
      param.attrib_02 AS param_value,
      param.attrib_12 AS eff_start_date,
      param.attrib_13 AS eff_end_date
FROM s_asset s,
      s_asset asset_for_udc,
      s_prod_int prod,
      s_asset_xm param,
      cx_alyt_changed_rec_info info
WHERE s.service_point_id = asset_for_udc.row_id(+)
      AND s.prod_id = prod.row_id
      AND param.par_row_id (+) = s.row_id
      AND info.record_type = 'Service'
      AND info.row_id = s.row_id;
{code}


h3. Initial load queries
The query structure will be same but the input source of query will be 7x S_ASSET, S_ASSET_REL etc. instead of trigger populated tables

h2. Mapping and look up table structure

h3. CHANNEL_PRODUCT_MAPPING

The table will be populated using Utility provided information related to register channel.

|| Column name || Type || Description ||
| channel_name_7x | Varchar | channel name from 7x |
| meas_type_udc_id_8x | Varchar | channel's meas_type udc id from 8x |
| meter_param_dials_value | Varchar | meter parameter 'Dials' value from 7x. The value is used to split Register channel from 7x to multiple Register channels in 8x. *The column name may change* |
| insert_time | Timestamp | insert time of asset |

h3. PRODUCT_NAME_MAPPING

|| Column name || Type || Description ||
| product_name_7x | Varchar | product name from 7x |
| type_7x | Varchar | entity type from 7x |
| sub_type_7x | Varchar | entity sub_type from 7x |
| product_name_8x | Varchar | product from 8x |
| type_8x | Varchar | entity type from 8x |
| sub_type_8x | Varchar | entity sub_type from 8x |
| insert_time | Timestamp | insert time of asset |


h2. XML templates

h3. SVC_PT template
{code:xml}
<servicePoint>
      <mRID>mRID</mRID>
      <idType>idType</idType>
      <pathName>pathName</pathName>
      <description>description</description>
      <universalId>universalId</universalId>
      <className>className</className>
      <serviceType>serviceType</serviceType>
      <type>type</type>
      <subType>subType</subType>
      <status>status</status>
      <feedLocation>feedLocation</feedLocation>
      <premiseId xsi:type= IdentifiedObject >
        <mRID>mRID</mRID>
        <idType>idType</idType>
        <pathName>pathName</pathName>
        <description>description</description>
      </premiseId>
      <accessInfo>accessInfo</accessInfo>
      <altAccessInfo>altAccessInfo</altAccessInfo>
      <locInfo>locInfo</locInfo>
      <altLocInfo>altLocInfo</altLocInfo>
      <sealInfo>sealInfo</sealInfo>
      <lockInfo>lockInfo</lockInfo>
      <pulseOutputBlock>pulseOutputBlock</pulseOutputBlock>
      <latitude>0.0</latitude>
      <longitude>0.0</longitude>
      <gpsLocAccuracy>0.0</gpsLocAccuracy>
      <virtualInd>virtualInd</virtualInd>
      <criticalCareInd>criticalCareInd</criticalCareInd>
      <criticalLoadInd>criticalLoadInd</criticalLoadInd>
      <timezone>timezone</timezone>
      <gisId>gisId</gisId>
      <parameter>
        <name>name</name>
        <value>value</value>
        <startDate>startDate</startDate>
        <endDate>endDate</endDate>
      </parameter>
</servicePoint>
{code}

h3. Meter (Device) template
{code:xml}
<device>
      <mRID>mRID</mRID>
      <idType>idType</idType>
      <pathName>pathName</pathName>
      <description>description</description>
      <model>model</model>
      <manufacturedDate>manufacturedDate</manufacturedDate>
      <mfgSerialNumber>mfgSerialNumber</mfgSerialNumber>
      <mfgLotNumber>mfgLotNumber</mfgLotNumber>
      <purchaseDate>purchaseDate</purchaseDate>
      <shipDate>shipDate</shipDate>
      <mfgTestDate>mfgTestDate</mfgTestDate>
      <testDate>testDate</testDate>
      <retireDate>retireDate</retireDate>
      <status>status</status>
      <networkId>networkId</networkId>
      <commTechnology>commTechnology</commTechnology>
      <parameter>
        <name>name</name>
        <value>value</value>
        <startDate>startDate</startDate>
        <endDate>endDate</endDate>
      </parameter>
      <badgeId>badgeId</badgeId>
      <standardId>standardId</standardId>
      <electronicId>electronicId</electronicId>
      <make>make</make>
      <type>type</type>
      <virtualInd>virtualInd</virtualInd>
      <className>className</className>
      <location>location</location>
      <deviceFunctionType>deviceFunctionType</deviceFunctionType>
</device>
{code}

h3. Channel template
{code:xml}
<channel xsi:type= Channel >
      <mRID>mRID</mRID>
      <idType>idType</idType>
      <pathName>pathName</pathName>
      <description>description</description>
      <channelType>channelType</channelType>
      <servicePointId>
        <mRID>mRID</mRID>
        <idType>idType</idType>
        <pathName>pathName</pathName>
        <description>description</description>
        <type>type</type>
      </servicePointId>
      <status>status</status>
      <parameter>
        <name>name</name>
        <value>value</value>
        <startDate>startDate</startDate>
        <endDate>endDate</endDate>
      </parameter>
</channel>
{code}

*************************************************************************************************************************

h4. Relationship Query for ROUTE-SDP and DISTRIBUTION-SDP
{code:xml}
SELECT rel.asset_id AS sdp_row_id
     , rel.x_asset_relation_type_cd
     , par_asset_id
     , (SELECT x_udc_asset_id
          FROM s_asset
         WHERE row_id = rel.par_asset_id) AS par_udc_id
     , asset_id AS child_asset_id
     , (SELECT x_udc_asset_id
          FROM s_asset
         WHERE row_id = rel.asset_id) AS child_udc_id
FROM s_asset_rel rel
     , cx_temp_sdp_ref_id ref
WHERE ref.sdp_row_id = rel.asset_id;
{code}

h4. Relationship Query for ACCOUNT-SDP
{code:xml}
SELECT rel.asset_id AS sdp_row_id
      , 'ACCOUNT-SDP'
      , accnt_id AS par_asset_id
      , (SELECT x_udc_accnt_id
           FROM s_org_ext
          WHERE row_id = rel.accnt_id) AS par_udc_id
      , asset_id AS child_asset_id
      , (SELECT x_udc_asset_id
           FROM s_asset
          WHERE row_id = rel.asset_id) AS child_udc_id
FROM cx_asset_accnt rel
      , temp_sdp_ref_id ref
WHERE ref.sdp_row_id = rel.asset_id;
{code}

h4. Relationship Query for SDP-METER and SDP-CT/PT
{code:xml}
SELECT rel.par_asset_id AS sdp_row_id
     , rel.x_asset_relation_type_cd
     , par_asset_id
     , (SELECT x_udc_asset_id
          FROM s_asset
         WHERE row_id = rel.par_asset_id) AS par_udc_id
     , asset_id AS child_asset_id
     , (SELECT x_udc_asset_id
          FROM s_asset
         WHERE row_id = rel.asset_id) AS child_udc_id
FROM s_asset_rel rel
     , temp_sdp_ref_id ref
WHERE ref.sdp_row_id = rel.par_asset_id;
{code}

h4. Relationship Query for METER-CHANNEL and COMMUNICATION-METER
{code:xml}
SELECT rel1.par_asset_id AS sdp_row_id
     , rel2.x_asset_relation_type_cd AS rel_type
     , rel2.par_asset_id
     , (SELECT x_udc_asset_id
          FROM s_asset
         WHERE row_id = rel2.par_asset_id) AS par_udc_id
     , rel2.asset_id AS child_asset_id
     , (SELECT x_udc_asset_id
          FROM s_asset
         WHERE row_id = rel2.asset_id) AS child_udc_id
FROM s_asset_rel rel1
     , cx_temp_sdp_ref_id ref
     , s_asset_rel rel2
WHERE ref.sdp_row_id = rel1.par_asset_id
   AND rel1.x_asset_relation_type_cd = 'SDP-METER'
   AND rel2.x_asset_relation_type_cd IN ('METER-CHANNEL','COMMUNICATION-METER')
   AND (rel1.asset_id = rel2.par_asset_id
   OR rel1.asset_id = rel2.asset_id);
{code}



8************************************************************

Cloud Analytics Structural data processing consist of extracting data from underlying MDM and use this data to load in EIP 8x schema. It performs following high level tasks:

* Structural data extraction: MDM data will be extracted into input XMLs.
* Structural data loader: It will load data from input XMLs to EIP 8x schema using FlexSync

Cloud Analytics will support data extraction for following MDMs
* EIP7
* EIP8 : Pending..
* Other MDM: Pending...

The loader process is common for all types of extraction. It will use optimized FlexSync for loading extracted data input XMLs in EIP8 schema.

--Note in case of channel: look_up table is populated to resolve references while processing transactional data 
--Edit 4.1.2 3rd point for handler type
--remove comonent diagram .. that is no longer valid



Vikas comment in last meeting

-write down content of file from top down...
-query in batches like 100 sdps then process it.
-suffix register channel like r1.  
-expose dataSrc / use pathName for storing refId - done added in assumption 


9/6/14
* updated the design page with Extrator details
Questions:
  No extract for channel-channel rel 
ANS: We may need in future.
  Service split based on overlapping param
ANS: Split service based on params. Only few has .. only split them 

* Ling - get update for approach in incremental change i.e Trigger Vs Query -- Done go ahead with triggers

comments:
seperate table to capture updated 
poc on sdp chunks in memory 
-get the code up in running soon and will be in incremental mode .. may add cases ongoing
-identify priority objects
	-services, (split logic later) 
	-

16/06
- Document updates, 
- Discuss multi ORG i.e org specific zip file
- RP : A new job needs to be written in 7x to identify the records for a batch id
- RP invoke R issues





**************************************************************

cglig 
asm - 2

pswd: 3998CC457D1BAAAB43FE30062B4EBEB1
user: BA317669626299CC

revenueprotection_netezza-1.0-200319.zip

amiDBDataSource.url=jdbc:oracle:thin:@(DESCRIPTION=(ADDRESS_LIST=(ADDRESS=(PROTOCOL=TCP)(HOST=emdb05)(PORT=1521)))(CONNECT_DATA=(SERVER=DEDICATED)(SERVICE_NAME=sebldb41)))
amiDBDataSource.username=BA317669626299CC
amiDBDataSource.password=3998CC457D1BAAAB43FE30062B4EBEB1
amiDBDataSource.encrypted=true

jdbc:oracle:thin:@(DESCRIPTION=(ADDRESS_LIST=(ADDRESS=(PROTOCOL=TCP)(HOST=emdb05)(PORT=1521)))(CONNECT_DATA=(SERVER=DEDICATED)(SERVICE_NAME=sebldb41)))



 score_new_data <- function (user_id, password, machine, database, Start_Date, End_Date, Database_Input_Table_Name,Database_Output_Table_Name, RP_Model) {
score_new_data <- function (user_id, password, machine, database, batchId, Database_Input_Table_Name,Database_Output_Table_Name, RP_Model) {

library(RODBC)
library(randomForest)
library(nza)
library(nzr)
library(tree)


 Netezza connection
nzConnect(user_id , password, machine, database) 
print(batchId)



**********************************************************

drop table RP_ALGORITHM_DEF;
drop table RP_ALGORITHM_DEF_PARAM;
drop table RP_ALGORITHM;
drop table RP_ALGORITHM_PARAM;
drop table RP_EXECUTION_STAT;
drop table RP_FEATURE;
drop table RP_MODEL_OUTPUT;
drop table RP_INVESTIGATION_REQUEST;
drop table RP_INVESTIGATION;


CREATE TABLE RP_ALGORITHM_DEF(
    WID                					BIGINT   NOT NULL,
    NAME               					 NATIONAL CHARACTER VARYING(100)  not null,
    DESCRIPTION              	 NATIONAL CHARACTER VARYING(1000),
    PROCEDURE_NAME  	 NATIONAL CHARACTER VARYING(100) not null,
    EIP_CD             					 NATIONAL CHARACTER VARYING(10) not null,
    MINUTE_BACKWARD    BIGINT not null,
    ORG_ID          									BIGINT,
    INSERT_TIME       TIMESTAMP    DEFAULT  TIMESTAMP ('now':: VARCHAR ) NOT NULL,
    LAST_UPD_TIME     TIMESTAMP    DEFAULT  TIMESTAMP ('now':: VARCHAR ) NOT NULL,
    CONSTRAINT PK_RP_ALGORITHM_DEF PRIMARY KEY (WID),
    CONSTRAINT UK_RP_ALGORITHM_DEF   UNIQUE (EIP_CD)
)
distribute on random;


CREATE TABLE RP_ALGORITHM_DEF_PARAM(
    WID                							BIGINT   NOT NULL,
    ALGORITHM_DEF_WID    	BIGINT   NOT NULL,
    NAME                 						 NATIONAL CHARACTER VARYING(50) not null,
    VALUE                						 NATIONAL CHARACTER VARYING(100),
    ORG_ID          									BIGINT,
    INSERT_TIME       TIMESTAMP    DEFAULT  TIMESTAMP ('now':: VARCHAR ) NOT NULL,
    LAST_UPD_TIME     TIMESTAMP    DEFAULT  TIMESTAMP ('now':: VARCHAR ) NOT NULL,
    CONSTRAINT PK_RP_ALGORITHM_DEF_PARAM PRIMARY KEY (WID),
    CONSTRAINT UK_RP_ALGORITHM_DEF_PARAM   UNIQUE (ALGORITHM_DEF_WID, NAME)
)
distribute on random;



ALTER TABLE RP_ALGORITHM_DEF_PARAM ADD CONSTRAINT FK_RP_ALGORITHM_DEF_PARAM_1
    FOREIGN KEY (ALGORITHM_DEF_WID)
    REFERENCES RP_ALGORITHM_DEF(WID)
;


CREATE TABLE RP_ALGORITHM(
    WID                       									BIGINT    NOT NULL,
    SDP_SEGMENT_CONFIG_WID    	BIGINT    NOT NULL,
    ALGORITHM_DEF_WID    				BIGINT   NOT NULL,
    ALGORITHM_EIP_CD             					 NATIONAL CHARACTER VARYING(10) not null,
    ACTIVE_FLAG               						CHAR(1) NOT NULL,
    MINUTE_BACKWARD    					BIGINT not null,
    ORG_ID          									BIGINT,
    INSERT_TIME       TIMESTAMP    DEFAULT  TIMESTAMP ('now':: VARCHAR ) NOT NULL,
    LAST_UPD_TIME     TIMESTAMP    DEFAULT  TIMESTAMP ('now':: VARCHAR ) NOT NULL,
    CONSTRAINT PK_RP_ALGORITHM PRIMARY KEY (WID),
    CONSTRAINT UK_RP_ALGORITHM   UNIQUE (SDP_SEGMENT_CONFIG_WID, ALGORITHM_DEF_WID)
)
distribute on random;


ALTER TABLE RP_ALGORITHM ADD CONSTRAINT FK_RP_ALGORITHM_1
    FOREIGN KEY (SDP_SEGMENT_CONFIG_WID)
    REFERENCES SDP_SEGMENT_CONFIG(WID)
;


ALTER TABLE RP_ALGORITHM ADD CONSTRAINT FK_RP_ALGORITHM_2
    FOREIGN KEY (ALGORITHM_DEF_WID)
    REFERENCES RP_ALGORITHM_DEF(WID)
;


CREATE TABLE RP_ALGORITHM_PARAM(
    WID                							BIGINT   NOT NULL,
    ALGORITHM_WID    				BIGINT   NOT NULL,
    NAME                 						 NATIONAL CHARACTER VARYING(50) not null,
    VALUE                						 NATIONAL CHARACTER VARYING(100),
  ORG_ID          									BIGINT,
    INSERT_TIME       TIMESTAMP    DEFAULT  TIMESTAMP ('now':: VARCHAR ) NOT NULL,
    LAST_UPD_TIME     TIMESTAMP    DEFAULT  TIMESTAMP ('now':: VARCHAR ) NOT NULL,
    CONSTRAINT PK_RP_ALGORITHM_PARAM PRIMARY KEY (WID),
    CONSTRAINT UK_RP_ALGORITHM_PARAM   UNIQUE (ALGORITHM_WID, NAME)
)
distribute on random;


ALTER TABLE RP_ALGORITHM_PARAM ADD CONSTRAINT FK_RP_ALGORITHM_PARAM_1
    FOREIGN KEY (ALGORITHM_WID)
    REFERENCES RP_ALGORITHM(WID)
;


CREATE TABLE RP_EXECUTION_STAT(
    BATCH_ID              						 BIGINT not null,
    ALGORITHM_WID    					BIGINT   NOT NULL,
    ANALYSIS_END_DATE             date not null,
    START_TIME             					timestamp not null,
    END_TIME               					timestamp,
    SUCCESS_FLAG              					CHAR(1) NOT NULL,
    SDP_OUTCOME_COUNT     BIGINT  null,
    SDP_SEGMENT_COUNT     BIGINT  null,
    PARAM_TEXT              					 NATIONAL CHARACTER VARYING(4000),
    ERROR_MSG              					 NATIONAL CHARACTER VARYING(4000),
    ORG_ID          									BIGINT,
    INSERT_TIME       TIMESTAMP    DEFAULT  TIMESTAMP ('now':: VARCHAR ) NOT NULL,
    LAST_UPD_TIME     TIMESTAMP    DEFAULT  TIMESTAMP ('now':: VARCHAR ) NOT NULL,
    CONSTRAINT PK_RP_EXECUTION_STAT PRIMARY KEY (BATCH_ID, ALGORITHM_WID, ANALYSIS_END_DATE)
)
distribute on random
ORGANIZE ON (ANALYSIS_END_DATE);

ALTER TABLE RP_EXECUTION_STAT ADD CONSTRAINT FK_RP_EXECUTION_STAT_1
    FOREIGN KEY (ALGORITHM_WID)
    REFERENCES RP_ALGORITHM(WID)
;

CREATE TABLE RP_FEATURE(
    BATCH_ID              						 BIGINT not null,
    ALGORITHM_WID        				BIGINT    NOT NULL,
    ANALYSIS_END_DATE             			date not null,
    SDP_WID              						BIGINT not null,
    ACTUAL_VALUE         				numeric(21,6) not null,
    DESCRIPTION 					NATIONAL CHARACTER VARYING(4000),
    ORG_ID          									BIGINT,
    INSERT_TIME       TIMESTAMP    DEFAULT  TIMESTAMP ('now':: VARCHAR ) NOT NULL,
    LAST_UPD_TIME     TIMESTAMP    DEFAULT  TIMESTAMP ('now':: VARCHAR ) NOT NULL,
    CONSTRAINT PK_RP_FEATURE PRIMARY KEY (BATCH_ID, ALGORITHM_WID, ANALYSIS_END_DATE)
)
distribute on (SDP_WID)
ORGANIZE ON (ANALYSIS_END_DATE);

ALTER TABLE RP_FEATURE ADD CONSTRAINT FK_RP_FEATURE_1
    FOREIGN KEY (ALGORITHM_WID)
    REFERENCES RP_ALGORITHM(WID)
;

/*
ALTER TABLE RP_FEATURE ADD CONSTRAINT FK_RP_FEATURE_2
    FOREIGN KEY (SDP_WID)
    REFERENCES SDP_D(WID)
;
*/

CREATE TABLE RP_MODEL_OUTPUT(
    WID                							BIGINT   NOT NULL,
    BATCH_ID              						 BIGINT not null,
    ANALYSIS_START_DATE             date not null,
    ANALYSIS_END_DATE             date not null,
    SDP_WID              						BIGINT not null,
    --SEQ_NUM              						BIGINT not null,
    SCORE            								BIGINT not null,
    MODEL_VERSION    					 NATIONAL CHARACTER VARYING(100),
  ORG_ID          									BIGINT,
    INSERT_TIME       TIMESTAMP    DEFAULT  TIMESTAMP ('now':: VARCHAR ) NOT NULL,
    LAST_UPD_TIME     TIMESTAMP    DEFAULT  TIMESTAMP ('now':: VARCHAR ) NOT NULL,
    CONSTRAINT PK_RP_MODEL_OUTPUT PRIMARY KEY (WID),
    CONSTRAINT UK_RP_MODEL_OUTPUT unique (BATCH_ID,  ANALYSIS_END_DATE, SDP_WID, MODEL_VERSION)
)
distribute on (SDP_WID)
ORGANIZE ON (ANALYSIS_END_DATE);

CREATE TABLE RP_INVESTIGATION_REQUEST(
    model_output_wid    BIGINT           NOT NULL,
    sdp_wid             BIGINT           NOT NULL,
    batch_id            BIGINT    NOT NULL,
    ORG_ID          	BIGINT,
    INSERT_TIME       TIMESTAMP    DEFAULT  TIMESTAMP ('now':: VARCHAR ) NOT NULL,
    LAST_UPD_TIME     TIMESTAMP    DEFAULT  TIMESTAMP ('now':: VARCHAR ) NOT NULL,
    CONSTRAINT PK_RP_INVESTIGATION_REQUEST PRIMARY KEY (model_output_wid)
)
distribute on (SDP_WID);

ALTER TABLE RP_INVESTIGATION_REQUEST ADD CONSTRAINT FK_RP_INVESTIGATION_REQUEST_1
    FOREIGN KEY (model_output_wid)
    REFERENCES RP_MODEL_OUTPUT(WID)
;


CREATE TABLE RP_INVESTIGATION(
    model_output_wid              BIGINT     NULL,
    batch_id                      BIGINT     NULL,
    service_request_id            BIGINT     NULL,
    sdp_wid                       BIGINT,
    sdp_udc_id                    NATIONAL CHARACTER VARYING(200),
    service_request_open_time     TIMESTAMP     NOT NULL,
    service_request_close_time    TIMESTAMP,
    investigation_outcome_cd      NATIONAL CHARACTER VARYING(50),
    sr_outcome_cd                 NATIONAL CHARACTER VARYING(50),
    sr_outcome_reason_cd          NATIONAL CHARACTER VARYING(50),
    sr_sub_type_cd                NATIONAL CHARACTER VARYING(50),
    ORG_ID          	BIGINT,
    INSERT_TIME       TIMESTAMP    DEFAULT  TIMESTAMP ('now':: VARCHAR ) NOT NULL,
    LAST_UPD_TIME     TIMESTAMP    DEFAULT  TIMESTAMP ('now':: VARCHAR ) NOT NULL,
    CONSTRAINT UK_RP_INVESTIGATION_1  UNIQUE (sdp_wid, sdp_udc_id, model_output_wid, service_request_id, service_request_open_time)
)
distribute on (SDP_WID)
ORGANIZE ON (service_request_open_time);

ALTER TABLE RP_INVESTIGATION ADD CONSTRAINT FK_RP_INVESTIGATION_1
    FOREIGN KEY (model_output_wid)
    REFERENCES RP_MODEL_OUTPUT(WID)
;




--1
insert into premise_d(WID,POSTAL_CODE_WID,SRC_ID,ADDR_LINE_1,ADDR_LINE_2,org_id)
values(1,1,1,'Addr11','Addr12',1);
insert into sdp_d (WID,SRC_ID,ORG_ID,UDC_ID,premise_wid,gps_lat,gps_long)
values(1, 'srcId-1',1,'udcId-1',1,0.34,0.56);
insert into RP_ALGORITHM_DEF(WID, NAME, DESCRIPTION, PROCEDURE_NAME,EIP_CD, MINUTE_BACKWARD,ORG_ID)
values(1,'Algo1','algoDesc1','AlgoProc1','Y1',10,1);
insert into RP_ALGORITHM(WID,SDP_SEGMENT_CONFIG_WID,ALGORITHM_DEF_WID,ALGORITHM_EIP_CD,ACTIVE_FLAG,MINUTE_BACKWARD,ORG_ID,INSERT_TIME,LAST_UPD_TIME)
values(1,1,1,'Y1','Y',10,1,now(),now());
insert into rp_feature(BATCH_ID,ALGORITHM_WID,ANALYSIS_END_DATE,SDP_WID,ACTUAL_VALUE,DESCRIPTION,ORG_ID,INSERT_TIME,LAST_UPD_TIME)
values(1,1,now(),1,0.1,'Desc 1',1,now(),now());
insert into RP_MODEL_OUTPUT(WID,BATCH_ID,ANALYSIS_END_DATE,ANALYSIS_START_DATE,SDP_WID,SCORE,MODEL_VERSION,ORG_ID,INSERT_TIME,LAST_UPD_TIME)
values(1,1,now(),now(),1,40,'modelVersion1',1,now(),now());

--2
insert into premise_d(WID,POSTAL_CODE_WID,SRC_ID,ADDR_LINE_1,ADDR_LINE_2,ORG_ID)
values(2,2,2,'Addr21','Addr22',1);
insert into sdp_d (WID,SRC_ID,ORG_ID,UDC_ID,premise_wid,gps_lat,gps_long)
values(2, 'srcId-2',1,'udcId-2',2,0.1,0.2);
insert into RP_ALGORITHM_DEF(WID, NAME, DESCRIPTION, PROCEDURE_NAME,EIP_CD, MINUTE_BACKWARD,ORG_ID)
values(2,'Algo2','algoDesc2','AlgoProc2','Y2',10,1);
insert into RP_ALGORITHM(WID,SDP_SEGMENT_CONFIG_WID,ALGORITHM_DEF_WID,ALGORITHM_EIP_CD,ACTIVE_FLAG,MINUTE_BACKWARD,ORG_ID,INSERT_TIME,LAST_UPD_TIME)
values(2,2,2,'Y2','Y',10,1,now(),now());
insert into rp_feature(BATCH_ID,ALGORITHM_WID,ANALYSIS_END_DATE,SDP_WID,ACTUAL_VALUE,DESCRIPTION,ORG_ID,INSERT_TIME,LAST_UPD_TIME)
values(1,2,now(),2,0.2,'Desc 2',1,now(),now());
insert into RP_MODEL_OUTPUT(WID,BATCH_ID,ANALYSIS_END_DATE,ANALYSIS_START_DATE,SDP_WID,SCORE,MODEL_VERSION,ORG_ID,INSERT_TIME,LAST_UPD_TIME)
values(2,1,now(),now(),2,80,'modelVersion2',1,now(),now());

--3
insert into premise_d(WID,POSTAL_CODE_WID,SRC_ID,ADDR_LINE_1,ADDR_LINE_2,ORG_ID)
values(3,3,3,'Addr31','Addr32',1);
insert into sdp_d (WID,SRC_ID,ORG_ID,UDC_ID,premise_wid,gps_lat,gps_long)
values(3, 'srcId-3',1,'udcId-3',3,0.3,0.4);
insert into RP_ALGORITHM_DEF(WID, NAME, DESCRIPTION, PROCEDURE_NAME,EIP_CD, MINUTE_BACKWARD,ORG_ID)
values(3,'Algo3','algoDesc3','AlgoProc3','Y3',10,1);
insert into RP_ALGORITHM(WID,SDP_SEGMENT_CONFIG_WID,ALGORITHM_DEF_WID,ALGORITHM_EIP_CD,ACTIVE_FLAG,MINUTE_BACKWARD,ORG_ID,INSERT_TIME,LAST_UPD_TIME)
values(3,3,3,'Y3','Y',10,1,now(),now());
insert into rp_feature(BATCH_ID,ALGORITHM_WID,ANALYSIS_END_DATE,SDP_WID,ACTUAL_VALUE,DESCRIPTION,ORG_ID,INSERT_TIME,LAST_UPD_TIME)
values(1,3,now(),3,0.3,'Desc 3',1,now(),now());
insert into RP_MODEL_OUTPUT(WID,BATCH_ID,ANALYSIS_END_DATE,ANALYSIS_START_DATE,SDP_WID,SCORE,MODEL_VERSION,ORG_ID,INSERT_TIME,LAST_UPD_TIME)
values(3,1,now(),now(),3,60,'modelVersion3',1,now(),now());
insert into rp_investigation_request(MODEL_OUTPUT_WID,SDP_WID,BATCH_ID,ORG_ID,INSERT_TIME,LAST_UPD_TIME)
values (3,3,1,1,now(),now());
--4
insert into premise_d(WID,POSTAL_CODE_WID,SRC_ID,ADDR_LINE_1,ADDR_LINE_2,ORG_ID)
values(4,4,4,'Addr41','Addr42',1);
insert into sdp_d (WID,SRC_ID,ORG_ID,UDC_ID,premise_wid,gps_lat,gps_long)
values(4, 'srcId-4',1,'udcId-4',4,0.5,0.6);
insert into RP_ALGORITHM_DEF(WID, NAME, DESCRIPTION, PROCEDURE_NAME,EIP_CD, MINUTE_BACKWARD,ORG_ID)
values(4,'Algo4','algoDesc4','AlgoProc4','Y4',10,1);
insert into RP_ALGORITHM(WID,SDP_SEGMENT_CONFIG_WID,ALGORITHM_DEF_WID,ALGORITHM_EIP_CD,ACTIVE_FLAG,MINUTE_BACKWARD,ORG_ID,INSERT_TIME,LAST_UPD_TIME)
values(4,4,4,'Y4','Y',10,1,now(),now());
insert into rp_feature(BATCH_ID,ALGORITHM_WID,ANALYSIS_END_DATE,SDP_WID,ACTUAL_VALUE,DESCRIPTION,ORG_ID,INSERT_TIME,LAST_UPD_TIME)
values(1,4,now(),4,0.4,'Desc 4',1,now(),now());
insert into RP_MODEL_OUTPUT(WID,BATCH_ID,ANALYSIS_END_DATE,ANALYSIS_START_DATE,SDP_WID,SCORE,MODEL_VERSION,ORG_ID,INSERT_TIME,LAST_UPD_TIME)
values(4,1,now(),now(),4,70,'modelVersion4',1,now(),now());

--5
insert into premise_d(WID,POSTAL_CODE_WID,SRC_ID,ADDR_LINE_1,ADDR_LINE_2,ORG_ID)
values(5,5,5,'Addr51','Addr52',1);
insert into sdp_d (WID,SRC_ID,ORG_ID,UDC_ID,premise_wid,gps_lat,gps_long)
values(5, 'srcId-5',1,'udcId-5',5,0.7,0.8);
insert into RP_ALGORITHM_DEF(WID, NAME, DESCRIPTION, PROCEDURE_NAME,EIP_CD, MINUTE_BACKWARD,ORG_ID)
values(5,'Algo5','algoDesc5','AlgoProc5','Y5',10,1);
insert into RP_ALGORITHM_DEF(WID, NAME, DESCRIPTION, PROCEDURE_NAME,EIP_CD, MINUTE_BACKWARD,ORG_ID)
values(6,'Algo6','algoDesc6','AlgoProc6','Y6',11,1);
insert into RP_ALGORITHM(WID,SDP_SEGMENT_CONFIG_WID,ALGORITHM_DEF_WID,ALGORITHM_EIP_CD,ACTIVE_FLAG,MINUTE_BACKWARD,ORG_ID,INSERT_TIME,LAST_UPD_TIME)
values(5,5,5,'Y5','Y',10,1,now(),now());
insert into RP_ALGORITHM(WID,SDP_SEGMENT_CONFIG_WID,ALGORITHM_DEF_WID,ALGORITHM_EIP_CD,ACTIVE_FLAG,MINUTE_BACKWARD,ORG_ID,INSERT_TIME,LAST_UPD_TIME)
values(6,5,6,'Y6','Y',10,1,now(),now());
insert into rp_feature(BATCH_ID,ALGORITHM_WID,ANALYSIS_END_DATE,SDP_WID,ACTUAL_VALUE,DESCRIPTION,ORG_ID,INSERT_TIME,LAST_UPD_TIME)
values(1,5,now(),5,0.5,'Desc 5',1,now(),now());
insert into rp_feature(BATCH_ID,ALGORITHM_WID,ANALYSIS_END_DATE,SDP_WID,ACTUAL_VALUE,DESCRIPTION,ORG_ID,INSERT_TIME,LAST_UPD_TIME)
values(1,6,now(),6,0.5,'Desc 6',1,now(),now());
insert into RP_MODEL_OUTPUT(WID,BATCH_ID,ANALYSIS_END_DATE,ANALYSIS_START_DATE,SDP_WID,SCORE,MODEL_VERSION,ORG_ID,INSERT_TIME,LAST_UPD_TIME)
values(5,1,now(),now(),5,60,'modelVersion6',1,now(),now());


insert into RP_MODEL_OUTPUT(WID,BATCH_ID,ANALYSIS_END_DATE,ANALYSIS_START_DATE,SDP_WID,SCORE,MODEL_VERSION,ORG_ID,INSERT_TIME,LAST_UPD_TIME)
values(6,1,now(),now(),6,60,'modelVersion6',1,now(),now());
insert into RP_MODEL_OUTPUT(WID,BATCH_ID,ANALYSIS_END_DATE,ANALYSIS_START_DATE,SDP_WID,SCORE,MODEL_VERSION,ORG_ID,INSERT_TIME,LAST_UPD_TIME)
values(7,1,now(),now(),7,60,'modelVersion6',1,now(),now());
*******************************************************************************


mutex & job chaining


Call out the diff between initial load and incremental. -- Done. May need to add more info if clarity is insufficient
Define the process to mark, sweep and purge the change data capture tables. -- Done. Updated 4.1.1.2 Structural data Extractor
Track counters for number of records processed. Logging of records rejected. -- Done. Updated 4.1.1.2.2.3 Extract the records and create XML (Sweeping of modified records)
Svc pt group is not ref data in 8.x. No such thing as svc pt group class.  -- Done. Removed from group diagram, updated gliffy image from 4.2 Loader Components
Need to define CTPT handling at both product and asset level. --- Will be handled as other devices
Multiplier in 7.x is at product level but at asset level in 8. ---- 4.1.1.2.2.3 Extract the records and create XML (Sweeping of modified records)
This is shipped as a standalone application scheduled via cron in 7.x environment. -- Done in deployment section but need to add more info
Orchestration components on load side need elaboration.
Multi org behavior needs to be called out. --- Done. Updated section 2.1.2.1 & 2.1.2.2
Also need to carry meas type objects. -- Need to discuss
Confirm the ability to carry row id in channel, svc pt, meter, and comm mod objects for meter data loading -- Already added in 1.4 Assumptions

***************************************************************************************************
adftransactionmonitor usage property expose issue 
http://jira.emeter.com/browse/EIP-24909 

***************************************************************************************************
RP jobs compatibility matrix for 7x environment:

* 32 bit machine (linux)
* energyIP-7.5.SP7 (or later) with job server configured [em-ui should be able to access job server]
* JDK1.6.xx or later
* ODBC Drivers for Linux must be installed. Verify it using:
	  rpm  -q  unixODBC
* Analytics 1.4
* nz-linuxclient-v7.0.4.2
* R version: R-2.15.3
* R packages: 
	bitops_1.0-6
	ca_0.53
	caTools_1.16
	e1071_1.6-1
	rgl_0.93.996
	RODBC_1.3-7
	tree_1.0-34
	XML_3.98-1.1
	randomForest_4.5-34
	rJava_0.9-5
* R Netezza packages: Netezza_Client_Packages-2.5.4
	nzr_2.5.4
	nza_2.5.4
	nzmatrix_2.5.4
* Ensure the netezza database server is accessible from installer machine


--MeasType
'N' AS cpp
               , prod.sub_type AS e_type
	       , prod.uom_cd AS uom



MAX(CASE 
                  WHEN param.attrib_01 = 'Physical Channel Number'
                         THEN param.attrib_02
                     END) AS e_channel_num
               , MAX(CASE 
                         WHEN param.attrib_01 = 'TOU Bin Number'
                         THEN param.attrib_02
                     END) AS e_tou_bin
        --, MAX(CASE 
        --          WHEN param.attrib_01 = 'MUDR Field Name'
        --          THEN param.attrib_02
               --      END) AS mudr_field_name
               , MAX(CASE 
                         WHEN param.attrib_01 = 'Direction'
                         THEN param.attrib_02
                     END) AS direction

               , MAX(CASE 
                         WHEN param.attrib_01 = 'Multiplier'
                         THEN param.attrib_02
                     END) AS metric_multiplier
               , MAX(CASE 
                         WHEN param.attrib_01 = 'Interval Length'
                         THEN param.attrib_02
                     END) AS e_interval_len
               , MAX(CASE 
                         WHEN param.attrib_01 = 'Channel Type'
                         THEN param.attrib_02
                     END) AS e_channel_type 

--Service split logic

 CAST (FROM_TZ (
                            CAST ((CASE
                                     WHEN src_n_param.start_dt >= src_n_param.param_start_dt
                                         THEN src_n_param.start_dt
                                     WHEN src_n_param.start_dt < src_n_param.param_start_dt
                                         THEN src_n_param.param_start_dt
                                  END) AS TIMESTAMP)
                            , p_source_timezone
                         )
                 AT TIME ZONE (p_target_timezone) AS TIMESTAMP(0))
           eff_start_time,
           CAST (FROM_TZ (
                           CAST ((CASE
                                     WHEN (src_n_param.end_dt IS NULL AND src_n_param.param_end_dt is NULL)
                                         THEN NULL
                                     WHEN (src_n_param.end_dt is NOT NULL AND src_n_param.param_end_dt is NULL)
                                         THEN src_n_param.end_dt
                                     WHEN (src_n_param.end_dt is NULL AND src_n_param.param_end_dt is NOT NULL)
                                         THEN src_n_param.param_end_dt
                                     WHEN src_n_param.end_dt <= src_n_param.param_end_dt
                                         THEN src_n_param.end_dt
                                     WHEN src_n_param.end_dt > src_n_param.param_end_dt
                                         THEN src_n_param.param_end_dt
                                  END)
                                      AS TIMESTAMP)
                           , p_source_timezone
                           )
                 AT TIME ZONE (p_target_timezone) AS TIMESTAMP(0))
           eff_end_time,
************************************************************************************
Login to DB using DBA privileges

  sqlplus eip_dba/eip_dba132@isebl_12

SQL> create user siebelhelper identified by siebelhelper;

grant connect to siebelhelper;
grant resource to siebelhelper;
grant create any table to siebelhelper;
grant create public synonym to siebelhelper;
grant drop public synonym to siebelhelper;
grant select on siebel.s_asset to siebelhelper;
grant select on siebel.s_prod_int to siebelhelper;
grant select on siebel.s_prod_int_xm to siebelhelper;


-- Grant from siebelhelper to siebel
grant all on siebelhelper.product_name_mapping to siebel;

-- Synonym on siebel
create synonym product_name_mapping for siebelhelper.product_name_mapping;

***************************************************************************************************

line,  ,( =([^\ ]*\ [^\ ]*\ )*[^\ ]* ) 




**************************************************************************************************
Incremental extraction:

Asset data  (Premise, Accnt, Route)
-----------
1) Mark the records

* Data identified by triggers are populated to output tables
e.g. Premise
	Premise(S_ADDR_PER)
	Trigger:TRG_ALYT_PREMISE_CAPTURE
	OutputTable: CX_ALYT_PREMISE_CDC	

	Trigger defn:
	
	CREATE OR REPLACE TRIGGER trg_alyt_premise_capture
	BEFORE INSERT OR UPDATE ON s_addr_per
	FOR EACH ROW
	BEGIN
	    INSERT INTO cx_alyt_premise_cdc
	        (
	            row_id
	          , org_name
	          , record_type
	          , last_upd_time
	          , status
	        )
	    VALUES
	        (
	            :new.row_id
	          , :new.x_data_src
	          , 'PREMISE'
	          , :new.last_upd
	          , 'ToProcess'
	        );
	 
	EXCEPTION
	    WHEN OTHERS
	    THEN
	        RAISE;
	END;
	
	
	

2) Sweep the records

	Step 1
	Identify the data to process. The status field of all trigger populated output tables is updated to 'Processing'.
		
	Step 2
	Load Bad records: Fetch duplicate Premise UDC_ID.

	Step 3
	Fetch the records from EIP 7. Select all records from output table i.e. CX_ALYT_PREMISE_CDC which has status as 'Processing' and  
	use src_id to fetch respective details from 7x schema table i.e S_ADDR_PER.
	Single query will perform above operations

	Step 4
	Filter bad records and Generate XML. Filtered records are captured for logging purpose. Valid records are used to create XML and captured 
	for logging successful records. 
	The query is ordered by orgId. 
	For each Org:
		* An orgId specific directory is created and the path is kept in Map to accessed across other extractor threads
		* XML template is loaded into a document and data is populated and nodes are appended to XML document.
		  Once all data is added to XML document, the document is written to a file in org specific directory.
	
	Step 5
	Create a log file to write user key for rejected records
	
	Step 6
	Create a stats file to track count for each modified asset: Logs the count of successful records
	
	Step 7
	All extractor threads extraction is done, a zip file, per Org, for the structural data files.
	
3) Purge the records
	Delete all records from output table with status 'Processing'

SDP Stack data
---------------
1) Mark the records
* Data identified by triggers are populated to output tables.
	Asset(S_ASSET) 
	Trigger: TRG_ALYT_ASSET_CAPTURE
	OutputTable: CX_ALYT_ASSET_CDC , CX_ALYT_DATA_SVC_CDC , CX_ALYT_SVC_AGREE_CDC
	
	Relationship(S_ASSET_REL)
	Trigger: TRG_ALYT_RELATION_CAPTURE
	OutputTable: CX_ALYT_REL_CDC
	
	Parameter(S_ASSET_XM)
	Trigger: TRG_ALYT_ASSET_PARAM_CAPTURE
	OutputTable: CX_ALYT_ASSET_CDC, CX_ALYT_DATA_SVC_CDC, CX_ALYT_SVC_AGREE_CDC 
	

2) Sweep the records
	Asset & Parameter
	Select the SDP_Id from CX_ALYT_ASSET_CDC
	
	Relationship 
	Select the SDP_Id from all changed relationship which has SDP as parent.
	Select the SDP_Id from all changed relationship which has SDP as child.
	Select the SDP_Id associated with the Meter-CommMod relation.
	Select the SDP_Id associated with the Meter-Channel relation.
	Select the SDP_Id present in the modified Data Services.
	Select the SDP_Id present in the modified Agreement Services.
	Select the SDP_Id which are modified in the specified interval.
	Select the SDP_Id present in the modified Channel


	Step 1 - Identify records to process
	* Result of above query is populated to CX_TEMP_SDP_REF_ID i.e table has distinct list of impacted SDP	
	  Look for Query: Extract unique SDP Ref Ids.
	* Identify the data to process. The status field of all trigger populated output tables is updated to 'Processing'.
	
	Step 2 - Load Bad records
	* Fetch duplicate SDP UDC_ID from S_ASSET.

	Step 3 - Fetch the records from EIP 7
	* Select list of impacted SDPs from output table i.e. CX_TEMP_SDP_REF_ID which has status as 'Processing'
	* Use worker thread to process for chunk of SDPs 


	Step 4 - Filter bad records and Generate XML by worker thread. 
	* Filtered records are captured for logging purpose. 
	* Valid records are used to create XML and captured for logging successful records. 
	The query is ordered by orgId. 
	For each Org:
		** An orgId specific directory is created and the path is kept in Map to accessed across other extractor threads
		** XML template is loaded into a document and data is populated and nodes are appended to XML document.
		  Once all data is added to XML document, the document is written to a file in org specific directory.

	* Following tasks are performed for creating complete SDP stack.
		** Each worker thread will receive chunk of SDPs to process e.g. 50 SDPs per worker thread.
		** For each SDP it will create an in-memory XML file.
		** Fetch SDP and SDP param details for each SDP and append the in-memory XML file with details.
		** Fetch Meter, DisconnectCollor, CT/PT and CommFunction with their params for each SDP and append the in-memory XML file with Device details.
		** Fetch DistributionNode and Equipment Location details for each SDP and append the inmemory XML file with details.
		** Fetch Channel and Channel param details for each SDP and append the in-memory XML file with details.
		** Fetch SDP-Meter relationship details for each SDP and append the in-memory XML file with relationship details.
		** Fetch Meter-Channel relationship details with all other relationships for each SDP and append the in-memory XML file with relationship details.
		** Fetch services details for each SDP and append the in-memory XML file with relationship details.
	
	Step 5
	Create a log file to write user key for rejected SDPs

	Step 6
	Create a stats file to track count for each modified asset: Logs the count of successful records

	Step 7
	All extractor threads extraction is done, a zip file, per Org, for the structural data files.

3) Purge the records
	Delete all records from output table with status 'Processing'



************************************************************************************************
-Need to discuss feasibility of helper schema for output tables. 
	* The siebel schema will have all grants on all o/p tables
	* The siebelHelper schema will have select grant on few siebel tables like s_asset, s_prod_int, s_prod_int_xm etc
	* siebel schema will only have
		** triggers
		** synonyms for siebelHelper tables
		
		
CX_ALYT_DATA_SVC_CDC
CX_ALYT_SVC_AGREE_CDC
CX_ALYT_ACCNT_SDP_REL_CDC
CX_TEMP_SDP_REF_ID

*************************************************************************************************
SELECT s.row_id AS src_id,
s.data_src AS data_src,
s.x_udc_asset_id AS udc_id,
s.prod_id AS class_src_id,
s.serial_num AS mfg_serial_num,
s.x_lot_num AS mfg_lot_num,
s.x_network_id AS network_id,
s.status_cd AS status_cd,
s.desc_text AS desc_text,
s.purch_dt AS purchase_date,
s.ship_dt AS ship_date,
NULL AS mfg_test_date,
s.x_retire_dt AS retire_date,
s.x_mfg_date AS mfg_date,
s.make_cd AS make,
s.model_cd AS model,
s.last_test_dt AS last_test_date,
s.x_virtual_asset AS is_virtual_flg,
s.x_universal_id AS badge_id,
s.x_aep_num AS standard_id,
s.x_electronic_id AS electronic_id,
NULL AS comm_technology,
NULL AS cur_inv_location_id,
NULL AS sku,
NULL AS part_num,
s.type_cd AS TYPE,
s.cfg_type_cd AS sub_type,
--DECODE (s.type_cd, 'Service', s.cfg_type_cd, s.type_cd) AS handler_type,
--s.per_addr_id AS premise_src_id,
--s.x_ax_feed_loc AS feed_loc,
--s.x_latitude_new AS gps_lat,
--s.x_longitude_new AS gps_long,
--s.x_pulse_output_blk AS pulse_output_block,
--s.x_seal_info AS seal_info,
--s.x_lock_info AS lock_info,
--s.x_util_access_info AS access_info,
--s.x_emeter_acces_info AS alt_access_info,
--s.x_util_premise_loc AS loc_info,
--s.x_emeter_premise_loc AS alt_loc_info,
--s.x_billing_cycle AS billing_cycle,
--s.x_reading_cycle AS reading_cycle,
--NULL AS timezone_id,
--NULL AS gis_id,
--s.x_read_dt AS billed_upto_time,
--s.x_power_status AS power_status,
--s.x_usage_status AS load_status,
--s.x_billing_hold AS billing_hold_status,
--s.meter_loc AS location_info,
--s.service_point_id AS svc_pt_src_id,
--s.start_dt AS eff_start_time,
--s.end_dt AS eff_end_time,
--a.svc_provider_id AS svc_provider_src_id,
--s.cur_agree_id AS svc_agree_src_id,
--s.created AS source_created,
--s.last_upd AS source_last_upd,
--NULL AS insert_time,
--NULL AS last_upd_time,
mapping.product_name_8x AS name, 
param.attrib_01 AS param_name,
param.attrib_02 AS param_value,
param.attrib_12 AS eff_start_date,
param.attrib_13 AS eff_end_date
FROM  s_asset s,
s_prod_int prod,
--cx_temp_sdp_ref_id ref,
cx_product_name_mapping mapping,
--s_asset_rel  rel,
s_asset_xm param
WHERE s.prod_id = prod.row_id
--AND ref.sdp_row_id = rel.par_asset_id
--AND info.record_type = 'Meter'
--AND rel.x_asset_relation_type_cd = 'SDP-METER'
--AND s.row_id  = rel.asset_id
AND mapping.product_name_7x = prod.name
AND mapping.type_7x = s.type_cd
AND param.par_row_id (+) = s.row_id
--AND status = 'Processing'
AND s.type_cd IN ('Meter','Communication Module','CT-PT','Disconnect Collar')
order by s.data_src, s.row_id;
*****************************************************************************************************************

Ankit conversation
emapp/emapp154@imudr_14 on ind-db02.emeter.com

@MUDR2SEBL
Ind
pipe/pipe154@isebl_14 on ind-db02.emeter.com

US 
pipe/piep198@sebldb18 on emdb05.emeter.com
***********************************************************************************

--TODO Critical Care, Critical Load not as parameter

**********************************************************************************
SELECT prms.row_id AS src_id, 
prms.x_client_prmse_id AS udc_id, 
prms.x_data_src AS data_src,
'Reason: UdcId is duplicate' as reason
FROM 
(
SELECT COUNT(1), x_client_prmse_id
FROM s_addr_per
--
GROUP BY x_client_prmse_id
HAVING COUNT(1) > 1
) dup,
s_addr_per prms
WHERE prms.x_client_prmse_id = dup.x_client_prmse_id;

***********************************************************************************

--SvcptClass
Insert into s_prod_int (ROW_ID,CREATED,CREATED_BY,LAST_UPD,LAST_UPD_BY,DCKING_NUM,MODIFICATION_NUM,CONFLICT_ID,ACTIVE_FLG,ADD_TO_QUOTE_FLG,APPLY_EC_RULE_FLG,APPROVAL_FLG,AUTO_UNGROUP_FLG,BILLABLE_FLG,BU_ID,CG_COMPETITOR_FLG,CMPND_FLG,COMMISIONED_FLG,COMPENSATABLE_FLG,CRT_AGREEMENT_FLG,CRT_AST_REC_FLG,CRT_INST_FLG,DESIGN_REG_FLG,ENTERPRISE_FLG,FEATURED_FLG,INCLSV_ELIG_RL_FLG,INCL_ALL_CRSE_FLG,LEAF_LEVEL_FLG,MODEL_PROD_FLG,NAME,ORDERABLE_FLG,POSTN_BL_PROD_FLG,SALES_PROD_FLG,SALES_SRVC_FLG,SERVICE_FLG,SERV_INST_FLG,SHIP_FLG,TARGET_VRSN_FLG,TAX_SUBCOMP_FLG,TRANSFERABLE_FLG,TWOBARCODES_FLG,UNIQUE_ASSET_FLG,CTRL_SUBS_FLG,ROLLUP_LEVEL,ROLLUP_TRGMKT_FLG,ACCRUAL_RATE,ALC_BELOW_SFTY_FLG,ALOC_ASSETS_FLG,APRX_COST_AMT,APX_CST_EXCH_DT,AUTO_ALLOCATE_FLG,AUTO_SUBST_FLG,AVG_CALL_CST,AVG_CALL_CST_DT,AVG_OP_LIFE,AVG_PROFIT,CAPACITY,CARY_COST,CARY_COST_DT,CASE_CONV_FACTOR,CASE_PACK,CONFIGURATION_FLG,CUTOFF_FLG,DELIV_INTERVAL,DEST_COST,DISP_CMPNT_FLG,DISP_CMPNT_PRI_FLG,EFFICIENCY_RATING,EFF_END_DT,EFF_START_DT,EXCH_DT,EXP_LEAD_DAYS,FRU_FLG,GROSS_MGN,INCL_CRSE_NUM,INVENTORY_FLG,ITEM_SIZE,LOAD_ADDED,LOCK_FLG,LOT_ACTIVE_FLG,LOY_ACTUAL_DIST,LOY_EXP_LEAD_TIME,LOY_SUG_POINTS,LOY_SUG_PRICE,LOY_SUG_R_POINTS,LOY_SUG_R_PRICE,MAX_ORDER_UNITS,MIN_ORDER_UNITS,MODEL_YR,MSRP,MTBF,MTTR,NUM_OCCURRENCE,ORDER_CST,ORDER_CST_DT,PRODUCT_LEVEL,PROD_UPD_DT,REQ_APPL_FLG,REQ_REFERRAL_FLG,RESERVABLE_FLG,RTRN_DEFECTIVE_FLG,RXAVPR_EXCH_DT,RX_AVG_PRICE,SCORE_NUM,SERIALIZED_FLG,SERVICE_LENGTH,SKIP_FLG,SUBSCN_DUR_DAY_NUM,TAXABLE_FLG,TOOL_FLG,UNITS_BCKORD,UNITS_BCKORD_AS_OF,UNITS_INVENT,UNITS_INVENT_AS_OF,UNIT_CONV_FACTOR,ADDL_LEAD_TIME,ALIAS_NAME,APX_CST_CURCY_CD,ASSET_REF_EXPR,ASSOC_LEVEL,ATM_TYPE_CD,AVGCALLCST_CURCYCD,BAR_CODE_NUM,BODY_STYLE_CD,BOOK_APPR_ID,BUILD,CARY_COST_CURCY_CD,CATEGORY_CD,CFG_MODEL_ID,CG_PR_CTLG_CAT_ID,CLASS_PARTNUM_ID,COMMENTS,CONFIG_RULE_FILE,CRITICAL_CD,CS_PATH_ID,CURCY_CD,DATA_SRC,DEF_MOD_PROD_ID,DESC_TEXT,DETAIL_TYPE_CD,DFLT_PROCSYS_ID,DIVN_CD,DOORS_TYPE_CD,DOWN_TIME,DRIVE_TRAIN_CD,ENGINE_TYPE_CD,FABRIC_CD,FUEL_TYPE_CD,GENDER_CD,GTIN,INTEGRATION_ID,INTL_BLD_LANG_CD,INVST_CATG_CD,INVST_OBJ_CD,LEAD_TM,LIFE_CYCLE_CD,LOY_DIST_UOM,LOY_EXP_UOM,LOY_FROMAPRT_CD,LOY_SUG_PTTYPE_ID,LOY_SUG_RPTTYPE_ID,LOY_TOAPRT_CD,MAKE_CD,MODEL,MODEL_CD,MOVEMENT_CLASS,MTBF_UOM_CD,MTTR_UOM_CD,NET_ELMT_TYPE_CD,OBJECTIVE_DESC,ONL_PAGESET_ID,ORDER_CST_CURCY_CD,PART_NUM,PAR_PROD_INT_ID,PLAN_STATUS,PM_DEPT_POSTN_ID,PREAPPR_STATE_ID,PREF_CARRIER_CD,PREF_SHIP_METH_CD,PRICE_TYPE_CD,PROD_ASSEMBLY_LVL,PROD_ATTRIB01_CD,PROD_ATTRIB02_CD,PROD_ATTRIB03_CD,PROD_ATTRIB04_CD,PROD_CATG_CD,PROD_CD,PROD_CLS_NUM,PROD_DIST_CD,PROD_GLOBAL_UID,PROD_IMAGE_ID,PROD_LCYCLE_STATUS,PROD_OPT1_MIX_ID,PROD_OPT2_MIX_ID,PROD_OPTION1_ID,PROD_OPTION2_ID,PROD_SUPPLY_CHAIN,PROD_TYPE_CD,PROFIT_RANK_CD,PROMO_INSTANCE_CD,PROMO_TYPE_CD,PR_CON_ID,PR_EQUIV_PROD_ID,PR_ERNG_ID,PR_FULFL_INVLOC_ID,PR_INDUST_ID,PR_POSTN_ID,PR_PROD_LN_ID,PR_SEASON_ID,PR_SHIP_CARPRIO_ID,PR_SRC_ID,PR_TRGT_MKT_SEG_ID,QUALITY_CD,REASON_TXT,REFERENCE_TYPE_CD,REF_NUMBER_1,REF_NUMBER_2,REF_NUMBER_3,REF_NUMBER_4,REF_NUMBER_5,REGION_ID,REQ_DATA_ID,RISK,ROOT_PROD_ID,RULE_ATTRIB1,RULE_ATTRIB2,RXAVPR_CURCY_CD,SCHEDULE_NUM_CD,SEQ_CD,SERVICE_TERMS,SERV_LENGTH_UOM_CD,SILHOUETTE_CD,STATUS_CD,STRATEGY,STRENGTH,SUB_TYPE,SUB_TYPE_CD,TGT_CUST_TYPE_CD,TGT_REGION_CD,THMBNL_IMAGE_ID,TRANSMISSION_CD,TRGT_MKT_ID,TRIM_CD,TYPE,UOM_CD,VALTN_SYS_ID,VALUE_CLASS,VENDR_OU_ID,VENDR_PART_NUM,VERSION,XA_CLASS_ID,X_GROUP_NAME) 
values ('1-ALYT-TEST-1BBD',to_timestamp('20-MAR-14 11.49.05.000000000 PM','DD-MON-RR HH.MI.SS.FF AM'),'1-36C',to_timestamp('20-MAR-14 11.49.05.000000000 PM','DD-MON-RR HH.MI.SS.FF AM'),'1-36C',0,0,'0','Y','N','N','N','N','Y','1-338','N','N','N','Y','N','Y','Y','N','N','N','N','N','N','N','Electric SDP 2','Y','N','Y','N','N','N','N','N','N','N','N','N','N',1,'Y',null,'Y',null,null,to_timestamp('21-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),'Y','Y',null,to_timestamp('21-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),null,0,null,null,to_timestamp('21-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),null,null,'N',null,null,null,null,null,null,null,null,to_timestamp('21-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),null,null,null,null,null,null,null,null,'N',null,null,null,null,null,null,null,null,null,null,null,null,1,null,to_timestamp('21-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),null,null,'Y','Y',null,null,to_timestamp('21-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),null,null,null,null,null,null,'N',null,null,null,null,null,null,null,null,'USD',null,null,null,'USD',null,null,null,null,'USD',null,'1-1BBD',null,null,null,null,'General',null,null,null,null,'Electric SDP 2',null,'No Match Row Id',null,null,null,null,null,null,null,null,null,'1-1BBD',null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,'USD',null,'No Match Row Id',null,null,null,null,null,'One-Time',null,null,null,null,null,null,'Product',null,null,null,null,null,null,null,null,null,null,'None',null,null,null,'No Match Row Id','No Match Row Id',null,null,null,null,'No Match Row Id',null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,'USD',null,null,null,null,null,null,null,null,'Electric',null,null,null,null,null,null,null,'Service Point','Each',null,null,null,null,null,null,null);

--DistNode
Insert into s_prod_int (ROW_ID,CREATED,CREATED_BY,LAST_UPD,LAST_UPD_BY,DCKING_NUM,MODIFICATION_NUM,CONFLICT_ID,ACTIVE_FLG,ADD_TO_QUOTE_FLG,APPLY_EC_RULE_FLG,APPROVAL_FLG,AUTO_UNGROUP_FLG,BILLABLE_FLG,BU_ID,CG_COMPETITOR_FLG,CMPND_FLG,COMMISIONED_FLG,COMPENSATABLE_FLG,CRT_AGREEMENT_FLG,CRT_AST_REC_FLG,CRT_INST_FLG,DESIGN_REG_FLG,ENTERPRISE_FLG,FEATURED_FLG,INCLSV_ELIG_RL_FLG,INCL_ALL_CRSE_FLG,LEAF_LEVEL_FLG,MODEL_PROD_FLG,NAME,ORDERABLE_FLG,POSTN_BL_PROD_FLG,SALES_PROD_FLG,SALES_SRVC_FLG,SERVICE_FLG,SERV_INST_FLG,SHIP_FLG,TARGET_VRSN_FLG,TAX_SUBCOMP_FLG,TRANSFERABLE_FLG,TWOBARCODES_FLG,UNIQUE_ASSET_FLG,CTRL_SUBS_FLG,ROLLUP_LEVEL,ROLLUP_TRGMKT_FLG,ACCRUAL_RATE,ALC_BELOW_SFTY_FLG,ALOC_ASSETS_FLG,APRX_COST_AMT,APX_CST_EXCH_DT,AUTO_ALLOCATE_FLG,AUTO_SUBST_FLG,AVG_CALL_CST,AVG_CALL_CST_DT,AVG_OP_LIFE,AVG_PROFIT,CAPACITY,CARY_COST,CARY_COST_DT,CASE_CONV_FACTOR,CASE_PACK,CONFIGURATION_FLG,CUTOFF_FLG,DELIV_INTERVAL,DEST_COST,DISP_CMPNT_FLG,DISP_CMPNT_PRI_FLG,EFFICIENCY_RATING,EFF_END_DT,EFF_START_DT,EXCH_DT,EXP_LEAD_DAYS,FRU_FLG,GROSS_MGN,INCL_CRSE_NUM,INVENTORY_FLG,ITEM_SIZE,LOAD_ADDED,LOCK_FLG,LOT_ACTIVE_FLG,LOY_ACTUAL_DIST,LOY_EXP_LEAD_TIME,LOY_SUG_POINTS,LOY_SUG_PRICE,LOY_SUG_R_POINTS,LOY_SUG_R_PRICE,MAX_ORDER_UNITS,MIN_ORDER_UNITS,MODEL_YR,MSRP,MTBF,MTTR,NUM_OCCURRENCE,ORDER_CST,ORDER_CST_DT,PRODUCT_LEVEL,PROD_UPD_DT,REQ_APPL_FLG,REQ_REFERRAL_FLG,RESERVABLE_FLG,RTRN_DEFECTIVE_FLG,RXAVPR_EXCH_DT,RX_AVG_PRICE,SCORE_NUM,SERIALIZED_FLG,SERVICE_LENGTH,SKIP_FLG,SUBSCN_DUR_DAY_NUM,TAXABLE_FLG,TOOL_FLG,UNITS_BCKORD,UNITS_BCKORD_AS_OF,UNITS_INVENT,UNITS_INVENT_AS_OF,UNIT_CONV_FACTOR,ADDL_LEAD_TIME,ALIAS_NAME,APX_CST_CURCY_CD,ASSET_REF_EXPR,ASSOC_LEVEL,ATM_TYPE_CD,AVGCALLCST_CURCYCD,BAR_CODE_NUM,BODY_STYLE_CD,BOOK_APPR_ID,BUILD,CARY_COST_CURCY_CD,CATEGORY_CD,CFG_MODEL_ID,CG_PR_CTLG_CAT_ID,CLASS_PARTNUM_ID,COMMENTS,CONFIG_RULE_FILE,CRITICAL_CD,CS_PATH_ID,CURCY_CD,DATA_SRC,DEF_MOD_PROD_ID,DESC_TEXT,DETAIL_TYPE_CD,DFLT_PROCSYS_ID,DIVN_CD,DOORS_TYPE_CD,DOWN_TIME,DRIVE_TRAIN_CD,ENGINE_TYPE_CD,FABRIC_CD,FUEL_TYPE_CD,GENDER_CD,GTIN,INTEGRATION_ID,INTL_BLD_LANG_CD,INVST_CATG_CD,INVST_OBJ_CD,LEAD_TM,LIFE_CYCLE_CD,LOY_DIST_UOM,LOY_EXP_UOM,LOY_FROMAPRT_CD,LOY_SUG_PTTYPE_ID,LOY_SUG_RPTTYPE_ID,LOY_TOAPRT_CD,MAKE_CD,MODEL,MODEL_CD,MOVEMENT_CLASS,MTBF_UOM_CD,MTTR_UOM_CD,NET_ELMT_TYPE_CD,OBJECTIVE_DESC,ONL_PAGESET_ID,ORDER_CST_CURCY_CD,PART_NUM,PAR_PROD_INT_ID,PLAN_STATUS,PM_DEPT_POSTN_ID,PREAPPR_STATE_ID,PREF_CARRIER_CD,PREF_SHIP_METH_CD,PRICE_TYPE_CD,PROD_ASSEMBLY_LVL,PROD_ATTRIB01_CD,PROD_ATTRIB02_CD,PROD_ATTRIB03_CD,PROD_ATTRIB04_CD,PROD_CATG_CD,PROD_CD,PROD_CLS_NUM,PROD_DIST_CD,PROD_GLOBAL_UID,PROD_IMAGE_ID,PROD_LCYCLE_STATUS,PROD_OPT1_MIX_ID,PROD_OPT2_MIX_ID,PROD_OPTION1_ID,PROD_OPTION2_ID,PROD_SUPPLY_CHAIN,PROD_TYPE_CD,PROFIT_RANK_CD,PROMO_INSTANCE_CD,PROMO_TYPE_CD,PR_CON_ID,PR_EQUIV_PROD_ID,PR_ERNG_ID,PR_FULFL_INVLOC_ID,PR_INDUST_ID,PR_POSTN_ID,PR_PROD_LN_ID,PR_SEASON_ID,PR_SHIP_CARPRIO_ID,PR_SRC_ID,PR_TRGT_MKT_SEG_ID,QUALITY_CD,REASON_TXT,REFERENCE_TYPE_CD,REF_NUMBER_1,REF_NUMBER_2,REF_NUMBER_3,REF_NUMBER_4,REF_NUMBER_5,REGION_ID,REQ_DATA_ID,RISK,ROOT_PROD_ID,RULE_ATTRIB1,RULE_ATTRIB2,RXAVPR_CURCY_CD,SCHEDULE_NUM_CD,SEQ_CD,SERVICE_TERMS,SERV_LENGTH_UOM_CD,SILHOUETTE_CD,STATUS_CD,STRATEGY,STRENGTH,SUB_TYPE,SUB_TYPE_CD,TGT_CUST_TYPE_CD,TGT_REGION_CD,THMBNL_IMAGE_ID,TRANSMISSION_CD,TRGT_MKT_ID,TRIM_CD,TYPE,UOM_CD,VALTN_SYS_ID,VALUE_CLASS,VENDR_OU_ID,VENDR_PART_NUM,VERSION,XA_CLASS_ID,X_GROUP_NAME) 
values ('1-ALYT-TEST-1B1D',to_timestamp('20-MAR-14 11.29.39.000000000 PM','DD-MON-RR HH.MI.SS.FF AM'),'1-36C',to_timestamp('20-MAR-14 11.29.39.000000000 PM','DD-MON-RR HH.MI.SS.FF AM'),'1-36C',0,0,'0','Y','N','N','N','N','Y','1-338','N','N','N','Y','N','Y','Y','N','N','N','N','N','N','N','Generic Substation79','Y','N','Y','N','N','N','N','N','N','N','N','N','N',1,'Y',null,'Y',null,null,to_timestamp('21-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),'Y','Y',null,to_timestamp('21-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),null,0,null,null,to_timestamp('21-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),null,null,'N',null,null,null,null,null,null,null,null,to_timestamp('21-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),null,null,null,null,null,null,null,null,'N',null,null,null,null,null,null,null,null,null,null,null,null,1,null,to_timestamp('21-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),null,null,'Y','Y',null,null,to_timestamp('21-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),null,null,null,null,null,null,'N',null,null,null,null,null,null,null,null,'USD',null,null,null,'USD',null,null,null,null,'USD',null,'1-1B1D',null,null,null,null,'General',null,null,null,null,'Generic Substation79',null,'No Match Row Id',null,null,null,null,null,null,null,null,null,'1-1B1D',null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,'USD',null,'No Match Row Id',null,null,null,null,null,'One-Time',null,null,null,null,null,null,'Product',null,null,null,null,null,null,null,null,null,null,'None',null,null,null,'No Match Row Id','No Match Row Id',null,null,null,null,'No Match Row Id',null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,'USD',null,null,null,null,null,null,null,null,'Substation',null,null,null,null,null,null,null,'Distribution Node','Each',null,null,null,null,null,null,null);

--Meter

--CommFn
Insert into s_prod_int (ROW_ID,CREATED,CREATED_BY,LAST_UPD,LAST_UPD_BY,DCKING_NUM,MODIFICATION_NUM,CONFLICT_ID,ACTIVE_FLG,ADD_TO_QUOTE_FLG,APPLY_EC_RULE_FLG,APPROVAL_FLG,AUTO_UNGROUP_FLG,BILLABLE_FLG,BU_ID,CG_COMPETITOR_FLG,CMPND_FLG,COMMISIONED_FLG,COMPENSATABLE_FLG,CRT_AGREEMENT_FLG,CRT_AST_REC_FLG,CRT_INST_FLG,DESIGN_REG_FLG,ENTERPRISE_FLG,FEATURED_FLG,INCLSV_ELIG_RL_FLG,INCL_ALL_CRSE_FLG,LEAF_LEVEL_FLG,MODEL_PROD_FLG,NAME,ORDERABLE_FLG,POSTN_BL_PROD_FLG,SALES_PROD_FLG,SALES_SRVC_FLG,SERVICE_FLG,SERV_INST_FLG,SHIP_FLG,TARGET_VRSN_FLG,TAX_SUBCOMP_FLG,TRANSFERABLE_FLG,TWOBARCODES_FLG,UNIQUE_ASSET_FLG,CTRL_SUBS_FLG,ROLLUP_LEVEL,ROLLUP_TRGMKT_FLG,ACCRUAL_RATE,ALC_BELOW_SFTY_FLG,ALOC_ASSETS_FLG,APRX_COST_AMT,APX_CST_EXCH_DT,AUTO_ALLOCATE_FLG,AUTO_SUBST_FLG,AVG_CALL_CST,AVG_CALL_CST_DT,AVG_OP_LIFE,AVG_PROFIT,CAPACITY,CARY_COST,CARY_COST_DT,CASE_CONV_FACTOR,CASE_PACK,CONFIGURATION_FLG,CUTOFF_FLG,DELIV_INTERVAL,DEST_COST,DISP_CMPNT_FLG,DISP_CMPNT_PRI_FLG,EFFICIENCY_RATING,EFF_END_DT,EFF_START_DT,EXCH_DT,EXP_LEAD_DAYS,FRU_FLG,GROSS_MGN,INCL_CRSE_NUM,INVENTORY_FLG,ITEM_SIZE,LOAD_ADDED,LOCK_FLG,LOT_ACTIVE_FLG,LOY_ACTUAL_DIST,LOY_EXP_LEAD_TIME,LOY_SUG_POINTS,LOY_SUG_PRICE,LOY_SUG_R_POINTS,LOY_SUG_R_PRICE,MAX_ORDER_UNITS,MIN_ORDER_UNITS,MODEL_YR,MSRP,MTBF,MTTR,NUM_OCCURRENCE,ORDER_CST,ORDER_CST_DT,PRODUCT_LEVEL,PROD_UPD_DT,REQ_APPL_FLG,REQ_REFERRAL_FLG,RESERVABLE_FLG,RTRN_DEFECTIVE_FLG,RXAVPR_EXCH_DT,RX_AVG_PRICE,SCORE_NUM,SERIALIZED_FLG,SERVICE_LENGTH,SKIP_FLG,SUBSCN_DUR_DAY_NUM,TAXABLE_FLG,TOOL_FLG,UNITS_BCKORD,UNITS_BCKORD_AS_OF,UNITS_INVENT,UNITS_INVENT_AS_OF,UNIT_CONV_FACTOR,ADDL_LEAD_TIME,ALIAS_NAME,APX_CST_CURCY_CD,ASSET_REF_EXPR,ASSOC_LEVEL,ATM_TYPE_CD,AVGCALLCST_CURCYCD,BAR_CODE_NUM,BODY_STYLE_CD,BOOK_APPR_ID,BUILD,CARY_COST_CURCY_CD,CATEGORY_CD,CFG_MODEL_ID,CG_PR_CTLG_CAT_ID,CLASS_PARTNUM_ID,COMMENTS,CONFIG_RULE_FILE,CRITICAL_CD,CS_PATH_ID,CURCY_CD,DATA_SRC,DEF_MOD_PROD_ID,DESC_TEXT,DETAIL_TYPE_CD,DFLT_PROCSYS_ID,DIVN_CD,DOORS_TYPE_CD,DOWN_TIME,DRIVE_TRAIN_CD,ENGINE_TYPE_CD,FABRIC_CD,FUEL_TYPE_CD,GENDER_CD,GTIN,INTEGRATION_ID,INTL_BLD_LANG_CD,INVST_CATG_CD,INVST_OBJ_CD,LEAD_TM,LIFE_CYCLE_CD,LOY_DIST_UOM,LOY_EXP_UOM,LOY_FROMAPRT_CD,LOY_SUG_PTTYPE_ID,LOY_SUG_RPTTYPE_ID,LOY_TOAPRT_CD,MAKE_CD,MODEL,MODEL_CD,MOVEMENT_CLASS,MTBF_UOM_CD,MTTR_UOM_CD,NET_ELMT_TYPE_CD,OBJECTIVE_DESC,ONL_PAGESET_ID,ORDER_CST_CURCY_CD,PART_NUM,PAR_PROD_INT_ID,PLAN_STATUS,PM_DEPT_POSTN_ID,PREAPPR_STATE_ID,PREF_CARRIER_CD,PREF_SHIP_METH_CD,PRICE_TYPE_CD,PROD_ASSEMBLY_LVL,PROD_ATTRIB01_CD,PROD_ATTRIB02_CD,PROD_ATTRIB03_CD,PROD_ATTRIB04_CD,PROD_CATG_CD,PROD_CD,PROD_CLS_NUM,PROD_DIST_CD,PROD_GLOBAL_UID,PROD_IMAGE_ID,PROD_LCYCLE_STATUS,PROD_OPT1_MIX_ID,PROD_OPT2_MIX_ID,PROD_OPTION1_ID,PROD_OPTION2_ID,PROD_SUPPLY_CHAIN,PROD_TYPE_CD,PROFIT_RANK_CD,PROMO_INSTANCE_CD,PROMO_TYPE_CD,PR_CON_ID,PR_EQUIV_PROD_ID,PR_ERNG_ID,PR_FULFL_INVLOC_ID,PR_INDUST_ID,PR_POSTN_ID,PR_PROD_LN_ID,PR_SEASON_ID,PR_SHIP_CARPRIO_ID,PR_SRC_ID,PR_TRGT_MKT_SEG_ID,QUALITY_CD,REASON_TXT,REFERENCE_TYPE_CD,REF_NUMBER_1,REF_NUMBER_2,REF_NUMBER_3,REF_NUMBER_4,REF_NUMBER_5,REGION_ID,REQ_DATA_ID,RISK,ROOT_PROD_ID,RULE_ATTRIB1,RULE_ATTRIB2,RXAVPR_CURCY_CD,SCHEDULE_NUM_CD,SEQ_CD,SERVICE_TERMS,SERV_LENGTH_UOM_CD,SILHOUETTE_CD,STATUS_CD,STRATEGY,STRENGTH,SUB_TYPE,SUB_TYPE_CD,TGT_CUST_TYPE_CD,TGT_REGION_CD,THMBNL_IMAGE_ID,TRANSMISSION_CD,TRGT_MKT_ID,TRIM_CD,TYPE,UOM_CD,VALTN_SYS_ID,VALUE_CLASS,VENDR_OU_ID,VENDR_PART_NUM,VERSION,XA_CLASS_ID,X_GROUP_NAME) 
values ('1-ALYT-TEST-7CH',to_timestamp('18-MAR-14 05.03.40.000000000 PM','DD-MON-RR HH.MI.SS.FF AM'),'1-36C',to_timestamp('18-MAR-14 05.03.40.000000000 PM','DD-MON-RR HH.MI.SS.FF AM'),'1-36C',0,0,'0','Y','N','N','N','N','Y','1-338','N','N','N','Y','N','Y','Y','N','N','N','N','N','N','N','GD EA300508 Phone Meter Module','Y','N','Y','N','N','N','N','N','N','N','N','N','N',1,'Y',null,'Y',null,null,to_timestamp('18-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),'Y','Y',null,to_timestamp('18-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),null,0,null,null,to_timestamp('18-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),null,null,'N',null,null,null,null,null,null,null,null,to_timestamp('18-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),null,null,null,null,null,null,null,null,'N',null,null,null,null,null,null,null,null,null,null,null,null,1,null,to_timestamp('18-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),null,null,'Y','Y',null,null,to_timestamp('18-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),null,null,null,null,null,null,'N',null,null,null,null,null,null,null,null,'USD',null,null,null,'USD',null,null,null,null,'USD',null,'1-7CH',null,null,null,null,'General',null,null,null,null,'GD, EA300508 Global Data Cellular Modem Communication Module',null,'No Match Row Id',null,null,null,null,null,null,null,null,null,'1-7CH',null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,'USD',null,'No Match Row Id',null,null,null,null,null,'One-Time',null,null,null,null,null,null,'Product',null,null,null,null,null,null,null,null,null,null,'None',null,null,null,'No Match Row Id','No Match Row Id',null,null,null,null,'No Match Row Id',null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,'USD',null,null,null,null,null,null,null,null,'GD Meter Module',null,null,null,null,null,null,null,'Communication Module','Each',null,null,null,null,null,null,null);

--CT-PT
Insert into s_prod_int (ROW_ID,CREATED,CREATED_BY,LAST_UPD,LAST_UPD_BY,DCKING_NUM,MODIFICATION_NUM,CONFLICT_ID,ACTIVE_FLG,ADD_TO_QUOTE_FLG,APPLY_EC_RULE_FLG,APPROVAL_FLG,AUTO_UNGROUP_FLG,BILLABLE_FLG,BU_ID,CG_COMPETITOR_FLG,CMPND_FLG,COMMISIONED_FLG,COMPENSATABLE_FLG,CRT_AGREEMENT_FLG,CRT_AST_REC_FLG,CRT_INST_FLG,DESIGN_REG_FLG,ENTERPRISE_FLG,FEATURED_FLG,INCLSV_ELIG_RL_FLG,INCL_ALL_CRSE_FLG,LEAF_LEVEL_FLG,MODEL_PROD_FLG,NAME,ORDERABLE_FLG,POSTN_BL_PROD_FLG,SALES_PROD_FLG,SALES_SRVC_FLG,SERVICE_FLG,SERV_INST_FLG,SHIP_FLG,TARGET_VRSN_FLG,TAX_SUBCOMP_FLG,TRANSFERABLE_FLG,TWOBARCODES_FLG,UNIQUE_ASSET_FLG,CTRL_SUBS_FLG,ROLLUP_LEVEL,ROLLUP_TRGMKT_FLG,ACCRUAL_RATE,ALC_BELOW_SFTY_FLG,ALOC_ASSETS_FLG,APRX_COST_AMT,APX_CST_EXCH_DT,AUTO_ALLOCATE_FLG,AUTO_SUBST_FLG,AVG_CALL_CST,AVG_CALL_CST_DT,AVG_OP_LIFE,AVG_PROFIT,CAPACITY,CARY_COST,CARY_COST_DT,CASE_CONV_FACTOR,CASE_PACK,CONFIGURATION_FLG,CUTOFF_FLG,DELIV_INTERVAL,DEST_COST,DISP_CMPNT_FLG,DISP_CMPNT_PRI_FLG,EFFICIENCY_RATING,EFF_END_DT,EFF_START_DT,EXCH_DT,EXP_LEAD_DAYS,FRU_FLG,GROSS_MGN,INCL_CRSE_NUM,INVENTORY_FLG,ITEM_SIZE,LOAD_ADDED,LOCK_FLG,LOT_ACTIVE_FLG,LOY_ACTUAL_DIST,LOY_EXP_LEAD_TIME,LOY_SUG_POINTS,LOY_SUG_PRICE,LOY_SUG_R_POINTS,LOY_SUG_R_PRICE,MAX_ORDER_UNITS,MIN_ORDER_UNITS,MODEL_YR,MSRP,MTBF,MTTR,NUM_OCCURRENCE,ORDER_CST,ORDER_CST_DT,PRODUCT_LEVEL,PROD_UPD_DT,REQ_APPL_FLG,REQ_REFERRAL_FLG,RESERVABLE_FLG,RTRN_DEFECTIVE_FLG,RXAVPR_EXCH_DT,RX_AVG_PRICE,SCORE_NUM,SERIALIZED_FLG,SERVICE_LENGTH,SKIP_FLG,SUBSCN_DUR_DAY_NUM,TAXABLE_FLG,TOOL_FLG,UNITS_BCKORD,UNITS_BCKORD_AS_OF,UNITS_INVENT,UNITS_INVENT_AS_OF,UNIT_CONV_FACTOR,ADDL_LEAD_TIME,ALIAS_NAME,APX_CST_CURCY_CD,ASSET_REF_EXPR,ASSOC_LEVEL,ATM_TYPE_CD,AVGCALLCST_CURCYCD,BAR_CODE_NUM,BODY_STYLE_CD,BOOK_APPR_ID,BUILD,CARY_COST_CURCY_CD,CATEGORY_CD,CFG_MODEL_ID,CG_PR_CTLG_CAT_ID,CLASS_PARTNUM_ID,COMMENTS,CONFIG_RULE_FILE,CRITICAL_CD,CS_PATH_ID,CURCY_CD,DATA_SRC,DEF_MOD_PROD_ID,DESC_TEXT,DETAIL_TYPE_CD,DFLT_PROCSYS_ID,DIVN_CD,DOORS_TYPE_CD,DOWN_TIME,DRIVE_TRAIN_CD,ENGINE_TYPE_CD,FABRIC_CD,FUEL_TYPE_CD,GENDER_CD,GTIN,INTEGRATION_ID,INTL_BLD_LANG_CD,INVST_CATG_CD,INVST_OBJ_CD,LEAD_TM,LIFE_CYCLE_CD,LOY_DIST_UOM,LOY_EXP_UOM,LOY_FROMAPRT_CD,LOY_SUG_PTTYPE_ID,LOY_SUG_RPTTYPE_ID,LOY_TOAPRT_CD,MAKE_CD,MODEL,MODEL_CD,MOVEMENT_CLASS,MTBF_UOM_CD,MTTR_UOM_CD,NET_ELMT_TYPE_CD,OBJECTIVE_DESC,ONL_PAGESET_ID,ORDER_CST_CURCY_CD,PART_NUM,PAR_PROD_INT_ID,PLAN_STATUS,PM_DEPT_POSTN_ID,PREAPPR_STATE_ID,PREF_CARRIER_CD,PREF_SHIP_METH_CD,PRICE_TYPE_CD,PROD_ASSEMBLY_LVL,PROD_ATTRIB01_CD,PROD_ATTRIB02_CD,PROD_ATTRIB03_CD,PROD_ATTRIB04_CD,PROD_CATG_CD,PROD_CD,PROD_CLS_NUM,PROD_DIST_CD,PROD_GLOBAL_UID,PROD_IMAGE_ID,PROD_LCYCLE_STATUS,PROD_OPT1_MIX_ID,PROD_OPT2_MIX_ID,PROD_OPTION1_ID,PROD_OPTION2_ID,PROD_SUPPLY_CHAIN,PROD_TYPE_CD,PROFIT_RANK_CD,PROMO_INSTANCE_CD,PROMO_TYPE_CD,PR_CON_ID,PR_EQUIV_PROD_ID,PR_ERNG_ID,PR_FULFL_INVLOC_ID,PR_INDUST_ID,PR_POSTN_ID,PR_PROD_LN_ID,PR_SEASON_ID,PR_SHIP_CARPRIO_ID,PR_SRC_ID,PR_TRGT_MKT_SEG_ID,QUALITY_CD,REASON_TXT,REFERENCE_TYPE_CD,REF_NUMBER_1,REF_NUMBER_2,REF_NUMBER_3,REF_NUMBER_4,REF_NUMBER_5,REGION_ID,REQ_DATA_ID,RISK,ROOT_PROD_ID,RULE_ATTRIB1,RULE_ATTRIB2,RXAVPR_CURCY_CD,SCHEDULE_NUM_CD,SEQ_CD,SERVICE_TERMS,SERV_LENGTH_UOM_CD,SILHOUETTE_CD,STATUS_CD,STRATEGY,STRENGTH,SUB_TYPE,SUB_TYPE_CD,TGT_CUST_TYPE_CD,TGT_REGION_CD,THMBNL_IMAGE_ID,TRANSMISSION_CD,TRGT_MKT_ID,TRIM_CD,TYPE,UOM_CD,VALTN_SYS_ID,VALUE_CLASS,VENDR_OU_ID,VENDR_PART_NUM,VERSION,XA_CLASS_ID,X_GROUP_NAME) 
values ('1-ALYT-TEST-98U',to_timestamp('18-MAR-14 05.05.00.000000000 PM','DD-MON-RR HH.MI.SS.FF AM'),'1-36C',to_timestamp('18-MAR-14 05.05.00.000000000 PM','DD-MON-RR HH.MI.SS.FF AM'),'1-36C',0,0,'0','Y','N','N','N','N','Y','1-338','N','N','N','Y','N','Y','Y','N','N','N','N','N','N','N','Generic CT/PT, Multiplier 75','Y','N','Y','N','N','N','N','N','N','N','N','N','N',1,'Y',null,'Y',null,null,to_timestamp('18-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),'Y','Y',null,to_timestamp('18-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),null,0,null,null,to_timestamp('18-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),null,null,'N',null,null,null,null,null,null,null,null,to_timestamp('18-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),null,null,null,null,null,null,null,null,'N',null,null,null,null,null,null,null,null,null,null,null,null,1,null,to_timestamp('18-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),null,null,'Y','Y',null,null,to_timestamp('18-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),null,null,null,null,null,null,'N',null,null,null,null,null,null,null,null,'USD',null,null,null,'USD',null,null,null,null,'USD',null,'1-98U',null,null,null,null,'General',null,null,null,null,'Generic CT/PT, Multiplier 75',null,'No Match Row Id',null,null,null,null,null,null,null,null,null,'1-98U',null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,'USD',null,'No Match Row Id',null,null,null,null,null,'One-Time',null,null,null,null,null,null,'Product',null,null,null,null,null,null,null,null,null,null,'None',null,null,null,'No Match Row Id','No Match Row Id',null,null,null,null,'No Match Row Id',null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,'USD',null,null,null,null,null,null,null,null,'CT-PT',null,null,null,null,null,null,null,'CT-PT','Each',null,null,null,null,null,null,null);

--Measurement
Insert into s_prod_int (ROW_ID,CREATED,CREATED_BY,LAST_UPD,LAST_UPD_BY,DCKING_NUM,MODIFICATION_NUM,CONFLICT_ID,ACTIVE_FLG,ADD_TO_QUOTE_FLG,APPLY_EC_RULE_FLG,APPROVAL_FLG,AUTO_UNGROUP_FLG,BILLABLE_FLG,BU_ID,CG_COMPETITOR_FLG,CMPND_FLG,COMMISIONED_FLG,COMPENSATABLE_FLG,CRT_AGREEMENT_FLG,CRT_AST_REC_FLG,CRT_INST_FLG,DESIGN_REG_FLG,ENTERPRISE_FLG,FEATURED_FLG,INCLSV_ELIG_RL_FLG,INCL_ALL_CRSE_FLG,LEAF_LEVEL_FLG,MODEL_PROD_FLG,NAME,ORDERABLE_FLG,POSTN_BL_PROD_FLG,SALES_PROD_FLG,SALES_SRVC_FLG,SERVICE_FLG,SERV_INST_FLG,SHIP_FLG,TARGET_VRSN_FLG,TAX_SUBCOMP_FLG,TRANSFERABLE_FLG,TWOBARCODES_FLG,UNIQUE_ASSET_FLG,CTRL_SUBS_FLG,ROLLUP_LEVEL,ROLLUP_TRGMKT_FLG,ACCRUAL_RATE,ALC_BELOW_SFTY_FLG,ALOC_ASSETS_FLG,APRX_COST_AMT,APX_CST_EXCH_DT,AUTO_ALLOCATE_FLG,AUTO_SUBST_FLG,AVG_CALL_CST,AVG_CALL_CST_DT,AVG_OP_LIFE,AVG_PROFIT,CAPACITY,CARY_COST,CARY_COST_DT,CASE_CONV_FACTOR,CASE_PACK,CONFIGURATION_FLG,CUTOFF_FLG,DELIV_INTERVAL,DEST_COST,DISP_CMPNT_FLG,DISP_CMPNT_PRI_FLG,EFFICIENCY_RATING,EFF_END_DT,EFF_START_DT,EXCH_DT,EXP_LEAD_DAYS,FRU_FLG,GROSS_MGN,INCL_CRSE_NUM,INVENTORY_FLG,ITEM_SIZE,LOAD_ADDED,LOCK_FLG,LOT_ACTIVE_FLG,LOY_ACTUAL_DIST,LOY_EXP_LEAD_TIME,LOY_SUG_POINTS,LOY_SUG_PRICE,LOY_SUG_R_POINTS,LOY_SUG_R_PRICE,MAX_ORDER_UNITS,MIN_ORDER_UNITS,MODEL_YR,MSRP,MTBF,MTTR,NUM_OCCURRENCE,ORDER_CST,ORDER_CST_DT,PRODUCT_LEVEL,PROD_UPD_DT,REQ_APPL_FLG,REQ_REFERRAL_FLG,RESERVABLE_FLG,RTRN_DEFECTIVE_FLG,RXAVPR_EXCH_DT,RX_AVG_PRICE,SCORE_NUM,SERIALIZED_FLG,SERVICE_LENGTH,SKIP_FLG,SUBSCN_DUR_DAY_NUM,TAXABLE_FLG,TOOL_FLG,UNITS_BCKORD,UNITS_BCKORD_AS_OF,UNITS_INVENT,UNITS_INVENT_AS_OF,UNIT_CONV_FACTOR,ADDL_LEAD_TIME,ALIAS_NAME,APX_CST_CURCY_CD,ASSET_REF_EXPR,ASSOC_LEVEL,ATM_TYPE_CD,AVGCALLCST_CURCYCD,BAR_CODE_NUM,BODY_STYLE_CD,BOOK_APPR_ID,BUILD,CARY_COST_CURCY_CD,CATEGORY_CD,CFG_MODEL_ID,CG_PR_CTLG_CAT_ID,CLASS_PARTNUM_ID,COMMENTS,CONFIG_RULE_FILE,CRITICAL_CD,CS_PATH_ID,CURCY_CD,DATA_SRC,DEF_MOD_PROD_ID,DESC_TEXT,DETAIL_TYPE_CD,DFLT_PROCSYS_ID,DIVN_CD,DOORS_TYPE_CD,DOWN_TIME,DRIVE_TRAIN_CD,ENGINE_TYPE_CD,FABRIC_CD,FUEL_TYPE_CD,GENDER_CD,GTIN,INTEGRATION_ID,INTL_BLD_LANG_CD,INVST_CATG_CD,INVST_OBJ_CD,LEAD_TM,LIFE_CYCLE_CD,LOY_DIST_UOM,LOY_EXP_UOM,LOY_FROMAPRT_CD,LOY_SUG_PTTYPE_ID,LOY_SUG_RPTTYPE_ID,LOY_TOAPRT_CD,MAKE_CD,MODEL,MODEL_CD,MOVEMENT_CLASS,MTBF_UOM_CD,MTTR_UOM_CD,NET_ELMT_TYPE_CD,OBJECTIVE_DESC,ONL_PAGESET_ID,ORDER_CST_CURCY_CD,PART_NUM,PAR_PROD_INT_ID,PLAN_STATUS,PM_DEPT_POSTN_ID,PREAPPR_STATE_ID,PREF_CARRIER_CD,PREF_SHIP_METH_CD,PRICE_TYPE_CD,PROD_ASSEMBLY_LVL,PROD_ATTRIB01_CD,PROD_ATTRIB02_CD,PROD_ATTRIB03_CD,PROD_ATTRIB04_CD,PROD_CATG_CD,PROD_CD,PROD_CLS_NUM,PROD_DIST_CD,PROD_GLOBAL_UID,PROD_IMAGE_ID,PROD_LCYCLE_STATUS,PROD_OPT1_MIX_ID,PROD_OPT2_MIX_ID,PROD_OPTION1_ID,PROD_OPTION2_ID,PROD_SUPPLY_CHAIN,PROD_TYPE_CD,PROFIT_RANK_CD,PROMO_INSTANCE_CD,PROMO_TYPE_CD,PR_CON_ID,PR_EQUIV_PROD_ID,PR_ERNG_ID,PR_FULFL_INVLOC_ID,PR_INDUST_ID,PR_POSTN_ID,PR_PROD_LN_ID,PR_SEASON_ID,PR_SHIP_CARPRIO_ID,PR_SRC_ID,PR_TRGT_MKT_SEG_ID,QUALITY_CD,REASON_TXT,REFERENCE_TYPE_CD,REF_NUMBER_1,REF_NUMBER_2,REF_NUMBER_3,REF_NUMBER_4,REF_NUMBER_5,REGION_ID,REQ_DATA_ID,RISK,ROOT_PROD_ID,RULE_ATTRIB1,RULE_ATTRIB2,RXAVPR_CURCY_CD,SCHEDULE_NUM_CD,SEQ_CD,SERVICE_TERMS,SERV_LENGTH_UOM_CD,SILHOUETTE_CD,STATUS_CD,STRATEGY,STRENGTH,SUB_TYPE,SUB_TYPE_CD,TGT_CUST_TYPE_CD,TGT_REGION_CD,THMBNL_IMAGE_ID,TRANSMISSION_CD,TRGT_MKT_ID,TRIM_CD,TYPE,UOM_CD,VALTN_SYS_ID,VALUE_CLASS,VENDR_OU_ID,VENDR_PART_NUM,VERSION,XA_CLASS_ID,X_GROUP_NAME) 
values ('1-ALYT-TEST-QOS',to_timestamp('18-MAR-14 05.21.09.000000000 PM','DD-MON-RR HH.MI.SS.FF AM'),'1-36C',to_timestamp('18-MAR-14 05.21.09.000000000 PM','DD-MON-RR HH.MI.SS.FF AM'),'1-36C',0,0,'0','Y','N','N','N','N','Y','1-338','N','N','N','Y','N','Y','Y','N','N','N','N','N','N','N','TNS S4 Max KW Demand','Y','N','Y','N','N','N','N','N','N','N','N','N','N',1,'Y',null,'Y',null,null,to_timestamp('18-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),'Y','Y',null,to_timestamp('18-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),null,0,null,null,to_timestamp('18-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),null,null,'N',null,null,null,null,null,null,null,null,to_timestamp('18-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),null,null,null,null,null,null,null,null,'N',null,null,null,null,null,null,null,null,null,null,null,null,1,null,to_timestamp('18-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),null,null,'Y','Y',null,null,to_timestamp('18-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),null,null,null,null,null,null,'N',null,null,null,null,null,null,null,null,'USD',null,null,null,'USD',null,null,null,null,'USD',null,'1-QOS',null,null,null,null,'General',null,null,null,null,'TNS S4 Max KW Demand',null,'No Match Row Id',null,null,null,null,null,null,null,null,null,'1-QOS',null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,'USD',null,'No Match Row Id',null,null,null,null,null,'One-Time',null,null,null,null,null,null,'Product',null,null,null,null,null,null,null,null,null,null,'None',null,null,null,'No Match Row Id','No Match Row Id',null,null,null,null,'No Match Row Id',null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,'USD',null,null,null,null,null,null,null,null,'DemandPeak',null,null,null,null,null,null,null,'Measurement','KW',null,null,null,null,null,null,null);

--service
Insert into s_prod_int (ROW_ID,CREATED,CREATED_BY,LAST_UPD,LAST_UPD_BY,DCKING_NUM,MODIFICATION_NUM,CONFLICT_ID,ACTIVE_FLG,ADD_TO_QUOTE_FLG,APPLY_EC_RULE_FLG,APPROVAL_FLG,AUTO_UNGROUP_FLG,BILLABLE_FLG,BU_ID,CG_COMPETITOR_FLG,CMPND_FLG,COMMISIONED_FLG,COMPENSATABLE_FLG,CRT_AGREEMENT_FLG,CRT_AST_REC_FLG,CRT_INST_FLG,DESIGN_REG_FLG,ENTERPRISE_FLG,FEATURED_FLG,INCLSV_ELIG_RL_FLG,INCL_ALL_CRSE_FLG,LEAF_LEVEL_FLG,MODEL_PROD_FLG,NAME,ORDERABLE_FLG,POSTN_BL_PROD_FLG,SALES_PROD_FLG,SALES_SRVC_FLG,SERVICE_FLG,SERV_INST_FLG,SHIP_FLG,TARGET_VRSN_FLG,TAX_SUBCOMP_FLG,TRANSFERABLE_FLG,TWOBARCODES_FLG,UNIQUE_ASSET_FLG,CTRL_SUBS_FLG,ROLLUP_LEVEL,ROLLUP_TRGMKT_FLG,ACCRUAL_RATE,ALC_BELOW_SFTY_FLG,ALOC_ASSETS_FLG,APRX_COST_AMT,APX_CST_EXCH_DT,AUTO_ALLOCATE_FLG,AUTO_SUBST_FLG,AVG_CALL_CST,AVG_CALL_CST_DT,AVG_OP_LIFE,AVG_PROFIT,CAPACITY,CARY_COST,CARY_COST_DT,CASE_CONV_FACTOR,CASE_PACK,CONFIGURATION_FLG,CUTOFF_FLG,DELIV_INTERVAL,DEST_COST,DISP_CMPNT_FLG,DISP_CMPNT_PRI_FLG,EFFICIENCY_RATING,EFF_END_DT,EFF_START_DT,EXCH_DT,EXP_LEAD_DAYS,FRU_FLG,GROSS_MGN,INCL_CRSE_NUM,INVENTORY_FLG,ITEM_SIZE,LOAD_ADDED,LOCK_FLG,LOT_ACTIVE_FLG,LOY_ACTUAL_DIST,LOY_EXP_LEAD_TIME,LOY_SUG_POINTS,LOY_SUG_PRICE,LOY_SUG_R_POINTS,LOY_SUG_R_PRICE,MAX_ORDER_UNITS,MIN_ORDER_UNITS,MODEL_YR,MSRP,MTBF,MTTR,NUM_OCCURRENCE,ORDER_CST,ORDER_CST_DT,PRODUCT_LEVEL,PROD_UPD_DT,REQ_APPL_FLG,REQ_REFERRAL_FLG,RESERVABLE_FLG,RTRN_DEFECTIVE_FLG,RXAVPR_EXCH_DT,RX_AVG_PRICE,SCORE_NUM,SERIALIZED_FLG,SERVICE_LENGTH,SKIP_FLG,SUBSCN_DUR_DAY_NUM,TAXABLE_FLG,TOOL_FLG,UNITS_BCKORD,UNITS_BCKORD_AS_OF,UNITS_INVENT,UNITS_INVENT_AS_OF,UNIT_CONV_FACTOR,ADDL_LEAD_TIME,ALIAS_NAME,APX_CST_CURCY_CD,ASSET_REF_EXPR,ASSOC_LEVEL,ATM_TYPE_CD,AVGCALLCST_CURCYCD,BAR_CODE_NUM,BODY_STYLE_CD,BOOK_APPR_ID,BUILD,CARY_COST_CURCY_CD,CATEGORY_CD,CFG_MODEL_ID,CG_PR_CTLG_CAT_ID,CLASS_PARTNUM_ID,COMMENTS,CONFIG_RULE_FILE,CRITICAL_CD,CS_PATH_ID,CURCY_CD,DATA_SRC,DEF_MOD_PROD_ID,DESC_TEXT,DETAIL_TYPE_CD,DFLT_PROCSYS_ID,DIVN_CD,DOORS_TYPE_CD,DOWN_TIME,DRIVE_TRAIN_CD,ENGINE_TYPE_CD,FABRIC_CD,FUEL_TYPE_CD,GENDER_CD,GTIN,INTEGRATION_ID,INTL_BLD_LANG_CD,INVST_CATG_CD,INVST_OBJ_CD,LEAD_TM,LIFE_CYCLE_CD,LOY_DIST_UOM,LOY_EXP_UOM,LOY_FROMAPRT_CD,LOY_SUG_PTTYPE_ID,LOY_SUG_RPTTYPE_ID,LOY_TOAPRT_CD,MAKE_CD,MODEL,MODEL_CD,MOVEMENT_CLASS,MTBF_UOM_CD,MTTR_UOM_CD,NET_ELMT_TYPE_CD,OBJECTIVE_DESC,ONL_PAGESET_ID,ORDER_CST_CURCY_CD,PART_NUM,PAR_PROD_INT_ID,PLAN_STATUS,PM_DEPT_POSTN_ID,PREAPPR_STATE_ID,PREF_CARRIER_CD,PREF_SHIP_METH_CD,PRICE_TYPE_CD,PROD_ASSEMBLY_LVL,PROD_ATTRIB01_CD,PROD_ATTRIB02_CD,PROD_ATTRIB03_CD,PROD_ATTRIB04_CD,PROD_CATG_CD,PROD_CD,PROD_CLS_NUM,PROD_DIST_CD,PROD_GLOBAL_UID,PROD_IMAGE_ID,PROD_LCYCLE_STATUS,PROD_OPT1_MIX_ID,PROD_OPT2_MIX_ID,PROD_OPTION1_ID,PROD_OPTION2_ID,PROD_SUPPLY_CHAIN,PROD_TYPE_CD,PROFIT_RANK_CD,PROMO_INSTANCE_CD,PROMO_TYPE_CD,PR_CON_ID,PR_EQUIV_PROD_ID,PR_ERNG_ID,PR_FULFL_INVLOC_ID,PR_INDUST_ID,PR_POSTN_ID,PR_PROD_LN_ID,PR_SEASON_ID,PR_SHIP_CARPRIO_ID,PR_SRC_ID,PR_TRGT_MKT_SEG_ID,QUALITY_CD,REASON_TXT,REFERENCE_TYPE_CD,REF_NUMBER_1,REF_NUMBER_2,REF_NUMBER_3,REF_NUMBER_4,REF_NUMBER_5,REGION_ID,REQ_DATA_ID,RISK,ROOT_PROD_ID,RULE_ATTRIB1,RULE_ATTRIB2,RXAVPR_CURCY_CD,SCHEDULE_NUM_CD,SEQ_CD,SERVICE_TERMS,SERV_LENGTH_UOM_CD,SILHOUETTE_CD,STATUS_CD,STRATEGY,STRENGTH,SUB_TYPE,SUB_TYPE_CD,TGT_CUST_TYPE_CD,TGT_REGION_CD,THMBNL_IMAGE_ID,TRANSMISSION_CD,TRGT_MKT_ID,TRIM_CD,TYPE,UOM_CD,VALTN_SYS_ID,VALUE_CLASS,VENDR_OU_ID,VENDR_PART_NUM,VERSION,XA_CLASS_ID,X_GROUP_NAME) 
values ('1-ALYT-TEST-9P9',to_timestamp('18-MAR-14 05.05.17.000000000 PM','DD-MON-RR HH.MI.SS.FF AM'),'1-36C',to_timestamp('18-MAR-14 05.05.17.000000000 PM','DD-MON-RR HH.MI.SS.FF AM'),'1-36C',0,0,'0','Y','N','N','N','N','Y','1-338','N','N','N','Y','N','Y','Y','N','N','N','N','N','N','N','Data Collection, Daily, CC','Y','N','Y','N','N','N','N','N','N','N','N','N','N',1,'Y',null,'Y',null,null,to_timestamp('18-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),'Y','Y',null,to_timestamp('18-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),null,0,null,null,to_timestamp('18-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),null,null,'N',null,null,null,null,null,null,null,null,to_timestamp('18-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),null,null,null,null,null,null,null,null,'N',null,null,null,null,null,null,null,null,null,null,null,null,1,null,to_timestamp('18-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),null,null,'Y','Y',null,null,to_timestamp('18-MAR-14 12.00.00.000000000 AM','DD-MON-RR HH.MI.SS.FF AM'),null,null,null,null,null,null,'N',null,null,null,null,null,null,null,null,'USD',null,null,null,'USD',null,null,null,null,'USD',null,'1-9P9',null,null,null,null,'General',null,null,null,null,'Data Collection, Daily, CC',null,'No Match Row Id',null,null,null,null,null,null,null,null,null,'1-9P9',null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,'USD',null,'No Match Row Id',null,null,null,null,null,'One-Time',null,null,null,null,null,null,'Product',null,null,null,null,null,null,null,null,null,null,'None',null,null,null,'No Match Row Id','No Match Row Id',null,null,null,null,'No Match Row Id',null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,'USD',null,null,null,null,null,null,null,null,'Data Collection',null,null,null,null,null,null,null,'Service','Each',null,null,null,null,null,null,null);

*****************************************************************

Generic CT/PT,Multiplier 15
Elster Meter Module
Data Collection, Daily, FlexNet
Generic Substation Distribution Node
Sample Electric Service Point


Data flow:
Output: 
Loader:
7x_register_index


drop table cx_country_mapping;
drop table cx_state_prov_mapping;
drop table cx_timezone_mapping;


Delete from s_prod_int where row_id like '1-ALYT- ';
Delete from s_asset where row_id like '1-ALYT- ';
Delete from s_addr_per where row_id like '1-ALYT- ';
commit;


Insert into s_asset (ROW_ID,CREATED,CREATED_BY,LAST_UPD,LAST_UPD_BY,MODIFICATION_NUM,CONFLICT_ID,ASSET_NUM,AUTO_SR_FLG,BU_ID,COMP_UND_WRNTY_FLG,CUSTOMIZABLE_FLG,DMNDNG_DR_FLG,FLEET_FLG,HH_CONSUMED_FLG,PROD_UND_WRNTY_FLG,REFERENCEABLE_FLG,SRL_NUM_VRFD_FLG,TEST_ASSET_FLG,UND_MFG_WRNTY_FLG,BILLPAY_FLG,ALT_FUEL_FLG,ANNUAL_REVN,ANNUAL_USG,APR,APY,ARCHIVE_FLG,ASGN_DT,ASGN_USR_EXCLD_FLG,ASSET_VALUE_AMT,ASSET_VAL_EXCH_DT,AVAIL_BAL,AVERAGE_BAL,CAPACITY,CAUTION_FLG,CFG_VALDN_STAT_DT,CURRENT_BAL,CUTOFF_FLG,DLR_INV_PRICE,EFFICIENCY_RATING,END_DT,EXCHG_DT,EXCH_DATE,EXTD_QTY,EXTENDED_QTY,INCEPTION_DT,INSTALL_DT,INTERNAL_ASSET_FLG,INTR_RATE,LAST_DOWNLOAD_DT,LAST_TEST_DT,LCNS_EXP_DT,LOAD_ADDED,METER_CNT,MFGD_DT,MODEL_YR,MSRP,ORIGINAL_COST,PREMIUM,PREMIUM_DT,PROFIT,PR_REP_DNRM_FLG,PR_REP_MANL_FLG,PR_REP_SYS_FLG,PURCH_ADJ_UNIT_PRI,PURCH_DT,PURCH_LST_UNIT_PRI,QTY,REGISTERED_DT,REL_LIMIT,REVENUE,RPLCMNT_VAL_AMT,SHIP_DT,START_DT,STATUS_CHG_DT,SVC_PER_PROD_AMT,UNINSTALL_DT,WARRANTY_END_DT,WARRANTY_START_DT,WRNTY_LAST_UPD_DT,AAGSVC_CON_ID,ALIAS_NAME,ASSEMBLY_PORT_ID,ASSET_COND_CD,ASSET_IMG_ID,ASSET_VAL_CURCY_CD,BAR_CODE_NUM,BASE_CURRENCY_CD,BILLACCT_ID,BILL_ACCNT_ID,BILL_STATUS_CD,BL_ADDR_ID,BODY_STYLE_CD,BRANCH_ID,BUILD,CAPITALIZED_CD,CASE_ID,CFG_STATE_CD,CFG_TYPE_CD,CFG_VALDN_STAT_CD,CHANNEL_TYPE_CD,COMMENTS,COMPANY_CD,CONDITION_CD,COST_LST_ID,CURCY_CD,CUR_AGREE_ID,DATA_SRC,DESC_TEXT,DLR_ID,DOORS_TYPE_CD,DRIVE_TRAIN_CD,ENGINE_TYPE_CD,EXTENSION_CD,EXT_COLOR_CD,FLEET_OU_ID,FLT_TYPE_CD,FUEL_CD,HLTHACCNT_TYPE_CD,INSCLM_EVT_ID,INTEGRATION_ID,INT_COLOR_CD,INVLOC_ID,LCNS_NUM,LCNS_STATE_CD,LOAD_PERIOD_CD,LOB_CD,LOT_NUM,MAKE_CD,METER_LOC,METER_TYPE_CD,METER_USE_CD,MODEL_CD,NAME,NEW_USD_CD,OPER_STATUS_CD,ORG_CENSUS_ID,ORIG_ORDER_ID,OU_ADDR_ID,OWNERSHIP_TYPE_CD,OWNER_ACCNT_ID,OWNER_ASSET_NUM,OWNER_CON_ID,OWNR_TYPE_CD,PARTNER_BANK,PARTNER_BRANCH,PAR_ASSET_ID,PER_ADDR_ID,PNSN_SUBTYPE_CD,POLICY_SOURCE_ID,PORT_VALID_PROD_ID,PREF_SRV_DLR_ID,PRI_DESC_TEXT,PRI_LST_ID,PRODUCER_CD,PROD_ID,PROD_INV_ID,PROMOTION_ID,PROM_INTEG_ID,PROM_ITEM_ID,PROM_SRC_INTG_ID,PR_ACCNT_ID,PR_ASSET_ID,PR_ASSET_WRNTY_ID,PR_CON_ID,PR_DISCNT_ID,PR_DRVR_ID,PR_EMP_ID,PR_FIN_DTL_ID,PR_GRP_OU_ID,PR_POSTN_ID,PR_TERR_ID,PURCH_LOC_DESC,RATE_CD,RATE_EXCEPTION,RATE_PLAN_CD,RATE_STATE,REASON_CD,REFERRAL_NAME,REF_NUMBER_1,REF_NUMBER_2,REF_NUMBER_3,REGION,REL_TYPE_CD,REV_NUM,ROOT_ASSET_ID,RTNG_DLR_ID,SERIAL_NUM,SERVICE_POINT_ID,SERV_ACCT_ID,SP_NUM,SRV_REGN_ID,SR_RTNG_CD,STATUS_CD,STAT_REASON_CD,STOCK_NUM,SUB_STATUS_CD,SUB_TYPE_CD,SUPPLIER_OU_ID,TERM_CD,THMBNL_IMAGE_ID,TIER_PRI_INFO,TIMEFRAME_CD,TRADEIN_TYPE_CD,TRANSMISSION_CD,TRIM_CD,TYPE_CD,USG_UOM_CD,VALUE_BASIS_CD,VERSION,WARRANTY_TYPE_CD,XA_CLASS_ID,X_AEP_NUM,X_AX_FEED_LOC,X_BILLING_CYCLE,X_ELECTRONIC_ID,X_EMETER_ACCES_INFO,X_EMETER_PREMISE_LOC,X_HIGH_VAL,X_LAST_READ_AMT,X_LATITUDE,X_LATITUDE_NEW,X_LOC_CODE,X_LOCK_FLG,X_LOCK_INFO,X_LOGICAL_CH_NUM,X_LONGITUDE,X_LONGITUDE_NEW,X_LOT_NUM,X_LOW_VAL,X_NETWORK_ID,X_PHYSICAL_CH_NUM,X_POWER_STATUS,X_PULSE_OUTPUT_BLK,X_READ_DT,X_READING_CYCLE,X_REGISTERED__PRVDR_STAT,X_RETIRE_DT,X_ROUTE_OWNER,X_SEAL_INFO,X_SERVICE_ID,X_UDC_ASSET_ID,X_UDC_EXP_USAGE,X_UNIVERSAL_ID,X_USAGE_STATUS,X_UTIL_ACCESS_INFO,X_UTIL_PREMISE_LOC,X_PLAN_APPROVED,X_PLAN_INSTALL_DT,X_MFG_DATE,X_VIRTUAL_ASSET,X_BILLING_HOLD,X_QUERY_LIST) 
values ('1-ALYT-1ROUTE',to_timestamp('19-MAR-14','DD-MON-RR HH.MI.SSXFF AM'),'1-6F8',to_timestamp('19-MAR-14','DD-MON-RR HH.MI.SSXFF AM'),'1-6F8',0,'0','1-ALYT-1ROUTE','N','1-338','N','N','N','N','N','N','N','N','N','N','N',null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,to_timestamp('19-MAR-14','DD-MON-RR HH.MI.SSXFF AM'),null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,'Sector',null,null,null,null,null,null,null,null,'SOURCE1','QA Automation Route',null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,'1-LOL',null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,'Accepted',null,null,null,null,null,null,null,null,null,null,null,null,'Route',null,null,null,null,null,null,null,'MyCycle_cd',null,null,null,null,null,null,null,null,'N',null,null,null,null,null,null,null,null,null,null,null,'MyCycle_cd',null,null,null,null,null,'OW37_20_1-2',null,null,null,null,null,null,null,null,null,null,null);


select name,type,sub_type from s_prod_int where row_id like '1-ALYT-1METER';

select * from s_prod_int;

select * from cx_cal_param;

select * from s_asset where type_cd IN ('Route');

SELECT s.row_id AS src_id,
						  NVL(s.data_src, 'SOURCE1') AS data_src,
						  s.x_udc_asset_id AS udc_id,
						  s.x_billing_cycle	AS billing_cycle,
						  s.x_reading_cycle	AS reading_cycle,
						  s.status_cd AS status_cd,
						  s.desc_text AS desc_text,
						  s.type_cd AS type,
						  s.cfg_type_cd AS sub_type,
						  cycle.cycle_cd_7x AS cycle_cd,
						  s.name	AS name,
						  s.region AS region,
						  param.attrib_01 AS param_name,
						  param.attrib_02 AS param_value,
						  param.attrib_12 AS param_eff_start_time,
						  param.attrib_13 AS param_eff_end_time
						FROM s_asset s,
							cx_cycle_cd_mapping cycle,
							s_asset_xm param
						WHERE s.type_cd = 'Route'	
					      AND param.par_row_id (+) = s.row_id
					      AND cycle.cycle_cd_7x = NVL(s.x_billing_cycle, s.x_reading_cycle)
					    ORDER BY NVL(s.data_src, 'SOURCE1'), s.row_id, s.type_cd;
           
select * from cx_product_name_mapping;          


SELECT s.row_id AS src_id,
						NVL(s.data_src, 'SOURCE1') AS data_src,
						s.x_udc_asset_id AS udc_id,
						s.serial_num AS mfg_serial_num,
						s.x_lot_num AS mfg_lot_num,
						s.x_network_id AS network_id,
						s.status_cd AS status_cd,
						s.desc_text AS desc_text,
						s.purch_dt AS purchase_date,
						s.ship_dt AS ship_date,
						NULL AS mfg_test_date,
						s.x_retire_dt AS retire_date,
						s.x_mfg_date AS mfg_date,
						s.make_cd AS make,
						s.model_cd AS model,
						s.last_test_dt AS last_test_date,
						s.x_virtual_asset AS is_virtual_flg,
						s.x_universal_id AS badge_id,
						s.x_aep_num AS standard_id,
						s.x_electronic_id AS electronic_id,
						NULL AS comm_technology,
						NULL AS sku,
						NULL AS part_num,
						s.type_cd AS type,
						s.cfg_type_cd AS sub_type,
						s.meter_loc AS location_info,
						--mapping.product_name_8x AS prod_name, 
            prod.name AS prod_name, 
						param.attrib_01 AS param_name,
						param.attrib_02 AS param_value,
						param.attrib_12 AS param_eff_start_time,
						param.attrib_13 AS param_eff_end_time
					FROM s_asset s,
						s_prod_int prod,
						cx_product_name_mapping mapping,
						s_asset_xm param
					WHERE s.prod_id = prod.row_id
						--AND mapping.product_name_7x = prod.name
						AND mapping.type_7x = s.type_cd
						AND param.par_row_id (+) = s.row_id
						AND s.type_cd IN (/*'Meter'),'Communication Module',*/'CT-PT'/*,'Disconnect Collar', 'FRU'*/)
            AND s.row_id = '1-ALYT-1CTPT';
						order by NVL(s.data_src, 'SOURCE1'), s.row_id, s.type_cd	;
						
						
						
						
						
						
*******************************************************
Hi Joel,

The //EnergyIP/opt/weather/ application refers following jars:
•	eMeterFrameworkCore2.jar
•	jmxremote.jar
The jars were checked in at //EnergyIP/opt/weather/trunk/lib directory and the build was fine.

But with Weather2.0 the i.e weather is released with project.eip.version 7.7. , we need eMeterFrameworkCore2.jar build on 7.7 (as jar file is updated in 7.7).

Made following changes to //EnergyIP/opt/weather/trunk/
•	Removed following jars from lib as latest jars need to picked
o	eMeterFrameworkCore2.jar
o	jmxremote.jar
o	em-core-7.1.17.jar (the jar is no longer required)
•	build.xml modified to fetch eMeterFrameworkCore2.jar and jmxremote.jar from pipe_home/lib directory
o	Added property ‘core.lib.dir’ which should point to emeter core jars, but the build is failing in order to resolve it.

Please do the needful to resolve the build issue //EnergyIP/opt/weather/trunk/. 

Once done same changes need to be propagated to //EnergyIP/opt/weather/release/2.0/
•	Remove following jars from lib folder
o	eMeterFrameworkCore2.jar
o	em-core-7.1.17.jar
o	jmxremote.jar
•	Merge 2.0 build.xml with trunk/build.xml to include emeter core jars. 



**********************

The issue is because the eMeterFrameworkCore2.jar is updated as part of 7.7 release and the packaged jar was of earlier version.
* The weather application has dependency on propset jmxmonitoring
* The resolution of above propset refers to XML file  {PIPE_CONFIG}/appXML/JMXEIPMBeanRegistration.xml (updated as part of 7.7)
* The above file is updated with addition of a class com.eMeter.PIPe.hydrofw.monitoring.jmx.registration.JMXRegistrationApplicationManager
* Above class is part of 7.7 compiled eMeterFrameworkCore2.jar

Also on this set up:
PIPE_HOME/tools does not have directory confXML (the directory contains all propset related files)

* Extracted egenergyIP-7.7FULL-208554.zip to tem location and copied only confXML folder.
* Ran ConfigurationManagement.sh
* Ran opt/weather/bin/setupConfiguration.sh
* Added required propsets in SystemConsole
* NZdatasource property now points to test env. Please revert them.



ServicePointClass,8
DeviceClass,10
MeasType,20
DataSvcClass,11
SvcAgreeClass,0
SvcPtGroup,4
DistNode,4
Account,10
Premise,4
Device,9
ServiceDeliveryPoint,2



**********************************************************
SELECT s.row_id AS src_id
       , NVL(s.data_src, 'SOURCE1') AS data_src
       , s.x_udc_asset_id AS udc_id
       --, s.prod_id AS class_src_id
       --, s.serial_num AS mfg_serial_num
       --, s.x_lot_num AS mfg_lot_num
       --, s.x_network_id AS network_id
       , s.status_cd AS status_cd
       , s.desc_text AS desc_text
       , s.x_virtual_asset AS is_virtual_flg
       , s.type_cd AS TYPE
       , s.cfg_type_cd AS sub_type
       /*, DECODE (s.type_cd, 'Service', s.cfg_type_cd, s.type_cd)
            AS handler_type
       , s.per_addr_id AS premise_src_id
       , s.x_ax_feed_loc AS feed_loc
       , s.x_latitude_new AS gps_lat
       , s.x_longitude_new AS gps_long
       , s.x_pulse_output_blk AS pulse_output_block
       , s.x_seal_info AS seal_info
       , s.x_lock_info AS lock_info
       , s.x_util_access_info AS access_info
       , s.x_emeter_acces_info AS alt_access_info
       , s.x_util_premise_loc AS loc_info
       , s.x_emeter_premise_loc AS alt_loc_info
       , s.x_billing_cycle AS billing_cycle
       , s.x_reading_cycle AS reading_cycle
       , NULL AS timezone_id
       , NULL AS gis_id
       , s.x_read_dt AS billed_upto_time
       , s.x_power_status AS power_status
       , s.x_usage_status AS load_status
       , s.x_billing_hold AS billing_hold_status
       , s.meter_loc AS location_info
       , s.service_point_id AS svc_pt_src_id
       , s.start_dt AS eff_start_time
       , s.end_dt AS eff_end_time
       , s.cur_agree_id AS svc_agree_src_id
       , s.created AS source_created
       , s.last_upd AS source_last_upd
       , NULL AS insert_time
       , NULL AS last_upd_time*/
       , asset_for_udc.x_udc_asset_id AS sdp_udc_id
       , mapping.udc_id_8x AS prod_name
       , param.attrib_01 AS param_name
       , param.attrib_02 AS param_value
       , param.attrib_12 AS param_eff_start_time
       , param.attrib_13 AS param_eff_end_time
       --, s.service_point_id sdp_row_id
       , mapping.register_index_7x
       , vc_rel.meter_loc_start_dt AS vc_start_dt -- Added for vc_contributor
       , vc_rel.meter_loc_end_dt AS vc_end_dt -- Added for vc_contributor
       , vc_mapping.udc_id_8x AS vc_prod_name -- Added for vc_contributor
    FROM s_asset s
       , s_asset asset_for_udc
       , s_prod_int prod
       --, cx_temp_sdp_ref_id ref
       --, channel_product_mapping mapping
       , cx_product_name_mapping mapping
       , s_asset_xm param
       , s_asset_rel vc_rel -- Added for vc_contributor
       , s_asset vcc -- Added for vc_contributor
       , s_prod_int vc_prod -- Added for vc_contributor
       , cx_product_name_mapping vc_mapping -- Added for vc_contributor
   WHERE s.service_point_id =   --IN ('1-2IWJ','1-2SOH','1-2SPY','1-32SM','1-33FH','1-32MP','1-336N')   -- 
     AND s.prod_id = prod.row_id
     AND s.type_cd = 'Channel' 
     --AND mapping.channel_name_7x = prod.name
     AND mapping.product_name_7x = prod.name
     AND param.par_row_id(+) = s.row_id
     --AND status = 'Processing'
     AND vc_rel.par_asset_id (+) = s.row_id -- Added for vc_contributor
     AND vc_rel.x_asset_relation_type_cd = 'CHANNEL-CHANNEL' -- Added for vc_contributor
     AND vcc.row_id = vc_rel.asset_id -- Added for vc_contributor
     AND vcc.prod_id = vc_prod.row_id -- Added for vc_contributor
     AND vc_prod.name = vc_mapping.product_name_7x -- Added for vc_contributor
     ORDER BY NVL(s.data_src, 'SOURCE1'), s.row_id, mapping.udc_id_8x; 


DistNode
========

SELECT dn.row_id, dn.prod_id, DN.TYPE_CD, eloc.row_id, eloc.type_cd, ELOC.PER_ADDR_ID, PR.X_CLIENT_PRMSE_ID, param.row_id, param.attrib_01 name, param.attrib_02 value 
FROM 
s_asset dn
JOIN s_prod_int p ON ( DN.PROD_ID=p.row_id)
LEFT OUTER JOIN s_asset eloc ON (DN.X_UDC_ASSET_ID=ELOC.X_UDC_ASSET_ID and dn.row_id <> eloc.row_id )--and eloc.type_cd='Equipment Location')
LEFT OUTER JOIN s_addr_per pr ON (eloc.per_addr_id = pr.row_Id) 
LEFT OUTER JOIN s_asset_xm param ON (DN.ROW_ID = PARAM.PAR_ROW_ID)
WHERE dn.type_cd='Distribution Node'  
AND 
EXISTS
(
SELECT 1 FROM s_asset_xm xm 
WHERE xm.par_row_id = dn.row_id AND xm.ATTRIB_01='Type' and xm.ATTRIB_02='1' 
HAVING COUNT=1
)
and PARAM.ATTRIB_01 !='Type'



sdp-rel, sdp - meter rel when it becomes sdp 
  handle parent Distnode rel
  


******************************************************************************************************************************************************
SVC_PT_SVC_AGREE_REL: TODO - Plan Name
=====================
SELECT asset.start_dt AS rel_start_time
     , asset.end_dt AS rel_end_time
     , sdp.x_udc_asset_Id AS sdp_udc_id
     , mapping.product_name_8x AS prod_name
     , NULL AS plan_name
     , param.attrib_12 AS param_start_dt
     , param.attrib_13 AS param_end_dt
     , param.attrib_01 AS param_name
     , param.attrib_02 AS param_value
     , param.attrib_03 AS param_status
FROM s_asset asset
     , s_asset_xm param
     , s_prod_int prod
     , cx_product_name_mapping mapping
     , s_asset sdp
WHERE asset.type_cd    = 'Service'
   AND asset.prod_id    = prod.row_id
   AND asset.row_id     = param.par_row_id (+)
   AND prod.name        = mapping.product_name_7x
   AND sdp.row_id       = asset.service_point_id
   AND sdp.row_id       =  
   AND asset.cfg_type_cd IN ('Energy Purchase'
			    ,'Generation Balance Provider'
			    ,'Consumption Balance Provider'
			    ,'Billing'
			    ,'AMI Operator'
			    ,'Energy Supplier'
			    ,'Energy Services Agent'
			    ,'Pricing Plan');


SVC_PT_DATA_SVC_REL: TODO - Splitting of Data Services based on parameters. Channel and Device details
====================
SELECT asset.row_id AS src_id
     , asset.start_dt AS rel_start_time
     , asset.end_dt AS rel_end_time
     , sdp.x_udc_asset_Id AS sdp_udc_id
     , mapping.product_name_8x AS prod_name
     , (SELECT mapping.udc_id_8x
          FROM s_asset ch
             , cx_product_name_mapping ch_mapping
             , s_prod_int ch_prod
         WHERE ch.row_id = asset.par_asset_id
           AND ch.type_cd = 'Channel'
           AND ch.prod_id = ch_prod.row_id
           AND ch_prod.name = ch_mapping.product_name_7x) AS channel_type
     , (SELECT mtr.x_udc_asset_Id
          FROM s_asset mtr
         WHERE mtr.row_id = asset.par_asset_id
           AND mtr.type_cd = 'Meter') AS device_udc_id
     , param.attrib_12 AS param_start_dt
     , param.attrib_13 AS param_end_dt
     , param.attrib_01 AS param_name
     , param.attrib_02 AS param_value
  FROM s_asset asset
     , s_asset_xm param
     , s_prod_int prod
     , cx_product_name_mapping mapping
     , s_asset sdp
 WHERE asset.type_cd    = 'Service'
   AND asset.prod_id    = prod.row_id
   AND asset.row_id     = param.par_row_id (+)
   AND prod.name        = mapping.product_name_7x
   AND sdp.row_id       = asset.service_point_id
   AND sdp.row_id       =  
   AND asset.cfg_type_cd IN ( 'Data Collection'
                            , 'Web Presentment'
                            , 'Data Delivery'
                            , 'VEE'
                            , 'Framing'
                            , 'Deployment Planning'
                            , 'Virtual Channel Persistence'
                            , 'Estimation'
                            , 'Outage'
                            , 'Data Transfer'
                            , 'CO2 Plan')
 ORDER BY asset.row_id, mapping.udc_id_8x;

SVC_PT_DEVICE_REL
===================
SELECT 'SDP-METER' AS relation_type
     , rel.meter_loc_start_dt
     , rel.meter_loc_end_dt
     , sdp.x_udc_asset_Id AS sdp_udc_id
     , mtr.x_udc_asset_Id AS mtr_udc_id
  FROM s_asset mtr
     , s_asset_rel rel
     , s_asset sdp
 WHERE rel.x_asset_relation_type_cd = 'SDP-METER'
   AND rel.par_asset_id             = mtr.row_id
   AND rel.asset_id                 = sdp.row_id
   AND sdp.row_id                   =  
   

DEVICE_CHANNEL_REL
==================
SELECT rel.row_id AS src_id
     , rel.meter_loc_start_dt AS rel_start_time
     , rel.meter_loc_end_dt AS rel_end_time
     , sdp.x_udc_asset_Id AS sdp_udc_id
     , mtr.x_udc_asset_Id AS device_udc_id
     , mapping.udc_id_8x AS prod_name
  FROM s_asset mtr
     , s_asset_rel rel
     , s_asset ch
     , s_asset sdp
     , s_prod_int prod
     , cx_product_name_mapping mapping
 WHERE rel.x_asset_relation_type_cd = 'METER-CHANNEL'
   AND rel.par_asset_id             = mtr.row_id
   AND rel.asset_id                 = ch.row_id
   AND ch.service_point_id          = sdp.row_id
   AND prod.row_id                  = ch.prod_id
   AND prod.name                    = mapping.product_name_7x
   AND sdp.row_id                   =  
   			


DEVICE_FUNCTION_REL
===================
SELECT 'COMMUNICATION-METER' AS relation_type
     , rel.meter_loc_start_dt AS rel_start_time
     , rel.meter_loc_end_dt AS rel_end_time
     , mtr.x_udc_asset_Id AS device_udc_id
     , commMod.x_udc_asset_Id AS comfunction_udc_id
  FROM s_asset mtr
     , s_asset_rel rel
     , s_asset commMod
 WHERE rel.x_asset_relation_type_cd = 'COMMUNICATION-METER'
   AND rel.par_asset_id             = commMod.row_id
   AND rel.asset_id                 = mtr.row_id
   AND mtr.row_id                   IN (@meterList@)
   

SVC_PT_GROUP_REL -- TODO: Read Sequence
===================
SELECT rel.meter_loc_start_dt AS rel_start_time
     , rel.meter_loc_end_dt AS rel_end_time
     , sdp.x_udc_asset_Id AS sdp_udc_id
     , route.x_udc_asset_Id AS route_udc_id
  FROM s_asset route
     , s_asset_rel rel
     , s_asset sdp
 WHERE rel.x_asset_relation_type_cd = 'ROUTE-SDP'
   AND rel.par_asset_id             = route.row_id
   AND rel.asset_id                 = sdp.row_id
   AND sdp.row_id                   =  


DistNode
========

SELECT rel.meter_loc_start_dt AS rel_start_time
     , rel.meter_loc_end_dt AS rel_end_time
     , sdp.x_udc_asset_Id AS sdp_udc_id
     , distnode.x_udc_asset_Id AS distnode_udc_id
     , 'DISTRIBUTION-SDP' AS rel_type
     , distnode.row_id AS distnode_src_id
  FROM s_asset distnode
     , s_asset_rel rel
     , s_asset sdp
 WHERE rel.x_asset_relation_type_cd = 'DISTRIBUTION-SDP'
   AND rel.par_asset_id             = distnode.row_id
   AND rel.asset_id                 = sdp.row_id
   AND sdp.row_id                   =  


SELECT dn.row_id
     , dn.prod_id
     , dn.type_cd
     --, eloc.row_id
     --, eloc.type_cd
     --, eloc.per_addr_id
     , pr.x_client_prmse_id as prms_uddc_id
     , param.row_id
     , param.attrib_01 AS param_name
     , param.attrib_02 AS param_value
     , param.attrib_12 AS param_start_dt
     , param.attrib_13 AS param_end_dt
FROM s_asset dn
JOIN s_prod_int p ON (dn.prod_id = p.row_id)
LEFT OUTER JOIN s_asset eloc ON (dn.x_udc_asset_id=eloc.x_udc_asset_id and dn.row_id <> eloc.row_id )--and eloc.type_cd='Equipment Location')
LEFT OUTER JOIN s_addr_per pr ON (eloc.per_addr_id = pr.row_Id) 
LEFT OUTER JOIN s_asset_xm param ON (dn.row_id = param.par_row_id)
WHERE dn.type_cd = 'Distribution Node'
  AND dn.row_id IN ( )
AND 
EXISTS
(
SELECT 1 FROM s_asset_xm xm 
WHERE xm.par_row_id = dn.row_id AND xm.attrib_01='Type' and xm.attrib_02='1' 
HAVING COUNT=1
)
and PARAM.ATTRIB_01 !='Type'


 SELECT rel.meter_loc_start_dt AS rel_start_time
      , rel.meter_loc_end_dt AS rel_end_time
      , par_distnode.x_udc_asset_Id AS par_distnode_udc_id
      , child_distnode.x_udc_asset_Id AS child_distnode_udc_id
      , 'DISTNODE-DISTNODE' AS rel_type
   FROM s_asset_rel rel
      , s_asset par_distnode
      , s_asset child_disnode
  WHERE rel.par_asset_id IN ( )
    AND rel.x_asset_relation_type_cd = 'DISTNODE-DISTNODE'
    AND rel.par_asset_id             = par_distnode.row_id
    AND rel.asset_id                 = child_distnode.row_id
UNION
 SELECT rel.meter_loc_start_dt AS rel_start_time
      , rel.meter_loc_end_dt AS rel_end_time
      , par_distnode.x_udc_asset_Id AS par_distnode_udc_id
      , child_distnode.x_udc_asset_Id AS child_distnode_udc_id
      , 'DISTNODE-DISTNODE' AS rel_type
   FROM s_asset_rel rel
      , s_asset par_distnode
      , s_asset child_disnode
  WHERE rel.asset_id IN ( )
    AND rel.x_asset_relation_type_cd = 'DISTNODE-DISTNODE'
    AND rel.par_asset_id             = par_distnode.row_id
    AND rel.asset_id                 = child_distnode.row_id
                         
                         
CHANNEL:
=======
SELECT ch.row_id AS src_id
     --, NVL(ch.data_src, @primaryOrg@) AS data_src
     , ch.x_udc_asset_id AS udc_id
     , ch.status_cd AS status_cd
     , ch.desc_text AS desc_text
     , NVL(ch.x_virtual_asset,'N') AS is_virtual_flg
     , ch.type_cd AS TYPE
     , ch.cfg_type_cd AS sub_type
     , sdp.x_udc_asset_id AS sdp_udc_id
     , mapping.udc_id_8x AS prod_name
     , param.attrib_01 AS param_name
     , param.attrib_02 AS param_value
     , param.attrib_12 AS param_eff_start_time
     , param.attrib_13 AS param_eff_end_time
     --, mapping.register_index_7x
  FROM s_asset ch
     , s_asset sdp
     , s_prod_int prod
     , s_asset_xm param
     , cx_product_name_mapping mapping
 WHERE ch.service_point_id = sdp.row_id
   AND ch.type_cd          = 'Channel'
   AND ch.row_id           = param.par_row_id (+)
   AND ch.prod_id          = prod.row_id
   AND prod.name           = mapping.product_name_7x
   AND NVL(ch.x_virtual_asset,'N') = 'N'
   AND sdp.row_id          =  
   ORDER BY NVL(s.data_src, @primaryOrg@), s.row_id, mapping.udc_id_8x


VIRTUAL CHANNEL:
================
SELECT ch.row_id AS src_id
     , NVL(ch.data_src, @primaryOrg@) AS data_src
     , ch.x_udc_asset_id AS udc_id
     , ch.status_cd AS status_cd
     , ch.desc_text AS desc_text
     , NVL(ch.x_virtual_asset,'N') AS is_virtual_flg
     , ch.type_cd AS TYPE
     , ch.cfg_type_cd AS sub_type
     , sdp.x_udc_asset_id AS sdp_udc_id
     , mapping.udc_id_8x AS prod_name
     , param.attrib_01 AS param_name
     , param.attrib_02 AS param_value
     , param.attrib_12 AS param_eff_start_time
     , param.attrib_13 AS param_eff_end_time
     , mapping.register_index_7x
     , rel.meter_loc_start_dt AS relation_start_dt
     , rel.meter_loc_end_dt AS relation_end_dt
     , rel.x_alias AS child_ch_symbol
     , vc_sdp.x_udc_asset_id as vc_sdp_udc_id
     , vc_mapping.udc_id_8x AS vc_prod_name
     , vc_ch.row_id AS vc_src_id
     , rel.row_id AS rel_src_id
     , CASE WHEN param.attrib_01 = 'Formula'
             AND param.type      = 'CHANNEL_ASSET_ATTRIB'
            THEN (SELECT caxx.x_attribute_02 AS formula_expr
                    FROM cx_asset_xm_x caxx
                   WHERE caxx.par_row_id = param.row_id)
        ELSE
             NULL
        END AS vc_formula
  FROM s_asset ch
     , s_asset sdp
     , s_prod_int prod
     , s_asset_xm param
     , cx_product_name_mapping mapping
     , s_asset_rel rel
     , s_asset vc_ch
     , s_asset vc_sdp
     , s_prod_int vc_prod
     , cx_product_name_mapping vc_mapping
 WHERE ch.service_point_id = sdp.row_id
   AND ch.type_cd          = 'Channel'
   AND ch.row_id           = param.par_row_id (+)
   AND ch.prod_id          = prod.row_id
   AND prod.name           = mapping.product_name_7x
   AND NVL(ch.x_virtual_asset,'N') = 'Y'
   AND sdp.row_id          =  
   AND rel.par_asset_id    = ch.row_id
   AND rel.x_asset_relation_type_cd = 'CHANNEL-CHANNEL'
   AND rel.asset_id        = vc_ch.row_id
   AND vc_sdp.row_id       = vc_ch.service_point_id
   AND vc_prod.row_id      = vc_ch.prod_id
   AND vc_prod.row_id      = vc_mapping.product_name_7x
   ORDER BY NVL(s.data_src, @primaryOrg@), s.row_id, mapping.udc_id_8x

|| Identifier for Register Channel Type || *Register Channel Types* ||
| R1 | CUM_READ |
| R2 | DEMAND_READ_BC |
| R3 | DEMAND_READ_DAILY |
| R4 | TOU1_CUM |
| R5 | TOU1_DEMAND_BC |
| R6 | TOU1_DEMAND_DAILY |
| R7 | TOU2_CUM |
| R8 | TOU2_DEMAND_BC |
| R9 | TOU2_DEMAND_DAILY |
| R10 | TOU3_CUM |
| R11 | TOU3_DEMAND_BC |
| R12 | TOU3_DEMAND_DAILY |
| R13 | TOU4_CUM |
| R14 | TOU4_DEMAND_BC |
| R15 | TOU4_DEMAND_DAILY |
| R16 | TOU5_CUM |
| R17 | TOU5_DEMAND_BC |
| R18 | TOU5_DEMAND_DAILY |



How to get the channel/device details on services
Need to split Data Services


IDEA SIM - 8505847708 (ANAND)





**********************
<!-- Channel Duplicate udc_id SQL -->
				<!-- <value>
					<![CDATA[
						SELECT asset.row_id as src_id, 
							asset.x_udc_asset_id as udc_id, 
							asset.data_src as data_src,
							'Reason:Udc_id is duplicate' as reason
						FROM 
							(
							SELECT COUNT(1), x_udc_asset_id
							FROM s_asset
							WHERE type_cd IN ('Channel')
							GROUP BY x_udc_asset_id
							HAVING COUNT(1) > 1
							) dup,
							s_asset asset
						WHERE asset.x_udc_asset_id = dup.x_udc_asset_id
							AND type_cd IN ('Channel')
					]]>
				</value> -->
				<!--Channel Null udc_id SQL -->
				<!-- <value>
					<![CDATA[
						SELECT  row_id as src_id,
								data_src as data_src, 
							    x_udc_asset_id as udc_id,
							    'Reason:Udc_id is Null' as reason
						FROM s_asset
						WHERE x_udc_asset_id is NULL
            			AND type_cd IN ('Channel')
					]]>
				</value> -->
				
				
				
				
				
				


/***********Old ********/

SELECT s.row_id AS src_id,
	NVL(s.data_src, @primaryOrg@) AS data_src,
	s.x_udc_asset_id AS udc_id,
	s.status_cd AS status_cd,
	s.desc_text AS desc_text,
	s.x_virtual_asset AS is_virtual_flg,
	s.type_cd AS type,
	s.cfg_type_cd AS sub_type,
	s.x_util_premise_loc 	AS location_info,
	s.x_emeter_premise_loc AS alt_loc_info,
	s.x_universal_id AS universal_id,
	s.cfg_type_cd AS service_type,
	s.x_ax_feed_loc AS feed_loc,
	s.x_util_access_info AS access_info,
	s.x_emeter_acces_info AS alt_access_info,
	s.x_pulse_output_blk AS pulse_output_block,
	s.x_seal_info AS seal_info,
	s.x_lock_info AS lock_info,
	s.x_latitude_new 		AS gps_lat,
	s.x_longitude_new 	AS gps_long,
	mapping.product_name_8x AS prod_name, 
	param.attrib_01 AS param_name,
	param.attrib_02 AS param_value,
	param.attrib_12 AS param_eff_start_time,
	param.attrib_13 AS param_eff_end_time,
prms.x_client_prmse_id 	AS premise_udc_id
FROM s_asset s,
	s_prod_int prod,
	cx_product_name_mapping mapping,
	s_asset_xm param,
s_addr_per prms
WHERE s.prod_id = prod.row_id
	AND mapping.product_name_7x = prod.name
	AND mapping.type_7x = s.type_cd
	AND param.par_row_id (+) = s.row_id
	AND s.type_cd IN ('Distribution Node')
AND s.per_addr_id = prms.row_id(+)
	order by NVL(s.data_src, @primaryOrg@), s.row_id, s.type_cd
	
	
	
SELECT s.row_id AS src_id,
	NVL(s.data_src, @primaryOrg@) AS data_src,
	s.x_udc_asset_id AS udc_id,
	s.status_cd AS status_cd,
	s.desc_text AS desc_text,
	s.x_virtual_asset AS is_virtual_flg,
	s.type_cd AS type,
	s.cfg_type_cd AS sub_type,
	s.x_util_premise_loc 	AS location_info,
	s.x_emeter_premise_loc AS alt_loc_info,
	s.x_universal_id AS universal_id,
	s.cfg_type_cd AS service_type,
	s.x_ax_feed_loc AS feed_loc,
	s.x_util_access_info AS access_info,
	s.x_emeter_acces_info AS alt_access_info,
	s.x_pulse_output_blk AS pulse_output_block,
	s.x_seal_info AS seal_info,
	s.x_lock_info AS lock_info,
	s.x_latitude_new 		AS gps_lat,
	s.x_longitude_new 	AS gps_long,
	mapping.product_name_8x AS prod_name, 
	param.attrib_01 AS param_name,
	param.attrib_02 AS param_value,
	param.attrib_12 AS param_eff_start_time,
	param.attrib_13 AS param_eff_end_time,
prms.x_client_prmse_id 	AS premise_udc_id
FROM s_asset s,
	s_prod_int prod,
	cx_product_name_mapping mapping,
	s_asset_xm param,
s_addr_per prms,
	cx_alyt_distnode_cdc cdc
WHERE s.prod_id = prod.row_id
	AND mapping.product_name_7x = prod.name
	AND mapping.type_7x = s.type_cd
	AND param.par_row_id (+) = s.row_id
	AND s.type_cd IN ('Distribution Node')
AND s.per_addr_id = prms.row_id(+)
	AND s.row_id= cdc.distnode_src_id
AND s.last_upd=cdc.src_last_upd_time 
	AND cdc.processed_flag='Y'		   		
Order by NVL(s.data_src, @primaryOrg@), s.row_id, s.type_cd
/***********Old ********/						



{composition-setup}
cloak.memory.duration = 100
cloak.toggle.type = defaultcloudAlytStructuralExtraction-1.0-xxx
{composition-setup}


 OR s_prod_int.sub_type
 
 
Measurement Param unused: 1-OQM::TOU Code
Measurement Param unused: 1-OQM::External Meas Code
Measurement Param unused: 1-OQM::Format Precision
Measurement Param unused: 1-OQM::Format Scale
Measurement Param unused: 1-OQM::MUDR Field Name
Measurement Param unused: 1-OQM::MUDR Table Name
Measurement Param unused: 1-OQM::Logical Channel Number
Measurement Param unused: 1-OQM::Read Sequence
Measurement Param unused: 1-OQM::TOU Code
Measurement Param unused: 1-OQM::Read Sequence
Measurement Param unused: 1-OQM::Logical Channel Number
Measurement Param unused: 1-OQM::MUDR Table Name
Measurement Param unused: 1-OQM::MUDR Field Name
Measurement Param unused: 1-OQM::Format Scale
Measurement Param unused: 1-OQM::Format Precision
Measurement Param unused: 1-OQM::External Meas Code
Measurement Param unused: 1-OQM::Read Sequence
Measurement Param unused: 1-OQM::Logical Channel Number
Measurement Param unused: 1-OQM::MUDR Table Name
Measurement Param unused: 1-OQM::MUDR Field Name
Measurement Param unused: 1-OQM::Format Precision
Measurement Param unused: 1-OQM::External Meas Code
Measurement Param unused: 1-OQM::TOU Code
Measurement Param unused: 1-OQM::Format Scale
Measurement Param unused: 1-OQM::External Meas Code
Measurement Param unused: 1-OQM::Format Precision
Measurement Param unused: 1-OQM::Format Scale
Measurement Param unused: 1-OQM::MUDR Field Name
Measurement Param unused: 1-OQM::MUDR Table Name
Measurement Param unused: 1-OQM::Logical Channel Number
Measurement Param unused: 1-OQM::Read Sequence
Measurement Param unused: 1-OQM::TOU Code
Measurement Param unused: 1-OQM::TOU Code
Measurement Param unused: 1-OQM::Read Sequence
Measurement Param unused: 1-OQM::Logical Channel Number
Measurement Param unused: 1-OQM::MUDR Table Name
Measurement Param unused: 1-OQM::Format Scale
Measurement Param unused: 1-OQM::Format Precision
Measurement Param unused: 1-OQM::External Meas Code
Measurement Param unused: 1-OQM::MUDR Field Name
Measurement Param unused: 1-OQM::TOU Code
Measurement Param unused: 1-OQM::Read Sequence
Measurement Param unused: 1-OQM::Logical Channel Number
Measurement Param unused: 1-OQM::MUDR Table Name
Measurement Param unused: 1-OQM::MUDR Field Name
Measurement Param unused: 1-OQM::Format Scale
Measurement Param unused: 1-OQM::Format Precision
Measurement Param unused: 1-OQM::External Meas Code
Measurement Param unused: 1-OQM::External Meas Code
Measurement Param unused: 1-OQM::TOU Code
Measurement Param unused: 1-OQM::Read Sequence
Measurement Param unused: 1-OQM::Logical Channel Number
Measurement Param unused: 1-OQM::MUDR Table Name
Measurement Param unused: 1-OQM::MUDR Field Name
Measurement Param unused: 1-OQM::Format Scale
Measurement Param unused: 1-OQM::Format Precision
Measurement Param unused: 1-OQM::TOU Code
Measurement Param unused: 1-OQM::Read Sequence
Measurement Param unused: 1-OQM::Logical Channel Number
Measurement Param unused: 1-OQM::MUDR Field Name
Measurement Param unused: 1-OQM::Format Scale
Measurement Param unused: 1-OQM::Format Precision
Measurement Param unused: 1-OQM::External Meas Code
Measurement Param unused: 1-OQM::MUDR Table Name
Measurement Param unused: 1-OQM::External Meas Code
Measurement Param unused: 1-OQM::Format Precision
Measurement Param unused: 1-OQM::Format Scale
Measurement Param unused: 1-OQM::MUDR Field Name
Measurement Param unused: 1-OQM::MUDR Table Name
Measurement Param unused: 1-OQM::Logical Channel Number
Measurement Param unused: 1-OQM::Read Sequence
Measurement Param unused: 1-OQM::TOU Code
Measurement Param unused: 1-OQM::External Meas Code
Measurement Param unused: 1-OQM::Format Precision
Measurement Param unused: 1-OQM::Format Scale
Measurement Param unused: 1-OQM::MUDR Field Name
Measurement Param unused: 1-OQM::Logical Channel Number
Measurement Param unused: 1-OQM::Read Sequence
Measurement Param unused: 1-OQM::TOU Code
Measurement Param unused: 1-OQM::MUDR Table Name
Measurement Param unused: 1-OQM::External Meas Code
Measurement Param unused: 1-OQM::Format Precision
Measurement Param unused: 1-OQM::Format Scale
Measurement Param unused: 1-OQM::MUDR Field Name
Measurement Param unused: 1-OQM::MUDR Table Name
Measurement Param unused: 1-OQM::Logical Channel Number
Measurement Param unused: 1-OQM::Read Sequence
Measurement Param unused: 1-OQM::TOU Code
Measurement Param unused: 1-OQM::External Meas Code
Measurement Param unused: 1-OQM::Format Precision
Measurement Param unused: 1-OQM::Format Scale
Measurement Param unused: 1-OQM::MUDR Field Name
Measurement Param unused: 1-OQM::MUDR Table Name
Measurement Param unused: 1-OQM::Logical Channel Number
Measurement Param unused: 1-OQM::Read Sequence
Measurement Param unused: 1-OQM::TOU Code
Measurement Param unused: 1-OQM::External Meas Code
Measurement Param unused: 1-OQM::Format Precision
Measurement Param unused: 1-OQM::Format Scale
Measurement Param unused: 1-OQM::MUDR Field Name
Measurement Param unused: 1-OQM::MUDR Table Name
Measurement Param unused: 1-OQM::Logical Channel Number
Measurement Param unused: 1-OQM::Read Sequence
Measurement Param unused: 1-OQM::TOU Code
Measurement Param unused: 1-OQM::Logical Channel Number
Measurement Param unused: 1-OQM::Read Sequence
Measurement Param unused: 1-OQM::TOU Code
Measurement Param unused: 1-OQM::External Meas Code
Measurement Param unused: 1-OQM::Format Precision
Measurement Param unused: 1-OQM::Format Scale
Measurement Param unused: 1-OQM::MUDR Field Name
Measurement Param unused: 1-OQM::MUDR Table Name
Measurement Param unused: 1-OQM::MUDR Field Name
Measurement Param unused: 1-OQM::Format Scale
Measurement Param unused: 1-OQM::Format Precision
Measurement Param unused: 1-OQM::External Meas Code
Measurement Param unused: 1-OQM::Logical Channel Number
Measurement Param unused: 1-OQM::Read Sequence
Measurement Param unused: 1-OQM::TOU Code
Measurement Param unused: 1-OQM::MUDR Table Name
Measurement Param unused: 1-OQM::MUDR Field Name
Measurement Param unused: 1-OQM::Format Scale
Measurement Param unused: 1-OQM::Format Precision
Measurement Param unused: 1-OQM::External Meas Code
Measurement Param unused: 1-OQM::MUDR Table Name
Measurement Param unused: 1-OQM::TOU Code
Measurement Param unused: 1-OQM::Read Sequence
Measurement Param unused: 1-OQM::Logical Channel Number
Measurement Param unused: 1-OQM::External Meas Code
Measurement Param unused: 1-OQM::Format Precision
Measurement Param unused: 1-OQM::Format Scale
Measurement Param unused: 1-OQM::MUDR Field Name
Measurement Param unused: 1-OQM::Read Sequence
Measurement Param unused: 1-OQM::Logical Channel Number
Measurement Param unused: 1-OQM::MUDR Table Name
Measurement Param unused: 1-OQM::TOU Code
Measurement Param unused: 1-OQM::TOU Code
Measurement Param unused: 1-OQM::External Meas Code
Measurement Param unused: 1-OQM::Format Precision
Measurement Param unused: 1-OQM::Format Scale
Measurement Param unused: 1-OQM::MUDR Field Name
Measurement Param unused: 1-OQM::MUDR Table Name
Measurement Param unused: 1-OQM::Logical Channel Number
Measurement Param unused: 1-OQM::Read Sequence
Measurement Param unused: 1-W38::Aggregate Time Period
Measurement Param unused: 1-W38::External Meas Code
Measurement Param unused: 1-W38::Format Precision
Measurement Param unused: 1-W38::Format Scale
Measurement Param unused: 1-W38::Value Units
Measurement Param unused: 1-W38::MUDR Field Name
Measurement Param unused: 1-W38::MUDR Table Name
Measurement Param unused: 1-W38::Read Sequence
Measurement Param unused: 1-W38::TOU Code
Measurement Param unused: 1-W38::Logical Channel Number


ServicePointClass,8
DeviceClass,10
MeasType,20
DataSvcClass,11
SvcAgreeClass,1
SvcPtGroup,4
Premise,4
Account,10
Device,9

ServiceDeliveryPoint,3
DistributionNode,0
TotalChannel,36,Channel,36,VirtualChannel,0
servicePointServiceAgreementAssociation,1
servicePointDataServiceAssociation,4
servicePointDeviceAssociation,3
deviceChannelAssociation,54
deviceFunctionAssociation,2
servicePointServicePointGroupAssociation,0
accountServicePointAssociation,2
servicePointServicePointAssociation SDP-DN,0
servicePointServicePointAssociation DN-DN,0


/*
1-2IWJ		SOURCE1
1-ALYT-1SDP	SOURCE1
1-32MP		TESTORG2
1-336N		TESTORG2


Dups 
1-2SOH		SOURCE1
1-2SPY		SOURCE1
1-32SM		TESTORG1
1-33FH		TESTORG1
*/

************************************************************
bkp product_mapping.csv

product_name_7x,type_7x,sub_type_7x,product_name_8x,type_8x,sub_type_8x,udc_id_8x,register_index_7x
Electric SDP,Service Point,Electric,Sample Electric Service Point,ServiceDeliveryPoint,Electric,,
Gas SDP,Service Point,Gas,Sample Gas Service Point,ServiceDeliveryPoint,Gas,,
Water SDP,Service Point,Water,Sample Water Service Point,ServiceDeliveryPoint,Water,,
Generic Feeder,Distribution Node,Feeder,Generic Feeder Distribution Node,DistributionNode,Feeder,,
Generic Substation,Distribution Node,Substation,Generic Substation Distribution Node,DistributionNode,Substation,,
Electric Meter,Meter,Electric,Sample Electric Meter,Meter,Electric,,
Gas Meter,Meter,Gas,Gas Meter,Meter,Gas,,
Water Meter,Meter,Water,Water Meter,Meter,Water,,
 Generic CT/PT,Multiplier 15 ,CT-PT,CT-PT, Generic CT/PT,Multiplier 15 ,CT-PT,CT-PT,,
Generic CT/PT ,CT-PT,CT-PT, Generic CT/PT, ,CT-PT,CT-PT,,
 Generic CT/PT-Test ,CT-PT,CT-PT, Generic CT/PT-Test ,CT-PT,CT-PT,,
Elster Meter Module,Communication Module,Elster Meter Module,Elster Meter Module,CommModule,Elster Meter Module,,
Itron 40E Meter Module,Communication Module,Itron Meter Module,Itron 40E Meter Module,CommModule,Itron 40E Meter Module,,
Cellnet Electric Module,Communication Module,Cellnet Meter Module,Cellnet Electric Module,CommModule,Cellnet Electric Module,,
 Data Collection, Daily, FlexNet ,Service,Data Collection, Data Collection, Daily, FlexNet ,Data Collection,,,
 Data Collection, Daily, EnergyAxis ,Service,Data Collection, Data Collection, Daily, EnergyAxis ,Data Collection,,,
 Data Collection, Daily, L+G CommandCenter ,Service,Data Collection, Data Collection, Daily, L+G CommandCenter ,Data Collection,,,
 Data Transfer, Daily ,Service,Data Transfer, Data Transfer, Daily ,Data Transfer,,,
 Data Transfer, Monthly ,Service,Data Transfer, Data Transfer, Monthly ,Data Transfer,,,
Sample DDS Service,Service,Data Delivery,Sample Residential Data Delivery Service,Data Delivery,,,
Generic Framing Service,Service,Framing,Generic Framing Service,Framing,,,
Generic Commercial Outage,Service,Outage,Generic Residential Outage,Outage,,,
Sample Register VEE Rule Service,Service,Estimation,Sample Register VEE Rule Service,Estimation,,,
Generic VEE Service,Service,VEE,Sample Large CI VEE Service,VEE,,,
 Energy Purchase Service, TOU/CPP (EST) ,Service, Energy Purchase , Energy Purchase Service, TOU/CPP (EST) ,Service, Energy Purchase ,,
Cellnet KWH RR Cum,Measurement,DemandPeak,KWH RR CUM,,,EM.ELECTRIC.KWH.1.1,R1
Cellnet KWH RR Cum,Measurement,DemandPeak,KWH RR DEMAND BC,,,EM.ELECTRIC.KWH.1.2,R2
Cellnet KWH RR Cum,Measurement,DemandPeak,KWH RR DEMAND Daily,,,EM.ELECTRIC.KWH.1.3,R3
Cellnet KWH RR Cum,Measurement,DemandPeak,KWH RR TOU1 CUM,,,EM.ELECTRIC.KWH.1.4,R4
Cellnet KWH RR Cum,Measurement,DemandPeak,KWH RR TOU1 DEMAND BC,,,EM.ELECTRIC.KWH.1.5,R5
Cellnet KWH RR Cum,Measurement,DemandPeak,KWH RR TOU1 DEMAND Daily,,,EM.ELECTRIC.KWH.1.6,R6
Cellnet KWH RR Cum,Measurement,DemandPeak,KWH RR TOU2 CUM,,,EM.ELECTRIC.KWH.1.7,R7
Cellnet KWH RR Cum,Measurement,DemandPeak,KWH RR TOU2 DEMAND BC,,,EM.ELECTRIC.KWH.1.8,R8
Cellnet KWH RR Cum,Measurement,DemandPeak,KWH RR TOU2 DEMAND Daily,,,EM.ELECTRIC.KWH.1.9,R9
Cellnet KWH RR Cum,Measurement,DemandPeak,KWH RR TOU3 CUM,,,EM.ELECTRIC.KWH.1.10,R10
Cellnet KWH RR Cum,Measurement,DemandPeak,KWH RR TOU3 DEMAND BC,,,EM.ELECTRIC.KWH.1.11,R11
Cellnet KWH RR Cum,Measurement,DemandPeak,KWH RR TOU3 DEMAND Daily,,,EM.ELECTRIC.KWH.1.12,R12
Cellnet KWH RR Cum,Measurement,DemandPeak,KWH RR TOU4 CUM,,,EM.ELECTRIC.KWH.1.13,R13
Cellnet KWH RR Cum,Measurement,DemandPeak,KWH RR TOU4 DEMAND BC,,,EM.ELECTRIC.KWH.1.14,R14
Cellnet KWH RR Cum,Measurement,DemandPeak,KWH RR TOU4 DEMAND Daily,,,EM.ELECTRIC.KWH.1.15,R15
Cellnet KWH RR Cum,Measurement,DemandPeak,KWH RR TOU5 CUM,,,EM.ELECTRIC.KWH.1.16,R16
Cellnet KWH RR Cum,Measurement,DemandPeak,KWH RR TOU5 DEMAND BC,,,EM.ELECTRIC.KWH.1.17,R17
Cellnet KWH RR Cum,Measurement,DemandPeak,KWH RR TOU5 DEMAND Daily,,,EM.ELECTRIC.KWH.1.18,R18
 Register, KWH ,Measurement,DemandPeak, Register, KWH RR CUM ,,,EM.ELECTRIC.KWH.1.21,R1
 Register, KWH ,Measurement,DemandPeak, Register, KWH RR DEMAND BC ,,,EM.ELECTRIC.KWH.1.22,R2
 Register, KWH ,Measurement,DemandPeak, Register, KWH RR DEMAND Daily ,,,EM.ELECTRIC.KWH.1.23,R3
 Register, KWH ,Measurement,DemandPeak, Register, KWH RR TOU1 CUM ,,,EM.ELECTRIC.KWH.1.24,R4
 Register, KWH ,Measurement,DemandPeak, Register, KWH RR TOU1 DEMAND BC ,,,EM.ELECTRIC.KWH.1.25,R5
 Register, KWH ,Measurement,DemandPeak, Register, KWH RR TOU1 DEMAND Daily ,,,EM.ELECTRIC.KWH.1.26,R6
 Register, KWH ,Measurement,DemandPeak, Register, KWH RR TOU2 CUM ,,,EM.ELECTRIC.KWH.1.27,R7
 Register, KWH ,Measurement,DemandPeak, Register, KWH RR TOU2 DEMAND BC ,,,EM.ELECTRIC.KWH.1.28,R8
 Register, KWH ,Measurement,DemandPeak, Register, KWH RR TOU2 DEMAND Daily ,,,EM.ELECTRIC.KWH.1.29,R9
 Register, KWH ,Measurement,DemandPeak, Register, KWH RR TOU3 CUM ,,,EM.ELECTRIC.KWH.1.30,R10
 Register, KWH ,Measurement,DemandPeak, Register, KWH RR TOU3 DEMAND BC ,,,EM.ELECTRIC.KWH.1.31,R11
 Register, KWH ,Measurement,DemandPeak, Register, KWH RR TOU3 DEMAND Daily ,,,EM.ELECTRIC.KWH.1.32,R12
 Register, KWH ,Measurement,DemandPeak, Register, KWH RR TOU4 CUM ,,,EM.ELECTRIC.KWH.1.33,R13
 Register, KWH ,Measurement,DemandPeak, Register, KWH RR TOU4 DEMAND BC ,,,EM.ELECTRIC.KWH.1.34,R14
 Register, KWH ,Measurement,DemandPeak, Register, KWH RR TOU4 DEMAND Daily ,,,EM.ELECTRIC.KWH.1.35,R15
 Register, KWH ,Measurement,DemandPeak, Register, KWH RR TOU5 CUM ,,,EM.ELECTRIC.KWH.1.36,R16
 Register, KWH ,Measurement,DemandPeak, Register, KWH RR TOU5 DEMAND BC ,,,EM.ELECTRIC.KWH.1.37,R17
 Register, KWH ,Measurement,DemandPeak, Register, KWH RR TOU5 DEMAND Daily ,,,EM.ELECTRIC.KWH.1.38,R18
TNS S4 Max KW Demand,Measurement,DemandPeak,TNS S4 Max KW Demand,,,EM.ELECTRIC.KVARH.1.18,R1
 Virtual Persisted, 60 Minute Interval Data, KWH ,Measurement,DemandPeak, Virtual Persisted, 60 Minute Interval Data, KWH ,,,EM.ELECTRIC.KWH.1.51,



*************************************************************


zip: 
ADF_StructuralData_<yyyyMMdd>_<hhmmss>.zip
ADF_StructuralData_20140911_194553.zip

filter file: 
File_Filtered_Record_<yyyyMMdd>_<HHmmss>.csv
File_Filtered_Record_20140911_194553.csv

Sample data
<assetName>,[<row_id>|<udc_id>|<data_src>|<Reason>]
e.g. 
Premise,[1-2SO7|5183686|SOURCE1|Reason:Udc_id is duplicate]


Rowcount file:
File_RowCount_Checksum_<yyyyMMdd>_<HHmmss>.csv
File_RowCount_Checksum_20140911_194553.csv

Sample data

ServicePointClass,6
DeviceClass,7
MeasType,11
DataSvcClass,10
SvcAgreeClass,1
SvcPtGroup,3
Premise,3
Account,9
Device,6
ServiceDeliveryPoint,3
DistributionNode,0
TotalChannel,30,Channel,30,VirtualChannel,0
servicePointServiceAgreementAssociation,2
servicePointDataServiceAssociation,7
servicePointDeviceAssociation,5
deviceChannelAssociation,50
deviceFunctionAssociation,3
servicePointServicePointGroupAssociation,0
accountServicePointAssociation,3
servicePointServicePointAssociation SDP-DN,0
servicePointServicePointAssociation DN-DN,0


AssetData file name:
ADF_AssetData_<assetName>_<yyyyMMdd>_<HHmmss>_<fileCounter>_<dataSrc>.xml
e.g.
ADF_AssetData_Account_20140911_194553_1_SOURCE1.xml

AssetData file header:
Following fields are filled at runtime, remaing have hardcoded values
    <dateTime>: 2014-09-11T19:45:53
    <source>: CLOUDALYT, configurable form system console
    <messageID>: MESSAGE_CLOUDALYT_AssetData_<row_id>_<data_src></messageID>

e.g. 
<header>
    <verb>request</verb>
    <noun>SDPSync</noun>
    <revision>1</revision>
    <dateTime>2014-09-11T19:45:53</dateTime>
    <source>CLOUDALYT</source>
    <messageID>MESSAGE_CLOUDALYT_AssetData_1-1TMF_SOURCE1</messageID>
    <asyncReplyTo>none</asyncReplyTo>
    <syncMode>sync</syncMode>
    <optimizationLevel>Optimistic</optimizationLevel>
</header>


SdpStackData file name:
ADF_SDPStackData_Sdp_<yyyyMMdd>_<HHmmss>_<fileCounter>_<dataSrc>.xml
e.g.
ADF_SDPStackData_Sdp_20140911_194553_1_SOURCE1.xml
SdpStackData file header:

Following fields are filled at runtime, remaing have hardcoded values
    <dateTime>: 2014-09-11T19:45:53
    <source>: CLOUDALYT, configurable form system console
    <messageID>: MESSAGE_CLOUDALYT_SdpStackData_<row_id>_<data_src></messageID>
    
e.g. 
<header>
    <verb>request</verb>
    <noun>SDPSync</noun>
    <revision>1</revision>
    <dateTime>2014-09-11T19:45:53</dateTime>
    <source>CLOUDALYT</source>
    <messageID>MESSAGE_CLOUDALYT_SdpStackData_1-2IWJ_SOURCE1</messageID>
    <asyncReplyTo>none</asyncReplyTo>
    <syncMode>sync</syncMode>
    <optimizationLevel>Optimistic</optimizationLevel>
</header>





******************************************
OLD DN-DN rel

SELECT rel.meter_loc_start_dt AS rel_start_time
	      , rel.meter_loc_end_dt AS rel_end_time
	      , par_distnode.x_udc_asset_Id AS sdp_udc_id -- it is parentDN
	      , child_distnode.x_udc_asset_Id AS distnode_udc_id
	      , 'DISTNODE-DISTNODE' AS rel_type
	      , child_distnode.row_id AS distnode_src_id
	   FROM s_asset_rel rel
	      , s_asset par_distnode
	      , s_asset child_distnode
	  WHERE rel.par_asset_id IN (@dnList@)
	    AND rel.x_asset_relation_type_cd = 'DISTNODE-DISTNODE'
	    AND rel.par_asset_id             = par_distnode.row_id
	    AND rel.asset_id                 = child_distnode.row_id
UNION
	 SELECT rel.meter_loc_start_dt AS rel_start_time
	      , rel.meter_loc_end_dt AS rel_end_time
	      , par_distnode.x_udc_asset_Id AS sdp_udc_id -- it is parentDN
	      , child_distnode.x_udc_asset_Id AS distnode_udc_id
	      , 'DISTNODE-DISTNODE' AS rel_type
	      , par_distnode.row_id AS distnode_src_id
	   FROM s_asset_rel rel
	      , s_asset par_distnode
	      , s_asset child_distnode
	  WHERE rel.asset_id IN (@dnList@)
	    AND rel.x_asset_relation_type_cd = 'DISTNODE-DISTNODE'
	    AND rel.par_asset_id             = par_distnode.row_id
	    AND rel.asset_id                 = child_distnode.row_id


**********************************************
* Route: As Route is no more a product in 8x
* Channel: Channel as a product is mapped to meas Type in 8x \\ |  |

***********************************************************************
h4. Extract unique SDP Ref Ids.
{code:xml}
INSERT INTO cx_temp_sdp_ref_id (sdp_row_id)
   -- To get SDP Assets
   SELECT DISTINCT row_id
     FROM cx_alyt_asset_cdc
    WHERE record_type = 'Service Point'
    AND status = 'Processing'
   UNION
   -- To get Channel Asset details
   SELECT DISTINCT sdp_row_id
     FROM cx_alyt_channel_cdc
    WHERE record_type = 'Channel'
    AND status = 'Processing'
   UNION
   -- To get Meter Asset details
   SELECT DISTINCT rel.par_asset_id
     FROM cx_alyt_asset_cdc mtr
        , s_asset_rel rel
    WHERE mtr.record_type = 'Meter'
      AND rel.x_asset_relation_type_cd = 'SDP-METER'
      AND rel.asset_id   = mtr.row_id
      AND status = 'Processing'
   UNION
   -- To get Communication Module Asset details
   SELECT DISTINCT rel2.par_asset_id
     FROM cx_alyt_asset_cdc cmd
        , s_asset_rel rel1
        , s_asset_rel rel2
    WHERE cmd.record_type = 'Communication Module'
      AND rel1.x_asset_relation_type_cd = 'COMMUNICATION-METER'
      AND rel1.par_asset_id = cmd.row_id
      AND rel2.x_asset_relation_type_cd = 'SDP-METER'
      AND rel1.asset_id   = rel2.asset_id
      AND status = 'Processing'
   UNION
   -- To get Data Service details
   SELECT DISTINCT sdp_row_id
     FROM cx_alyt_data_svc_cdc
     WHERE status = 'Processing'
   UNION
   -- To get Agreement Service details
   SELECT DISTINCT sdp_row_id
     FROM cx_alyt_svc_agree_cdc 
     WHERE status = 'Processing'
   UNION
   -- To get Account Relationship details
   SELECT DISTINCT sdp_row_id
     FROM cx_alyt_accnt_sdp_rel_cdc
     WHERE status = 'Processing'
   UNION
   -- To get SDP-METER Relationship details
   SELECT DISTINCT par_asset_id
     FROM cx_alyt_rel_cdc
    WHERE rel_type = 'SDP-METER'
    AND status = 'Processing'
   UNION
   -- To get ROUTE-SDP Relationship details
   SELECT DISTINCT child_asset_id
     FROM cx_alyt_rel_cdc
    WHERE rel_type = 'ROUTE-SDP'
    AND status = 'Processing'
   UNION
   SELECT par_asset_id
     FROM s_asset_rel rel
        , (SELECT child_asset_id AS asset_id
             FROM cx_alyt_rel_cdc
            WHERE rel_type = 'COMMUNICATION-METER'
            AND status = 'Processing'
           UNION
           SELECT par_asset_id AS asset_id
             FROM cx_alyt_rel_cdc
            WHERE rel_type = 'METER-CHANNEL'
            AND status = 'Processing'
           ) meter
    WHERE rel.asset_id = meter.asset_id
      AND rel.x_asset_relation_type_cd = 'SDP-METER'
      AND status = 'Processing' 
{code}


h4. Asset meter extraction query with product_name resolution
{code:xml}
SELECT s.row_id AS src_id, 
      s.data_src AS data_src, 
      s.x_udc_asset_id AS udc_id, 
      s.prod_id AS class_src_id, 
      s.serial_num AS mfg_serial_num, 
      s.x_lot_num AS mfg_lot_num, 
      s.x_network_id AS network_id, 
      s.status_cd AS status_cd, 
      s.desc_text AS desc_text, 
      s.purch_dt AS purchase_date, 
      s.ship_dt AS ship_date, 
      NULL AS mfg_test_date, 
      s.x_retire_dt AS retire_date, 
      s.x_mfg_date AS mfg_date, 
      s.make_cd AS make, 
      s.model_cd AS model, 
      s.last_test_dt AS last_test_date, 
      s.x_virtual_asset AS is_virtual_flg, 
      s.x_universal_id AS badge_id, 
      s.x_aep_num AS standard_id, 
      s.x_electronic_id AS electronic_id, 
      NULL AS comm_technology, 
      NULL AS cur_inv_location_id, 
      NULL AS sku, 
      NULL AS part_num, 
      s.type_cd AS TYPE, 
      s.cfg_type_cd AS sub_type, 
      DECODE (s.type_cd, 'Service', s.cfg_type_cd, s.type_cd) AS handler_type, 
      s.per_addr_id AS premise_src_id, 
      s.x_ax_feed_loc AS feed_loc, 
      s.x_latitude_new AS gps_lat, 
      s.x_longitude_new AS gps_long, 
      s.x_pulse_output_blk AS pulse_output_block, 
      s.x_seal_info AS seal_info, 
      s.x_lock_info AS lock_info, 
      s.x_util_access_info AS access_info, 
      s.x_emeter_acces_info AS alt_access_info, 
      s.x_util_premise_loc AS loc_info, 
      s.x_emeter_premise_loc AS alt_loc_info, 
      s.x_billing_cycle AS billing_cycle, 
      s.x_reading_cycle AS reading_cycle, 
      NULL AS timezone_id, 
      NULL AS gis_id, 
      s.x_read_dt AS billed_upto_time, 
      s.x_power_status AS power_status, 
      s.x_usage_status AS load_status, 
      s.x_billing_hold AS billing_hold_status, 
      s.meter_loc AS location_info, 
      s.service_point_id AS svc_pt_src_id, 
      s.start_dt AS eff_start_time, 
      s.end_dt AS eff_end_time, 
      --a.svc_provider_id AS svc_provider_src_id, 
      s.cur_agree_id AS svc_agree_src_id, 
      s.created AS source_created, 
      s.last_upd AS source_last_upd, 
      NULL AS insert_time, 
      NULL AS last_upd_time, 
      mapping.product_name_8x AS name, 
      param.attrib_01 AS param_name, 
      param.attrib_02 AS param_value, 
      param.attrib_12 AS eff_start_date, 
      param.attrib_13 AS eff_end_date 
FROM s_asset s, 
      s_prod_int prod, 
      cx_temp_sdp_ref_id ref, 
      cx_product_name_mapping mapping,
      s_asset_rel  rel, 
      s_asset_xm param 
WHERE s.prod_id = prod.row_id 
      AND ref.sdp_row_id = rel.par_asset_id 
      --AND info.record_type = 'Meter' 
      AND rel.x_asset_relation_type_cd = 'SDP-METER'
      AND s.row_id  = rel.asset_id
      AND mapping.product_name_7x = prod.name 
      AND mapping.type_7x = s.type_cd
      AND param.par_row_id (+) = s.row_id
      AND status = 'Processing';
{code}

h4. Asset SDP and param details extraction query with product_name, premise_udc_id resolution
{code:xml}
SELECT s.row_id AS src_id,
      s.data_src AS data_src,
      s.x_udc_asset_id AS udc_id,
      s.prod_id AS class_src_id,
      s.serial_num AS mfg_serial_num,
      s.x_lot_num AS mfg_lot_num,
      s.x_network_id AS network_id,
      s.status_cd AS status_cd,
      s.desc_text AS desc_text,
      s.purch_dt AS purchase_date,
      s.ship_dt AS ship_date,
      NULL AS mfg_test_date,
      s.x_retire_dt AS retire_date,
      s.x_mfg_date AS mfg_date,
      s.make_cd AS make,
      s.model_cd AS model,
      s.last_test_dt AS last_test_date,
      s.x_virtual_asset AS is_virtual_flg,
      s.x_universal_id AS badge_id,
      s.x_aep_num AS standard_id,
      s.x_electronic_id AS electronic_id,
      NULL AS comm_technology,
      NULL AS cur_inv_location_id,
      NULL AS sku,
      NULL AS part_num,
      s.type_cd AS TYPE,
      s.cfg_type_cd AS sub_type,
      DECODE (s.type_cd, 'Service', s.cfg_type_cd, s.type_cd) AS handler_type,
      s.per_addr_id AS premise_src_id,
      s.x_ax_feed_loc AS feed_loc,
      s.x_latitude_new AS gps_lat,
      s.x_longitude_new AS gps_long,
      s.x_pulse_output_blk AS pulse_output_block,
      s.x_seal_info AS seal_info,
      s.x_lock_info AS lock_info,
      s.x_util_access_info AS access_info,
      s.x_emeter_acces_info AS alt_access_info,
      s.x_util_premise_loc AS loc_info,
      s.x_emeter_premise_loc AS alt_loc_info,
      s.x_billing_cycle AS billing_cycle,
      s.x_reading_cycle AS reading_cycle,
      NULL AS timezone_id,
      NULL AS gis_id,
      s.x_read_dt AS billed_upto_time,
      s.x_power_status AS power_status,
      s.x_usage_status AS load_status,
      s.x_billing_hold AS billing_hold_status,
      s.meter_loc AS location_info,
      s.service_point_id AS svc_pt_src_id,
      s.start_dt AS eff_start_time,
      s.end_dt AS eff_end_time,
      s.cur_agree_id AS svc_agree_src_id,
      s.created AS source_created,
      s.last_upd AS source_last_upd,
      NULL AS insert_time,
      NULL AS last_upd_time,
      premise_tab.x_client_prmse_id AS premise_udc_id,
      mapping.product_name_8x AS name,
      param.attrib_01 AS param_name,
      param.attrib_02 AS param_value,
      param.attrib_12 AS eff_start_date,
      param.attrib_13 AS eff_end_date
 FROM s_asset s,
      s_addr_per premise_tab,
      s_prod_int prod,
      cx_temp_sdp_ref_id ref,
      cx_product_name_mapping mapping,
      s_asset_xm param
WHERE s.per_addr_id = premise_tab.row_id
      AND s.prod_id = prod.row_id
      AND ref.sdp_row_id = s.row_id
      AND mapping.product_name_7x = prod.name
      AND param.par_row_id (+) = s.row_id
      AND status = 'Processing';
{code}

h4. Asset channel extraction query with product_name, svc_pt_udc_id and register channel split resolution
{code:xml}
SELECT s.row_id AS src_id
       , s.data_src AS data_src
       , s.x_udc_asset_id AS udc_id
       , s.prod_id AS class_src_id
       , s.serial_num AS mfg_serial_num
       , s.x_lot_num AS mfg_lot_num
       , s.x_network_id AS network_id
       , s.status_cd AS status_cd
       , s.desc_text AS desc_text
       , s.purch_dt AS purchase_date
       , s.ship_dt AS ship_date
       , NULL AS mfg_test_date
       , s.x_retire_dt AS retire_date
       , s.x_mfg_date AS mfg_date
       , s.make_cd AS make
       , s.model_cd AS model
       , s.last_test_dt AS last_test_date
       , s.x_virtual_asset AS is_virtual_flg
       , s.x_universal_id AS badge_id
       , s.x_aep_num AS standard_id
       , s.x_electronic_id AS electronic_id
       , NULL AS comm_technology
       , NULL AS cur_inv_location_id
       , NULL AS sku
       , NULL AS part_num
       , s.type_cd AS TYPE
       , s.cfg_type_cd AS sub_type
       , DECODE (s.type_cd, 'Service', s.cfg_type_cd, s.type_cd)
            AS handler_type
       , s.per_addr_id AS premise_src_id
       , s.x_ax_feed_loc AS feed_loc
       , s.x_latitude_new AS gps_lat
       , s.x_longitude_new AS gps_long
       , s.x_pulse_output_blk AS pulse_output_block
       , s.x_seal_info AS seal_info
       , s.x_lock_info AS lock_info
       , s.x_util_access_info AS access_info
       , s.x_emeter_acces_info AS alt_access_info
       , s.x_util_premise_loc AS loc_info
       , s.x_emeter_premise_loc AS alt_loc_info
       , s.x_billing_cycle AS billing_cycle
       , s.x_reading_cycle AS reading_cycle
       , NULL AS timezone_id
       , NULL AS gis_id
       , s.x_read_dt AS billed_upto_time
       , s.x_power_status AS power_status
       , s.x_usage_status AS load_status
       , s.x_billing_hold AS billing_hold_status
       , s.meter_loc AS location_info
       , s.service_point_id AS svc_pt_src_id
       , s.start_dt AS eff_start_time
       , s.end_dt AS eff_end_time
       , s.cur_agree_id AS svc_agree_src_id
       , s.created AS source_created
       , s.last_upd AS source_last_upd
       , NULL AS insert_time
       , NULL AS last_upd_time
       , asset_for_udc.x_udc_asset_id AS sdp_udc_id
       , mapping.meas_type_udc_id_8x AS name
       , param.attrib_01 AS param_name
       , param.attrib_02 AS param_value
       , param.attrib_12 AS eff_start_date
       , param.attrib_13 AS eff_end_date
       , s.service_point_id sdp_row_id
    FROM s_asset s
       , s_asset asset_for_udc
       , s_prod_int prod
       , cx_temp_sdp_ref_id ref
       , channel_product_mapping mapping
       , s_asset_xm param
   WHERE s.service_point_id = asset_for_udc.row_id
     AND s.prod_id = prod.row_id
     AND ref.sdp_row_id = s.service_point_id
     AND s.type_cd = 'Channel' 
     AND mapping.channel_name_7x = prod.name
     AND param.par_row_id(+) = s.row_id
     AND status = 'Processing'
     ORDER BY s.row_id, mapping.meas_type_udc_id_8x;
{code}

h4. Services Query for SDP-DataService and SDP-ServiceAgree 
{code:xml}
SELECT s.row_id AS src_id
     , s.data_src AS data_src
     , s.x_udc_asset_id AS udc_id
     , s.prod_id AS class_src_id
     , s.serial_num AS mfg_serial_num
     , s.x_lot_num AS mfg_lot_num
     , s.x_network_id AS network_id
     , s.status_cd AS status_cd
     , s.desc_text AS desc_text
     , s.purch_dt AS purchase_date
     , s.ship_dt AS ship_date
     , NULL AS mfg_test_date
     , s.x_retire_dt AS retire_date
     , s.x_mfg_date AS mfg_date
     , s.make_cd AS make
     , s.model_cd AS model
     , s.last_test_dt AS last_test_date
     , s.x_virtual_asset AS is_virtual_flg
     , s.x_universal_id AS badge_id
     , s.x_aep_num AS standard_id
     , s.x_electronic_id AS electronic_id
     , NULL AS comm_technology
     , NULL AS cur_inv_location_id
     , NULL AS sku
     , NULL AS part_num
     , s.type_cd AS TYPE
     , s.cfg_type_cd AS sub_type
     , DECODE (s.type_cd, 'Service', s.cfg_type_cd, s.type_cd)
          AS handler_type
     , s.per_addr_id AS premise_src_id
     , s.x_ax_feed_loc AS feed_loc
     , s.x_latitude_new AS gps_lat
     , s.x_longitude_new AS gps_long
     , s.x_pulse_output_blk AS pulse_output_block
     , s.x_seal_info AS seal_info
     , s.x_lock_info AS lock_info
     , s.x_util_access_info AS access_info
     , s.x_emeter_acces_info AS alt_access_info
     , s.x_util_premise_loc AS loc_info
     , s.x_emeter_premise_loc AS alt_loc_info
     , s.x_billing_cycle AS billing_cycle
     , s.x_reading_cycle AS reading_cycle
     , NULL AS timezone_id
     , NULL AS gis_id
     , s.x_read_dt AS billed_upto_time
     , s.x_power_status AS power_status
     , s.x_usage_status AS load_status
     , s.x_billing_hold AS billing_hold_status
     , s.meter_loc AS location_info
     , s.service_point_id AS svc_pt_src_id
     , s.start_dt AS eff_start_time
     , s.end_dt AS eff_end_time
     , s.cur_agree_id AS svc_agree_src_id
     , s.created AS source_created
     , s.last_upd AS source_last_upd
     , NULL AS insert_time
     , NULL AS last_upd_time
     , asset_for_udc.x_udc_asset_id AS sdp_udc_id
     , mapping.product_name_8x AS name,
     , param.attrib_01 AS param_name
     , param.attrib_02 AS param_value
     , param.attrib_12 AS eff_start_date
     , param.attrib_13 AS eff_end_date
  FROM s_asset s
     , s_asset asset_for_udc
     , s_prod_int prod
     , s_asset_xm param
     , cx_temp_sdp_ref_id ref
     , cx_product_name_mapping mapping
 WHERE s.service_point_id = asset_for_udc.row_id(+)
   AND s.prod_id = prod.row_id
   AND param.par_row_id(+) = s.row_id
   AND s.type_cd = 'Service'
   AND ref.sdp_row_id = s.service_point_id
   AND mapping.product_name_7x = prod.name 
   AND mapping.type_7x = s.type_cd
   AND mapping.sub_type_7x = s.cfg_type_cd
   AND status = 'Processing'
{code}
**********************************************************************

h3. CHANNEL_PRODUCT_MAPPING

The table will be populated using Utility provided information related to register channel.

|| Column name || Type || Description ||
| channel_name_7x | Varchar | channel name from 7x |
| meas_type_udc_id_8x | Varchar | channel's meas_type udc id from 8x |
| register_index_7x | Varchar | Identifier for Register channel split value for TOU1, TOU2. Will have null value for Interval and Usage channel |
| insert_time | Timestamp | insert time of asset |
**********************************************************************************************************
Lookup mapping:

h2. Look-up mapping

h3. premise_lookup mapping 

||Column name||Type||Description||
|src_id|Varchar|source id of asset from 7x|
|udc_id|Varchar|udc id of asset from 7x|
|org_name|Varchar|org name of utility from 7x|
|insert_time|Timestamp|insert time of asset|

h3. svc_pt_lookup mapping

||Column name||Type||Description||
|src_id|Varchar|source id of asset from 7x|
|udc_id|Varchar|udc id of asset from 7x|
|premise_src_id|Varchar|source id of asset from 7x|
|svc_pt_class_src_id|Varchar|source id of asset class from 7x|
|org_name|Varchar|org name of utility from 7x|
|insert_time|Timestamp|insert time of asset|


h3. svc_pt_param_lookup mapping

||Column name||Type||Description||
|src_id|Varchar|source id of asset from 7x|
|svc_pt_src_id|Varchar|parent svc_pt src_id from 7x|
|name|Varchar|param name from 7x|
|value|Varchar|param value from 7x|
|start_time|Timestamp|param eff_start_time from 7x|
|end_time|Timestamp|param eff_end_time from 7x|
|org_name|Varchar|org name of utility from 7x|
|insert_time|Timestamp|insert time of asset|

h3. svc_pt_class_lookup mapping

||Column name||Type||Description||
|src_id|Varchar|source id of asset from 7x|
|udc_id|Varchar|udc id of asset from 7x|
|name|Varchar|svc_pt class name from 7x|
|org_name|Varchar|org name of utility from 7x|
|insert_time|Timestamp|insert time of asset|

h3. channel_lookup mapping

||Column name||Type||Description||
|src_id|Varchar|source id of asset from 7x|
|svc_pt_udc_id|Varchar|svc_pt udc_id from 7x|
|meas_type_udc_id_8x|Varchar|channel measType udcId for 8x|
|org_name|Varchar|org name of utility from 7x|
|insert_time|Timestamp|insert time of asset|

h3. device_lookup mapping

||Column name||Type||Description||
|src_id|Varchar|source id of asset from 7x|
|udc_id|Varchar|udc id of asset from 7x|
|device_class_src_id|Varchar|source id of asset class from 7x|
|type|Varchar|device type from 7x|
|sub_type|Varchar|device sub_type from 7x|
|org_name|Varchar|org name of utility from 7x|
|insert_time|Timestamp|insert time of asset|

h3. device_class_lookup mapping

||Column name||Type||Description||
|src_id|Varchar|source id of asset from 7x|
|udc_id|Varchar|udc id of asset from 7x|
|name|Varchar|device class name from 7x|
|org_name|Varchar|org name of utility from 7x|
|insert_time|Timestamp|insert time of asset|

h3. device_param_lookup mapping

||Column name||Type||Description||
|src_id|Varchar|source id of asset from 7x|
|par_src_id|Varchar|parent device src_id from 7x|
|name|Varchar|param name from 7x|
|value|Varchar|param value from 7x|
|start_time|Timestamp|param eff_start_time from 7x|
|end_time|Timestamp|param eff_end_time from 7x|
|org_name|Varchar|org name of utility from 7x|
|insert_time|Timestamp|insert time of asset|

h3. svc_pt_meter_lookup mapping

||Column name||Type||Description||
|src_id|Varchar|source id of rel from 7x|
|svc_pt_src_id|Varchar|svc_pt src_id from 7x|
|meter_src_id|Varchar|meter src_id from 7x|
|rel_type|Varchar|rel type from 7x|
|start_time|Timestamp|param eff_start_time from 7x|
|end_time|Timestamp|param eff_end_time from 7x|
|org_name|Varchar|org name of utility from 7x|
|insert_time|Timestamp|insert time of asset|

h3. meter_commFn_lookup mapping

||Column name||Type||Description||
|src_id|Varchar|source id of rel from 7x|
|meter_src_id|Varchar|meter src_id from 7x|
|commfn_src_id|Varchar|commfn src_id from 7x|
|rel_type|Varchar|rel type from 7x|
|start_time|Timestamp|param eff_start_time from 7x|
|end_time|Timestamp|param eff_end_time from 7x|
|org_name|Varchar|org name of utility from 7x|
|insert_time|Timestamp|insert time of asset|

h3. meter_channel_lookup mapping

||Column name||Type||Description||
|src_id|Varchar|source id of rel from 7x|
|meter_src_id|Varchar|meter src_id from 7x|
|channel_src_id|Varchar|channel src_id from 7x|
|rel_type|Varchar|rel type from 7x|
|start_time|Timestamp|param eff_start_time from 7x|
|end_time|Timestamp|param eff_end_time from 7x|
|org_name|Varchar|org name of utility from 7x|
|insert_time|Timestamp|insert time of asset|

h3. channel_product_mapping

The table will be populated using Utility provided information related to register channel.

||Column name||Type||Description||
|channel_name_7x|Varchar|channel name from 7x|
|meas_type_udc_id_8x|Varchar|channel's meas_type udc id from 8x|
|meter_param_dials_value|Varchar|meter parameter 'Dials' value from 7x. The value is used to split Register channel from 7x to multiple Register channels in 8x. *The column name may change*|
|insert_time|Timestamp|insert time of asset|

h3. product_name_mapping

||Column name||Type||Description||
|product_name_7x|Varchar|product name from 7x|
|type_7x|Varchar|entity type from 7x|
|sub_type_7x|Varchar|entity sub_type from 7x|
|product_name_8x|Varchar|product from 8x|
|type_8x|Varchar|entity type from 8x|
|sub_type_8x|Varchar|entity sub_type from 8x|
|insert_time|Timestamp|insert time of asset|

**********************************************************************************************************
|| 8.x fieldName || Siebel fieldName || Comments ||
|svc_pt_svc_agree_rel.id |-|ebo populated | 
|svc_pt_svc_agree_rel.svc_pt_id |s_asset.x_udc_asset_id  | sdp udc_id| 
|svc_pt_svc_agree_rel.svc_agree_class_id |cx_product_name_mapping.product_name_8x |s_prod_int.name mapped with cx_product_name_mapping.product_name_7x| 
|svc_pt_svc_agree_rel.eff_start_time|s_asset.start_dt| | 
|svc_pt_svc_agree_rel.eff_end_time |s_asset.end_dt | | 
|svc_pt_svc_agree_rel.service_plan_cd |{color:red}unmapped{color} |( )| 
|svc_pt_svc_agree_rel.svc_provider_id |{color:red}unmapped{color} |( )|
|svc_pt_svc_agree_rel.svc_agree_id |{color:red}unmapped{color} |( )| 
|svc_pt_svc_agree_rel.svc_agree_udc_id |{color:red}unmapped{color} |( )|
|svc_pt_svc_agree_rel.svc_agree_type |{color:red}unmapped{color} |( )|
|svc_pt_svc_agree_rel.data_src |-|siebel row_id ie. s_asset.row_id| 
|svc_pt_svc_agree_rel.org_id|-|ebo populated | 
|svc_pt_svc_agree_rel.insert_time |-|ebo populated | 
|svc_pt_svc_agree_rel.insert_by |-|ebo populated | 
|svc_pt_svc_agree_rel.last_upd_by |-|ebo populated | 
|svc_pt_svc_agree_rel.last_upd_time |-|ebo populated | 
|svc_pt_svc_agree_rel.rec_version_num |-|ebo populated |
|svc_pt_svc_agree_attr.id |-|ebo populated |
|svc_pt_svc_agree_attr.svc_pt_svc_agree_rel_id |-|ebo populated |
|svc_pt_svc_agree_attr.name |s_asset_xm.attrib_01 | |
|svc_pt_svc_agree_attr.value |s_asset_xm.attrib_02 | | 
|svc_pt_svc_agree_attr.active |s_asset_xm.attrib_03 |only 1st character of s_asset_xm.attrib_03 | 
|svc_pt_svc_agree_attr.data_src |-|ebo populated |
|svc_pt_svc_agree_attr.org_id |-|ebo populated |
|svc_pt_svc_agree_attr.insert_time |-|ebo populated |
|svc_pt_svc_agree_attr.insert_by |-|ebo populated |
|svc_pt_svc_agree_attr.last_upd_by |-|ebo populated |
|svc_pt_svc_agree_attr.last_upd_time |-|ebo populated |
|svc_pt_svc_agree_attr.rec_version_num |-|ebo populated |
|svc_pt_svc_agree_attr.id |-|ebo populated |




|svc_pt_svc_agree_rel.role |DISTRIBUTION-SDP | | 
|svc_pt_svc_agree_rel.rel_type |SDP-METER | | 
|svc_pt_svc_agree_attr.id |s_prod_ |ebo populated |
|svc_pt_svc_agree_attr.svc_pt_svc_agree_rel_id |-|ebo populated |
|svc_pt_svc_agree_attr.name |s_asset_xm.attrib_01 | |
|svc_pt_svc_agree_attr.value |s_asset_xm.attrib_02 | | 
|svc_pt_svc_agree_attr.eff_start_time |s_asset_xm.attrib_12 | | 
|svc_pt_svc_agree_attr.eff_end_time |s_asset_xm.attrib_13 | | 
|svc_pt_svc_agree_attr.data_src |-|ebo populated |
|svc_pt_svc_agree_attr.org_id |-|ebo populated |
|svc_pt_svc_agree_attr.insert_time |-|ebo populated |
|svc_pt_svc_agree_attr.insert_by |-|ebo populated |
|svc_pt_svc_agree_attr.last_upd_by |-|ebo populated |
|svc_pt_svc_agree_attr.last_upd_time |-|ebo populated |
|svc_pt_svc_agree_attr.rec_version_num |-|ebo populated |
|svc_pt_svc_agree_attr.id |-|ebo populated |



********************************************************************
|svc_pt_svc_agree_rel.cur_inv_location_id|{color:red}UnMapped{color} |( )| 
|svc_pt_svc_agree_rel.sku|NULL| | 
|svc_pt_svc_agree_rel.part_num|NULL| | 
|svc_pt_svc_agree_rel.mfg_date|s_asset.x_mfg_date | | 
|svc_pt_svc_agree_rel.svc_pt_svc_agree_rel_function_type|N |Y only in case of CommModule| 

|svc_pt_svc_agree_rel.alt_udc_id |s_asset.x_esp_svc_pt_svc_agree_rel_id | |
|svc_pt_svc_agree_rel.universal_id |s_asset.x_universal_id | |
|svc_pt_svc_agree_rel.pri_consumer_id |{color:red}UnMapped{color} |( )| 
|svc_pt_svc_agree_rel.alt_name|s_asset.x_name_2 | |
|svc_pt_svc_agree_rel.name |s_asset.x_name_1 | | 
|svc_pt_svc_agree_rel.desc_text |s_asset_xm.desc_text | | 
|svc_pt_svc_agree_rel.type |cx_product_name_mapping.type_8x |s_addr_per.type mapped with cx_product_name_mapping.type_7x | 
|svc_pt_svc_agree_rel.sub_type |cx_product_name_mapping.sub_type_8x | | 
|svc_pt_svc_agree_rel.name |cx_product_name_mapping.product_name_8x |s_prod_int.name mapped with cx_product_name_mapping.product_name_7x| 
|svc_pt_svc_agree_rel.service_type |s_prod_int.cx_product_name_mapping.sub_type_8x | | 
|svc_pt_svc_agree_rel.make |{color:red}UnMapped{color} |( )| 
|svc_pt_svc_agree_rel.model |{color:red}UnMapped{color} |( )| 
|svc_pt_svc_agree_rel.default_svc_pt_svc_agree_rel_config_id |{color:red}UnMapped{color} |( )| 
|svc_pt_svc_agree_rel.default_warranty_days |{color:red}UnMapped{color} |( )| 
|svc_pt_svc_agree_rel.default_comm_technology |{color:red}UnMapped{color} |( )|

|svc_pt_svc_agree_attr.id |s_prod_int_xm.attrib_01 | |
|svc_pt_svc_agree_attr.svc_pt_svc_agree_rel_id |-|ebo populated |
|svc_pt_svc_agree_attr.name |s_prod_int_xm.attrib_01 | |
|svc_pt_svc_agree_attr.value |s_prod_int_xm.attrib_02 | | 
|svc_pt_svc_agree_attr.eff_start_time |s_prod_int_xm.attrib_03 | | 
|svc_pt_svc_agree_attr.eff_end_time |s_prod_int_xm.attrib_03 | | 
|svc_pt_svc_agree_attr.premise_id |-|ebo populated |
|svc_pt_svc_agree_attr.data_src |-|ebo populated |
|svc_pt_svc_agree_attr.org_id |-|ebo populated |
|svc_pt_svc_agree_attr.insert_time |-|ebo populated |
|svc_pt_svc_agree_attr.insert_by |-|ebo populated |
|svc_pt_svc_agree_attr.last_upd_by |-|ebo populated |
|svc_pt_svc_agree_attr.last_upd_time |-|ebo populated |
|svc_pt_svc_agree_attr.rec_version_num |-|ebo populated |
|svc_pt_svc_agree_attr.id |-|ebo populated |

**********************************************************************************************************
	SELECT par_row_id child_dnode,
	attrib_02 parent_dnode,
	(SELECT x_udc_asset_id FROM s_asset sa1 WHERE sa1.row_id = sam.par_row_id) child_x_udc_asset_id,
	(SELECT x_udc_asset_id FROM s_asset sa2 WHERE sa2.row_id = sam.attrib_02) par_x_udc_asset_id,
	attrib_12 rel_start_date,
	attrib_13 rel_end_date, 
	'DISTNODE-DISTNODE' AS rel_type
	 FROM s_asset_xm  sam
	WHERE attrib_02 IN (@dnList@)
	  AND attrib_01 = @paramNameParentDN@
UNION ALL
	SELECT row_id child_dnode,
	par_asset_id parent_dnode,
	x_udc_asset_id child_x_udc_asset_id,
	(SELECT x_udc_asset_id FROM s_asset sa1 WHERE sa1.row_id = sa.par_asset_id) par_x_udc_asset_id,
	TO_DATE('01/01/1970 00:00:00','MM/DD/YYYY HH24:MI:SS' ) rel_start_date,
	NULL rel_end_date , 
	'DISTNODE-DISTNODE' AS rel_type
	FROM s_asset sa WHERE type_cd = 'Distribution Node'
	AND par_asset_id IN (@dnList@)
	AND par_asset_id NOT IN
	(SELECT attrib_02 FROM s_asset_xm  sam
	WHERE attrib_02 IN (@dnList@)
	  AND attrib_01 = @paramNameParentDN@)
UNION ALL
	SELECT par_row_id child_dnode,
	attrib_02 parent_dnode,
	(SELECT x_udc_asset_id FROM s_asset sa1 WHERE sa1.row_id = sam.par_row_id) child_x_udc_asset_id,
	(SELECT x_udc_asset_id FROM s_asset sa2 WHERE sa2.row_id = sam.attrib_02) par_x_udc_asset_id,
	attrib_12 rel_start_date,
	attrib_13 rel_end_date, 
	'DISTNODE-DISTNODE' AS rel_type
	 FROM s_asset_xm  sam
	WHERE par_row_id IN (@dnList@)
	  AND attrib_01 = @paramNameParentDN@
UNION ALL
	SELECT row_id child_dnode,
	par_asset_id parent_dnode,
	x_udc_asset_id child_x_udc_asset_id,
	(SELECT x_udc_asset_id FROM s_asset sa1 WHERE sa1.row_id = sa.par_asset_id) par_x_udc_asset_id,
	TO_DATE('01/01/1970 00:00:00','MM/DD/YYYY HH24:MI:SS' ) rel_start_date,
	NULL rel_end_date , 
	'DISTNODE-DISTNODE' AS rel_type
	FROM s_asset sa WHERE type_cd = 'Distribution Node'
	AND row_id IN (@dnList@)
	AND row_id NOT IN
	(SELECT par_row_id FROM s_asset_xm  sam
	WHERE par_row_id IN (@dnList@)
	  AND attrib_01 = @paramNameParentDN@);
	  
***********************************************************************************	  

Account,[0-5200|null|null|Reason:Udc_id is Null],
[0-55RNV|null|null|Reason:Udc_id is Null],
[0-R9NH|null|null|Reason:Udc_id is Null],
[1-13CS|null|null|Reason:Udc_id is Null],
[1-338|null|null|Reason:Udc_id is Null],
[1-J69|null|null|Reason:Udc_id is Null],[1-Z0N|null|null|Reason:Udc_id is Null],[INT_COMPANY_ID|null|null|Reason:Udc_id is Null],
**********************************************************************************************************************************

old rel distNode Query
SELECT TO_DATE(NVL((SELECT attrib_02
	       FROM s_asset_xm
	      WHERE par_row_id = par_distnode.row_id
		AND attrib_01 = @paramNameDNRelStartDate@), '01/01/1970 00:00:00'),'MM/DD/YYYY HH24:MI:SS') AS rel_start_time
	     , TO_DATE((SELECT attrib_02
	       FROM s_asset_xm
	      WHERE par_row_id = par_distnode.row_id
		AND attrib_01 = @paramNameDNRelEndDate@),'MM/DD/YYYY HH24:MI:SS')  AS rel_end_time
	     , par_distnode.x_udc_asset_id AS sdp_udc_id -- it is parentDN
	     , child_distnode.x_udc_asset_id AS distnode_udc_id
			 , 'DISTNODE-DISTNODE' AS rel_type
	     , par_distnode.row_id AS distnode_src_id
	  FROM s_asset child_distnode
	     , s_asset par_distnode
	 WHERE child_distnode.type_cd = 'Distribution Node'
	   AND child_distnode.row_id <> NVL(child_distnode.par_asset_id,'NULL')
	   AND par_distnode.row_id = child_distnode.par_asset_id
	   AND par_distnode.type_cd = 'Distribution Node'
	   AND child_distnode.row_id IN (@dnList@)
UNION
	SELECT TO_DATE(NVL((SELECT attrib_02
	       FROM s_asset_xm
	      WHERE par_row_id = par_distnode.row_id
		AND attrib_01 = @paramNameDNRelStartDate@), '01/01/1970 00:00:00'),'MM/DD/YYYY HH24:MI:SS') AS rel_start_time
	     , TO_DATE((SELECT attrib_02
	       FROM s_asset_xm
	      WHERE par_row_id = par_distnode.row_id
		AND attrib_01 = @paramNameDNRelEndDate@),'MM/DD/YYYY HH24:MI:SS')  AS rel_end_time
	     , par_distnode.x_udc_asset_id AS sdp_udc_id -- it is parentDN
	     , child_distnode.x_udc_asset_id AS distnode_udc_id
	     , 'DISTNODE-DISTNODE' AS rel_type
	     , child_distnode.row_id AS distnode_src_id
	  FROM s_asset child_distnode
	     , s_asset par_distnode
	 WHERE child_distnode.type_cd = 'Distribution Node'
	   AND NVL(child_distnode.par_asset_id,'NULL') <> child_distnode.row_id
	   AND par_distnode.row_id = child_distnode.par_asset_id
	   AND par_distnode.row_id IN (@dnList@)
***************************************************************************************************************					   
Old seed data query

<![CDATA[
SELECT            
	 p.row_id         AS src_id,
	 pxm.row_id       AS param_src_id, 
	 NULL             AS data_src, 
	 par_row_id       AS par_src_id, 
	 mapping.product_name_8x  AS name, 
	 attrib_01        AS param_name, 
	 attrib_02        AS param_value, 
	 attrib_03        AS param_status_cd, 
	 attrib_26    AS param_eff_start_time,      
	 attrib_27    AS param_eff_end_time,       
	 mapping.type_8x  AS type, 
	 mapping.sub_type_8x AS sub_type, 
	 DECODE(p.type, 'Service', p.sub_type, p.type) AS handler_type, 
	 pxm.created      AS source_created, 
	 pxm.last_upd     AS source_last_upd, 
	 NULL                 AS insert_time, 
	 NULL                AS last_upd_time, 
	 p.desc_text AS desc_text, 
	 p.status_cd AS status_cd, 
	 p.make_cd AS make, 
	 p.model AS model, 
	 p.uom_cd AS uom, 
	 x_group_name AS group_name, 
	 mapping.udc_id_8x as udc_id_8x,
	 mapping.register_index_7x as register_index_7x 
FROM  
	s_prod_int_xm pxm, 
	s_prod_int p, 
	cx_product_name_mapping mapping 
WHERE  
	 pxm.par_row_id  (+) = p.row_id 
	 AND p.name     = mapping.product_name_7x 
	 AND p.type     = mapping.type_7x 
	 AND p.type in ('Service Point','Distribution Node','Meter','CT-PT','Communication Module','Disconnect Collar','FRU','Service')  

UNION ALL	

SELECT            
	 p.row_id         AS src_id,
	 pxm.row_id       AS param_src_id, 
	 NULL             AS data_src, 
	 par_row_id       AS par_src_id, 
	 mapping.product_name_8x  AS name, 
	 attrib_01        AS param_name, 
	 attrib_02        AS param_value, 
	 attrib_03        AS param_status_cd, 
	 attrib_26    AS param_eff_start_time,      
	 attrib_27    AS param_eff_end_time,       
	 mapping.type_8x  AS type, 
	 mapping.sub_type_8x AS sub_type, 
	 DECODE(p.type, 'Service', p.sub_type, p.type) AS handler_type, 
	 pxm.created      AS source_created, 
	 pxm.last_upd     AS source_last_upd, 
	 NULL                 AS insert_time, 
	 NULL                AS last_upd_time, 
	 p.desc_text AS desc_text, 
	 p.status_cd AS status_cd, 
	 p.make_cd AS make, 
	 p.model AS model, 
	 p.uom_cd AS uom, 
	 x_group_name AS group_name, 
	 mapping.udc_id_8x as udc_id_8x,
	 mapping.register_index_7x as register_index_7x 
FROM  
	s_prod_int_xm pxm, 
	s_prod_int p, 
	cx_product_name_mapping mapping 
WHERE  
	 pxm.par_row_id  (+) = p.row_id 
	 AND p.name     = mapping.product_name_8x 
	 AND p.type     = mapping.type_8x 
	 AND p.type in ('Measurement')  
order by handler_type, src_id, register_index_7x
]]>
***************************************************************************************************************					   
Old sdp dataSvc rel

SELECT asset.row_id AS src_id
     , asset.start_dt AS rel_start_time
     , asset.end_dt AS rel_end_time
     , sdp.x_udc_asset_Id AS sdp_udc_id
     , mapping.product_name_8x AS prod_name
	 , asset.par_asset_id AS channel_src_id
     /*, (SELECT ch_mapping.udc_id_8x
	  FROM s_asset ch
	     --, cx_product_name_mapping ch_mapping
	     , s_prod_int ch_prod
	 WHERE ch.row_id = asset.par_asset_id
	   AND ch.type_cd = 'Channel'
	   AND ch.prod_id = ch_prod.row_id
	   --AND ch_prod.name = ch_mapping.product_name_7x) AS channel_type*/
     , (SELECT mtr.x_udc_asset_Id
	  FROM s_asset mtr
	 WHERE mtr.row_id = asset.par_asset_id
	   AND mtr.type_cd = 'Meter') AS device_udc_id
     , param.attrib_12 AS param_start_dt
     , param.attrib_13 AS param_end_dt
     , param.attrib_01 AS param_name
     , param.attrib_02 AS param_value
     , param.attrib_03 AS param_status_cd
 FROM s_asset asset
     , s_asset_xm param
     , s_prod_int prod
     , cx_product_name_mapping mapping
     , s_asset sdp
 WHERE asset.type_cd    = 'Service'
   AND asset.prod_id    = prod.row_id
   AND asset.row_id     = param.par_row_id (+)
   AND prod.name        = mapping.product_name_7x
   AND sdp.row_id       = asset.service_point_id
   AND sdp.row_id       =  
   AND asset.cfg_type_cd IN ( 'Data Collection'
			    , 'Web Presentment'
			    , 'Data Delivery'
			    , 'VEE'
			    , 'Framing'
			    , 'Deployment Planning'
			    , 'Virtual Channel Persistence'
			    , 'Estimation'
			    , 'Outage'
			    , 'Data Transfer'
			    , 'CO2 Plan')
 ORDER BY asset.row_id, mapping.udc_id_8x 
 
***************************************************************************************************************					   
Old sdpSvcAgree rel

SELECT asset.row_id AS src_id
     , asset.start_dt AS rel_start_time
     , asset.end_dt AS rel_end_time
     , sdp.x_udc_asset_Id AS sdp_udc_id
     , mapping.product_name_8x AS prod_name
     , NULL AS plan_name
     , param.attrib_12 AS param_start_dt
     , param.attrib_13 AS param_end_dt
     , param.attrib_01 AS param_name
     , param.attrib_02 AS param_value
     , param.attrib_03 AS param_status_cd
FROM s_asset asset
     , s_asset_xm param
     , s_prod_int prod
     , cx_product_name_mapping mapping
     , s_asset sdp
WHERE asset.type_cd  = 'Service'
   AND asset.prod_id = prod.row_id
   AND asset.row_id = param.par_row_id (+)
   AND prod.name = mapping.product_name_7x
   AND sdp.row_id = asset.service_point_id
   AND sdp.row_id =  
   AND asset.cfg_type_cd IN ('Energy Purchase'
			    ,'Generation Balance Provider'
			    ,'Consumption Balance Provider'
			    ,'Billing'
			    ,'AMI Operator'
			    ,'Energy Supplier'
			    ,'Energy Services Agent'
			    ,'Pricing Plan')
  ORDER BY asset.row_id, mapping.udc_id_8x 
***************************************************************************************************************					   
old sdp datasvcrel query:
=========================
SELECT asset.row_id AS src_id
					     , asset.start_dt AS rel_start_time
					     , asset.end_dt AS rel_end_time
					     , sdp.x_udc_asset_Id AS sdp_udc_id
					     , mapping.product_name_8x AS prod_name
		   				 , asset.par_asset_id AS channel_src_id
					     /*, (SELECT ch_mapping.udc_id_8x
					          FROM s_asset ch
					             --, cx_product_name_mapping ch_mapping
					             , s_prod_int ch_prod
					         WHERE ch.row_id = asset.par_asset_id
					           AND ch.type_cd = 'Channel'
					           AND ch.prod_id = ch_prod.row_id
					           --AND ch_prod.name = ch_mapping.product_name_7x) AS channel_type*/
					     , (SELECT mtr.x_udc_asset_Id
					          FROM s_asset mtr
					         WHERE mtr.row_id = asset.par_asset_id
					           AND mtr.type_cd = 'Meter') AS device_udc_id
					     , param.attrib_12 AS param_start_dt
					     , param.attrib_13 AS param_end_dt
					     , param.attrib_01 AS param_name
					     , param.attrib_02 AS param_value
		   				 , param.attrib_03 AS param_status_cd
					 FROM s_asset asset
					     , s_asset_xm param
					     , s_prod_int prod
					     , cx_product_name_mapping mapping
					     , s_asset sdp
					 WHERE asset.type_cd    = 'Service'
					   AND asset.prod_id    = prod.row_id
					   AND asset.row_id     = param.par_row_id (+)
					   AND prod.name        = mapping.product_name_7x
					   AND sdp.row_id       = asset.service_point_id
					   AND sdp.row_id       =  
					   AND asset.cfg_type_cd IN ( 'Data Collection'
					                            , 'Web Presentment'
					                            , 'Data Delivery'
					                            , 'VEE'
					                            , 'Framing'
					                            , 'Deployment Planning'
					                            , 'Virtual Channel Persistence'
					                            , 'Estimation'
					                            , 'Outage'
					                            , 'Data Transfer'
					                            , 'CO2 Plan')
					 ORDER BY asset.row_id, mapping.udc_id_8x  
***************************************************************************************************************		
Old svc_agree_rel Query:
=======================
		   			SELECT asset.row_id AS src_id
		   				 , asset.start_dt AS rel_start_time
					     , asset.end_dt AS rel_end_time
					     , sdp.x_udc_asset_Id AS sdp_udc_id
					     , mapping.product_name_8x AS prod_name
					     , NULL AS plan_name
					     , param.attrib_12 AS param_start_dt
					     , param.attrib_13 AS param_end_dt
					     , param.attrib_01 AS param_name
					     , param.attrib_02 AS param_value
					     , param.attrib_03 AS param_status_cd
					FROM s_asset asset
					     , s_asset_xm param
					     , s_prod_int prod
					     , cx_product_name_mapping mapping
					     , s_asset sdp
					WHERE asset.type_cd  = 'Service'
					   AND asset.prod_id = prod.row_id
					   AND asset.row_id = param.par_row_id (+)
					   AND prod.name = mapping.product_name_7x
					   AND sdp.row_id = asset.service_point_id
					   AND sdp.row_id =  
					   AND asset.cfg_type_cd IN ('Energy Purchase'
					                            ,'Generation Balance Provider'
					                            ,'Consumption Balance Provider'
					                            ,'Billing'
					                            ,'AMI Operator'
					                            ,'Energy Supplier'
					                            ,'Energy Services Agent'
					                            ,'Pricing Plan')
					  ORDER BY asset.row_id, mapping.udc_id_8x                          

***************************************************************************************************************					   
/*create table cx_country_mapping(
country_name_7x varchar2(50),
country_name_8x varchar2(50)
);

create table cx_state_prov_mapping(
stateProv_name_7x varchar2(50),
stateProv_name_8x varchar2(50)
);

create table cx_timezone_mapping(
timezone_name_7x varchar2(50),
timezone_name_8x varchar2(50)
);

create table cx_cycle_cd_mapping(
cycle_cd_7x varchar2(50),
cycle_cd_8x varchar2(50)
);

create table cx_alyt_distnode_cdc(
    distnode_row_id   varchar2(15),
    processed_flag    char(1),
    src_last_upd_time date,
    last_upd_time     date
);
*/
***************************************************************************************************************					   
delete  from  cx_alyt_sdp_cdc;--8
delete  from  cx_alyt_device_cdc;
delete  from  cx_alyt_route_cdc;
delete  from  cx_alyt_premise_cdc; --7
delete  from  cx_alyt_account_cdc; --23
commit;

select row_id, par_asset_id from s_asset where type_cd = 'Distribution Node';

select count(*) from s_asset;
update s_asset set last_upd= sysdate;
--where cfg_type_cd IN ( 'Data Collection', 'Data Delivery', 'VEE', 'Framing', 'Deployment Planning', 'Virtual Channel Persistence', 'Estimation', 'Outage', 'Data Transfer', 'Web Presentment', 'CO2 Plan', 'Energy Purchase', 'Generation Balance Provider', 'Consumption Balance Provider', 'Billing', 'AMI Operator', 'Energy Supplier', 'Pricing Plan');
update s_asset_rel set last_upd= sysdate;
update s_asset_xm set last_upd= sysdate;
update s_addr_per set last_upd= sysdate;
update s_org_ext set last_upd= sysdate;
update s_org_ext_xm set last_upd= sysdate;
update cx_asset_accnt set last_upd= sysdate;
commit;

select * from  cx_alyt_sdp_cdc;--8
select * from  cx_alyt_device_cdc;
select * from  cx_alyt_route_cdc;
select * from  cx_alyt_premise_cdc; --7
select * from  cx_alyt_account_cdc; --23

------------------------------------------------------
drop table cx_alyt_sdp_cdc;
drop table cx_alyt_device_cdc;
drop table cx_alyt_route_cdc;
drop table cx_alyt_premise_cdc;
drop table cx_alyt_account_cdc;


--product_name_mapping


create table cx_alyt_sdp_cdc(
    sdp_row_id        varchar2(15),
    processed_flag    char(1) default 'N',
    marked_flag    char(1) default 'N',
    src_last_upd_time date,
    last_upd_time  TIMESTAMP default SYSTIMESTAMP,
    insert_time  TIMESTAMP default SYSTIMESTAMP
);

create table cx_alyt_device_cdc(
    device_row_id     varchar2(15),
    processed_flag    char(1) default 'N',
    marked_flag    char(1) default 'N',
    src_last_upd_time date,
    last_upd_time  TIMESTAMP default SYSTIMESTAMP,
    insert_time  TIMESTAMP default SYSTIMESTAMP
);

create table cx_alyt_premise_cdc(
    premise_row_id    varchar2(15),
    processed_flag    char(1) default 'N',
    marked_flag    char(1) default 'N',
    src_last_upd_time date,
    last_upd_time  TIMESTAMP default SYSTIMESTAMP,
    insert_time  TIMESTAMP default SYSTIMESTAMP
);


create table cx_alyt_account_cdc (
	account_row_id varchar2(15),
	processed_flag    char(1) default 'N',
    marked_flag    char(1) default 'N',
    src_last_upd_time date,
    last_upd_time  TIMESTAMP default SYSTIMESTAMP,
    insert_time  TIMESTAMP default SYSTIMESTAMP
);


create table cx_alyt_route_cdc (
	route_row_id varchar2(15),
	processed_flag    char(1) default 'N',
    marked_flag    char(1) default 'N',
    src_last_upd_time date,
    last_upd_time  TIMESTAMP default SYSTIMESTAMP,
    insert_time  TIMESTAMP default SYSTIMESTAMP
);

CREATE OR REPLACE TRIGGER trg_alyt_asset_capture
FOR INSERT OR UPDATE ON s_asset
COMPOUND TRIGGER

-- Defining Record Type
TYPE tab_dn_row_chg IS RECORD (
row_id            s_asset.row_id TYPE,
src_last_upd_time cx_alyt_sdp_cdc.src_last_upd_time TYPE
);

TYPE t_row_id IS TABLE OF tab_dn_row_chg;
v_row_id t_row_id := t_row_id();

BEFORE EACH ROW IS
BEGIN
   IF :new.type_cd IN ('Service Point', 'Channel')
   THEN 
          INSERT INTO cx_alyt_sdp_cdc 
              (
                  sdp_row_id
                , src_last_upd_time
               
              )
          VALUES
              (
                  CASE
                      WHEN :new.type_cd = 'Service Point'
                      THEN :new.row_id
                      ELSE :new.service_point_id
                   END
                , :new.last_upd
           );
    ELSIF :new.type_cd IN ('Meter', 'Communication Module', 'CT-PT', 'FRU', 'Disconnect Collar')
    THEN 
          INSERT INTO cx_alyt_device_cdc 
              (
                  device_row_id
                , src_last_upd_time
              )
          VALUES
              (
                  :new.row_id
                , :new.last_upd
           ); 
    ELSIF :new.type_cd IN ('Route')
    THEN 
          INSERT INTO cx_alyt_route_cdc 
              (
                  route_row_id
                , src_last_upd_time
              )
          VALUES
              (
                  :new.row_id
                , :new.last_upd
           );       
    ELSIF :new.type_cd = 'Distribution Node' 
    THEN 
           INSERT INTO cx_alyt_sdp_cdc
           		(
                	sdp_row_id
                      , src_last_upd_time
                         )
	                SELECT asset_id
		             , :new.last_upd
	                FROM s_asset_rel
	                WHERE par_asset_id = :new.row_id
	                  AND x_asset_relation_type_cd = 'DISTNODE-SDP';
                   
                  v_row_id.extend;
                  v_row_id(v_row_id.last).row_id := :new.row_id;
                  v_row_id(v_row_id.last).src_last_upd_time := :new.last_upd;
                  
    ELSIF (:new.type_cd = 'Service'
      AND :new.cfg_type_cd IN ( 'Data Collection', 'Data Delivery', 'VEE', 'Framing', 'Deployment Planning', 'Virtual Channel Persistence', 'Estimation', 'Outage', 'Data Transfer', 'Web Presentment', 'CO2 Plan', 'Energy Purchase', 'Generation Balance Provider', 'Consumption Balance Provider', 'Billing', 'AMI Operator', 'Energy Supplier', 'Pricing Plan'))
    THEN 
       INSERT INTO cx_alyt_sdp_cdc
           (
               sdp_row_id
             , src_last_upd_time
           )
       VALUES
           (
              :new.service_point_id
            , :new.last_upd
           ); 
     END IF;
END BEFORE EACH ROW;

AFTER STATEMENT IS
BEGIN
    IF (v_row_id.COUNT > 0)
    THEN
	    FOR i IN v_row_id.FIRST .. v_row_id.LAST
	    LOOP
	        INSERT INTO cx_alyt_sdp_cdc
	            (
	                 sdp_row_id
	               , src_last_upd_time
	            )
	        SELECT rel.asset_id
		    , v_row_id(i).src_last_upd_time
		 FROM s_asset asset
		    , s_asset_rel rel
		 WHERE asset.par_asset_id = v_row_id(i).row_id
		   AND asset.row_id       = rel.par_asset_id
	           AND rel.x_asset_relation_type_cd = 'DISTNODE-SDP';
	        
	    END LOOP;
    END IF;
END AFTER STATEMENT;

END trg_alyt_asset_capture;
/


CREATE OR REPLACE TRIGGER trg_alyt_relation_capture
   FOR INSERT OR UPDATE ON s_asset_rel
   COMPOUND TRIGGER
   
-- Defining Record Type
TYPE tab_commmod_meter_row_chg IS RECORD (
    asset_id           s_asset_rel.asset_id TYPE,
    src_last_upd_time  cx_alyt_sdp_cdc.src_last_upd_time TYPE,
    meter_loc_start_dt s_asset_rel.meter_loc_start_dt TYPE,
    meter_loc_end_dt   s_asset_rel.meter_loc_end_dt TYPE
);

    TYPE t_row_id IS TABLE OF tab_commmod_meter_row_chg;
    v_row_id t_row_id := t_row_id();

BEFORE EACH ROW IS
BEGIN
   IF :new.x_asset_relation_type_cd IN ('DISTNODE-SDP', 'ROUTE-SDP')
   THEN
      INSERT INTO cx_alyt_sdp_cdc
      	(
              sdp_row_id
            , src_last_upd_time
        )
      VALUES (
        	:new.asset_id
              , :new.last_upd
            );
   ELSIF :new.x_asset_relation_type_cd IN ('SDP-METER', 'SDP-CTPT')
   THEN
      INSERT INTO cx_alyt_sdp_cdc(
                  sdp_row_id
                , src_last_upd_time
                 )
           VALUES (
                     :new.par_asset_id
                   , :new.last_upd
                   );
   ELSIF :new.x_asset_relation_type_cd IN ('METER-CHANNEL')
   THEN
      INSERT INTO cx_alyt_sdp_cdc(
                  sdp_row_id
                , src_last_upd_time
                 )
                 SELECT service_point_id
                      , :new.last_upd
                 FROM s_asset
                       WHERE row_id = :new.asset_id;
   ELSIF :new.x_asset_relation_type_cd IN ('COMMUNICATION-METER')
   THEN
      v_row_id.extend;
      v_row_id(v_row_id.last).asset_id := :new.asset_id;
      v_row_id(v_row_id.last).src_last_upd_time := :new.last_upd;
      v_row_id(v_row_id.last).meter_loc_start_dt := :new.meter_loc_start_dt;
      v_row_id(v_row_id.last).meter_loc_end_dt := :new.meter_loc_end_dt;
      
   END IF;
END BEFORE EACH ROW;

AFTER STATEMENT IS
BEGIN
    IF (v_row_id.COUNT > 0)
    THEN
	    FOR i IN v_row_id.FIRST .. v_row_id.LAST
	    LOOP
	        INSERT INTO cx_alyt_sdp_cdc
	            (
	                 sdp_row_id
	               , src_last_upd_time
	            )
	        SELECT par_asset_id
	             , v_row_id(i).src_last_upd_time
	          FROM s_asset_rel
	         WHERE asset_id = v_row_id(i).asset_id 
	           AND x_asset_relation_type_cd = 'SDP-METER'
	           AND meter_loc_start_dt BETWEEN v_row_id(i).meter_loc_start_dt 
	                                      AND v_row_id(i).meter_loc_end_dt;
	        
	    END LOOP;
    END IF;
END AFTER STATEMENT;

END trg_alyt_relation_capture;
/

CREATE OR REPLACE TRIGGER trg_alyt_asset_param_capture
AFTER INSERT OR UPDATE
ON s_asset_xm
FOR EACH ROW
DECLARE
   v_cfg_type_cd        VARCHAR2(50);
   v_service_point_id   VARCHAR2(15);
BEGIN
   IF :new.type = 'SDP_ASSET_ATTRIB'
   THEN
      INSERT INTO cx_alyt_sdp_cdc  (
                sdp_row_id
              , src_last_upd_time
              )
        VALUES (
                :new.par_row_id
              , :new.last_upd
   );
   ELSIF :new.type = 'METER_ASSET_ATTRIB'
   THEN
      INSERT INTO cx_alyt_device_cdc  (
                device_row_id
              , src_last_upd_time
              )
        VALUES (
                :new.par_row_id
              , :new.last_upd
   );
   ELSIF :new.type = 'COMMMOD_ASSET_ATTRIB'
   THEN
      INSERT INTO cx_alyt_device_cdc  (
                device_row_id
              , src_last_upd_time
              )
        VALUES (
                :new.par_row_id
              , :new.last_upd
   );
   ELSIF :new.type = 'DISTNODE_ASSET_ATTRIB'
   THEN
   		INSERT INTO cx_alyt_sdp_cdc
           		(
                	sdp_row_id
                    , src_last_upd_time
                )
	                SELECT asset_id
		                , :new.last_upd
	                FROM s_asset_rel
	                WHERE par_asset_id = :new.par_row_id
	                         AND x_asset_relation_type_cd = 'DISTNODE-SDP'
                UNION
                    SELECT rel.asset_id
                    	, :new.last_upd
                    FROM s_asset asset
                       	, s_asset_rel rel
                    WHERE asset.par_asset_id = :new.par_row_id
                    	AND asset.row_id       = rel.par_asset_id
                        AND rel.x_asset_relation_type_cd = 'DISTNODE-SDP';
   ELSIF :new.type = 'SERVICE_ASSET_ATTRIB'
   THEN   
      SELECT cfg_type_cd, service_point_id 
        INTO v_cfg_type_cd, v_service_point_id
        FROM s_asset
       WHERE row_id = :NEW.par_row_id;
 
      IF v_cfg_type_cd  IN ( 'Data Collection', 'Data Delivery', 'VEE', 'Framing', 'Deployment Planning', 'Virtual Channel Persistence', 'Estimation', 'Outage', 'Data Transfer', 'Web Presentment', 'CO2 Plan', 'Energy Purchase', 'Generation Balance Provider', 'Consumption Balance Provider', 'Billing', 'AMI Operator', 'Energy Supplier', 'Pricing Plan')
      THEN
         INSERT INTO cx_alyt_sdp_cdc 
           (
               sdp_row_id  
             , src_last_upd_time
           )
       VALUES
           (
              v_service_point_id
            , :new.last_upd
           );      
 
      END IF;
   END IF;
EXCEPTION
   WHEN OTHERS
   THEN
      RAISE;
END;
/

CREATE OR REPLACE TRIGGER trg_alyt_premise_capture
AFTER INSERT OR UPDATE ON s_addr_per
FOR EACH ROW
BEGIN
    INSERT INTO cx_alyt_premise_cdc
        (
            premise_row_id
          , src_last_upd_time
        )
    VALUES
        (
            :new.row_id
          , :new.last_upd
        );
 
EXCEPTION
    WHEN OTHERS
    THEN
        RAISE;
END;
/

CREATE OR REPLACE TRIGGER trg_alyt_account_capture
AFTER INSERT OR UPDATE ON s_org_ext
FOR EACH ROW
BEGIN
    INSERT INTO cx_alyt_account_cdc
        (
            account_row_id
          , src_last_upd_time
        )
    VALUES
        (
            :new.row_id
          , :new.last_upd
        );
 
EXCEPTION
    WHEN OTHERS
    THEN
        RAISE;
END;
/

CREATE OR REPLACE TRIGGER trg_alyt_account_param_capture
AFTER INSERT OR UPDATE ON s_org_ext_xm
FOR EACH ROW
BEGIN
    INSERT INTO cx_alyt_account_cdc
        (
            account_row_id
          , src_last_upd_time
        )
    VALUES
        (
            :new.par_row_id
          , :new.last_upd
        );
 
EXCEPTION
    WHEN OTHERS
    THEN
        RAISE;
END;
/


CREATE OR REPLACE TRIGGER trg_alyt_accnt_svc_pt_capture
AFTER INSERT OR UPDATE ON cx_asset_accnt
FOR EACH ROW
BEGIN
    INSERT INTO cx_alyt_sdp_cdc
        (
            sdp_row_id
          , src_last_upd_time
        )
    VALUES
        (
            :new.asset_id
          , :new.last_upd
        );
  
EXCEPTION
    WHEN OTHERS
    THEN
      RAISE;
END;
/
***************************************************************************************************************					   
Old svcptDataSvcRel query

SELECT DISTINCT asset.row_id AS src_id
					     , asset.start_dt AS rel_start_time
					     , asset.end_dt AS rel_end_time
					     , sdp.x_udc_asset_Id AS sdp_udc_id
					     , mapping.product_name_8x AS prod_name
		   				 , asset.par_asset_id AS channel_src_id
					     /*, (SELECT ch_mapping.udc_id_8x
					          FROM s_asset ch
					             --, cx_product_name_mapping ch_mapping
					             , s_prod_int ch_prod
					         WHERE ch.row_id = asset.par_asset_id
					           AND ch.type_cd = 'Channel'
					           AND ch.prod_id = ch_prod.row_id
					           --AND ch_prod.name = ch_mapping.product_name_7x) AS channel_type*/
					     , (SELECT mtr.x_udc_asset_Id
					          FROM s_asset mtr
					         WHERE mtr.row_id = asset.par_asset_id
					           AND mtr.type_cd = 'Meter') AS device_udc_id
					     , FIRST_VALUE(param.attrib_12) OVER(PARTITION BY param.attrib_01, asset.row_id ORDER BY param.attrib_13 NULLS FIRST, param.attrib_12 desc) AS param_start_dt
               	     	 , FIRST_VALUE(param.attrib_13) OVER(PARTITION BY param.attrib_01, asset.row_id ORDER BY param.attrib_13 NULLS FIRST, param.attrib_12 desc) AS param_end_dt
               	     	 , FIRST_VALUE(param.attrib_01) OVER(PARTITION BY param.attrib_01, asset.row_id ORDER BY param.attrib_13 NULLS FIRST, param.attrib_12 desc) AS param_name
               	     	 , FIRST_VALUE(param.attrib_02) OVER(PARTITION BY param.attrib_01, asset.row_id ORDER BY param.attrib_13 NULLS FIRST, param.attrib_12 desc) AS param_value
               	     	 , FIRST_VALUE(param.attrib_03) OVER(PARTITION BY param.attrib_01, asset.row_id ORDER BY param.attrib_13 NULLS FIRST, param.attrib_12 desc) AS param_status_cd
					 FROM s_asset asset
					     , s_asset_xm param
					     , s_prod_int prod
					     , cx_product_name_mapping mapping
					     , s_asset sdp
					 WHERE asset.type_cd    = 'Service'
					   AND asset.prod_id    = prod.row_id
					   AND asset.row_id     = param.par_row_id (+)
					   AND prod.name        = mapping.product_name_7x
					   AND sdp.row_id       = asset.service_point_id
					   AND sdp.row_id       =  
					   AND asset.cfg_type_cd IN ( 'Data Collection'
					                            , 'Web Presentment'
					                            , 'Data Delivery'
					                            , 'VEE'
					                            , 'Framing'
					                            , 'Deployment Planning'
					                            , 'Virtual Channel Persistence'
					                            , 'Estimation'
					                            , 'Outage'
					                            , 'Data Transfer'
					                            , 'CO2 Plan')
					 ORDER BY src_id, prod_name

***************************************************************************************************************					   
//EnergyIP/opt8/AnalyticsDataLoader/trunk/common.build.xml does not exist in the Perforce repository.
***************************************************************************************************************					   
8x systemconsole:
https://ind-lnxapp53.emeter.com:9553/systemconsole/config.html
pipe
pipepass1

8x opt ConfigManagement command:
./ConfigurationManagementTool.sh -DconfigDataTransferService.configDataSourcePath=/home/eip/opt/em-ac-dataloader/tools/confXML


Create org:
./orgAdmin.sh -c create -o JITENORG -u JITENORG_USER -p JITENORG_USER -f JITENORG -l DEV
./orgAdmin.sh -c importRefData -o JITENORG 

RDU execute:
/home/eip/bin/ReferenceDataUtil.sh -Dapplication.scope=org -Dapplication.orgName=FLEXSYNCORG -Dapplication.command=import -DimportReferenceDataService.referenceDataSourcePath=/home/eip/data/acdataloader/in/ADF_StructuralData_20141007_110631_temp

/home/eip/bin/ReferenceDataUtil.sh -Dapplication.scope=org -Dapplication.orgName=FLEXSYNCORG

																								       													
Application launcher:
./ApplicationLauncher.sh start ACDataLoader 1 -DkeyStoreProperties.keyStorePath=/home/eip/conf/systemProperties/keystore -DkeyStoreProperties.keyStoreType=jceks -DkeySs.keyStorePassword=OBF:1uuu1vgj1jjz1u2u1sar1m171vv11ym91x1b1ym51vu91lwz1saj1u2g1jnj1vgb1uvc -DkeyStoreProperties.keyManagerPassword=OBF:1uuu1vgj1jjz1u2u1sar1m171vv11ym91x1b1ym51vg1jnj1vgb1uvc -DkeyStoreProperties.secretKeyAlias=aeskey -DmonitoringClient.checkDependency=false


***************************************************************************************************************					   
Eclipse 8x debug configurations
1:
D:/home/eip/conf/systemProperties/eipenv.properties -Dapplication.name=ACDataLoader -Dapplication.instanceId=1 -DmonitoringClient.checkDependency=false -DkeyStoreProperties.keyStorePath=D:/home/eip/conf/systemProperties/localhostkeystore -DcasProperties.baseUrl=https://ind-lnxapp53.emeter.com:9443/em-cas -DkeyStoreProperties.keyStorePath=/home/eip/conf/systemProperties/keystore -DkeyStoreProperties.keyStoreType=jceks -DkeyStoreProperties.keyStorePassword=OBF:1uuu1vgj1jjz1u2u1sar1m171vv11ym91x1b1ym51vu91lwz1saj1u2g1jnj1vgb1uvc -DkeyStoreProperties.keyManagerPassword=OBF:1uuu1vgj1jjz1u2u1sar1m171vv11ym91x1b1ym51vu91lwz1saj1u2g1jnj1vgb1uvc

2:
D:/home/eip/conf/systemProperties/eipenv.properties -Dapplication.name=ACDataLoader -Dapplication.instanceId=1 -DmonitoringClient.checkDependency=false -DkeyStoreProperties.keyStorePath=D:/home/eip/conf/systemProperties/localhostkeystore -DcasProperties.baseUrl=https://ind-lnxapp53.emeter.com:9443/em-cas -DkeyStoreProperties.keyStorePath=/home/eip/conf/systemProperties/keystore -DkeyStoreProperties.keyStoreType=jceks -DkeyStoreProperties.keyStorePassword=OBF:1uuu1vgj1jjz1u2u1sar1m171vv11ym91x1b1ym51vu91lwz1saj1u2g1jnj1vgb1uvc -DkeyStoreProperties.keyManagerPassword=OBF:1uuu1vgj1jjz1u2u1sar1m171vv11ym91x1b1ym51vu91lwz1saj1u2g1jnj1vgb1uvc -Da109-assetDataLoader.loaderWaitPerodInMilliSec=1000 -Da109-dataLoaderImpl.unZipDirPath=/home/eip/data/acdataloader/in -Da109-assetDataLoader.maxWaitCounter=55

3:
com.emeter.hydrofw.application.PropertiesBootstrap
D:/home/eip/conf/systemProperties/eipenv.properties -Dapplication.name=ACDataLoader -Dapplication.instanceId=1 -DspringBeansXml.application=classpath:conf/appXML/AnalyticsDataLoader.xml -DmonitoringClient.checkDependency=false -DkeyStoreProperties.keyStorePath=D:/home/eip/conf/systemProperties/localhostkeystore -DcasProperties.baseUrl=https://ind-lnxapp53.emeter.com:9443/em-cas -DkeyStoreProperties.keyStorePath=/home/eip/conf/systemProperties/keystore -DkeyStoreProperties.keyStoreType=jceks -DkeyStoreProperties.keyStorePassword=OBF:1uuu1vgj1jjz1u2u1sar1m171vv11ym91x1b1ym51vu91lwz1saj1u2g1jnj1vgb1uvc -DkeyStoreProperties.keyManagerPassword=OBF:1uuu1vgj1jjz1u2u1sar1m171vv11ym91x1b1ym51vu91lwz1saj1u2g1jnj1vgb1uvc -Da109-assetDataLoader.loaderWaitPerodInMilliSec=1000 -Da109-dataLoaderImpl.unZipDirPath=/home/eip/data/acdataloader/in -Da109-assetDataLoader.maxWaitCounter=10 -Da109-dataLoaderImpl.flexSyncfailedMsgDirPath=/home/eip/data/synchronization/flexsync/failedMsg
***************************************************************************************************************					   
<bean id= universalSyncInterfacePortTypeFilePoller  class= com.emeter.energyip.syncinterface.file.UniversalSyncInterfacePortTypeFilePoller >
	<property name= universalSyncInterfacePortTypeDelegate  ref= universalSyncInterfacePortTypeDelegate  />
</bean>


<bean id= universalSyncInterfacePortTypeDelegate  class= com.emeter.flexsync.request.SyncRequestProcessor >
		<property name= syncEngine  ref= syncEngine  />
		<property name= srcMapAction  ref= sourceMappingAction />
</bean>	





***************************************************************************************************************					   
-Xms512m -Xmx1024m



./ApplicationLauncher.sh start CloudAlytStructuralExtraction 1 -Xms512m -Xmx1024m -Da108-structuralExtractApplication.threadSizeSdpStack=32 -Da108-structuralExtractApplication.countOfSdpMsgInXML=300 -Da108-structuralExtractApplication.threadSizeAsset=32 -Da108-structuralExtractApplication.countOfMsgInXML=500 -DcloudExtDataSource.encrypted=false -DcloudExtDataSource.driverClassName=oracle.jdbc.driver.OracleDriver -DcloudExtDataSource.maxActive=100 -DcloudExtDataSource.maxWait=10000 -DcloudExtDataSource.maxIdle=80 -DcloudExtDataSource.minIdle=80 -DcloudExtDataSource.initialSize=10 -DcloudExtDataSource.url='jdbc:oracle:thin:@(DESCRIPTION=(ADDRESS_LIST=(ADDRESS=(PROTOCOL=TCP)(HOST=emdb36.emeter.com)(PORT=1521)))(CONNECT_DATA=(SERVER=DEDICATED)(SERVICE_NAME=sebldb80)))' -DcloudExtDataSource.username=siebel -DcloudExtDataSource.password=siebel
./ApplicationLauncher.sh start CloudAlytStructuralExtraction 1 -Xms512m -Xmx1024m -Da108-structuralExtractApplication.threadSizeSdpStack=32 -Da108-structuralExtractApplication.countOfSdpMsgInXML=300 -Da108-structuralExtractApplication.threadSizeAsset=32 -Da108-structuralExtractApplication.countOfMsgInXML=500 -DcloudExtDataSource.encrypted=false -DcloudExtDataSource.driverClassName=oracle.jdbc.driver.OracleDriver -DcloudExtDataSource.maxActive=100 -DcloudExtDataSource.maxWait=10000 -DcloudExtDataSource.maxIdle=80 -DcloudExtDataSource.minIdle=80 -DcloudExtDataSource.initialSize=10 -DcloudExtDataSource.url={siebelDataSource.url} -DcloudExtDataSource.username=siebel -DcloudExtDataSource.password=siebel -Da108-structuralExtractorDao.fetchSize=50 -DspringBeansXml.cloudExtDataSource=classpath:com/emeter/extractor/beanXMLs/cloudExtDataSource.xml

-bash-3.2  ./ApplicationLauncher.sh start CloudAlytStructuralExtraction 1 -Xms512m -Xmx1024m
-bash-3.2  ./ApplicationLauncher.sh start CloudAlytStructuralExtraction 1 -Da108-structuralExtractApplication.threadSizeAsset=100 -Da108-structuralExtractApplication.countOfSdpMsgInXML=50

./ApplicationLauncher.sh start CloudAlytStructuralExtraction 1 -Xms256m -Xmx1024m -Da108-structuralExtractApplication.threadSizeSdpStack=20 -Da108-structuralExtractApplication.countOfSdpMsgInXML=10 -Da108-structuralExtractApplication.threadSizeAsset=100 -Da108-structuralExtractApplication.countOfMsgInXML=100


./ApplicationLauncher.sh start CloudAlytStructuralExtraction 1 -Xms512m -Xmx1024m a108-structuralExtractApplication.threadSizeSdpStack=50 -Da108-structuralExtractApplication.countOfSdpMsgInXML=50 -Da108-structuralExtractApplication.threadSizeAsset=100 -Da108-structuralExtractApplication.countOfMsgInXML=100
-Dcom.sun.management.jmxremote.port=9876 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false

./ApplicationLauncher.sh start CloudAlytStructuralExtraction 1 a108-structuralExtractApplication.threadSizeSdpStack=50 -Da108-structuralExtractApplication.countOfSdpMsgInXML=300 -Da108-structuralExtractApplication.threadSizeAsset=100 -Da108-structuralExtractApplication.countOfMsgInXML=500 


run application 
attach pid using wizard 
as ./<directory with unpacked content>/bin/yjp.sh -attach <PID> 
or 

use UI console on windows 

verify agent load 
java -agentpath:/home/pipe/RGA/yjp-12.0.3/bin/linux-x86-64/libyjpagent.so

2014-11-04 04:34:31,640 com.emeter.extractor.runner.sdpstackdata.SdpStackData [main] INFO  - SdpStackData stat : 
ServiceDeliveryPoint,121
DistributionNode,242
TotalChannel,1573,Channel,1452,VirtualChannel,121
servicePointServiceAgreementAssociation,242
servicePointDataServiceAssociation,2177
servicePointDeviceAssociation,363
deviceChannelAssociation,1573
deviceFunctionAssociation,121
servicePointServicePointGroupAssociation,121
accountServicePointAssociation,121
servicePointServicePointAssociation SDP-DN,121
servicePointServicePointAssociation DN-DN,121




*******************************************************************************************************************
QE-lnxapp110:

2014-11-05 04:10:08,194 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - APPLICATION SUCCESSFULLY STARTED
2014-11-05 04:10:08,194 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - StructuralExtractApplication Mode:: initial
2014-11-05 04:10:08,612 com.emeter.extractor.runner.seeddata.SeedDataCycleCdExtractRunner [pool-1-thread-2] INFO  - Total Records for cycleCd extraction:1
2014-11-05 04:10:18,767 com.emeter.extractor.runner.seeddata.SeedDataExtractRunner [pool-1-thread-1] INFO  - Total Records for Seed data extraction:71
2014-11-05 04:10:24,590 com.emeter.extractor.runner.assetdata.route.AssetDataRoute [pool-2-thread-4] INFO  - AssetDataRoute Total Records:121
2014-11-05 04:10:25,043 com.emeter.extractor.runner.assetdata.account.AssetDataAccount [pool-2-thread-3] INFO  - AssetDataAccount Total Records:121
2014-11-05 04:10:26,750 com.emeter.extractor.runner.assetdata.premise.AssetDataPremise [pool-2-thread-1] INFO  - AssetDataPremise Total Records:121
2014-11-05 04:10:33,746 com.emeter.extractor.runner.assetdata.device.AssetDataDevice [pool-2-thread-2] INFO  - AssetDataDevice Total Records:484
2014-11-05 04:11:08,696 com.emeter.extractor.runner.sdpstackdata.SdpStackData [main] INFO  - SdpStackData stat : 
ServiceDeliveryPoint,121
DistributionNode,242
TotalChannel,1573,Channel,1452,VirtualChannel,121
servicePointServiceAgreementAssociation,242
servicePointDataServiceAssociation,2177
servicePointDeviceAssociation,363
deviceChannelAssociation,1573
deviceFunctionAssociation,121
servicePointServicePointGroupAssociation,121
accountServicePointAssociation,121
servicePointServicePointAssociation SDP-DN,121
servicePointServicePointAssociation DN-DN,121
2014-11-05 04:11:08,985 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - Done Structural execution in 60.790875 sec
*******************************************************************************************************************
QE - zone lnxapp29

2014-11-04 21:00:51,302 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - StructuralExtractApplication Mode:: initial
2014-11-04 21:00:51,393 com.emeter.extractor.runner.seeddata.SeedDataCycleCdExtractRunner [pool-1-thread-2] INFO  - SeedDataCycleCdExtractRunner No cycleCd data from DB to process
2014-11-04 21:00:51,393 com.emeter.extractor.runner.seeddata.SeedDataCycleCdExtractRunner [pool-1-thread-2] INFO  - Total Records for cycleCd extraction:0
2014-11-04 21:00:57,702 com.emeter.extractor.runner.seeddata.SeedDataExtractRunner [pool-1-thread-1] INFO  - Total Records for Seed data extraction:67
2014-11-04 21:01:01,104 com.emeter.extractor.runner.assetdata.device.AssetDataDevice [pool-2-thread-2] INFO  - AssetDataDevice Total Records:8
2014-11-04 21:01:16,649 com.emeter.extractor.runner.assetdata.account.AssetDataAccount [pool-2-thread-3] INFO  - AssetDataAccount Total Records:1517
2014-11-04 21:01:17,146 com.emeter.extractor.runner.assetdata.route.AssetDataRoute [pool-2-thread-4] INFO  - AssetDataRoute Total Records:1466
2014-11-04 21:01:31,722 com.emeter.extractor.runner.assetdata.premise.AssetDataPremise [pool-2-thread-1] INFO  - AssetDataPremise Total Records:1516
2014-11-04 21:04:38,790 com.emeter.extractor.runner.sdpstackdata.SdpStackData [main] INFO  - SdpStackData stat : 
ServiceDeliveryPoint,1466
DistributionNode,1
TotalChannel,22265,Channel,22265,VirtualChannel,0
servicePointServiceAgreementAssociation,372
servicePointDataServiceAssociation,4536
servicePointDeviceAssociation,2934
deviceChannelAssociation,22265
deviceFunctionAssociation,1466
servicePointServicePointGroupAssociation,1452
accountServicePointAssociation,1466
servicePointServicePointAssociation SDP-DN,66
servicePointServicePointAssociation DN-DN,80
2014-11-04 21:04:39,616 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - Done Structural execution in 228.314410362 sec


updated jar
*******************************************************************************************************************
QE - env 2
2014-11-12 14:50:54,956 INFO  - Total Records for Seed data extraction:70
2014-11-12 14:51:00,666 INFO  - Total Records for cycleCd extraction:4
2014-11-12 14:51:05,977 INFO  - AssetDataAccount Total Records:149
2014-11-12 14:51:06,057 INFO  - AssetDataRoute Total Records:144
2014-11-12 14:51:07,753 INFO  - AssetDataPremise Total Records:149
2014-11-12 14:51:13,808 INFO  - AssetDataDevice Total Records:405
2014-11-12 14:52:04,810 INFO  - SdpStackData stat :
ServiceDeliveryPoint,149  --chk
DistributionNode,200
TotalChannel,1680,Channel,1580,VirtualChannel,100
servicePointServiceAgreementAssociation,202
servicePointDataServiceAssociation,1887
servicePointDeviceAssociation,400
deviceChannelAssociation,1698
deviceFunctionAssociation,147
servicePointServicePointGroupAssociation,145
accountServicePointAssociation,148
servicePointServicePointAssociation SDP-DN,100
servicePointServicePointAssociation DN-DN,100

  QA for orgs
a108-structuralExtractorDao.listOfOrgs=CLDANALYTICSORG1,CLDANALYTICSORG2,SOURCE1

2014-12-03 10:48:37,889 [main] INFO  - APPLICATION SUCCESSFULLY STARTED
2014-12-03 10:48:37,889 [main] INFO  - StructuralExtractApplication Mode:: initial
2014-12-03 10:48:52,132 [pool-2-thread-2] INFO  - Total Records for cycleCd extraction:4
2014-12-03 10:48:52,133 [pool-2-thread-1] INFO  - Total Records for Seed data extraction:284
2014-12-03 10:48:56,236 [main] INFO  - AssetDataAccount Total Records:124
2014-12-03 10:48:56,252 [main] INFO  - AssetDataPremise Total Records:126
2014-12-03 10:48:56,252 [main] INFO  - AssetDataDevice Total Records:480
2014-12-03 10:48:56,252 [main] INFO  - AssetDataRoute Total Records:121
2014-12-03 10:49:02,726 [main] INFO  - SdpStackData stat : 
ServiceDeliveryPoint,125
DistributionNode,236
TotalChannel,1571,Channel,1453,VirtualChannel,118
servicePointServiceAgreementAssociation,237
servicePointDataServiceAssociation,2175
servicePointDeviceAssociation,368
deviceChannelAssociation,1571
deviceFunctionAssociation,124
servicePointServicePointGroupAssociation,121
accountServicePointAssociation,123
servicePointServicePointAssociation SDP-DN,118
servicePointServicePointAssociation DN-DN,118
2014-12-03 10:49:03,824 [main] INFO  - Done Structural execution in 25.941688319 sec

*******************************************************************************************************************
FortCollins test data

2014-11-19 02:29:18,035 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - APPLICATION SUCCESSFULLY STARTED
2014-11-19 02:29:18,036 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - StructuralExtractApplication Mode:: initial
2014-11-19 02:33:39,098 com.emeter.extractor.runner.sdpstackdata.SdpStackData [main] INFO  - SdpStackData stat : 
ServiceDeliveryPoint,100332
DistributionNode,0
TotalChannel,409768,Channel,409020,VirtualChannel,748
servicePointServiceAgreementAssociation,0
servicePointDataServiceAssociation,181949
servicePointDeviceAssociation,111672
deviceChannelAssociation,466465
deviceFunctionAssociation,113103
servicePointServicePointGroupAssociation,100464
accountServicePointAssociation,147689
servicePointServicePointAssociation SDP-DN,206883
servicePointServicePointAssociation DN-DN,0
*********************************************************************************************************************

log4j.logger.com.emeter.extractor DEBUG 
log4j.logger.com.eMeter.util.Application  DEBUG 
log4j.rootLogger DEBUG, FileLog 

0
5
3
1000

/home/pipe/logs/out/heapdump.20141104.235157.422126.0001.phd


*********************************************************************************************************************************************
Dev env

2014-11-23 09:36:41,858 [pool-2-thread-2] INFO  - Total Records for cycleCd extraction:0
2014-11-23 09:36:50,007 [pool-2-thread-1] INFO  - Total Records for Seed data extraction:275
2014-11-23 09:40:13,634 [main] INFO  - AssetDataAccount Total Records:9979
2014-11-23 09	:40:13,635 [main] INFO  - AssetDataPremise Total Records:9868
2014-11-23 09:40:13,635 [main] INFO  - AssetDataDevice Total Records:39633
2014-11-23 09:40:13,636 [main] INFO  - AssetDataRoute Total Records:9893
2014-11-23 09:44:56,965 [main] INFO  - SdpStackData stat : 
ServiceDeliveryPoint,9854
DistributionNode,9725
TotalChannel,68539,Channel,68539,VirtualChannel,0
servicePointServiceAgreementAssociation,0
servicePointDataServiceAssociation,67765
servicePointDeviceAssociation,29277
deviceChannelAssociation,67956
deviceFunctionAssociation,9820
servicePointServicePointGroupAssociation,9754
accountServicePointAssociation,9766
servicePointServicePointAssociation SDP-DN,9725
servicePointServicePointAssociation DN-DN,0

**********************************************************************************************************************************************
CNP Asset data 

2014-12-06 02:11:24,308 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - APPLICATION SUCCESSFULLY STARTED
2014-12-06 02:11:24,309 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - StructuralExtractApplication Mode:: initial
2014-12-06 02:11:27,682 com.emeter.extractor.runner.seeddata.SeedDataCycleCdExtractRunner [pool-2-thread-2] INFO  - Total Records for cycleCd extraction:31
2014-12-06 02:11:28,004 com.emeter.extractor.runner.seeddata.SeedDataExtractRunner [pool-2-thread-1] INFO  - Total Records for Seed data extraction:360
2014-12-06 02:12:18,635 com.emeter.extractor.runner.assetdata.device.AssetDataDevice [main] INFO  -  Done truncating initial load table Device for data:4899754
2014-12-06 02:44:10,784 com.emeter.extractor.runner.assetdata.route.AssetDataRoute [main] INFO  -  Done truncating initial load table ROUTE for data:21
2014-12-06 02:56:28,871 com.emeter.extractor.runner.assetdata.device.AssetDataDevice [main] INFO  - AssetDataDevice Total Records:4899754
2014-12-06 02:56:28,871 com.emeter.extractor.runner.assetdata.route.AssetDataRoute [main] INFO  - AssetDataRoute Total Records:21
2014-12-06 02:56:28,871 com.emeter.extractor.runner.assetdata.account.AssetDataAccount [main] INFO  - AssetDataAccount Total Records:4064969
2014-12-06 02:56:28,871 com.emeter.extractor.runner.assetdata.premise.AssetDataPremise [main] INFO  - AssetDataPremise Total Records:2272500


********************************************************************************************************
./ApplicationLauncher.sh start CloudAlytStructuralExtraction 1 -Da108-structuralExtractApplication.threadSizeSdpStack=20 -Da108-structuralExtractApplication.countOfSdpMsgInXML=50 -Da108-structuralExtractApplication.threadSizeAsset=32 -Da108-structuralExtractApplication.countOfMsgInXML=500 -DsiebelDataSource.maxActive=100 -DsiebelDataSource.maxIdle=80 -DsiebelDataSource.minIdle=80

com.sun.org.apache.xpath.internal.jaxp.XPathExpressionImpl@4726df42
com.sun.org.apache.xpath.internal.jaxp.XPathExpressionImpl@45b6867


-Dcom.sun.org.apache.xml.internal.dtm.DTMManager=com.sun.org.apache.xml.internal.dtm.ref.DTMManagerDefault


JAVA_OPTS   -Xms128m -Xmx256m 

JAVA_OPTS  -Xms512m -Xmx1024m

{siebelDataSource.url}
{siebelDataSource.username}
{siebelDataSource.password}

***********************************************************************************************************************************************

public static void writeFile(File tempFile, StringBuilder sdpXML) throws SAXException, IOException, ParserConfigurationException, TransformerException{
		DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();  
        DocumentBuilder builder;  
        builder = factory.newDocumentBuilder();  
        
        // Use String reader  
        Document document = builder.parse(new InputSource(new StringReader(sdpXML.toString())));  

        TransformerFactory tranFactory = TransformerFactory.newInstance();  
        Transformer aTransformer = tranFactory.newTransformer();  
        Source src = new DOMSource( document );  
        Result dest = new StreamResult(tempFile);  
        aTransformer.transform(src, dest);
	}
	
**************************************************************************************************************************************************
CREATE TABLE CX_ALYT_INITIAL_LOAD_DEVICE
(
    src_id    VARCHAR2(15),
    rownum_id        NUMBER
);

CREATE TABLE CX_ALYT_INITIAL_LOAD_ROUTE
(
    src_id    VARCHAR2(15),
    rownum_id        NUMBER
);

CREATE TABLE CX_ALYT_INITIAL_LOAD_SVCPT
(
    src_id    VARCHAR2(15),
    rownum_id        NUMBER
);

INSERT INTO CX_ALYT_INITIAL_LOAD_DEVICE
    (
        src_id
      , rownum_id
    )
SELECT  row_id
      , rownum
  FROM s_asset
WHERE type_cd IN ('Meter','Communication Module','CT-PT','Disconnect Collar'/*, 'FRU'*/);
 
 
 
select * from CX_ALYT_INITIAL_LOAD_DEVICE 
where rownum_id between 101 AND 200
order by rownum_id;

select count(*) from s_asset WHERE type_cd IN ('Meter','Communication Module','CT-PT','Disconnect Collar'/*, 'FRU'*/); 
 
WHERE type_cd = 'Service Point';
**************************************************************************************************************************************************
Perf data:
FC without DN


2014-12-07 23:42:43,909 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - APPLICATION SUCCESSFULLY STARTED
2014-12-07 23:42:43,909 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - StructuralExtractApplication Mode:: initial
2014-12-07 23:42:46,852 com.emeter.extractor.runner.seeddata.SeedDataCycleCdExtractRunner [pool-2-thread-2] INFO  - Total Records for cycleCd extraction:16
2014-12-07 23:42:47,205 com.emeter.extractor.runner.seeddata.SeedDataExtractRunner [pool-2-thread-1] INFO  - Total Records for Seed data extraction:346
2014-12-07 23:42:48,223 com.emeter.extractor.runner.assetdata.device.AssetDataDevice [main] INFO  -  Done truncating initial load table Device for data:204235
2014-12-07 23:43:36,082 com.emeter.extractor.runner.assetdata.route.AssetDataRoute [main] INFO  -  Done truncating initial load table ROUTE for data:398
2014-12-07 23:43:52,234 com.emeter.extractor.runner.assetdata.device.AssetDataDevice [main] INFO  - AssetDataDevice Total Records:204235
2014-12-07 23:43:52,234 com.emeter.extractor.runner.assetdata.route.AssetDataRoute [main] INFO  - AssetDataRoute Total Records:398
2014-12-07 23:43:52,234 com.emeter.extractor.runner.assetdata.account.AssetDataAccount [main] INFO  - AssetDataAccount Total Records:106778
2014-12-07 23:43:52,234 com.emeter.extractor.runner.assetdata.premise.AssetDataPremise [main] INFO  - AssetDataPremise Total Records:71500
2014-12-07 23:43:53,401 com.emeter.extractor.runner.sdpstackdata.SdpStackData [main] INFO  -  Done truncating & loading initial load table SVCPT:100334
2014-12-07 23:47:45,740 com.emeter.extractor.runner.sdpstackdata.SdpStackData [main] INFO  - SdpStackData stat : 
ServiceDeliveryPoint,100200
DistributionNode,0
TotalChannel,408656,Channel,407910,VirtualChannel,747
servicePointServiceAgreementAssociation,0
servicePointDataServiceAssociation,0
servicePointDeviceAssociation,111234
deviceChannelAssociation,463318
deviceFunctionAssociation,112356
servicePointServicePointGroupAssociation,99432
accountServicePointAssociation,146156
servicePointServicePointAssociation SDP-DN,204786
servicePointServicePointAssociation DN-DN,0
2014-12-07 23:48:16,382 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - Done Structural execution in 332.47301 sec
2014-12-07 23:48:16,382 com.eMeter.PIPe.hydrofw.application.HydroApplicationContextImpl [main] INFO  - SHUTDOWN


size of zip file: 45 M
actual time Time: 332.47301 sec = 5:30 min
Total time Application startup + Extract = 6 min 


2) 
FortCollins: Without DN-DN rel

JAVA_OPTS: -Xms512m -Xmx1024m

No of DataBaseConnections: 50

AssetThreadCount:32
No Of AssetMessages/file:500

SdpStackThreadCount:16
No Of SdpStackMessages/file:300

2014-12-08 04:51:19,699 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - APPLICATION SUCCESSFULLY STARTED
2014-12-08 04:51:19,699 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - StructuralExtractApplication Mode:: initial
2014-12-08 04:51:22,346 com.emeter.extractor.runner.seeddata.SeedDataCycleCdExtractRunner [pool-2-thread-2] INFO  - Total Records for cycleCd extraction:16
2014-12-08 04:51:22,878 com.emeter.extractor.runner.seeddata.SeedDataExtractRunner [pool-2-thread-1] INFO  - Total Records for Seed data extraction:346
2014-12-08 04:51:24,448 com.emeter.extractor.runner.assetdata.device.AssetDataDevice [main] INFO  -  Done truncating initial load table Device for data:204235
2014-12-08 04:52:16,232 com.emeter.extractor.runner.assetdata.route.AssetDataRoute [main] INFO  -  Done truncating initial load table ROUTE for data:398
2014-12-08 04:52:35,001 com.emeter.extractor.runner.assetdata.device.AssetDataDevice [main] INFO  - AssetDataDevice Total Records:204235
2014-12-08 04:52:35,001 com.emeter.extractor.runner.assetdata.route.AssetDataRoute [main] INFO  - AssetDataRoute Total Records:398
2014-12-08 04:52:35,001 com.emeter.extractor.runner.assetdata.account.AssetDataAccount [main] INFO  - AssetDataAccount Total Records:106778
2014-12-08 04:52:35,002 com.emeter.extractor.runner.assetdata.premise.AssetDataPremise [main] INFO  - AssetDataPremise Total Records:71500
2014-12-08 04:52:36,121 com.emeter.extractor.runner.sdpstackdata.SdpStackData [main] INFO  -  Done truncating & loading initial load table SVCPT:100334
2014-12-08 04:56:52,334 com.emeter.extractor.runner.sdpstackdata.SdpStackData [main] INFO  - SdpStackData stat : 
ServiceDeliveryPoint,100201
DistributionNode,36971
TotalChannel,408031,Channel,407285,VirtualChannel,746
servicePointServiceAgreementAssociation,0
servicePointDataServiceAssociation,175299
servicePointDeviceAssociation,107595
deviceChannelAssociation,449812
deviceFunctionAssociation,108680
servicePointServicePointGroupAssociation,96127
accountServicePointAssociation,141148
servicePointServicePointAssociation SDP-DN,197970
servicePointServicePointAssociation DN-DN,0
2014-12-08 04:57:29,496 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - Done Structural execution in 369.797354 sec
2014-12-08 04:57:29,496 com.eMeter.PIPe.hydrofw.application.HydroApplicationContextImpl [main] INFO  - SHUTDOWN

Time: ~ 6.13 sec

Size of zip: 
49M     /home/pipe/Outgoing/Utility/CloudStructuralExtract/ADF_StructuralData_20141208_045119.zip



*******************************************************************************************************


2014-12-08 16:31:22,808 [pool-2-thread-2] INFO  - Total Records for cycleCd extraction:0
2014-12-08 16:31:28,886 [pool-2-thread-1] INFO  - Total Records for Seed data extraction:275
2014-12-08 16:31:29,546 [main] INFO  -  Done truncating & loading initial load table SVCPT:9854
2014-12-08 16:31:48,189 [pool-3-thread-1] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::1.045611196 sec
2014-12-08 16:31:48,442 [pool-3-thread-4] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::1.097684381 sec
2014-12-08 16:31:48,719 [pool-3-thread-5] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::1.290358905 sec
2014-12-08 16:31:49,382 [pool-3-thread-7] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::1.845058391 sec
2014-12-08 16:31:50,039 [pool-3-thread-11] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::1.052585515 sec
2014-12-08 16:31:51,237 [pool-3-thread-8] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::2.059969531 sec
2014-12-08 16:31:51,238 [pool-3-thread-10] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::1.816740745 sec
2014-12-08 16:31:51,239 [pool-3-thread-2] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::1.838355426 sec
2014-12-08 16:31:51,239 [pool-3-thread-3] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::2.599556146 sec
2014-12-08 16:31:51,542 [pool-3-thread-9] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::4.000831713 sec
2014-12-08 16:31:52,489 [pool-3-thread-6] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::2.366545605 sec
2014-12-08 16:31:52,598 [pool-3-thread-13] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::1.229567103 sec
2014-12-08 16:31:53,884 [pool-3-thread-15] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::1.520139068 sec
2014-12-08 16:31:53,916 [pool-3-thread-12] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::1.916070489 sec
2014-12-08 16:31:55,874 [pool-3-thread-14] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::1.055675907 sec
2014-12-08 16:31:57,420 [pool-3-thread-16] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::1.50803818 sec
2014-12-08 16:31:57,431 [pool-3-thread-17] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::1.302257406 sec
2014-12-08 16:31:59,732 [pool-3-thread-20] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::1.302515213 sec
2014-12-08 16:31:59,782 [pool-3-thread-19] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::1.237343577 sec
2014-12-08 16:31:59,844 [pool-3-thread-18] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::2.054046143 sec
2014-12-08 16:32:03,331 [pool-3-thread-1] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::1.763805468 sec
2014-12-08 16:32:10,920 [pool-3-thread-4] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::3.246051519 sec
2014-12-08 16:32:13,887 [pool-3-thread-7] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::1.354795711 sec
2014-12-08 16:32:13,897 [pool-3-thread-5] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::1.532953849 sec
2014-12-08 16:32:16,919 [pool-3-thread-13] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::1.462437247 sec
2014-12-08 16:32:17,197 [pool-3-thread-8] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::1.245335573 sec
2014-12-08 16:32:17,562 [pool-3-thread-11] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::1.348697031 sec
2014-12-08 16:32:19,178 [pool-3-thread-12] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::0.822784826 sec
2014-12-08 16:32:21,282 [pool-3-thread-9] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::1.413539431 sec
2014-12-08 16:32:21,284 [pool-3-thread-15] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::1.422039651 sec
2014-12-08 16:32:21,286 [pool-3-thread-10] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::1.971047281 sec
2014-12-08 16:32:22,469 [pool-3-thread-3] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::1.088546625 sec
2014-12-08 16:32:22,522 [pool-3-thread-6] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::1.109584526 sec


******************************************************************************************************************************

CNP 1  - No DN. 
Asset : Th 32 - 500
Th: 16 SDP 200 per messages

2014-12-10 04:06:49,016 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - APPLICATION SUCCESSFULLY STARTED
2014-12-10 04:06:49,016 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - StructuralExtractApplication Mode:: initial
2014-12-10 04:06:52,473 com.emeter.extractor.runner.seeddata.SeedDataCycleCdExtractRunner [pool-2-thread-2] INFO  - Total Records for cycleCd extraction:31
2014-12-10 04:06:52,564 com.emeter.extractor.runner.seeddata.SeedDataExtractRunner [pool-2-thread-1] INFO  - Total Records for Seed data extraction:360
2014-12-10 04:07:51,993 com.emeter.extractor.runner.assetdata.device.AssetDataDevice [main] INFO  -  Done truncating initial load table Device for data:4899754
2014-12-10 04:41:44,279 com.emeter.extractor.runner.assetdata.route.AssetDataRoute [main] INFO  -  Done truncating initial load table ROUTE for data:21
2014-12-10 04:53:59,116 com.emeter.extractor.runner.assetdata.device.AssetDataDevice [main] INFO  - AssetDataDevice Total Records:4899754
2014-12-10 04:53:59,116 com.emeter.extractor.runner.assetdata.route.AssetDataRoute [main] INFO  - AssetDataRoute Total Records:21
2014-12-10 04:53:59,116 com.emeter.extractor.runner.assetdata.account.AssetDataAccount [main] INFO  - AssetDataAccount Total Records:4064969
2014-12-10 04:53:59,116 com.emeter.extractor.runner.assetdata.premise.AssetDataPremise [main] INFO  - AssetDataPremise Total Records:2272964
2014-12-10 04:56:11,474 com.emeter.extractor.runner.sdpstackdata.SdpStackData [main] INFO  -  Done truncating & loading initial load table SVCPT:2272939
2014-12-10 06:18:26,086 com.emeter.extractor.runner.sdpstackdata.SdpStackData [main] INFO  - SdpStackData stat : 
ServiceDeliveryPoint,2272939
DistributionNode,0
TotalChannel,15910540,Channel,15910540,VirtualChannel,0
servicePointServiceAgreementAssociation,0
servicePointDataServiceAssociation,5001552
servicePointDeviceAssociation,2575702
deviceChannelAssociation,17637915
deviceFunctionAssociation,2519663
servicePointServicePointGroupAssociation,2273761
accountServicePointAssociation,4077534
servicePointServicePointAssociation SDP-DN,2129383
servicePointServicePointAssociation DN-DN,0
2014-12-10 06:56:44,113 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - Done Structural execution in 10195.096598 sec
2014-12-10 06:56:44,146 com.eMeter.PIPe.hydrofw.application.HydroApplicationContextImpl [main] INFO  - SHUTDOWN



size: 1.1G    ADF_StructuralData_20141210_040648_CNP1.zip
----------------------------------

FC - With No crush

2014-12-10 08:00:10,748 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - APPLICATION SUCCESSFULLY STARTED
2014-12-10 08:00:10,749 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - StructuralExtractApplication Mode:: initial
2014-12-10 08:00:13,609 com.emeter.extractor.runner.seeddata.SeedDataCycleCdExtractRunner [pool-2-thread-2] INFO  - Total Records for cycleCd extraction:16
2014-12-10 08:00:14,475 com.emeter.extractor.runner.seeddata.SeedDataExtractRunner [pool-2-thread-1] INFO  - Total Records for Seed data extraction:346
2014-12-10 08:00:16,401 com.emeter.extractor.runner.assetdata.device.AssetDataDevice [main] INFO  -  Done truncating initial load table Device for data:204235
2014-12-10 08:01:03,067 com.emeter.extractor.runner.assetdata.route.AssetDataRoute [main] INFO  -  Done truncating initial load table ROUTE for data:398
2014-12-10 08:01:20,888 com.emeter.extractor.runner.assetdata.device.AssetDataDevice [main] INFO  - AssetDataDevice Total Records:204235
2014-12-10 08:01:20,890 com.emeter.extractor.runner.assetdata.route.AssetDataRoute [main] INFO  - AssetDataRoute Total Records:398
2014-12-10 08:01:20,890 com.emeter.extractor.runner.assetdata.account.AssetDataAccount [main] INFO  - AssetDataAccount Total Records:106778
2014-12-10 08:01:20,890 com.emeter.extractor.runner.assetdata.premise.AssetDataPremise [main] INFO  - AssetDataPremise Total Records:71916
2014-12-10 08:01:21,819 com.emeter.extractor.runner.sdpstackdata.SdpStackData [main] INFO  -  Done truncating & loading initial load table SVCPT:100334
2014-12-10 08:02:14,558 com.emeter.extractor.dao.StructuralExtractorDao [main] INFO  - SdpStackData, Done SDP from 1 to 19200
2014-12-10 08:03:01,799 com.emeter.extractor.dao.StructuralExtractorDao [main] INFO  - SdpStackData, Done SDP from 19201 to 38400
2014-12-10 08:03:51,746 com.emeter.extractor.dao.StructuralExtractorDao [main] INFO  - SdpStackData, Done SDP from 38401 to 57600
2014-12-10 08:04:39,167 com.emeter.extractor.dao.StructuralExtractorDao [main] INFO  - SdpStackData, Done SDP from 57601 to 76800
2014-12-10 08:05:26,438 com.emeter.extractor.dao.StructuralExtractorDao [main] INFO  - SdpStackData, Done SDP from 76801 to 96000
2014-12-10 08:05:35,669 com.emeter.extractor.dao.StructuralExtractorDao [main] INFO  - SdpStackData, Done SDP from 96001 to 115200
2014-12-10 08:05:37,995 com.emeter.extractor.runner.sdpstackdata.SdpStackData [main] INFO  - SdpStackData stat : 
ServiceDeliveryPoint,100334
DistributionNode,0
TotalChannel,409768,Channel,409020,VirtualChannel,748
servicePointServiceAgreementAssociation,0
servicePointDataServiceAssociation,181949
servicePointDeviceAssociation,111665
deviceChannelAssociation,465396
deviceFunctionAssociation,105595
servicePointServicePointGroupAssociation,100466
accountServicePointAssociation,147688
servicePointServicePointAssociation SDP-DN,206883
servicePointServicePointAssociation DN-DN,0
2014-12-10 08:06:10,912 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - Done Structural execution in 360.163545 sec


---------------------------------------------------------------------------------------------------------------------------------
CNP 2 - With DN
2014-12-17 01:53:17,444 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - APPLICATION SUCCESSFULLY STARTED
2014-12-17 01:53:17,444 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - StructuralExtractApplication Mode:: initial
2014-12-17 01:53:20,659 com.emeter.extractor.runner.seeddata.SeedDataCycleCdExtractRunner [pool-2-thread-2] INFO  - Total Records for cycleCd extraction:31
2014-12-17 01:53:20,662 com.emeter.extractor.runner.seeddata.SeedDataExtractRunner [pool-2-thread-1] INFO  - Total Records for Seed data extraction:361
2014-12-17 02:34:22,969 com.emeter.extractor.runner.assetdata.device.AssetDataDevice [main] INFO  - AssetDataDevice Total Records:4899753
2014-12-17 02:34:22,971 com.emeter.extractor.runner.assetdata.route.AssetDataRoute [main] INFO  - AssetDataRoute Total Records:21
2014-12-17 02:34:22,971 com.emeter.extractor.runner.assetdata.account.AssetDataAccount [main] INFO  - AssetDataAccount Total Records:4064969
2014-12-17 02:34:22,971 com.emeter.extractor.runner.assetdata.premise.AssetDataPremise [main] INFO  - AssetDataPremise Total Records:2272964
2014-12-17 05:58:21,321 com.emeter.extractor.runner.sdpstackdata.SdpStackData [main] INFO  - SdpStackData stat :
ServiceDeliveryPoint,2272912
DistributionNode,16217708
TotalChannel,15910351,Channel,15910351,VirtualChannel,0
servicePointServiceAgreementAssociation,0
servicePointDataServiceAssociation,4994095
servicePointDeviceAssociation,2574242
deviceChannelAssociation,17622717
deviceFunctionAssociation,2518365
servicePointServicePointGroupAssociation,2273720
accountServicePointAssociation,4077142
servicePointServicePointAssociation SDP-DN,2129383
servicePointServicePointAssociation DN-DN,2129379
2014-12-17 06:46:37,476 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - Done Structural execution in 17600.032596 sec
2014-12-17 06:46:37,476 com.eMeter.PIPe.hydrofw.application.HydroApplicationContextImpl [main] INFO  - SHUTDOWN


~5 hrs


---------------------------------------------------------------------------------------------------------------------------------
18/12

CLDALYT-433	Fixed. Arvind to update bug status after regression. Also attached the updated sample file on Docs page
Document	Updated as per e-mail: 
Loader performance data for FCU	Will update in Thursday call

DN-DN Query

initial time without DN ~2hrs 20 minutes
Total time: 4 hrs with DN



CNP 4 - With DN - The DN query now takes ~9sec. 
From 400 sec to avg of 9 sec

DN-rel count: 2129379 = 2.1 million


2014-12-17 06:53:48,306 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - APPLICATION SUCCESSFULLY STARTED
2014-12-17 06:53:48,306 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - StructuralExtractApplication Mode:: initial
2014-12-17 06:53:51,737 com.emeter.extractor.runner.seeddata.SeedDataCycleCdExtractRunner [pool-2-thread-2] INFO  - Total Records for cycleCd extraction:31
2014-12-17 06:53:51,793 com.emeter.extractor.runner.seeddata.SeedDataExtractRunner [pool-2-thread-1] INFO  - Total Records for Seed data extraction:361
2014-12-17 07:35:11,979 com.emeter.extractor.runner.assetdata.device.AssetDataDevice [main] INFO  - AssetDataDevice Total Records:4899753
2014-12-17 07:35:11,979 com.emeter.extractor.runner.assetdata.route.AssetDataRoute [main] INFO  - AssetDataRoute Total Records:21
2014-12-17 07:35:11,979 com.emeter.extractor.runner.assetdata.account.AssetDataAccount [main] INFO  - AssetDataAccount Total Records:4064969
2014-12-17 07:35:11,979 com.emeter.extractor.runner.assetdata.premise.AssetDataPremise [main] INFO  - AssetDataPremise Total Records:2272964
2014-12-17 10:03:09,169 com.emeter.extractor.runner.sdpstackdata.SdpStackData [main] INFO  - SdpStackData stat :
ServiceDeliveryPoint,2272912
DistributionNode,16217708
TotalChannel,15910351,Channel,15910351,VirtualChannel,0
servicePointServiceAgreementAssociation,0
servicePointDataServiceAssociation,4994095
servicePointDeviceAssociation,2574242
deviceChannelAssociation,17622717
deviceFunctionAssociation,2518365
servicePointServicePointGroupAssociation,2273720
accountServicePointAssociation,4077142
servicePointServicePointAssociation SDP-DN,2129383
servicePointServicePointAssociation DN-DN,2129379
2014-12-17 10:03:09,187 com.emeter.extractor.impl.StructuralExtractManager [main] INFO  - creating zip file...
2014-12-17 10:54:21,879 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - Done Structural execution in 14433.572781 sec


folder size= 56 GB
zip size: 1.5 GB



-----------------------------------------------------------------------------------------
asset: 
1- no fix

2014-12-18 13:13:29,553 [main] INFO  - INIT
2014-12-18 13:13:31,098 [main] INFO  - APPLICATION SUCCESSFULLY STARTED
2014-12-18 13:13:31,098 [main] INFO  - StructuralExtractApplication Mode:: initial
2014-12-18 13:13:49,449 [pool-2-thread-2] INFO  - Total Records for cycleCd extraction:1
2014-12-18 13:13:51,633 [pool-2-thread-1] INFO  - Total Records for Seed data extraction:282
2014-12-18 13:13:56,786 [pool-3-thread-2] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::0.156343753 sec
2014-12-18 13:13:56,881 [pool-3-thread-1] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::0.197644574 sec
2014-12-18 13:13:57,623 [main] INFO  - SdpStackData stat : 
ServiceDeliveryPoint,132
DistributionNode,255
TotalChannel,1615,Channel,1489,VirtualChannel,126
servicePointServiceAgreementAssociation,251
servicePointDataServiceAssociation,2301
servicePointDeviceAssociation,385
deviceChannelAssociation,1677
deviceFunctionAssociation,129
servicePointServicePointGroupAssociation,129
accountServicePointAssociation,129
servicePointServicePointAssociation SDP-DN,129
servicePointServicePointAssociation DN-DN,124
2014-12-18 13:13:59,885 [main] INFO  - creating zip file... 
2014-12-18 13:14:00,467 [main] INFO  - Done Structural execution in 29.373624127 sec


2 - fix, only for sdp-dn rel wale dn
2014-12-18 14:07:35,202 [main] INFO  - SdpStackData stat : 
ServiceDeliveryPoint,132
DistributionNode,127
TotalChannel,1615,Channel,1489,VirtualChannel,126
servicePointServiceAgreementAssociation,251
servicePointDataServiceAssociation,2301
servicePointDeviceAssociation,385
deviceChannelAssociation,1677
deviceFunctionAssociation,129
servicePointServicePointGroupAssociation,129
accountServicePointAssociation,129
servicePointServicePointAssociation SDP-DN,129
servicePointServicePointAssociation DN-DN,0
2014-12-18 14:07:35,250 [main] INFO  - creating zip file... 
2014-12-18 14:07:40,514 [main] INFO  - Done Structural execution in 31.258008513 sec
2014-12-18 14:07:40,514 [main] INFO  - SHUTDOWN

3 - with fix -- mode asset

2014-12-18 14:10:56,208 [pool-3-thread-1] INFO  - dnHierarchyMode:asset
2014-12-18 14:10:56,208 [pool-3-thread-2] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::0.066312849 sec
2014-12-18 14:10:57,284 [pool-3-thread-1] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::1.088048892 sec
2014-12-18 14:10:57,987 [main] INFO  - SdpStackData stat : 
ServiceDeliveryPoint,132
DistributionNode,251
TotalChannel,1615,Channel,1489,VirtualChannel,126
servicePointServiceAgreementAssociation,251
servicePointDataServiceAssociation,2301
servicePointDeviceAssociation,385
deviceChannelAssociation,1677
deviceFunctionAssociation,129
servicePointServicePointGroupAssociation,129
accountServicePointAssociation,129
servicePointServicePointAssociation SDP-DN,129
servicePointServicePointAssociation DN-DN,124
2014-12-18 14:10:58,002 [main] INFO  - creating zip file... 
2014-12-18 14:11:01,309 [main] INFO  - Done Structural execution in 16.441014193 sec

param:
======
2014-12-18 15:57:15,291 [pool-2-thread-2] INFO  - Total Records for cycleCd extraction:1
2014-12-18 15:57:18,115 [pool-2-thread-1] INFO  - Total Records for Seed data extraction:282
2014-12-18 15:57:22,581 [pool-3-thread-2] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::0.021004078 sec
2014-12-18 15:57:23,066 [pool-3-thread-1] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::0.41913693 sec
2014-12-18 15:57:23,721 [main] INFO  - SdpStackData stat : 
ServiceDeliveryPoint,132
DistributionNode,127
TotalChannel,1615,Channel,1489,VirtualChannel,126
servicePointServiceAgreementAssociation,251
servicePointDataServiceAssociation,2301
servicePointDeviceAssociation,385
deviceChannelAssociation,1677
deviceFunctionAssociation,129
servicePointServicePointGroupAssociation,129
accountServicePointAssociation,129
servicePointServicePointAssociation SDP-DN,129
servicePointServicePointAssociation DN-DN,1


both:
=====
2014-12-18 15:56:30,233 [pool-2-thread-2] INFO  - Total Records for cycleCd extraction:1
2014-12-18 15:56:33,318 [pool-2-thread-1] INFO  - Total Records for Seed data extraction:282
2014-12-18 15:56:40,185 [pool-3-thread-2] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::0.112676669 sec
2014-12-18 15:56:40,404 [pool-3-thread-1] INFO  - perf writeSdpData time taken by relSdpDistNodeWriter DN-DN::0.277407028 sec
2014-12-18 15:56:41,247 [main] INFO  - SdpStackData stat : 
ServiceDeliveryPoint,132
DistributionNode,252
TotalChannel,1615,Channel,1489,VirtualChannel,126
servicePointServiceAgreementAssociation,251
servicePointDataServiceAssociation,2301
servicePointDeviceAssociation,385
deviceChannelAssociation,1677
deviceFunctionAssociation,129
servicePointServicePointGroupAssociation,129
accountServicePointAssociation,129
servicePointServicePointAssociation SDP-DN,129
servicePointServicePointAssociation DN-DN,125





----------------------------------------------------------------------------------------------------------------------------------
CNP6 - With DN (both) - [DN-DN query takes ~1.2 sec ]
2014-12-18 06:46:59,751 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - APPLICATION SUCCESSFULLY STARTED
2014-12-18 06:46:59,751 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - StructuralExtractApplication Mode:: initial
2014-12-18 06:47:03,149 com.emeter.extractor.runner.seeddata.SeedDataCycleCdExtractRunner [pool-2-thread-2] INFO  - Total Records for cycleCd extraction:31
2014-12-18 06:47:03,208 com.emeter.extractor.runner.seeddata.SeedDataExtractRunner [pool-2-thread-1] INFO  - Total Records for Seed data extraction:361
2014-12-18 07:27:34,910 com.emeter.extractor.runner.assetdata.device.AssetDataDevice [main] INFO  - AssetDataDevice Total Records:4899753
2014-12-18 07:27:34,911 com.emeter.extractor.runner.assetdata.route.AssetDataRoute [main] INFO  - AssetDataRoute Total Records:21
2014-12-18 07:27:34,911 com.emeter.extractor.runner.assetdata.account.AssetDataAccount [main] INFO  - AssetDataAccount Total Records:4064969
2014-12-18 07:27:34,911 com.emeter.extractor.runner.assetdata.premise.AssetDataPremise [main] INFO  - AssetDataPremise Total Records:2272964
ServiceDeliveryPoint,2272912
DistributionNode,1757848
TotalChannel,15910351,Channel,15910351,VirtualChannel,0
servicePointServiceAgreementAssociation,0
servicePointDataServiceAssociation,4994095
servicePointDeviceAssociation,2574242
deviceChannelAssociation,17622717
deviceFunctionAssociation,2518343
servicePointServicePointGroupAssociation,2273720
accountServicePointAssociation,4077142
servicePointServicePointAssociation SDP-DN,2129383
servicePointServicePointAssociation DN-DN,2129379
2014-12-18 08:47:46,904 com.emeter.extractor.impl.StructuralExtractManager [main] INFO  - creating zip file... 
2014-12-18 09:27:01,047 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - Done Structural execution in 9601.29622 sec

~ 2 hrs 40 mins 


CNP5 - With DN (assetMode)
In progress

FCU5 - With DN (bothMode)
2014-12-18 20:49:42,246 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - APPLICATION SUCCESSFULLY STARTED
2014-12-18 20:49:42,246 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - StructuralExtractApplication Mode:: initial
2014-12-18 20:49:45,595 com.emeter.extractor.runner.seeddata.SeedDataCycleCdExtractRunner [pool-2-thread-2] INFO  - Total Records for cycleCd extraction:16
2014-12-18 20:49:45,757 com.emeter.extractor.runner.seeddata.SeedDataExtractRunner [pool-2-thread-1] INFO  - Total Records for Seed data extraction:347
2014-12-18 20:50:40,650 com.emeter.extractor.runner.assetdata.device.AssetDataDevice [main] INFO  - AssetDataDevice Total Records:204235
2014-12-18 20:50:40,650 com.emeter.extractor.runner.assetdata.route.AssetDataRoute [main] INFO  - AssetDataRoute Total Records:398
2014-12-18 20:50:40,650 com.emeter.extractor.runner.assetdata.account.AssetDataAccount [main] INFO  - AssetDataAccount Total Records:106778
2014-12-18 20:50:40,650 com.emeter.extractor.runner.assetdata.premise.AssetDataPremise [main] INFO  - AssetDataPremise Total Records:71916
2014-12-18 20:53:45,761 com.emeter.extractor.runner.sdpstackdata.SdpStackData [main] INFO  - SdpStackData stat : 
ServiceDeliveryPoint,100334
DistributionNode,38278
TotalChannel,410079,Channel,409020,VirtualChannel,1059
servicePointServiceAgreementAssociation,0
servicePointDataServiceAssociation,181212
servicePointDeviceAssociation,111340
deviceChannelAssociation,463714
deviceFunctionAssociation,104710
servicePointServicePointGroupAssociation,100433
accountServicePointAssociation,147499
servicePointServicePointAssociation SDP-DN,202594
servicePointServicePointAssociation DN-DN,0
2014-12-18 20:53:45,860 com.emeter.extractor.impl.StructuralExtractManager [main] INFO  - creating zip file... 
2014-12-18 20:54:19,604 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - Done Structural execution in 277.357754 sec
2014-12-18 20:54:19,604 com.eMeter.PIPe.hydrofw.application.HydroApplicationContextImpl [main] INFO  - SHUTDOWN


FCU6 - With DN (assetMode)
2014-12-18 20:55:47,005 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - APPLICATION SUCCESSFULLY STARTED
2014-12-18 20:55:47,005 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - StructuralExtractApplication Mode:: initial
2014-12-18 20:55:50,026 com.emeter.extractor.runner.seeddata.SeedDataCycleCdExtractRunner [pool-2-thread-2] INFO  - Total Records for cycleCd extraction:16
2014-12-18 20:55:50,632 com.emeter.extractor.runner.seeddata.SeedDataExtractRunner [pool-2-thread-1] INFO  - Total Records for Seed data extraction:347
2014-12-18 20:56:36,989 com.emeter.extractor.runner.assetdata.device.AssetDataDevice [main] INFO  - AssetDataDevice Total Records:204235
2014-12-18 20:56:36,989 com.emeter.extractor.runner.assetdata.route.AssetDataRoute [main] INFO  - AssetDataRoute Total Records:398
2014-12-18 20:56:36,989 com.emeter.extractor.runner.assetdata.account.AssetDataAccount [main] INFO  - AssetDataAccount Total Records:106778
2014-12-18 20:56:36,989 com.emeter.extractor.runner.assetdata.premise.AssetDataPremise [main] INFO  - AssetDataPremise Total Records:71916
2014-12-18 20:59:37,967 com.emeter.extractor.runner.sdpstackdata.SdpStackData [main] INFO  - SdpStackData stat : 
ServiceDeliveryPoint,100334
DistributionNode,38278
TotalChannel,410079,Channel,409020,VirtualChannel,1059
servicePointServiceAgreementAssociation,0
servicePointDataServiceAssociation,181212
servicePointDeviceAssociation,111340
deviceChannelAssociation,463714
deviceFunctionAssociation,104710
servicePointServicePointGroupAssociation,100433
accountServicePointAssociation,147499
servicePointServicePointAssociation SDP-DN,202594
servicePointServicePointAssociation DN-DN,0
2014-12-18 20:59:37,970 com.emeter.extractor.impl.StructuralExtractManager [main] INFO  - creating zip file... 
2014-12-18 21:00:12,467 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - Done Structural execution in 265.461503 sec

---------------------------------------------------------------------------------------------------------------------------------





FC no crush - With DN






- Stats
- Filter crush - all param
- VC without contributor - dont extract
- RDU timezone - no need
- DN-DN
- Incrmental test
- Doc









*********************************************
        sqlSdpAccountRel
        SELECT DISTINCT sdp.x_udc_asset_Id AS sdp_udc_id
					     , FIRST_VALUE(to_char(rel.x_rel_start_dt, 'YYYY-MM-DD T HH24:MI:SS')) OVER(PARTITION BY rel.asset_id, rel.accnt_id, rel.x_rel_start_dt ORDER BY rel.x_rel_end_dt NULLS FIRST) AS rel_start_time
					     , FIRST_VALUE(to_char(rel.x_rel_end_dt, 'YYYY-MM-DD T HH24:MI:SS')) OVER(PARTITION BY rel.asset_id, rel.accnt_id, rel.x_rel_start_dt ORDER BY rel.x_rel_end_dt NULLS FIRST) AS rel_end_time
					     , rel.rel_type_cd AS rel_type
					     , accnt.x_udc_accnt_id AS account_udc_id
					     , accnt.row_id AS account_src_id
		   				 , sdp.type_cd AS type_7x
		   			 	 , sdp.row_id sdp_src_id
					FROM cx_asset_accnt rel,
					    s_asset sdp,
					    s_org_ext accnt
					WHERE rel.asset_id = sdp.row_id
						AND rel.accnt_id = accnt.row_id
		   				AND sdp.row_id  IN ('1-43FJ')
		   			ORDER BY sdp.row_id, accnt.row_id;
		   			
		   			
	sdp-meter	   			
	SELECT DISTINCT FIRST_VALUE(rel.row_id) OVER(PARTITION BY rel.par_asset_id, rel.asset_id, rel.meter_loc_start_dt ORDER BY rel.meter_loc_end_dt NULLS FIRST) AS src_id
		   				 , rel.x_asset_relation_type_cd AS rel_type  
						  , FIRST_VALUE(to_char(rel.meter_loc_start_dt, 'YYYY-MM-DD T HH24:MI:SS')) OVER(PARTITION BY rel.par_asset_id, rel.asset_id, rel.meter_loc_start_dt ORDER BY rel.meter_loc_end_dt NULLS FIRST) AS rel_start_time
					     , FIRST_VALUE(to_char(rel.meter_loc_end_dt, 'YYYY-MM-DD T HH24:MI:SS')) OVER(PARTITION BY rel.par_asset_id, rel.asset_id, rel.meter_loc_start_dt ORDER BY rel.meter_loc_end_dt NULLS FIRST) AS rel_end_time			   		
					     , sdp.x_udc_asset_Id AS sdp_udc_id
					     , mtr.x_udc_asset_Id AS device_udc_id
					     , mtr.row_id AS device_src_id
		   				 , mtr.type_cd AS type_7x
		   			     , sdp.row_id AS sdp_src_id	
					FROM s_asset mtr
					     , s_asset_rel rel
					     , s_asset sdp
					WHERE rel.x_asset_relation_type_cd IN ('SDP-METER', 'SDP-CT/PT', 'SDP-DC')		   		
					   AND rel.par_asset_id = sdp.row_id
					   AND rel.asset_id = mtr.row_id
		   			   AND sdp.row_id  IN ('1-43FJ')  
		   			ORDER BY src_id, sdp.row_id;		   			
		   			
		   			
	
	sqlMeterCommFnRel -- no DISTINMCT

		SELECT DISTINCT 'COMMUNICATION-METER' AS rel_type
		     --, to_char(rel.meter_loc_start_dt, 'YYYY-MM-DD T HH24:MI:SS') AS rel_start_time
		     --, to_char(rel.meter_loc_end_dt, 'YYYY-MM-DD T HH24:MI:SS') AS rel_end_time
		     , FIRST_VALUE(to_char(rel.meter_loc_start_dt, 'YYYY-MM-DD T HH24:MI:SS')) OVER(PARTITION BY rel.par_asset_id, rel.asset_id, rel.meter_loc_start_dt ORDER BY rel.meter_loc_end_dt NULLS FIRST) AS rel_start_time
		     , FIRST_VALUE(to_char(rel.meter_loc_end_dt, 'YYYY-MM-DD T HH24:MI:SS')) OVER(PARTITION BY rel.par_asset_id, rel.asset_id, rel.meter_loc_start_dt ORDER BY rel.meter_loc_end_dt NULLS FIRST) AS rel_end_time			   		
		     , mtr.x_udc_asset_Id AS device_udc_id
		     , commMod.x_udc_asset_Id AS comfunction_udc_id
		     , commMod.row_id AS comfunction_src_id
		     , sdp_mtr_rel.par_asset_id AS sdp_src_id
		FROM s_asset mtr
		     , s_asset_rel rel
		     , s_asset commMod
			 , s_asset_rel sdp_mtr_rel
		WHERE rel.x_asset_relation_type_cd = 'COMMUNICATION-METER'
		   AND rel.par_asset_id = commMod.row_id
		   AND rel.asset_id = mtr.row_id
	       AND sdp_mtr_rel.asset_id = mtr.row_id
		   AND sdp_mtr_rel.x_asset_relation_type_cd = 'SDP-METER'
		   AND sdp_mtr_rel.asset_id  IN (@meterList@)
		   AND sdp_mtr_rel.par_asset_id IN (@sdpSrcIdList@)
		ORDER BY sdp_mtr_rel.par_asset_id
	
	
	sqlMeterChannel
	
	SELECT rel.row_id AS src_id
		, FIRST_VALUE(to_char(rel.meter_loc_start_dt, 'YYYY-MM-DD T HH24:MI:SS')) OVER(PARTITION BY rel.par_asset_id, rel.asset_id, rel.meter_loc_start_dt ORDER BY rel.meter_loc_end_dt NULLS FIRST) AS rel_start_time
	     	, FIRST_VALUE(to_char(rel.meter_loc_end_dt, 'YYYY-MM-DD T HH24:MI:SS')) OVER(PARTITION BY rel.par_asset_id, rel.asset_id, rel.meter_loc_start_dt ORDER BY rel.meter_loc_end_dt NULLS FIRST) AS rel_end_time			   								     , sdp.x_udc_asset_Id AS sdp_udc_id
		, mtr.x_udc_asset_Id AS device_udc_id
		, mapping.udc_id_8x AS prod_name
			   				 , sdp.row_id AS sdp_src_id
						  FROM s_asset mtr
						     , s_asset_rel rel
						     , s_asset ch
						     , s_asset sdp
						     , s_prod_int prod
						     , cx_product_name_mapping mapping
						 WHERE rel.x_asset_relation_type_cd = 'METER-CHANNEL'
						   AND rel.par_asset_id             = mtr.row_id
						   AND rel.asset_id                 = ch.row_id
						   AND ch.service_point_id          = sdp.row_id
						   AND prod.row_id                  = ch.prod_id
						   AND prod.name                    = mapping.product_name_7x
						   AND sdp.row_id  IN (@sdpSrcIdList@)
			   		       --AND NVL(sdp.data_src, @primaryOrg@) IN (@listOfOrgs@)
		   			 ORDER BY rel.row_id, mapping.udc_id_8x, sdp.row_id
	
	
	
	
	sqlSdpRouteRel
	
	 SELECT DISTINCT 
	               --to_char(rel.meter_loc_start_dt, 'YYYY-MM-DD T HH24:MI:SS') AS rel_start_time
						     --, to_char(rel.meter_loc_end_dt, 'YYYY-MM-DD T HH24:MI:SS') AS rel_end_time
	               FIRST_VALUE(to_char(rel.meter_loc_start_dt, 'YYYY-MM-DD T HH24:MI:SS')) OVER(PARTITION BY rel.par_asset_id, rel.asset_id, rel.meter_loc_start_dt ORDER BY rel.meter_loc_end_dt NULLS FIRST) AS rel_start_time
	              , FIRST_VALUE(to_char(rel.meter_loc_end_dt, 'YYYY-MM-DD T HH24:MI:SS')) OVER(PARTITION BY rel.par_asset_id, rel.asset_id, rel.meter_loc_start_dt ORDER BY rel.meter_loc_end_dt NULLS FIRST) AS rel_end_time			   		
						     , sdp.x_udc_asset_Id AS sdp_udc_id
						     , route.x_udc_asset_Id AS route_udc_id
						     , route.row_id AS route_src_id
			   				 , rel.quantity AS read_seq
			   		     , sdp.row_id AS sdp_src_id
	            FROM s_asset route
	                 , s_asset_rel rel
	                 , s_asset sdp
	            WHERE rel.x_asset_relation_type_cd = 'ROUTE-SDP'
	               AND rel.par_asset_id             = route.row_id
	               AND rel.asset_id                 = sdp.row_id
	                 AND sdp.row_id  IN ('1-43FJ')
	                   --AND NVL(sdp.data_src, @primaryOrg@) IN (@listOfOrgs@)
              ORDER BY sdp.row_id;
              
              
          SDP-DN 
          
          SELECT DISTINCT
	                FIRST_VALUE(to_char(rel.meter_loc_start_dt, 'YYYY-MM-DD T HH24:MI:SS')) OVER(PARTITION BY rel.par_asset_id, rel.asset_id, rel.meter_loc_start_dt ORDER BY rel.meter_loc_end_dt NULLS FIRST) AS rel_start_time
	  	          , FIRST_VALUE(to_char(rel.meter_loc_end_dt, 'YYYY-MM-DD T HH24:MI:SS')) OVER(PARTITION BY rel.par_asset_id, rel.asset_id, rel.meter_loc_start_dt ORDER BY rel.meter_loc_end_dt NULLS FIRST) AS rel_end_time			   		
	               --to_char(rel.meter_loc_start_dt, 'YYYY-MM-DD T HH24:MI:SS') AS rel_start_time
	  				     --, to_char(rel.meter_loc_end_dt, 'YYYY-MM-DD T HH24:MI:SS') AS rel_end_time
	  				     , sdp.x_udc_asset_Id AS sdp_udc_id
	  				     , distnode.x_udc_asset_Id AS distnode_udc_id
	  				     , 'DISTRIBUTION-SDP' AS rel_type
	  				     , distnode.row_id AS distnode_src_id
	  		   			 , sdp.row_id AS sdp_src_id
	  				  FROM s_asset distnode
	  				     , s_asset_rel rel
	  				     , s_asset sdp
	  				 WHERE rel.x_asset_relation_type_cd = 'DISTRIBUTION-SDP'
	  				   AND rel.par_asset_id             = distnode.row_id
	  				   AND rel.asset_id                 = sdp.row_id
	  		   		   AND sdp.row_id  IN ('1-43FJ')
	  		   		   --AND NVL(sdp.data_src, @primaryOrg@) IN (@listOfOrgs@)
		   		 ORDER BY sdp.row_id;
		   		 
		   		 
		   		 
		   		 
		   		 
		   		 
		   		 
		   		 
		   		 
		   		 
		   		 *****************
CASE WHEN oxm.attrib_12 = NVL(oxm.attrib_13, TO_DATE('01-JAN-9999 23:59:59', 'DD-MON-YYYY HH24:MI:SS'))
	                   		THEN FIRST_VALUE(oxm.attrib_02) OVER(PARTITION BY o.row_id, oxm.attrib_01 ORDER BY oxm.last_upd DESC) 	
	                   		ELSE oxm.attrib_02
	              		END AS param_value,


		   			
             SELECT DISTINCT 
					    o.row_id 	   	AS src_id,
					    o.x_accnt_class AS prod_name,
					    o.ou_type_cd   	AS type,
					    o.x_name_1 	   	AS name,
					    o.x_name_2 	   	AS alt_name,
					    o.x_udc_accnt_id 	AS udc_id,
					    o.x_esp_accnt_id 	AS alt_udc_id,
					    o.x_universal_id 	AS universal_id,
					    NVL(o.x_data_src, 'SOURCE1') AS data_src,
					    o.cust_stat_cd  AS status_cd,
					    o.desc_text	   	AS desc_text,
					    o.created 	   	AS source_created,
					    o.last_upd 	   	AS source_last_upd,
					    oxm.attrib_01 	AS param_name,
					    FIRST_VALUE(oxm.attrib_02) OVER(PARTITION BY o.row_id, oxm.attrib_01, oxm.attrib_12 ORDER BY oxm.attrib_13 NULLS FIRST, oxm.last_upd DESC) 	 	AS param_value,
					    to_char(oxm.attrib_12, 'YYYY-MM-DD T HH24:MI:SS')	AS param_eff_start_time,
					    FIRST_VALUE(to_char(oxm.attrib_13, 'YYYY-MM-DD T HH24:MI:SS')) OVER(PARTITION BY o.row_id, oxm.attrib_01, oxm.attrib_12 ORDER BY oxm.attrib_13 NULLS FIRST, oxm.last_upd DESC) AS param_eff_end_time,
					    NULL   	AS insert_time,
					    NULL   	AS last_upd_time
					FROM 
					    s_org_ext_xm oxm
		   		      , s_org_ext o
		   			  , (SELECT DISTINCT account_row_id
		   			       FROM cx_alyt_account_cdc
		   		          WHERE marked_flag='Y'
		   		            AND processed_flag='N') cdc
					WHERE
						o.row_id=cdc.account_row_id
					AND oxm.par_row_id (+) = o.row_id
		   			AND o.ou_type_cd != 'Partner'
            AND o.row_id = '1-3YGD'
		   		    AND NVL(o.x_data_src, 'SOURCE1') IN ('CLDANALYTICSORG1')
					order by NVL(o.x_data_src, 'SOURCE1'), o.row_id, o.ou_type_cd  ;
					
					
					
					
FIRST_VALUE(oxm.attrib_02) OVER(PARTITION BY o.row_id, oxm.attrib_01, oxm.attrib_12 ORDER BY oxm.attrib_13 NULLS FIRST, oxm.last_upd DESC) AS param_value,
FIRST_VALUE(to_char(oxm.attrib_13, 'YYYY-MM-DD T HH24:MI:SS')) OVER(PARTITION BY o.row_id, oxm.attrib_01, oxm.attrib_12 ORDER BY oxm.attrib_13 NULLS FIRST, oxm.last_upd DESC) AS param_eff_end_time,					

FIRST_VALUE(param.attrib_02) OVER(PARTITION BY s.row_id, param.attrib_01, param.attrib_12 ORDER BY param.attrib_13 NULLS FIRST, param.last_upd DESC) AS param_value,
FIRST_VALUE(to_char(param.attrib_13, 'YYYY-MM-DD T HH24:MI:SS')) OVER(PARTITION BY s.row_id, param.attrib_01, param.attrib_12 ORDER BY param.attrib_13 NULLS FIRST, param.last_upd DESC) AS param_eff_end_time,



*******************************************************************************************************************************

Docs backup 
--------------
Introduction
Cloud Analytics Structural data processing consist of extracting data from underlying MDM and load this data to Analytics cloud (EIP 8x ) schema. It performs the following tasks:
Structural data extraction: Reference data, asset data, services data, relationship and parameter data from the underlying MDM will be extracted into input XMLs. The extraction can be executed in following modes.
Initial mode: All assets and relationships are extracted in iniital mode. Refer Initial extraction for more details
Incremental mode: Only those assets and relationships are extracted that were modified since last successful execution of extraction. Refer incremental extraction for more details.

Structural data loading: The loader will load extracted data to Analytics cloud (EIP 8x) schema using Reference data util and FlexSync.
Cloud Analytics will support data extraction for following EIP releases
EIP7.5
EIP7.6
EIP7.7
After the data has been extracted, it uses FlexSync to load the extracted data into EIP8 schema.


Assumptions
Org is pre-created in EIP8. Addition of new Orgs will be a manual step using OrgAdmin.sh.  Refers to OrgAdmin Utility for details.
Org name in 7x should not have '_' (underscore) in name.
This process does not synchronize / create 7x Users in EIP8. A default user in EIP8 will be created to cater Structural extract and load process. The same user will be used in the insert_by and last_upd_by columns in the database.
Sdp-DataSvcRel & Sdp-SvcAgreeRel: there could be multiple parameters under these entities in EnergyIP 7.  However, only the parameter with the latest start date in EnergyIP 7 will be used as the attribute in EnergyIP 8.

Data handling during 7x to 8x extraction and loading
 7.x row siebel row id is stored in respective asset's data_src column in 8.x.
'pathName' in the input XMLs, for respective assets' will be used to save src_id from 7x. The pathName will be mapped to respective table column data_src
The Asset parameter's data_src will have its parent 'pathName' i.e. 7x src_id value as FlexSync input XML does not support 'pathName' for parameter.
Channel
Interval channel: DataSource column contains siebel row id like 1-xyz.
Usage channel: DataSource column contains siebel row id like 1-xyz.
Register channel: Register channel may be split upto 18 channels. So datasource column contains SIEBEL row_id, splitted channel register_index_7x like 1-xyz,R1
DistNode extraction
DistNode-DistNode hierarchy can come either in field par_row_id (parent dist node) or as a parameter 'Parent DN' and is fetched in following priority i.e. first preference is for parameter name 'Parent DN' and second is par_row_id field. 
Parameter 'Parent DN': The DN will have parameter 'Parent DN' and its value will be src_id of its parent DN. If the DN-DN hierarchy is fetched from param name 'Parent DN' then rel start date will be param start date. 
Field par_row_id: The DN will have the par_row_id as src_id of its parent DN. If the DN-DN hierarchy is fetched field par_row_id (parent dist node) then consider the epoc date as rel start date.
Distnode extraction is done as per scenario 1. Single level of hierarchy is traversed for DistNode-DistNode rel i.e DistNode parent and child is fetched only and not parent of parent of a distnode
status_cd: If status_cd in 7x is null, the value is saved as 'Active' in 8x.
Dependencies
Following are prerequisites for Cloud analytics application:
Execution of ReferenceDataUtil.sh to create master org
Run configuration.sh to create application instance data
Org & User are pre created in 8.x database
Country, TimeZone, StateProvince should be pre-created in 8.x DB
The Distribution Node in 7x does not have premise associated with it but the 8.x schema expects a premise for servicepoint. Pre-create a dummy premise in 8.x and set its udcId in Extractor property a108-structuralExtractApplication.dummyPremiseUdcId e.g. a108-structuralExtractApplication.dummyPremiseUdcId=DummyDN_premise_10_1
The configuration file located at /home/pipe/opt/em-ac-adf/conf/appXML/product_mapping.csv, needs to be pre populated. The file will have 7x to 8x product mapping details. The CSV file will have the following headers:
 product_name_7x	type_7x 	sub_type_7x 	product_name_8x 	type_8x 	sub_type_8x 	udc_id_8x 	register_index_7x 
Note: udc_id_8x & register_index_7x are only applicable for type Measurement. Refer attached product_mapping.csv, sample configurations for product mapping. Only configured measurement, channels and services products will be extracted (mentioned in product_mapping.csv file).
Supported entities for extraction 
Reference data
CycleCd
Note: CycleCd with day_type_class LIKE ' Cycle' are only extracted as part of seed data.
servicePointClass with attributes
Service Point
Distribution Node
deviceClass with attributes
Meter
CT-PT
Communication Module
Disconnect Collar
measType
dataSvcClass with attributes
Data Collection
Data Delivery
VEE
Framing
Deployment Planning
Virtual Channel Persistence
Estimation 
Note: Estimation data from 7x is modified to VEE in 8x
Outage
Data Transfer
svcAgreeClass with attributes
Energy Purchase
Generation Balance Provider
Consumption Balance Provider
Billing
AMI Operator
Energy Supplier
Energy Services Agent
Asset data
Premise
Route with param
Account with param
Device with param
Meter
CT-PT
Communication Module
Disconnect Collar
SdpStack data
Svcpt with param
Channel with param
DistNode with param
Relationship
SDP-Route
SDP-DistNode
DistNode-DistNode
Accnt-SDP
SDP-Meter
Meter-Channel
Meter-Commmod
SDP-DataSvc
SDP-SvcAgree
Known issues
EIP-27521: RDU will be enhanced to add support for svc_provider data. Partner account data is not extracted as part of Reference data extraction as RDU does not support this feature.


*****************************************************************************************************************

	
CREATE TABLE CX_ALYT_INITIAL_LOAD_DEVICE(
    src_id    VARCHAR2(15),
    rownum_id        NUMBER
);

CREATE TABLE CX_ALYT_INITIAL_LOAD_ROUTE(
    src_id    VARCHAR2(15),
    rownum_id        NUMBER
);

CREATE TABLE CX_ALYT_INITIAL_LOAD_SVCPT(
    src_id    VARCHAR2(15),
    rownum_id        NUMBER
);


 {EIP_HOME}/data/synchronization/flexsync/failedMsg
DummyDN_premise_10_1


universalSyncInterfacePortTypeFilePoller.asyncProcessorNoOfThreads
universalSyncInterfacePortTypeFilePoller.noOfThreads



poll interval less
poller thread 1

***************************************************************************
create org
./orgAdmin.sh -c create -d /home/eip -o CLDANALYTICSORG1 -u CLDANALYTICSORG1_USR_1 -p CLDANALYTICSORG1_USR_1 -f fname_CLDANALYTICSORG1 -l lname_CLDANALYTICSORG1


FCU
./orgAdmin.sh -c create -d /home/eip -o FCU -u FCU_USR_1 -p FCU_USR_1 -f fname_FCU -l lname_FCU
./orgAdmin.sh –c importRefData –o FCU
./ReferenceDataUtil.sh -DimportReferenceDataService.referenceDataSourcePath=/home/eip/cloud/rdu -Dapplication.scope=org -Dapplication.orgName=FCU

CNP
./orgAdmin.sh -c create -d /home/eip -o 957877905 -u 957877905_USR_1 -p 957877905_USR_1 -f fname_957877905 -l lname_957877905

(not executed on ind-lnxapp122)
./orgAdmin.sh –c importRefData –o 957877905 

(not executed on ind-lnxapp122)
./ReferenceDataUtil.sh -DimportReferenceDataService.referenceDataSourcePath=/home/eip/cloud/rdu -Dapplication.scope=org -Dapplication.orgName=957877905


******************************************
Loader performance

16 threads

Premise 10k rec
start: 2014-12-16 13:38:09,042 [ConsumerThread14] INFO  - SyncRequest processing Start for Message Id : MESSAGE_CLOUDALYT_AssetData_FCU-1-44L6O
end: 2014-12-16 13:59:28,384 [ConsumerThread2] INFO  - rename file [/home/eip/data/synchronization/flexsync/in/ADF_AssetData_Premise_20141215_215911_119_FCU.xml] to [/home/eip/data/synchronization/flexsync/processed/ADF_AssetData_Premise_20141215_215911_119_FCU.xml
time: 21 min

Accnt 10k rec
start: 2014-12-16 15:39:50,615 [ConsumerThread4] INFO  - SyncRequest processing Start for Message Id : MESSAGE_CLOUDALYT_AssetData_FCU-1-126YQ
end: 2014-12-16 15:58:39,525 [ConsumerThread15] INFO  - rename file [/home/eip/data/synchronization/flexsync/in/ADF_AssetData_Account_20141215_215911_19_FCU.xml] to [/home/eip/data/synchronization/flexsync/processed/ADF_AssetData_Account_20141215_215911_19_FCU.xml
time: 19 min

Device 10k rec
start: 2014-12-16 16:05:11,891 [ConsumerThread11] INFO  - SyncRequest processing Start for Message Id : MESSAGE_CLOUDALYT_AssetData_FCU-1-1481K
end: 2014-12-16 16:24:30,238 [ConsumerThread0] INFO  - rename file [/home/eip/data/synchronization/flexsync/in/ADF_AssetData_Device_20141215_215911_17_FCU.xml] to [/home/eip/data/synchronization/flexsync/processed/ADF_AssetData_Device_20141215_215911_17_FCU.xml]
time: 19 min

32 thread 
Route 10k rec
start:
end:
time:




**************************
<property name= sqlRelDistNode >
		   <value>
		   		<![CDATA[
			   		SELECT DISTINCT * FROM 
		   				(SELECT sam.par_row_id child_dnode
							, sam.attrib_02 parent_dnode
							, (SELECT x_udc_asset_id FROM s_asset sa1 WHERE sa1.row_id = sam.par_row_id) sdp_udc_id 
							, (SELECT x_udc_asset_id FROM s_asset sa2 WHERE sa2.row_id = sam.attrib_02)  distnode_udc_id -- parentDN
		   		 			, to_char(sam.attrib_12, 'YYYY-MM-DD T HH24:MI:SS') AS rel_start_time
							, to_char(sam.attrib_13, 'YYYY-MM-DD T HH24:MI:SS') AS rel_end_time 
							, 'DISTNODE-DISTNODE' AS rel_type
		   					, sam.par_row_id AS distnode_src_id
		   			        --, dn_sdp_rel.asset_id AS sdp_src_id
						FROM s_asset_xm  sam
		   			       --, s_asset_rel dn_sdp_rel
						WHERE sam.attrib_02 IN (@dnList@)
						  	AND sam.attrib_01 = @paramNameParentDN@
		   		            --AND dn_sdp_rel.x_asset_relation_type_cd = 'DISTRIBUTION-SDP' 
		   		            --AND dn_sdp_rel.par_asset_id = sam.attrib_02
		   		            --AND dn_sdp_rel.asset_id IN (@sdpSrcIdList@)
		   		            AND (
		   		                 sam.attrib_12 <> sam.attrib_13
		   		        	     OR
		   			             sam.attrib_13 IS NULL
		   			            )
					UNION ALL
						SELECT sa.row_id child_dnode
							, sa.par_asset_id parent_dnode
							, sa.x_udc_asset_id sdp_udc_id 
		   					, (SELECT x_udc_asset_id FROM s_asset sa1 WHERE sa1.row_id = sa.par_asset_id) distnode_udc_id -- parentDN
							, to_char(TO_DATE('01/01/1970 00:00:00','MM/DD/YYYY HH24:MI:SS' ), 'YYYY-MM-DD T HH24:MI:SS') rel_start_time
							, NULL rel_end_time
							, 'DISTNODE-DISTNODE' AS rel_type
		   					, sa.row_id AS distnode_src_id
		   			        --, dn_sdp_rel.asset_id AS sdp_src_id
						FROM s_asset sa
		   			       --, s_asset_rel dn_sdp_rel
		   		       WHERE sa.type_cd = 'Distribution Node'
							AND sa.par_asset_id IN (@dnList@)
							AND sa.par_asset_id NOT IN
							(SELECT attrib_02 FROM s_asset_xm  sam
							WHERE attrib_02 IN (@dnList@)
						  		AND attrib_01 = @paramNameParentDN@)
		   		            --AND dn_sdp_rel.x_asset_relation_type_cd = 'DISTRIBUTION-SDP' 
		   		            --AND dn_sdp_rel.par_asset_id = sa.par_asset_id
		   			        AND sa.par_asset_id IS NOT NULL
		   		            --AND dn_sdp_rel.asset_id IN (@sdpSrcIdList@)
		   			/*UNION ALL
						SELECT sam.par_row_id child_dnode
							, sam.attrib_02 parent_dnode
							, (SELECT x_udc_asset_id FROM s_asset sa1 WHERE sa1.row_id = sam.par_row_id) sdp_udc_id 
							, (SELECT x_udc_asset_id FROM s_asset sa2 WHERE sa2.row_id = sam.attrib_02) distnode_udc_id --parentDN
							, to_char(sam.attrib_12, 'YYYY-MM-DD T HH24:MI:SS') AS rel_start_time
							, to_char(sam.attrib_13, 'YYYY-MM-DD T HH24:MI:SS') AS rel_end_time
							, 'DISTNODE-DISTNODE' AS rel_type
		   				 	, sam.attrib_02 AS distnode_src_id
		   			        --, dn_sdp_rel.asset_id AS sdp_src_id
						FROM s_asset_xm  sam
		   			       --, s_asset_rel dn_sdp_rel
						WHERE sam.par_row_id IN (@dnList@)
						  	AND sam.attrib_01 = @paramNameParentDN@
		   		            --AND dn_sdp_rel.x_asset_relation_type_cd = 'DISTRIBUTION-SDP' 
		   		            --AND dn_sdp_rel.par_asset_id = sam.par_row_id
		   			        --AND dn_sdp_rel.asset_id IN (@sdpSrcIdList@)
		   		            AND (
		   		                 sam.attrib_12 <> sam.attrib_13
		   		        	     OR
		   			             sam.attrib_13 IS NULL
		   			            )
					UNION ALL
						SELECT sa.row_id child_dnode
							, sa.par_asset_id parent_dnode
		   					, sa.x_udc_asset_id sdp_udc_id 
							, (SELECT x_udc_asset_id FROM s_asset sa1 WHERE sa1.row_id = sa.par_asset_id) distnode_udc_id --parentDN
							, to_char(TO_DATE('01/01/1970 00:00:00','MM/DD/YYYY HH24:MI:SS' ), 'YYYY-MM-DD T HH24:MI:SS') rel_start_time
							, NULL rel_end_time
							, 'DISTNODE-DISTNODE' AS rel_type
		   					, sa.par_asset_id AS distnode_src_id
		   			        --, dn_sdp_rel.asset_id AS sdp_src_id
						FROM s_asset sa
		   			       --, s_asset_rel dn_sdp_rel
		   		       WHERE sa.type_cd = 'Distribution Node'
							AND sa.row_id IN (@dnList@)
		   					AND sa.row_id NOT IN
							(SELECT par_row_id FROM s_asset_xm  sam
							WHERE par_row_id IN (@dnList@)
						  		AND attrib_01 = @paramNameParentDN@)
		   		            --AND dn_sdp_rel.x_asset_relation_type_cd = 'DISTRIBUTION-SDP' 
		   		            --AND dn_sdp_rel.par_asset_id = sa.row_id	
		   			        AND sa.par_asset_id IS NOT NULL*/
		   		            --AND dn_sdp_rel.asset_id IN (@sdpSrcIdList@)
		   		        ) a
		   		        , s_asset_rel dn_sdp_rel
		   		        WHERE dn_sdp_rel.x_asset_relation_type_cd = 'DISTRIBUTION-SDP'
		   		          AND dn_sdp_rel.asset_id IN (@sdpSrcIdList@)
		   		          AND dn_sdp_rel.par_asset_id = a.parent_dnode
						ORDER BY sdp_src_id
		   		]]>
		   </value>
		</property>














<!--
		<property name= sqlRelDistNode >
		   <value>
		   		<![CDATA[
			   		SELECT DISTINCT * FROM 
		   				(SELECT sam.par_row_id child_dnode
							, sam.attrib_02 parent_dnode
							, (SELECT x_udc_asset_id FROM s_asset sa1 WHERE sa1.row_id = sam.par_row_id) sdp_udc_id 
							, (SELECT x_udc_asset_id FROM s_asset sa2 WHERE sa2.row_id = sam.attrib_02)  distnode_udc_id -- parentDN
		   		 			, to_char(sam.attrib_12, 'YYYY-MM-DD T HH24:MI:SS') AS rel_start_time
							, to_char(sam.attrib_13, 'YYYY-MM-DD T HH24:MI:SS') AS rel_end_time 
							, 'DISTNODE-DISTNODE' AS rel_type
		   					, sam.par_row_id AS distnode_src_id
		   			        , dn_sdp_rel.asset_id AS sdp_src_id
						FROM s_asset_xm  sam
		   			       , s_asset_rel dn_sdp_rel
						WHERE sam.attrib_02 IN (@dnList@)
						  	AND sam.attrib_01 = @paramNameParentDN@
		   		            AND dn_sdp_rel.x_asset_relation_type_cd = 'DISTRIBUTION-SDP' 
		   		            AND dn_sdp_rel.par_asset_id = sam.attrib_02
		   		            AND dn_sdp_rel.asset_id IN (@sdpSrcIdList@)
		   		            AND (
		   		                 sam.attrib_12 <> sam.attrib_13
		   		        	     OR
		   			             sam.attrib_13 IS NULL
		   			            )
					UNION ALL
						SELECT sa.row_id child_dnode
							, sa.par_asset_id parent_dnode
							, sa.x_udc_asset_id sdp_udc_id 
		   					, (SELECT x_udc_asset_id FROM s_asset sa1 WHERE sa1.row_id = sa.par_asset_id) distnode_udc_id -- parentDN
							, to_char(TO_DATE('01/01/1970 00:00:00','MM/DD/YYYY HH24:MI:SS' ), 'YYYY-MM-DD T HH24:MI:SS') rel_start_time
							, NULL rel_end_time
							, 'DISTNODE-DISTNODE' AS rel_type
		   					, sa.row_id AS distnode_src_id
		   			        , dn_sdp_rel.asset_id AS sdp_src_id
						FROM s_asset sa
		   			       , s_asset_rel dn_sdp_rel
		   		       WHERE sa.type_cd = 'Distribution Node'
							AND sa.par_asset_id IN (@dnList@)
							AND sa.par_asset_id NOT IN
							(SELECT attrib_02 FROM s_asset_xm  sam
							WHERE attrib_02 IN (@dnList@)
						  		AND attrib_01 = @paramNameParentDN@)
		   		            AND dn_sdp_rel.x_asset_relation_type_cd = 'DISTRIBUTION-SDP' 
		   		            AND dn_sdp_rel.par_asset_id = sa.par_asset_id
		   			        AND sa.par_asset_id IS NOT NULL
		   		            AND dn_sdp_rel.asset_id IN (@sdpSrcIdList@)
		   			UNION ALL
						SELECT sam.par_row_id child_dnode
							, sam.attrib_02 parent_dnode
							, (SELECT x_udc_asset_id FROM s_asset sa1 WHERE sa1.row_id = sam.par_row_id) sdp_udc_id 
							, (SELECT x_udc_asset_id FROM s_asset sa2 WHERE sa2.row_id = sam.attrib_02) distnode_udc_id --parentDN
							, to_char(sam.attrib_12, 'YYYY-MM-DD T HH24:MI:SS') AS rel_start_time
							, to_char(sam.attrib_13, 'YYYY-MM-DD T HH24:MI:SS') AS rel_end_time
							, 'DISTNODE-DISTNODE' AS rel_type
		   				 	, sam.attrib_02 AS distnode_src_id
		   			        , dn_sdp_rel.asset_id AS sdp_src_id
						FROM s_asset_xm  sam
		   			       , s_asset_rel dn_sdp_rel
						WHERE sam.par_row_id IN (@dnList@)
						  	AND sam.attrib_01 = @paramNameParentDN@
		   		            AND dn_sdp_rel.x_asset_relation_type_cd = 'DISTRIBUTION-SDP' 
		   		            AND dn_sdp_rel.par_asset_id = sam.par_row_id
		   			        AND dn_sdp_rel.asset_id IN (@sdpSrcIdList@)
		   		            AND (
		   		                 sam.attrib_12 <> sam.attrib_13
		   		        	     OR
		   			             sam.attrib_13 IS NULL
		   			            )
					UNION ALL
						SELECT sa.row_id child_dnode
							, sa.par_asset_id parent_dnode
		   					, sa.x_udc_asset_id sdp_udc_id 
							, (SELECT x_udc_asset_id FROM s_asset sa1 WHERE sa1.row_id = sa.par_asset_id) distnode_udc_id --parentDN
							, to_char(TO_DATE('01/01/1970 00:00:00','MM/DD/YYYY HH24:MI:SS' ), 'YYYY-MM-DD T HH24:MI:SS') rel_start_time
							, NULL rel_end_time
							, 'DISTNODE-DISTNODE' AS rel_type
		   					, sa.par_asset_id AS distnode_src_id
		   			        , dn_sdp_rel.asset_id AS sdp_src_id
						FROM s_asset sa
		   			       , s_asset_rel dn_sdp_rel
		   		       WHERE sa.type_cd = 'Distribution Node'
							AND sa.row_id IN (@dnList@)
		   					AND sa.row_id NOT IN
							(SELECT par_row_id FROM s_asset_xm  sam
							WHERE par_row_id IN (@dnList@)
						  		AND attrib_01 = @paramNameParentDN@)
		   		            AND dn_sdp_rel.x_asset_relation_type_cd = 'DISTRIBUTION-SDP' 
		   		            AND dn_sdp_rel.par_asset_id = sa.row_id	
		   			        AND sa.par_asset_id IS NOT NULL
		   		            AND dn_sdp_rel.asset_id IN (@sdpSrcIdList@)
		   		        )	   		
						ORDER BY sdp_src_id
		   		]]>
		   </value>
		</property>-->
		
		******************
		
		
		
		Westar Org: 006943781
		sebl03
		cldext01
		
		
********************************************	
Eclipse

com.eMeter.PIPe.hydrofw.application.PropertiesBootstrap
D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/opt/em-ac-adf/release/1.0/test/conf/appProperties/extractorProp.properties
D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/opt/em-ac-adf/trunk/test/conf/appProperties/extractorProp.properties

D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/opt/Analytics-adf2/core/trunk/em-ac-adf/test/conf/appProperties/extractorProp.properties

***************************************************************************

2014-12-23 13:47:49,210 [main] INFO  - StructuralExtractApplication Mode:: initial
2014-12-23 13:47:55,133 [pool-2-thread-2] INFO  - Total Records for cycleCd extraction:1
2014-12-23 13:47:58,293 [pool-2-thread-1] INFO  - Total Records for Seed data extraction:282
2014-12-23 13:48:00,394 [main] INFO  - AssetDataDevice Total Records:546
2014-12-23 13:48:00,394 [main] INFO  - AssetDataRoute Total Records:133
2014-12-23 13:48:00,394 [main] INFO  - AssetDataAccount Total Records:133
2014-12-23 13:48:00,394 [main] INFO  - AssetDataPremise Total Records:132
2014-12-23 13:48:05,068 [main] INFO  - SdpStackData stat : 
ServiceDeliveryPoint,132
DistributionNode,251
TotalChannel,1613,Channel,1488,VirtualChannel,125
servicePointServiceAgreementAssociation,251
servicePointDataServiceAssociation,2301
servicePointDeviceAssociation,385
deviceChannelAssociation,1677
deviceFunctionAssociation,129
servicePointServicePointGroupAssociation,129
accountServicePointAssociation,129
servicePointServicePointAssociation SDP-DN,129
servicePointServicePointAssociation DN-DN,124
2014-12-23 13:48:05,079 [main] INFO  - creating zip file... 
2014-12-23 13:48:11,617 [main] INFO  - Done Structural execution in 22.408120093 sec





properties.db=D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/opt/em-ac-adf/release/1.0/test/conf/systemProperties/db.properties
 properties.db=conf/systemProperties/db.properties
DatabaseProperties=false

 Application XMLs
springBeansXml.app=classpath:com/emeter/extractor/beanXMLs/structuralExtractApp.xml
springBeansXml.seedData=classpath:com/emeter/extractor/beanXMLs/structuralExtractSeedData.xml
springBeansXml.assetData=classpath:com/emeter/extractor/beanXMLs/structuralExtractAssetData.xml
springBeansXml.sdpStackData=classpath:com/emeter/extractor/beanXMLs/structuralExtractSdpStackData.xml
springBeansXml.extractDB=classpath:com/emeter/extractor/beanXMLs/structuralExtractDB.xml
 springBeansXml.sibelDataSource=classpath:com/emeter/extractor/beanXMLs/SiebelDataSource.xml
springBeansXml.cloudExtDataSource=classpath:com/emeter/extractor/beanXMLs/cloudExtDataSource.xml
 springBeansXml.structuralExtractFilter= {PIPE_HOME}/opt/em-ac-adf/conf/appXML/cloudAlytExtraction/structuralExtractFilterConfig.xml
springBeansXml.structuralExtractFilter=classpath:conf/appXML/structuralExtractFilterConfig.xml

 DB locking properties in seconds
lockManager.defaultTimeOut=60
lockManager.expiryTime=300

 application properties 
  incremental | initial
a108-structuralExtractApplication.mode=initial
a108-structuralExtractApplication.extractOutputDirPath= {PIPE_HOME}/Outgoing/Utility/CloudStructuralExtract
a108-structuralExtractApplication.threadSizeAsset=32
a108-structuralExtractApplication.countOfMsgInXML=500
a108-structuralExtractApplication.threadSizeSdpStack=16
a108-structuralExtractApplication.countOfSdpMsgInXML=200
 a108-structuralExtractApplication.configDirPath= {PIPE_HOME}/opt/em-ac-adf/conf/appXML/cloudAlytExtraction/
a108-structuralExtractApplication.configDirPath=D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/opt/em-ac-adf/release/1.0/output/conf/appXML


Clover
Extractor:
**/com/emeter/extractor/StructuralExtTestSuite.java,**/com/emeter/extractor/intrfc/Extractor.java,**/com/emeter/extractor/intrfc/ExtractRunner.java,**/com/emeter/extractor/utils/ExtractorErrorCode.java,**/com/emeter/extractor/utils/ExtractorException.java,**/com/emeter/extractor/utils/StructuralExtConstants.java,**/com/emeter/extractor/AbstractPropertiesBootstrapBasedTest.java,**/com/emeter/extractor/TestAnalyticsStructuralExtractor.java,**/com/emeter/extractor/TestExtractAssetDataHelper.java,**/com/emeter/extractor/TestExtractSdpStackDataHelper.java,**/com/emeter/extractor/TestExtractSeedDataHelper.java,

Loader
**/com/emeter/analytics/dataloader/impl/ReferenceDataLoaderImpl.java,**/com/emeter/analytics/dataloader/AnalyticsDataLoader.java,**/com/emeter/analytics/exception/ValidationException.java,**/com/emeter/analytics/utils/OSProcess.java,**/com/emeter/analytics/utils/OSProcessExecutionException.java,**/com/emeter/analytics/AnalyticsDataLoaderRunOnceApplication.java,

***********************************************************************************************************************
   QA cloud -1 
 cloudExtDataSource.url=jdbc:oracle:thin:@(DESCRIPTION=(ADDRESS_LIST=(ADDRESS=(PROTOCOL=TCP)(HOST=ind-db08.emeter.com)(PORT=1521)))(CONNECT_DATA=(SERVER=DEDICATED)(SERVICE_NAME=isebl_54)))
 cloudExtDataSource.username=pipe
 cloudExtDataSource.password=pipe594

   QA cloud -2 
 cloudExtDataSource.url=jdbc:oracle:thin:@(DESCRIPTION=(ADDRESS_LIST=(ADDRESS=(PROTOCOL=TCP)(HOST=ind-db06.emeter.com)(PORT=1521)))(CONNECT_DATA=(SERVER=DEDICATED)(SERVICE_NAME=isebl_45)))
 cloudExtDataSource.username=pipe
 cloudExtDataSource.password=pipe495

    QA Migration
 cloudExtDataSource.url=jdbc:oracle:thin:@(DESCRIPTION=(ADDRESS_LIST=(ADDRESS=(PROTOCOL=TCP)(HOST=ind-db05.emeter.com)(PORT=1521)))(CONNECT_DATA=(SERVER=DEDICATED)(SERVICE_NAME=isebl_38)))
 cloudExtDataSource.username=siebel
 cloudExtDataSource.password=siebel318

    CNP test data siebel
 cloudExtDataSource.url=jdbc:oracle:thin:@(DESCRIPTION=(ADDRESS_LIST=(ADDRESS=(PROTOCOL=TCP)(HOST=emdb36.emeter.com)(PORT=1521)))(CONNECT_DATA=(SERVER=DEDICATED)(SERVICE_NAME=SEBP)))
 cloudExtDataSource.username=siebel
 cloudExtDataSource.password=siebel

    FortCollins data siebel
 cloudExtDataSource.url=jdbc:oracle:thin:@(DESCRIPTION=(ADDRESS_LIST=(ADDRESS=(PROTOCOL=TCP)(HOST=emdb36.emeter.com)(PORT=1521)))(CONNECT_DATA=(SERVER=DEDICATED)(SERVICE_NAME=sebldb80)))
 cloudExtDataSource.username=siebel
 cloudExtDataSource.password=siebel

    Dev - old
cloudExtDataSource.url=jdbc:oracle:thin:@(DESCRIPTION=(ADDRESS_LIST=(ADDRESS=(PROTOCOL=TCP)(HOST=ind-db02.emeter.com)(PORT=1521)))(CONNECT_DATA=(SERVER=DEDICATED)(SERVICE_NAME=isebl_12)))
cloudExtDataSource.username=siebel
cloudExtDataSource.password=siebel132

    Dev - CLDEXT12 
 cloudExtDataSource.url=jdbc:oracle:thin:@(DESCRIPTION=(ADDRESS_LIST=(ADDRESS=(PROTOCOL=TCP)(HOST=ind-db02.emeter.com)(PORT=1521)))(CONNECT_DATA=(SERVER=DEDICATED)(SERVICE_NAME=isebl_12)))
 cloudExtDataSource.username=CLDEXT12
 cloudExtDataSource.password=CLDEXT12

    Dev - Local
 cloudExtDataSource.url=jdbc:oracle:thin:@(DESCRIPTION=(ADDRESS_LIST=(ADDRESS=(PROTOCOL=TCP)(HOST=localhost)(PORT=1521)))(CONNECT_DATA=(SERVER=DEDICATED)(SERVICE_NAME=XE)))
 cloudExtDataSource.username=siebel
 cloudExtDataSource.password=siebel

***********************************************************************************************************************

**/com/emeter/extractor/runner/AbstractSdpStackRunner.java

getThreadSizeSdpStack
getEnvServerTimeZone


https://www.youtube.com/watch v=WHDFxm7ZU4Q

***********************************************************************************************************************
Docs

*resources.zip missing
*sample flexSync message obsolete

6- optional if they do .. EnergyIP ..  then follow .. add sample directory and add resources.zip
6- remove country etc.. comment everything and ask utility to uncomment them as per need

point to flexSync 8x messages link.


Check that the reference data for Data Transfer service, DistNode-DistNode relation, country, timeZone, stateProvince exist in EnergyIP 8. Else, create them using the following sample resources.zip file.




The following preparation must be completed prior to data extraction:
Org must be precreated in EnergyIP 8 in Cloud Analytics. Addition of new Orgs will be a manual step using OrgAdmin.sh.  For more information, refer to OrgAdmin Utility.
A default user must be precreated in EnergyIP 8 to cater to the structural extract and load process. The same user is used in the insert_by and last_upd_by columns in the database. This extractor process does not synchronize or create EnergyIP 7 Users in EnergyIP 8.
Execution of ReferenceDataUtil.sh to create master org in EIP schema in EnergyIP 8.
Ensure that Org name in EnergyIP 7 does not contain _ (underscore) as part of the name. 
Use SQL query on EnergyIP 7 data base to determine the org name
select org_name from org;

Run configuration.sh to create application instance in EnergyIP 8.
Org and User are precreated in EIP database in EnergyIP 8.
Country, TimeZone, StateProvince, used in EnergyIP 7, should be precreated in the EIP database in EnergyIP 8.
Optional, If the Utility has Data Transfer service and DistNode-DistNode relation, follow below steps:
Download the resources.zip OR copy /home/eip/opt/em-ac-adf/trunk/sample/resources.zip file to a location such as /home/eip/cloud/rdu.
Extract the zip file at the resources directory.
Remove the resources.zip file.
Uncomment and edit the respective file and save the XML configuration files.
Execute the following command for each Org to create data.
cd /home/eip/bin
./ReferenceDataUtil.sh -DimportReferenceDataService.referenceDataSourcePath=/home/eip/cloud/rdu -Dapplication.scope=org -Dapplication.orgName=<ORG_NAME>

A default premise should be created in EnergyIP 8 as it will be referenced in the DistNode entity in EnergyIP 8. For information on creating the default premise, refer to FlexSync Message Samples.



**********************************************spark config**************************************************************
WS: D:\work\workspace\eclipseWS2jdk8

Resolve from ivy
move guava1.6 + 4 etc jars up the class path
check eclipse version Luna 442 kind of 


1) add -Spropfile=file:/....

2) Exception in thread "main" org.apache.spark.SparkException: A master URL must be set in your configuration
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:185)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at com.emeter.spark.SparkWf.init(SparkWf.java:22)
	at com.emeter.spark.SparkWf.execute(SparkWf.java:42)
	at com.emeter.test.sdp.LoadSdp.main(LoadSdp.java:27)

	Use sparkConf.setMaster("local[1]");
	

3) HADOOP_HOME to the <projectLoaction>/winutils/bin
	-Use path till winutils do not include bin
	
4) create namespace in HBase
	create_namespace 'ravigu'
	create 'ravigu:sdp', 'sdpdetails'
	
put 'ravigu:sdp', '1', 'sdpdetails:UDC_ID', 'RGA-Test-SdpUdcId1'
put 'ravigu:sdp', '1', 'sdpdetails:PREMISE_UDC_ID', 'RGA-Test-PrmsUdcId1' 

put 'ravigu:sdp', '2', 'sdpdetails:UDC_ID', 'RGA-Test-SdpUdcId2'
put 'ravigu:sdp', '2', 'sdpdetails:PREMISE_UDC_ID', 'RGA-Test-PrmsUdcId2' 



eclipse : 
arguements: 
D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/core/sandbox/RaviGupta/em-revenueprotection2.0/trunk/input -SpropFile=file:/D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/core/sandbox/RaviGupta/em-revenueprotection2.0/trunk/test/conf/env.properties 1

VM
-Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Dsun.io.serialization.extendedDebugInfo=true

HADOOP_HOME
D:\work\codebase\P4-WS\Perforce-WS\EnergyIP\core\sandbox\RaviGupta\em-revenueprotection2.0\trunk\winutils




-----------------------------------------------------------------------------------


RP 8x db details: (Sanjiv)
eip139/eip139@oltp01:192.168.199.11 

8x Mudr details (Manas)
mudr263
emdb36.emeter.com
oltp08
1521 



18/2 & 19/2
- Understanding existing Flow
- Understanding 7x - Algorithm Framework & Machine Learning Framework flow
- Create draft for Cloud - Algorithm Framework & Machine Learning Framework flow

20/2 21/2 22/2
- Understand Case Management Flow
- Deploy existing Case management on server
- Understand Flows and all 



Understanding:
---------------
- 7x - Algorithm Framework & Machine Learning Framework flow will be moved to Cloud EIP 
- Case management system will be modified as per above changes
- Feedback system will be removed as now only EIP exixts
- minutes_backward is used to calculate value for past '||p_minutes_backward/(24*60)||' days

High level Todos
----------------
- 7x Algorithm Framework & Machine Learning Framework flow will be moved to Cloud EIP 
	
	- POC on 8x Job creation process
	- R changes 
	- Multi Org handling
	- Creation of required BOs from CodeGen
	- JUnit with 80   coverage and SonarQube integration
	
- Case management system will be modified as per above changes
	- CAS Authentication
	- UI -> Sample application with extJs OR Click view (POC)
		- Charts creation etc
	- Multi Org handling
	- Creation of required BOs from CodeGen
	- JUnit with 80   coverage and SonarQube integration

- Security & Multitenancy
- Timezone: All timestamps are in env.serverTimeZone in that environment unless specifically called out.



7x Algorithm Framework & Machine Learning Framework flow will be moved to Cloud EIP
-----------------------------------------------------------------------------------

Tasks for 8x 
*** All insert & ddl according to EIP 8x schema
*** Create SDP-SEGMENT_CONFIG table in EIP 8 with values. The sdp_id in analytics is part of interval_f table.
	In EIP 8 lp_interval is in mudrDB and table does not have sdp_id. Need to update sdp_segment_config table with 
	-Required sdp and their meter = sdp(sdp_seg_config query) + sdp-Meter Rel -> use meterId in lp_interval
	- OR SDP join with channel table = sdp(sdp_seg_config query) + channel (sdpid) -> use channelID in lp_interval
*** Insert DMLs for algo_def etc do not have org_id i.e have NULL as ORGID. Need to replicate insert DMLs for ListOfOrgs 
*** Procedures update
	- EIP 8x schema compatible
	- Remove netezza specific SQL commands like distribute etc
*** replace use interval_f from Analytics DB with EIP 8 schema
*** May be we can replace WID and ID in procs with UDC_ID this will solve many issues
*** RP 1.0 procedures use analytics DB to get all SDP wid and interval data etc from same DB
	But in Cloud we'll have EIP and MUDR schema for structural and transactional data
	
	Two approaches:
	1) If Using procedure, add db link for MUDR DB
	2) If using Java, store SDP data in tmp table and then query MUDR table for procs


Will cloud RP use
   add new Algorithms
   SQL procedures approach
   


Process begins with either a scheduled job or the job is run manually from EM-UI 
1. Revenue protection Job---------------------7x job 
2. Analyze SDP score Job (invoke R)-----------7x job
3. Create Investigation Request Job-----------7x job
4. Migrate Investigation Ticket Job-----------8x job
5. Sync Feedback Info Job - not required------8x job


SDP_SEGMENT_CONFIG 
-------------------
contains query that segments the SDP population. The SEGMENT_QUERY column contains the select statement used to 
generate a set of SDPs that the algorithms will be used against.

Current Query:
insert  into SDP_SEGMENT_CONFIG (
        WID, APPLICATION_NAME,  SEGMENT_NAME, SEGMENT_QUERY, PROCEDURE_NAME, ACTIVE_IND, ORG_ID, segment_code )
values  (

	select  nvl(max(wid) + 1, 1) from    SDP_SEGMENT_CONFIG), -- WID
    'Revenue Protection Calculation', -- APPLICATION_NAME
    'Revenue_Protection_SDP', --SEGMENT_NAME
    'select distinct sd.wid as sdp_wid -- SEGMENT_QUERY
		from svc_pt_data_svc_rel_ps spdsr,
			data_svc_class_ps dsc,
			sdp_d sd 
		where spdsr.class_src_id = dsc.src_id
			and sd.src_id = spdsr.svc_pt_src_id
			and dsc.sub_type = ''VEE'' 
			and dsc.name = ''VEE Service, Residential'' ', 
	null, -- PROCEDURE_NAME
	'Y', -- ACTIVE_IND
	NULL, -- ORG_ID
	'RPS' -- segment_code

	
	8x variant  - Query will fetch sdp along with -> sdpid - channelid - measTypeid & its interval length - premiseid - postalcd 
	----------

		insert  into SDP_SEGMENT_CONFIG (
				ID, APPLICATION_NAME,  SEGMENT_NAME, SEGMENT_QUERY, PROCEDURE_NAME, ACTIVE_IND, ORG_ID, segment_code )
		values  (

			select  nvl(max(id) + 1, 1) from    SDP_SEGMENT_CONFIG), -- ID
			'Revenue Protection Calculation', -- APPLICATION_NAME
			'Revenue_Protection_SDP', --SEGMENT_NAME
			-- SEGMENT_QUERY
			'select distinct sd.id as svc_pt_id 
				from svc_pt_data_svc_rel spdsr,
					data_svc_class dsc,
					svc_pt sd 
				where spdsr.data_svc_class_id = dsc.id
					and sd.id = spdsr.svc_pt_id
					and dsc.type = ''VEE'' 
					and dsc.name = ''VEE Service, Residential'' ', 
			NULL, -- PROCEDURE_NAME
			'Y', -- ACTIVE_IND
			NULL, -- ORG_ID
			'RPS' -- segment_code


NOTE: While executing Procs the sdp_segement_cd query is appended with eff_start_time & eff_end_time b/w analysis end date



1. Revenue protection Job:
--------------------------

input param: processRunId, processStartTime, analysisStartDate, analysisEndDate, segmentCode, algorithmCodes

segmentCode: sdp_segment_config.segment_code eg. 'RPS'
algorithmCodes: rp_algorithm_def.eip_cd 

1.1 Validating Inputs
1.2 TaskBuilder queries to fetch list of AlgoTasks (taskValueObjects) using above segmentcode and algorithmCodes
	taskValueObjects is List of {
									segConfig.WID, 
									RP_ALGORITHM.MINUTE_BACKWARD, 
									RP_ALGORITHM.ORG_ID, 
									RP_ALGORITHM.WID, 
									RP_ALGORITHM_DEF.PROCEDURE_NAME
									}
		
		
		SELECT segConfig.WID as SEGMENT_WID, algo.MINUTE_BACKWARD, algo.ORG_ID, algo.WID as ALGORITHM_WID,algoDef.PROCEDURE_NAME  
		from RP_ALGORITHM algo,  
		RP_ALGORITHM_DEF algoDef, 
		SDP_SEGMENT_CONFIG segConfig 
		 where algo.ALGORITHM_DEF_WID = algoDef.WID  
		 and algo.SDP_SEGMENT_CONFIG_WID = segConfig.WID  
		 and segConfig.ACTIVE_IND = 'Y'  
		 and algo.ACTIVE_FLAG = 'Y'  		
		 and segConfig.SEGMENT_CODE IN (   getFlattened(segmentCodes)  ) 
 		 and algoDef.EIP_CD IN (   getFlattened(algoCodes)  );

1.3  Get total Number of Days for which Analysis needs to be done i.e start and end date gives no of days 
1.4  analysisDate = analysisStartDate (recevd Job input param)
1.4  Loop for No Of Days {
			
			1.4.1 Set analysisDate in executionContext- to be used by job
			1.4.2 Create JobList for each Algorithm list from 1.2 (setting all algos procedure name etc)
			1.4.3 Execute Job for analysisDate
			1.4.3.1 While executing Procs the sdp_segement_cd query is appended with dates b/w analysis start and end date
			1.4.4 analysisDate = addDaysToDate(1, analysisDate)
	 }
	 
1.5 Algos will populate data in RP_FEATURE table


-What hadoop ecosystem is used in transactional db
-some design flows
-Specific issues/donts 
-dev env for reference ...


Pip_consulting/stefina/hadoop-study-

spark ..
sqoop
hbase
hive- query purpose


hbase;
describe

spark can be tried locally




Algorithmns: All algorithms need parameters to run (RP_ALGORITHM_DEF_PARAM). All algorithm call stat proc in end z_rp_sp_load_execution_stat
------------				
			 Minutes_backward is used to calculate value for past '||p_minutes_backward/(24*60)||' days

		1. Estimated Intervals (EI): At least 50  of intervals are estimated in the past 30 days  - Z_RP_SP_ALGORITHM_EI
			- Fetch Algo params from rp_algorithm_param like THRESHOLD_PERCENT_INTERVALS & MEASUREMENT_WID
				THRESHOLD_PERCENT_INTERVALS: minimum percent of estimated intervals to be considered. Value checked in 50
				MEASUREMENT_WID: Unique Identifier of the channel measurement. 
			- Append segment_query with dates 
			- Create tmp table tmp_sdp_ei with data fetched from segment_query 
			- v_description := 'Estimated intervals should at least '||v_threshold_percent||'  of the total interval reads in the past '||p_minutes_backward/(24*60)||' days';
			- *** Create tmp table tmp_tot_int Using Join from tmp_sdp_ei & interval_f (get the total interval count for all the SDP's).
				  [TODO] replace use interval_f from Analytics DB with EIP 8 schema
			- *** Create tmp table tmp_est_int (get the estimated interval count for all the SDP's) . 
				  [TODO] replace use interval_f from Analytics DB with EIP 8 schema
			- *** May be we can replace WID and ID in procs with UDC_ID this will solve many issues
			- *** The sdp_id in analytics is part of interval_f table.
					In EIP 8 lp_interval is in mudrDB and table does not have sdp_id. Need to update sdp_segment_config table with 
						-Required sdp and their meter = sdp(sdp_seg_config query) + sdp-Meter Rel -> use meterId in lp_interval
						- OR SDP join with channel table = sdp(sdp_seg_config query) + channel (sdpid) -> use channelID in lp_interval
			- Get the SDP's that are estimated at or over THRESHOLD_PERCENT_INTERVALS during the past x days.
			  Insert into RP_FEATURE with SDP's that are estimated at or over THRESHOLD_PERCENT_INTERVALS
			- Drop temp table tmp_sdp_ei, tmp_tot_int, tmp_est_int


		2. USAGE WITH NO ACTIVE ACCOUNT (UWNAA): Number of intervals with greater than 0 usage when account is not active in the 
												 past 30 days. - Z_RP_SP_ALGORITHM_UWNAA
												 
			
			
			Algo_Params:
			THRESHOLD_USAGE:  Threshold value of usage that usage must be greater than or equal to be considered.
			THRESHOLD_SUM_INTERVAL_MINUTES: Threshold number of interval minutes when summed up that must be met or exceeded before the record is inserted into RP_FEATURE
			
												 


		3. Reverse Rotation (RR): Number of intervals with presence of   reverse rotation  events in the past 30 days. - Z_RP_SP_ALGORITHM_RR
		
						*** will need meas_type interval length

		4. XXX (uses deviceEvent in algo)- Night Time Bypass (NTB): Number of days in the past 30 days where the total usage at night  is less than 0.1 
									and standard deviation is less than 0.1 and during the day  is greater than 1 and 
									standard deviation is less than 0.1 - Z_RP_SP_ALGORITHM_NTB


						*** The proc uses following table accnt_sdp_r, device_event_f, device_event_type_d, interval_f
							-EIP 8 has sdp & accnt info in accnt_svc_pt_rel
							-MUDR 8 device_event has svc_pt_id column and device_event_type_id 
							-MUDR 8 lp_interval refer EI algo

		5. Opposite Usage Pattern Compared to Neighbors (OUPCTN): For the past 30 days, calculate the average usage per interval for each SDP 
																  and its surrounding SDPs ( say at ZIP+3 level). Then calculate the correlation 
																  between each SDP and its neighbors. The algorithm should flag those SDPs with 
																  correlation less than -0.75 and usage of at least - Z_RP_SP_ALGORITHM_OUPCTN
																  
						*** The proc uses sdp_d sd,premise_d pd,postal_code_d
							-sdp - channel - measType - premise - postalcd 
										  
																  k
		6. Tamper With Usage Drop to Zero in the Past 30 Days (TWUDZ): Tamper With Usage Drop to Zero in the Past 30 Days - Z_RP_SP_ALGORITHM_TWUDZ
						
		7. Load Side Voltage and Zero Usage Within a Week of Remote Disconnect (RDLSVZU): - Z_RP_SP_ALGORITHM_RDLSVZU

		8. Intervals With No Active Account (IWNAA): Minutes of intervals with greater than 0 usage when account is not active in the 
													 past 30 days - Z_RP_SP_ALGORITHM_IWNAA

						*** The proc has join with accnt_sdp_r in analytics DB (RP1.0). EIP 8 has sdp & accnt info in accnt_svc_pt_rel										 

		9. Estimated Usage(EU): At least 50  of usage are estimated in the past 30 days -  Z_RP_SP_ALGORITHM_EU

		10. SDP Low Voltage (SDPLV): At least 2 days of low voltage in the past 7 days -  Z_RP_SP_ALGORITHM_SDPLV
						*** The proc uses register_read_f
							- Using sdp - channel id we ccan get register read values
						
		11. No Usage For X Days (NUFXD) : SDPs which have active accounts but 0 usage for consecutive X days - Z_RP_SP_ALGORITHM_NUFXD
						*** The proc uses accnt_sdp_r, interval_f, measurement_d				
						-EIP 8 has sdp & accnt info in accnt_svc_pt_rel
						-MUDR 8 lp_interval refer EI algo
						-lp_interval has MEAS_TYPE_ID column OR we need to get it from Structural EIP



2. Analyze SDP score Job (invoke R)-----------7x job
-----------------------------------------------------
The job will invoke R machine learning language which internally creates list of SDPs with a confidence number. 
R package is used to utilize machine learning technics and bulid a predictive model so we can score indivual SDPs 
for potential power stealing activities

3. Create Investigation Request Job-----------7x job
-----------------------------------------------------
 The SDP score created during Analyze SDP score job acts as input for this job. Job creates entry in RP_INVESTIGATION_REQUEST 
 table in analytics DB. The job deployed over on 7.x deployment (preferably Netezza). 
 This job is event triggered on Analytics Score Job.
 
 Job creates tickets only when:
 -Sdp score is greater than the threshold score.
 -Sdp should not be exempt for the period analysis start time and analysis end time.
 -Entry should not be present in the RP_INVESTIGATION_REQUEST table for a sdp.

4. Migrate Investigation Ticket Job-----------8x job
-----------------------------------------------------
The Migrate investigation ticket Job creates investigation ticket for all records of RP_INVESTIGATION_REQUEST table 
which have status  New  those tickets can then be handled at Investigation Workflow at RP UI and after the successful 
migration will mark the status to Sent of every record. The job interacts with 8.x schema & 7.x

Ticket that already migrated in previous run should not be migrated again for this check the existence of request 
by looking model_output_wid value in service_request.ext_id column having external_system value em-ra-rp.

5. Sync Feedback Info Job - not required------8x job
-----------------------------------------------------
The Sync Feedback Info Job sync the result into the analytics DB for all the SR's that have status  Closed . 
The job select all the records of RP_FEEDBACK_INFO table and fetch the relevant information from SERVICE_REQUEST 
and create a entry in the Analytics RP_INVESTIGATION table and delete entry from the RP_INVESTIGATION_REQUEST 
and RP_FEEDBACK_INFO table 






Case Management system
----------------------











------------------------------------------------------------------------------------------------------------------------------
Transactional data load:

-Interval Data Load
-Register Read Data Load


EnergIP Analytics Platform jobs must be executed in the following order:
1. LoadSDP: Loads all the SDPs where type = ServiceDeliveryPoint, into svc_pt table in HBase. It also loads interval and register channels associated with the SDPs in channel_lookup table in HBase.
2. LoadLpIntervals: Loads interval reads into lp_intervals table in HBase.
3. LoadRegisterReads (These can be run at the same time as LoadLpIntervals, in any order): Loads register reads into register_read_f table in HBase.
4. ReprocessBadLpIntervals: Moves the Bad Interval Channel files into incoming directory to be reprocessed
5. ReprocessBadRegisterReads: Moves the Bad Register Channel files into incoming directory to be reprocessed.


https://wiki.emeter.com/display/ENG/Analytics+Cloud+-+ELM+HLD
https://wiki.emeter.com/display/ENG/Meter+Data+Load+into+HBase
https://docs.emeter.com/display/EIPDE/Transactional+Data+Extraction
https://docs.emeter.com/display/ALYTPLAT/ELM+Configuration+and+Deployment


https://docs.emeter.com/display/ALYTPLAT/Deploy+the+HBase+Schema+for+ELM
https://docs.emeter.com/display/ALYTPLAT/Deploy+the+Hive+Schema+for+ELM

https://docs.emeter.com/display/ALYTPLAT/Configuration+and+Deployment
https://docs.emeter.com/display/ALYTPLAT/Deploy+the+HBase+Schema
https://docs.emeter.com/display/ALYTPLAT/Preparing+Your+Environment+for+EnergyIP+Analytics+Platform



https://docs.emeter.com/display/ALYTPLAT/Configure+FlexSync+Extension+And+Loader


Hadoop: 
http://www.revelytix.com/ q=content/hadoop-ecosystem

apache spark prog guide:
https://spark.apache.org/docs/latest/programming-guide.html

-awsapp02 server or similar. The server has to be provisioned in CDH cluster as HBase/Hive/Yarn/HDFS Gateway.
 Verify that the server can see HDFS file system and can access HBase and Hive.




Tech stack 

-hadoop (Hadoop 2.3.0-cdh5.1.0)

-hdfs: 	Hadoop Distributed File System: HDFS, the storage layer of Hadoop, is a distributed, scalable, 
		Java-based file system adept at storing large volumes of unstructured data
		
-Mapreduce: MapReduce: MapReduce is a software framework that serves as the compute layer of Hadoop. 
			MapReduce jobs are divided into two (obviously named) parts. The 'Map' function divides 
			a query into multiple parts and processes data at the node level. The 'Reduce' function 
			aggregates the results of the 'Map' function to determine the 'answer' to the query.
			
-hbase (Version 0.98.1-cdh5.1.0): HBase: HBase is a non-relational database that allows for low-latency, quick lookups in Hadoop. 
		It adds transactional capabilities to Hadoop, allowing users to conduct updates, inserts and deletes. 
		EBay and Facebook use HBase heavily. HBase depends on Zookeeper and runs a Zookeeper instance by default.
		
-Sqoop: Sqoop ( SQL-to-Hadoop ) is a tool which transfers data in both directions between relational systems and 
		HDFS or other Hadoop data stores, e.g. Hive or HBase.

-hive: 	Hive is a Hadoop-based data warehousing-like framework originally developed by Facebook. It allows 
		users to write queries in a SQL-like language caled HiveQL, which are then converted to MapReduce. 
		This allows SQL programmers with no MapReduce experience to use the warehouse and makes it easier 
		to integrate with business intelligence and visualization tools such as Microstrategy, Tableau, 
		Revolutions Analytics, etc.

-oozie, Create Oozie Workflow folders on HDFS: Oozie: 	Oozie is a workflow processing system that lets users 
														define a series of jobs written in multiple languages 
														- such as Map Reduce, Pig and Hive -- then intelligently 
														link them to one another. Oozie allows users to specify, 
														for example, that a particular query is only to be initiated 
														after specified previous jobs on which it relies for data are completed.

-YARN(yarn version: Hadoop 2.3.0-cdh5.1.0): Yet Another Resource Negotiator. MapReduce 1.0 had issues with scalability, 
											memory usage, synchronization, and had its own SPOF issues. n response, 
											YARN (Yet Another Resource Negotiator) was begun as a subproject in the Apache Hadoop Project,
											on par with other subprojects like HDFS, MapReduce, and Hadoop Common.

-Zookeeper 

-Cloudera documentation: http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/introduction.html

	-Cloudera provides a scalable, flexible, integrated platform that makes it easy to manage Apache Hadoop and related projects
	 keep that data secure and protected.
	-Cloudera provides the following products and tools:
		-CDH - The Cloudera distribution of Apache Hadoop and other related open-source projects
		-Cloudera impala
		-Cloudera Search
	-Cloudera Manager: 	A sophisticated application used to deploy, manage, monitor, and diagnose issues with your 
						CDH deployments. Cloudera Manager provides the Admin Console, a web-based user interface that 
						makes administration of your enterprise data simple and straightforward. It also includes the 
						Cloudera Manager API, which you can use to obtain cluster health information and metrics
	-Cloudera Navigator: An end-to-end data management tool for the CDH platform. Cloudera Navigator enables administrators, 
						 data managers, and analysts to explore the large amounts of data in Hadoop	
	
	
	
	Clodera terminology:
			-deployment
			A configuration of Cloudera Manager and all the clusters it manages.
	
			-dynamic resource pool
			A named configuration of resources and a policy for scheduling the resources among YARN applications or Impala queries running in the pool.
	
			-cluster
			A logical entity that contains a set of hosts, a single version of CDH installed on the hosts, and the service and role instances running on the hosts. A host can belong to only one cluster.
	
			-host
			A physical or virtual machine that runs role instances.
	
			-rack
			A physical entity that contains a set of physical hosts typically served by the same switch.
	
			-service
			A category of managed functionality, which may be distributed or not, running in a cluster. Sometimes referred to as a service type. For example: MapReduce, HDFS, YARN, Spark, Accumulo. Whereas in traditional environments multiple services run on one host, in distributed systems a service runs on many hosts.
	
			-service instance
			An instance of a service running on a cluster. A service instance spans many role instances. For example:  HDFS-1  and  yarn .
	
			-role
			A category of functionality within a service. For example, the HDFS service has the following roles: NameNode, SecondaryNameNode, DataNode, and Balancer. Sometimes referred to as a role type.
	
			-role instance
			An instance of a role running on a host. It typically maps to a Unix process. For example:  NameNode-h1  and  DataNode-h1 .
	
			-role group
			A set of configuration properties for a set of role instances.
	
			-host template
			A set of role groups. When a template is applied to a host, a role instance from each role group is created and assigned to that host.
	
			-gateway
			A role that designates a host that should receive a client configuration for a service when the host does not have any role instances for that service running on it.
	
			-parcel
			A binary distribution format that contains compiled code and meta-information such as a package description, version, and dependencies.
	
			-static service pool
			A static partitioning of total cluster resources—CPU, memory, and I/O weight—across a set of services.
		
	
	
	
	-Hive and other frameworks built on MapReduce are best suited for long running batch jobs, such as those involving 
	 batch processing of Extract, Transform, and Load (ETL) type jobs.
	 
	-CDH:
		-CDH is Apache-licensed open source and is the only Hadoop solution to offer unified batch processing, 
			interactive SQL and interactive search, and role-based access controls.
		-
	
	-It is worth noting that stopping Cloudera Manager and the Cloudera Manager Agents will not bring down your cluster; any running instances will keep running.
	
	-Service instances started by Cloudera Manager do not read configurations from the default locations. 
		In contrast, the HDFS role instances (for example, NameNode and DataNode) obtain their configurations from a 
		private per-process directory, under /var/run/cloudera-scm-agent/process/unique-process-name. Giving each process 
		its own private execution and configuration environment allows Cloudera Manager to control each process independently
	
	
	JDK support: CDH 5.1.x	(1.6x)Not Supported	1.7.0_55	(1.8x)Not Supported
	
	Cloudera location in cloudera:
	/var/run/cloudera-scm-agent
	/etc/hadoop/conf
	/etc/hadoop/conf/hdfs-site.xml
	/etc/hbase/conf
	/etc/hive/conf
	/var/run/cloudera-scm-agent/process/unique-process-name
	/opt/cloudera/parcels/CDH/lib/ (With parcel software distribution, the path to the CDH libraries is /opt/cloudera/parcels/CDH/lib/)
	
	
	http://awsapp06.emeter.com:11000/oozie/

-QlikView Server (I guess it is for front end)
awsapp06.emeter.com

hbase commands:

hbase shell
 >list

o/p
<orgId>:channel_lookup
<orgId>:lp_interval_f
<orgId>:register_read_f
<orgId>:svc_pt



Install the CDH5.x cluster.
Our current Hadoop environment includes the following servers:
Hadoop Cluster Nodes
awsapp06.emeter.com          
awsapp07.emeter.com          
awsapp08.emeter.com          
awsapp04.emeter.com          
awsapp05.emeter.com          
 
Additional Servers
awsapp02.emeter.com         VM for Cloudera Manager (8GB RAM/4vCPUs)
emdb35.emeter.com           Dedicated Oracle DB server with 11gR2 database installed and running
lnxapp227.emeter.com        EIP 8.1.3 VM Server
qlikview-01.emeter.com      VM for qlikview



All the deployment will take place on awsapp02.
EM-UI --> https://awsapp02.emeter.com:8080/em-ui
SystemConsole --> https://awsapp02.emeter.com:9553/systemconsole


We use cloudera distributed hadoop .. CDH for transactional deployment

awsapp06.emeter.com
-------------------
hadoop_home:
-Dhadoop.home.dir=/opt/cloudera/parcels/CDH-5.1.0-1.cdh5.1.0.p0.53/lib/hadoop

namenode process:
hdfs      2822  1585  4 Feb02          21:33:40 /usr/java/jdk1.7.0_55-cloudera/bin/java -Dproc_namenode -Xmx1000m -Dhdfs.audit.logger=INFO,RFAAUDIT -Dsecurity.audit.logger=INFO
,RFAS -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/var/log/hadoop-hdfs -Dhadoop.log.file=hadoop-cmf-hdfs-NAMENODE-awsapp06.emeter.com.log.out -Dhadoop.home.dir=/opt/cloudera/parcels/CDH-5.1.0-1.cdh5.1.0.p0.53/lib/hadoop -Dhadoop.id.str=hdfs -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/opt/cloudera/parcels/CDH-5.1.0-1.cdh5.1.0.p0.53/lib/hadoop/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Xms1073741824 -Xmx1073741824 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:-CMSConcurrentMTEnabled -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled -XX:OnOutOfMemoryError=/usr/lib64/cmf/service/common/killparent.sh -Dhadoop.security.logger=INFO,RF
AS org.apache.hadoop.hdfs.server.namenode.NameNode

datanode process:
eip@awsapp06:/home/eip->ps -aef | grep -i datanode
hdfs      3040  1585  0 Feb02          01:24:39 /usr/java/jdk1.7.0_55-cloudera/bin/java -Dproc_datanode -Xmx1000m -Dhdfs.audit.logger=INFO,RFAAUDIT -Dsecurity.audit.logger=INFO
,RFAS -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/var/log/hadoop-hdfs -Dhadoop.log.file=hadoop-cmf-hdfs-DATANODE-awsapp06.emeter.com.log.out -Dhadoop.home.dir=/opt/cloudera/parcels/CDH-5.1.0-1.cdh5.1.0.p0.53/lib/hadoop -Dhadoop.id.str=hdfs -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/opt/cloudera/parcels/CDH-5.1.0-1.cdh5.1.0.p0.53/lib/hadoop/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -server -Xms559939584 -Xmx559939584 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:-CMSConcurrentMTEnabled -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled -XX:OnOutOfMemoryError=/usr/lib64/cmf/service/common/killparent.sh -Dhadoop.security.logger=I
NFO,RFAS org.apache.hadoop.hdfs.server.datanode.DataNode


wget http://archive.cloudera.com/cm5/installer/latest/cloudera-manager-installer.bin

chmodu+x cloudera-manager-installer.bin

./cloudera-manager-installer.bin


hadoop programming:

Design 
-Partitioning  and shuffling are common design patterns that go along with mapping and reducing

hadoop commands:
bin/hadoop
We get
Usage: hadoop [--config confdir] COMMAND
where COMMAND is one of:
  namenode -format     format the DFS filesystem
  secondarynamenode    run the DFS secondary namenode
  namenode             run the DFS namenode
  datanode             run a DFS datanode
  dfsadmin             run a DFS admin client
  fsck                 run a DFS filesystem checking utility
  fs                   run a generic filesystem user client
  balancer             run a cluster balancing utility
  jobtracker           run the MapReduce job Tracker node
  pipes                run a Pipes job
  tasktracker          run a MapReduce task Tracker node
  job                  manipulate MapReduce jobs
  version              print the version
  jar <jar>            run a jar file
  distcp <srcurl> <desturl> copy file or directories recursively
  archive -archiveName NAME <src>* <dest> create a hadoop archive
  daemonlog            get/set the log level for each daemon
 or
  CLASSNAME            run the class named CLASSNAME
Most commands print help when invoked w/o parameters.



The hadoop daemons   include
-Secondary NameNode 
-NameNode
-JobTracker

-DataNode
-TaskTracker


Yahoo developer network

-Hadoop configuration directory/conf/hadoop-defaults.xml: file contains default values for every parameter in Hadoop.
-Hadoop configuration directory/conf/hadoop-site.xml: You override configuration by setting new values in conf/hadoop-site.xml. 
													This file should be replicated consistently across all machines in the cluster
													
		Configuration settings are a set of key-value pairs of the format:
		  <property>
			<name>property-name</name>
			<value>property-value</value>

		  </property>
		Adding the line <final>true</final> inside the property body will prevent properties from being overridden 
		by user applications. This is useful for most system-wide configuration options.

		e.g.
		<configuration>
		  <property>
			<name>fs.default.name</name>
			<value>hdfs://your.server.name.com:9000</value>
		  </property>
		  <property>
			<name>dfs.data.dir</name>
			<value>/home/username/hdfs/data</value>
		  </property>
		  <property>
			<name>dfs.name.dir</name>
			<value>/home/username/hdfs/name</value>
		  </property>
		</configuration>

	fs.default.name - This is the URI (protocol specifier, hostname, and port) that describes the NameNode for the cluster. Each node in the system on which Hadoop is expected to operate needs to know the address of the NameNode. The DataNode instances will register with this NameNode, and make their data available through it. Individual client programs will connect to this address to retrieve the locations of actual file blocks.
	dfs.data.dir - This is the path on the local file system in which the DataNode instance should store its data. It is not necessary that all DataNode instances store their data under the same local path prefix, as they will all be on separate machines; it is acceptable that these machines are heterogeneous. However, it will simplify configuration if this directory is standardized throughout the system. By default, Hadoop will place this under /tmp. This is fine for testing purposes, but is an easy way to lose actual data in a production system, and thus must be overridden.
	dfs.name.dir - This is the path on the local file system of the NameNode instance where the NameNode metadata is stored. It is only used by the NameNode instance to find its information, and does not exist on the DataNodes. The caveat above about /tmp applies to this as well; this setting must be overridden in a production system.
	dfs.replication - Another configuration parameter, not listed above, is dfs.replication. This is the default replication factor for each block of data in the file system. For a production cluster, this should usually be left at its default value of 3.


-After copying above information into your conf/hadoop-site.xml file, copy this to the conf/ directories on all machines in the cluster.
-In the conf/ directory, edit the file slaves so that it contains a list of fully-qualified hostnames for the slave instances, one host per line. On a multi-node setup, the master node (e.g., localhost) is not usually present in this file.
-Then make the directories necessary:
  	user@EachMachine  mkdir -p  HOME/hdfs/data
  	user@namenode  mkdir -p  HOME/hdfs/name
  	
-The user who owns the Hadoop instances will need to have read and write access to each of these directories. 
-The bulk of commands that communicate with the cluster are performed by a monolithic script named bin/hadoop. This will load the Hadoop system with the Java virtual machine and execute a user command. 
	user@machine:hadoop  bin/hadoop moduleName -cmd args...
	The moduleName tells the program which subset of Hadoop functionality to use. -cmd is the name of a specific command within this module to execute.
	
	Two such modules are relevant to HDFS: dfs and dfsadmin
-commands:
	bin/start-dfs.sh
	bin/hadoop dfs -put ...
	bin/hadoop dfs -get ...
	bin/hadoop dfs -ls ...
	bin/hadoop dfs -mkdir ...
	
	bin/hadoop dfs -help
	bin/stop-dfs.sh
	
	
	bin/hadoop dfsadmin -report: This returns basic information about the overall health of the HDFS cluster
	bin/hadoop dfsadmin -metasave filename: will record this information in filename
	
-Upgrading HDFS versions: When upgrading from one version of Hadoop to the next, the file formats used by the NameNode and DataNodes may change. When you first start the new version of Hadoop on the cluster, you need to tell Hadoop to change the HDFS version
	bin/start-dfs.sh -upgrade: It will then begin upgrading the HDFS version
	bin/start-dfs.sh -rollback:  To back out the changes, stop the cluster, re-install the older version of Hadoop, and then use the command: bin/start-dfs.sh -rollback. It will restore the previous HDFS state.

-HDFS Permissions and Security
	HDFS security is based on the POSIX model of users and groups. Each file or directory has 3 permissions (read, write and execute) associated with it at three different granularities: the file's owner, users in the same group as the owner, and all other users in the system. As the HDFS does not provide the full POSIX spectrum of activity, some combinations of bits will be meaningless. For example, no file can be executed; the +x bits cannot be set on files (only directories). Nor can an existing file be written to, although the +w bits may still be set.
	bin/hadoop dfs -chmod, -chown, and -chgrp: Security permissions and ownership can be modified using the bin/hadoop dfs -chmod, -chown, and -chgrp

-HDFS Web Interface
	http://namenode:50070/  : show data similar to bin/hadoop dfsadmin -report
	address and port can be changed: dfs.http.address in conf/hadoop-site.xml
	

-Installation error:

[root@ind-lnxapp122 RGA]  vi /var/log/cloudera-manager-installer/4.install-cloudera-manager-server-db-2.log
Loaded plugins: product-id, refresh-packagekit, rhnplugin, security,
              : subscription-manager
This system is not registered to Red Hat Subscription Management. You can use subscription-manager to register.
This system is not registered with RHN Classic or RHN Satellite.
You can use rhn_register to register.
RHN Satellite or RHN Classic support will be disabled.
Setting up Install Process
Resolving Dependencies
--> Running transaction check
---> Package cloudera-manager-server-db-2.x86_64 0:5.3.2-1.cm532.p0.209.el6 will be installed
--> Processing Dependency: postgresql-server >= 8.4 for package: cloudera-manager-server-db-2-5.3.2-1.cm532.p0.209.el6.x86_64
--> Finished Dependency Resolution
Error: Package: cloudera-manager-server-db-2-5.3.2-1.cm532.p0.209.el6.x86_64 (cloudera-manager)
           Requires: postgresql-server >= 8.4
 You could try using --skip-broken to work around the problem
 You could try running: rpm -Va --nofiles --nodigest





Cloudera recommends setting /proc/sys/vm/swappiness to 0. Current setting is 60. Use the sysctl command to change this setting at runtime and edit /etc/sysctl.conf for this setting to be saved after a reboot. You may continue with installation, but you may run into issues with Cloudera Manager reporting that your hosts are unhealthy because they are swapping. The following hosts are affected: 
ind-lnxapp122.emeter.com


https://www.youtube.com/watch v=z0l87tArUDk



AAEINGN533070L.ww888.siemens.net:1521
ind-lnxapp122.emeter.com:7432
hive
hive
1ppTBL8h7M

create user hive identified by hive profile default account unlock;
grant connect, create session, resource to hive;

create user rman identified by rman profile default account unlock;
grant connect, create session, resource to rman;

--------------------------------hadoop-------------------------------
The core components of Hadoop
	Data storage: Hadoop Distributed File System (HDFS) 
	Data processing: MapReduce 

-How HDFS works
	Files are divided into blocks 
	Blocks are replicated across nodes

-FsShell: hadoop fs (probably fileSystem shell)
-Sub/commands: -get, -put, -ls, -cat, etc. 

-MapReduce is a method for distribuing a task across multiple nodes 


-Map task  intermediate data is stored on the local disk (not HDFS)

- Hadoop uses speculative,execution to mitigate against this 
-If a Mapper appears to be running signi cantly more slowly than the  others, a new instance of the Mapper will be started on another 
	machine, operaGng on the same data 
	-A new task)a<empt for the same task 
-The results of the first Mapper to finish will be used 
-Hadoop will kill off the Mapper which is still running 


-Write the Mapper and Reducer classes 
	Write a Driver class that con gures the job and submits it to the cluster 

		Driver classes are covered in the “WriGng MapReduce” chapter 

		Compile the Mapper, Reducer, and Driver classes 
			javac -classpath `hadoop classpath` MyMapper.java MyReducer.java MyDriver.java

		Create a jar  le with the Mapper, Reducer, and Driver classes 
		  jar cf MyMR.jar MyMapper.class MyReducer.class MyDriver.class 


		-Run the hadoop jar command to submit the job to the Hadoop cluster 
		  hadoop jar MyMR.jar MyDriver in_file out_dir 


-killing mapred job;
	  mapred job -list
	  mapred job -kill jobid
	

-Keys and values in Hadoop are Java Objects Not primiAves 
		Values are objects which implement Writable 
		Keys are objects which implement WritableComparable 

-Any optons not explicitly set in your driver code will be read from your  Hadoop configuration files 
	Usually located in /etc/hadoop/conf
	
-Any options not specified in your configuration files will use Hadoop’s default values	

-The default InputFormat (TextInputFormat) will be used unless you  specify otherwise 
	To use an InputFormat other than the default, use e.g. 
		job.setInputFormatClass(KeyValueTextInputFormat.class)

key -> WritableComparable
value -> Writable
Context is used to write intermediate data. It contains info about job's configuration
			
-ToolRunner allows the user to specify configuration options on the command line
	Commonly used to specify Hadoop properHes using the -D flag 
		-Will override any default or site properEes in the configuration 
		-But will not override those set in the driver code
		e.g. 
		  hadoop jar myjar.jar MyDriver -D mapred.reduce.tasks=10 myinputdir myoutputdir
		-Note that -D opHons must appear before any addiHonal program arguments
		-Can specify an XML configuration file with -conf
		-Can specify the default filesystem with -fs uri 
			Shortcut for  -D fs.default.name=uri
			
			
-The setup Method 
	The setup method is run before the map or reduce method is called for the first time
	It is common to want your Mapper or Reducer to execute some code before the map or reduce method is called for the first time 
		-Initialize data structures 
		-Read data from an external file 
		-Set parameters 
		
	public class MyMapper extends Mapper { 
	 
			 public void setup(Context context) { 
					   Configuration conf = context.getConfiguration(); 
					 int myParam = conf.getInt( paramname , 0); 
					 ... 
			 } 
				public void map...  
	} 
	
-The cleanup Method 
	The cleanup method is called before the Mapper or Reducer terminates 
		public void cleanup(Context context) throws IOException, InterruptedException

-Per mapper and reducer, setup and cleanup is executed once

-Enable Mapper Logging for the Job
	-Dmapred.map.child.log.level=DEBUG
	-  hadoop jar toolrunner.jar solution.AvgWordLength -Dmapred.map.child.log.level=DEBUG shakespeare outdir 
	
-JobTracker UI
	-http://localhost:50030/jobtracker.jsp

-The Combiner
	-Combiners run as part of the Map phase 
	-Output from the Combiners is passed to the Reducers 
	-The Combiner uses the same signature as the Reducer 
	-Some Reducers may be used as Combiners  
		-If operation is associative and commutative, e.g., SumReducer
	-Some Reducers cannot be used as a Combiner, e.g., AverageReducer 	
	-Specify the Combiner class to be used in your MapReduce code in the driver 	
		job.setMapperClass(WordMapper.class); 
		job.setReducerClass(SumReducer.class); 
		job.setCombinerClass(SumReducer.class); 
	-Input and output data types for the Combiner and the Reducer for a job must be idenHcal 
	- The Combiner may run once, or more than once, on the output from any given Mapper 
		Do not put code in the Combiner which could influence your results if it runs more than once 

-HDFS
	-Files cannot be modified once they have been written
	
	-FileSystem API
		-Configuration conf = new Configuration(); 
		 FileSystem fs = FileSystem.get(conf);
	
	-A file in HDFS is represented by a Path object 
		-Path p = new Path( /path/to/my/file ); 
		
	-Some useful API methods: 
		FSDataOutputStream create(...) 
		Extends java.io.DataOutputStream 
		Provides methods for wriEng primiEves, raw bytes etc 
		FSDataInputStream open(...) 
		Extends java.io.DataInputStream 
		Provides methods for reading primiEves, raw bytes etc 
		boolean delete(...) 
		boolean mkdirs(...) 
		void copyFromLocalFile(...) 
		void copyToLocalFile(...) 
		FileStatus[] listStatus(...) 	
	
	-e.g. Get Directory
		Path p = new Path( /my/path ); 
		 
		Configuration conf = new Configuration(); 
		FileSystem fs = FileSystem.get(conf); 
		FileStatus[] fileStats = fs.listStatus(p); 
		 
		for (int i = 0; i < fileStats.length; i++) { 
		    Path f = fileStats[i].getPath(); 
		 
		    // do something interesting 
		} 
		
	-e.g. Write data
		-Configuration conf = new Configuration(); 
		FileSystem fs = FileSystem.get(conf); 

		Path p = new Path( /my/path/foo ); 

		FSDataOutputStream out = fs.create(p, false); 

		// write some raw bytes 
		out.write(getBytes()); 

		// write an int 
		out.writeInt(getInt()); 

		... 

		out.close(); 

-The Distributed Cache
	-The Distributed Cache lets you copy local  les to worker nodes 
		-Mappers and Reducers can access directly as regular  les
	-The Distributed Cache provides an API to push data to all slave nodes 	
	-Transfer happens behind the scenes before any task is executed 
	-Data is only transferred once to each node, rather  
		Note: Distributed Cache is read/only 
	-Files in the Distributed Cache are automaEcally deleted from slave nodes when the job finishes
	-Using the Distributed Cache: The Di cult Way
		-Place the  les into HDFS 
		-Con gure the Distributed Cache in your driver code
				Configuration conf = new Configuration(); 
				DistributedCache.addCacheFile(new URI( /myapp/lookup.dat ),conf); 
				DistributedCache.addFileToClassPath(new Path( /myapp/mylib.jar ),conf); 
				DistributedCache.addCacheArchive(new URI( /myapp/map.zip ,conf)); 
				DistributedCache.addCacheArchive(new URI( /myapp/mytar.tar ,conf)); 
				DistributedCache.addCacheArchive(new URI( /myapp/mytgz.tgz ,conf)); 
				DistributedCache.addCacheArchive(new URI( /myapp/mytargz.tar.gz ,conf)); 	
	
	-Using the DistributedCache: The Easy Way
		-If you are using ToolRunner, you can add  les to the Distributed Cache directly from the command line when you run the job 
		-No need to copy the  les to HDFS  rst 
		-Use the -files opHon to add files
			-hadoop jar myjar.jar MyDriver -files file1, file2, file3, ... 
			-archives  ag adds archived files
			-libjars  ag adds jar files
			
	-Files added to the Distributed Cache are made available in your task’s local working directory 
		File f = new File( file_name_here ); 
	
-The FileSystem API lets you read and write HDFS  les programmatically

-There are many types of job where only a Mapper is needed 
		Examples: 
		-Image processing 
		-File format conversion 
		-Input data sampling 
		-ETL 
	-To create a Map only job, set the number of Reducers to 0 in your Driver code 
		job.setNumReduceTasks(0); 
	-Anything written using the Context.write method in the Mapper will be written to HDFS
	
-Performance key points
	-LocalJobRunner lets you test jobs on your local machine 
	-Hadoop uses the Log4J framework for logging 
	-Reusing objects is a best practice 
	-Counters provide a way of passing numeric data back to the driver 
	-Create Map only MapReduce jobs by sebng the number of Reducers to zero 
	-With a single Reducer, one task receives all keys in sorted order 
	-If a job needs to output a  le where all keys are listed in sorted order, a single Reducer must be used 

-All keys in MapReduce must be WritableComparable	
	
-If the MapReduce framework receives a non-splittable file (such as a GZipped file) it passes the entire file to a single Mapper 	
	-This can result in one Mapper running for far longer than the others
	-Typically it is not a good idea to use GZip to compress MapReduce input files 

-Snappy is a relaAvely new compression codec 
	-Developed at Google
	-Very fast 
	-Works well with SequenceFiles, Avro files
	-Snappy is now preferred over LZO
	
-TeraSort
	-terasort is one of the sample jobs provided with Hadoop 
		-Creates and sorts very large files



services.msc


----------------------------------------------------------HBASE----------------------------------------------------------
-Use HBase if
	-You need random write, random read, or both (but not neither) 
	-You need to do many thousands of operaAons per second on mulAple terabytes of data 
	-Your access pa>erns are well-known and simple 

-Don’t use HBase if
	-You only append to your dataset, and tend to read the whole thing when processing 
	-You primarily do ad/hoc analyAcs (ill-defined access patterns) 
	-Your data easily fits on one large node 
	
-Steps to designing an HBase schema 
	-Determine ways that data will be accessed 
	-Determine types of data to be stored 
	-Create data layouts and keys 	

-HBase schema design is ‘data-centric’ not ‘relationship-centric’

-HBase stores data in tables 

-Architecturally, HBase tables are sorted, distributed maps

-Table data is stored on the Hadoop Distributed File System (HDFS)

-HBase tables are split into Regions 

-Regions are served to clients by RegionServer daemons 
	-RegionServer runs on each slave node in the cluster
	-Very unlikely that one RegionServer will serve all the regions for a parCcular table

-HBase Master is a daemon which coordinates the RegionServers 
	-Coordinates which Regions are managed by each RegionServer 
	-Handles new table creation and other housekeeping operations
	
-An HBase cluster can have multiple Masters for high availability 
	-Only one Master controls the cluster 
	-The ZooKeeper service handles coordination of the Masters 

-ZooKeeper daemons run on master nodes on the cluster
	-Upon startup all Masters connect to ZooKeeper 
	-They compete to run the cluster 
	-The  rst Master to connect wins control 
	-If the controlling Master fails, the remaining Masters will compete to run the cluster again 



-A column consists of a column family prefix + qualifier

-Separate column families are useful for 
	-Data that is not frequently accessed together 
	-Data that uses di erent column family opCons   compression 

-Data in HBase tables is stored as byte arrays 
	-Anything that can be converted to an array of bytes can be stored 
	-Strings, numbers, complex objects, images, etc.

-An HBase table is a distributed sorted map 

-An HBase table is a distributed sorted map 
	-Row key + column key + timestamp --> value 
	-Row key and value are just bytes 
	
Row key       Column key  		 Timestamp  		Cell value  }--------sorted by key and column
jdupont       contacCnfo:fname   1273746289103      Jean 		}

-GET PUT SCAN DELETE
-Increment allows atomic counters

-By default, HBase keeps three versions of each cell

-The versions are sorted by their timestamp (in descending order) 

-HBase shell uses JRuby
	- Makes parameter usage a little different than most shells 
		-Command parameters are single quoted (') 
		hbase> command 'parameter1', 'parameter2' 

- to start hbase master:
	  sudo service hbase-master start

- API calls like Get, Delete and Put support batching i.e.  Batch all Get in single API call: To get multiple rows

- Getting Previous Versions in HBase
	get.setMaxVersions(3)

- Oracle transaction: ensure you provide ojdbc6.jar to hbase/lib

-Get
	get 'tablename', 'rowkey'
 	get 'table1', 'row1', {COLUMN => 'fam1:col1', VERSIONS => 2}  
	get 'table1', 'row1', { COLUMN => 'fam1.*'} 

-Put
 	put 'svc_pt_premise', '551', 'sdpdetails:PREMISE_UDC_ID', 'RGA-Test-PrmsUdcId'
 	
 	
 	put 'svc_pt_premise', '551', 'cf1:PREMISE_UDC_ID', 'cf2:RGA-Test-PrmsUdcId'
 
 http://ind-lnxapp122.emeter.com:7180/cmf/home
 
-Delete
	Rows are deleted by row key 
	Deletes can be done on row’s column family or column descriptors 
		All other data will stay intact
	Batching is supported
			
-Some important ones for HBase are: 
		-HBase Master http://<master_address>:60010 
		-RegionServer http://<regionserver_address>:60030 
		-NameNode http://<namenode_address>:50070 
		
-Recall: HBase does not disDnguish an insert from an update 
		-Insert a row with Put and a row key that does not exist yet 
		-Update a row with Put on an existing row 		

-Updates can occur on speci c column descriptors, leaving the row’s other columns unchanged


-Put: The same table object can be used to insert multiple rows
	-Batching allows mulDple Puts to be performed in a single call

- A Scan is used when: 
		-The exact row key is not known 
		-A group of rows needs to be accessed 

-Scans can be bounded by a start and stop row key 
		-The start row key is included in the results 
		-The stop row is not included in the results and the Scan will exhaust its data upon hitting the stop row key 		

-Scans can be limited to certain column families or column descriptors

-A scan without a start and stop row will scan the enDre table

-Scan results can be retrieved in batches to improve performance 
	-Performance will improve but memory usage will increase

-Atomic Puts and Deletes 
	-boolean success = table.checkAndPut(rowkey, FAMILY_BYTES,  COLUMN_BYTES, valuebytes, p); 
	-boolean success = userTable.checkAndDelete(rowkey,  FAMILY_BYTES, COLUMN_BYTES, valuebytes, d);

-Coprocessors
	-HBase has a feature similar to stored procedures in a tradiDonal RDBMS called Coprocessors
		-EndPoints: like strored proc
		-Observers: like triggers
		
-Debugging and supporting issues as a result of Coprocessors is extremely difficult		

-Scans can be augmented with Filters
	-Filters allow logic to be run on RegionServers before the data is returned 
	-RegionServers run the logic on the rows and only send what passes the logic 
	-Causes less data to be sent over the wire 

-Filters
	-ByteArrayComparable: 
	-CompareOp: are: EQUAL, GREATER, GREATER_OR_EQUAL, LESS,LESS_OR_EQUAL, NOT_EQUAL
	
	
-Block Cache is a read cache; Memstore is a write buffer
	- When$the$Block$Cache$is$full,$the$least$recently$used$block$will$get$evicted$or$freed$
	-
	
	
3- implement 3
		
		
-------------------------------------------SPARK-------------------------------------------------------------------------
-Hadoop job starts in new JVM but Spark job starts in existing JVM, hence faster
-spark does not store intermediate result to disk hence faster
-spark in memory RDD can be transformed/manipulated in memory hence fast
-RDDs are immutable

-------------------------------------------------------------------------------------------------------------------------






Developer training - ~437
Hbase training - 
spark training - 
Introduction to Yarn hadoop 2 and Map red available on cloudera

Bigdata (cloudera) developer path :
http://www.cloudera.com/content/cloudera/en/training/roles/developers.html
-Developer training
-Designing and building Big data application
-spark
-hbase
-Introduction to data science


QQQ
@Stefina

-How security is managed in hadoop i.e which user is allowed to execute map red job  
-Need some overview in testing and debugging the applications i.e. How to debug on cluster
	-How to ensure things working on dev single node setup will also work in cluster env.
	-How to deal with performance issue on cluster
	-Dos and Donts will be helpful
	
	-Any write up / guide on above would be helpful
	
-spark is supported on Java 8... but CDH distribution uses JDK 1.6...Is is that different component uses own java 
-hadoop ecosystem supports map red and not spark so how do we manage that .. will it not hit the performance
-how in spark we load data from hbase 
-there are limitation in spark less tested , some bugs ... some pieces missing
-spark has machine learning mlib, shipped with spark ... does it replace R
-What actually we need to study in detail spark or Hbase


@Ling
-RP1.0 uses tables created in netezza / Oracle, || We assume that RP2.0 will use RP tables created in HBASE, 
-RP1.0 1st job evaluates SDP based on theft alorithms. || RP2.0, are we expecting some more algorthms
-RP1.0 evaluates an algorithm 'Night Time Bypass (NTB)' which uses device events. || RP2.0 the cloud transactional load does not load device events, so how to go about it.
-RP1.0 uses R machine learning to calculate the score which has interaction with netezza DB || RP2.0, are we intended to use hadoop / spark provided machine learing version e.g mlib or any other library. If yes then who will work on this.
-RP1.0 case management system i.e. UI uses extjs for display || RP2.0, are we going to use extjs / qlikView
-Need to have overview on what changes are we expecting in RP2.0 


---------------------------------------------------------------------



Spark:

Hadoop is 3 apache proj
	-HDFS
	-Yarn
	-MapReduce (spark core is a contender of replacing it)

JavaRDD<String> inputRDD = sc.textFile("Log.txt");
JavaRDD<String> errorsRDD = inputRDD.filter(
  new Function<String, Boolean>() {
    public Boolean call(String x) { return x.contains("error"); }
  }
});

-An RDD in Spark is simply an immutable distributed collection of objects. Each RDD is split into multiple partitions, which may be computed on different nodes of the cluster
-Transformation and action: If you are ever confused whether a given function is a transformation or an action, you can look at its return type: transformations return RDDs, whereas actions return some other data type.
-cache() is the same as calling persist() with the default storage level
-parallelize() The simplest way to create RDDs is to take an existing collection in your program and pass it to SparkContext’s parallelize() method 
	JavaRDD<String> lines = sc.parallelize(Arrays.asList("pandas", "i like pandas"));
-Spark keeps track of the set of dependencies between different RDDs, called the lineage graph.

-Although transformations are lazy, you can force Spark to execute them at any time by running an action, such as count(). This is an easy way to test out just part of your program.
-Both anonymous inner classes and lambda expressions can reference any final variables in the method enclosing them, so you can pass these variables to Spark just as in Python and Scala.
-Note that distinct() is expensive
-The performance of intersection() is much worse than union()
-Cartesian product is very expensive for large RDDs.
-The persist() call on its own doesn’t force evaluation.
-RDDs come with a method called unpersist() that lets you manually remove them from the cache.



operations: Basic RDD transformations on an RDD containing {1, 2, 3, 3}


1-RDD transformations 

map() - Apply a function to each element in the RDD and return an RDD of the result.
flatmap() - Apply a function to each element in the RDD and return an RDD of the contents of the iterators returned. Often used to extract words
			-  as “flattening” the iterators returned to it, so that instead of ending up with an RDD of lists we have an RDD of the elements in those lists.
filter() - 
distinct()
sample(withReplacement, fraction, [seed]) - 

2-RDD transformations 

subtract()
union()
intersection()
cartesian()

RDD actions
collect() - Return all elements from the RDD.
count() - Number of elements in the RDD.
countByValue() - Number of times each element occurs in the RDD
take(num) - Return num elements from the RDD.
top(num) - Return the top num elements the RDD.
takeOrdered(num)(ordering) - Return num elements based on provided ordering.
takeSample(withReplacement, num, [seed]) - Return num elements at random.
reduce(func) - Combine the elements of the RDD together in parallel (e.g., sum).
fold(zero)(func) - Same as reduce() but with the provided zero value
aggregate(zeroValue)(seqOp, combOp) - Similar to reduce() but used to return a different type.
foreach(func) - Apply the provided function to each element of the RDD.


fold()

-require that the return type of our result be the same type as that of the elements in the RDD we are operating over



spark Tutorial : https://www.youtube.com/watch?v=7ooZ4S7Ay6Y
--------------

Spark:
	scheduling 
	monitring 
	distributing

Spark universe
	
1) spark core (at centre)

2) spark library: push computation to core
	SQL - hive queries work automatically
	Streaming: flume and kafka buffer straming
	Mlib
	GraphX
	BlinkDB: query result in x sec with y error rate..
	TackYon: data distribution 

3) Resource managers
	Node	
	yarn
	Mesos
	spark standalone node

4) File systems
	HDFS
	HBASE
	MongoDB
	...

Oozie shoots MR jobs in order in a cluster


Data read speed
1) RAM 10GB/s
2) HDD 100MB/s
3) External 
4) N/w slow

White paper : 
	-spark 
	-RDD
	-streaming
	-sparkSql
	-blinkDB
	-graphX


	RDD
----------
-Partitions: More partitions leads to more parallelism. 
-For each partition we need a task/thread to complete operations on that RDD
-RDD can be created:
	-parallize a collection 
		e.g.: sc.parallelize(Arrays.asList("fish","cat","dogs"));
		Not used outside testing or prototyping
	-read from file, cluster etc
		e.g. sc.textFile("xxx.txt")
	-Types:
		HadoopRDD
		FilteredRDD
		JdbcRDD
		ShuffledRDD
		
		....
		....

-RDD interface has:
	-Partitions
	-dependencies
	-
-Base RDD (with some partitions)- transforms to new RDD (it gets same no of partitions) -> Action like collect() [give me data back at driver] 
-All tranformations in RDD are lazy. So it keep on creating meta data that this 
	RDD depends on this etc. But no read happen.
-Dont call collect action on RDD of TB size, out of memory 
-Which RDD to cache: intermediate RDD used many times, should be cached (cleaned RDD shld be cached)
-caching a RDD is also lazy. Cachced in JVM (cached RDD are visible in spark UI)
-when RDD is cached, where its gets cached ?
-LifeCycle of spark program
	-create RDD(parallelize / external data) in driver program
	-Lazy transform them to new RDDs using transformations like filter() or map()
	-Cache any intermediate RDDs that can be reused
	-Launch actions like count() and collect() to kick of parallel computation. Optimized and executed by spark

-Transformations(lazy): it work on every item of RDD e.g. map() howeever some transformations work on per partition base e.g. 
	open DB connection save and then close DB connection. 
-Actions: 
-download spark scala source code to understand transormations and actions in details. API comments explains better


.count() -> no f items in RDD
.collect() -> collect all items from RDDs and bring back to driver



Fast:
1) keeps intermediate data in memory unlike Hadoop MR it does not write to HDFS 10-100x speed
2) MapRed has hardcoded map and reduce slots so CPU usage is not 100%. In spark has generic slots that can be used either by map/reduce
3) Empty slots for map or reduce are not filled aggressively in Map-reduce (Face book noticed that and used corona to 
   agressively start next map/reduce job). In spark it does
4) parallelism in MapRed - is by process Id on a node (???). Slots in MapRed is called processId
   parallelism in spark - is by threads on a executor. So better. Spark calls them cores. So eg. 6 cores are started in a Executor 



Spark architecture:

1) Local mode	shell starts JVM runs and start
		executor and
		driver process
		
	Jvm has slots called cores	eg. 6 cores i.e. 6 threads can run simultenously 	
2) Standalone mode: Driver tells the master that need some worker JVM to run my tasks
3) Yarn
4) 


spark mailing lists	 - q are answered quickly
	

Questions
---------
?coalesce : to unite so as to form one mass
?

----------------------------------------------------------------------

JDK 8 

Lambda expressions:
http://viralpatel.net/blogs/lambda-expressions-java-tutorial/
-The CheckPerson interface is a functional interface. A functional interface is any interface that contains only one abstract method. (A functional interface may contain one or more default methods or static methods.)
-The interface Predicate<T> is an example of a generic interface

***************************************************************************************************************************************************
 
Weather 
DEV DB
database: eip_ra_nz_06
user: eip_ra_nz_06_usr
Tables: weather_data_f, weather_data_s, weather_data_s_bad;


RP
QA DB
database: eip_ra_nz_284
user: eip_ra_nz_284_usr 



DEV DB: 
database: eip_ra_nz_30
user: eip_ra_nz_30_usr


Tables: RP_*
Views: RP_MODEL_INPUT_V
 
 
sudo -u hdfs hadoop jar \ /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar \ pi 10 100

scp -r eip@awsapp06.emeter.com:/opt/cloudera/parcels/CDH-5.1.0-1.cdh5.1.0.p0.53/* CDH-5.1.0-1.cdh5.1.0.p0.53/



-- 
understanding hadoop.
planning to install and create single noode setup for transaction load .... using cloudera



hdfs hadoop jar /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar pi 10 100





org create 
----------
./orgAdmin.sh -o CLOUDORG -u CLOUDUSER -f CLOUDUSER -l CLOUDUSER -p CLOUDUSER -c create
ref data creation 
-----------------
-mkdir /home/eip/resources/org/CLOUDORG
-cp -r /home/eip/resources/org/Sample/* to /home/eip/resources/org/CLOUDORG/
-./orgAdmin.sh -c importRefData -o CLOUDORG


cloudera-Quickstart-vm-4.4.0-1-vmware








1/4 intrvw 

2345


1-SLKT,R1



8x 
Structural Extractor: will have to code. Almost similar to 7x just underlying queries will change.
Will work on minor details this week

Structural Loader: Same


Transactional extractor: Will be almost same in functionality with structural extrator:
	-Org based
	-incremental / initial 
	-zip file
Transactional loader: We are thinking of reusing loader .... 
-Need to knw - is this loader also takes zip as input file..



Started looking in 8x transactional loader



MUDR100	emdb32.emeter.com:1521/oltp05		
MUDR196	EMDB25.EMETER.COM:1521/PPERF01		
MUDR263	emdb36.emeter.com:1521/oltp08		
MUDR302	emdb36.emeter.com:1521/oltp06		
MUDR304	emdb36.emeter.com:1521/oltp06		
MUDR67	ind-db03.emeter.com:1521/ieip_01


-********************************************

LP_INTERVALS

-----7x-------

CHANNEL_ID
UTILITY_ID
INTERVAL_START_TIME
INTERVAL_END_TIME
INSERT_TIME
INTERVAL_LENGTH
INTERVAL_VALUE
VALIDATION_STATUS
CHANGE_METHOD
VALIDATION_FAIL_CODES
INTERVAL_FLAGS
LOCKED
EXT_VERSION_TIME


----8x-----

CHANNEL_ID
INTERVAL_END_TIME
LP_VALUE
VALIDATION_STATUS
CHANGE_METHOD
FAIL_CODE
LOCKED
ESTIMATED
EXT_VERSION_TIME
MEAS_TYPE_ID
FLAGS
INTERVAL_LEN
DEVICE_ID
DEVICE_DATA_SRC_ID
AMI_RECORD_NUM
ORG_ID
LAST_UPD_BY
LAST_UPD_TIME
REC_VERSION_NUM

MeasType ID ???

*************************************************


RR

-------7x-------
CHANNEL_ID
READ_TIME
INSERT_TIME
DC_TIME
CUM_READ
DEMAND_DAILY
DEMAND_DAILY_TIME
DEMAND_BC
DEMAND_BC_TIME
TOU1_CUM_READ
TOU1_DEMAND_DAILY
TOU1_DEMAND_DAILY_TIME
TOU1_DEMAND_BC
TOU1_DEMAND_BC_TIME
TOU2_CUM_READ
TOU2_DEMAND_DAILY
TOU2_DEMAND_DAILY_TIME
TOU2_DEMAND_BC
TOU2_DEMAND_BC_TIME
TOU3_CUM_READ
TOU3_DEMAND_DAILY
TOU3_DEMAND_DAILY_TIME
TOU3_DEMAND_BC
TOU3_DEMAND_BC_TIME
TOU4_CUM_READ
TOU4_DEMAND_DAILY
TOU4_DEMAND_DAILY_TIME
TOU4_DEMAND_BC
TOU4_DEMAND_BC_TIME
TOU5_CUM_READ
TOU5_DEMAND_DAILY
TOU5_DEMAND_DAILY_TIME
TOU5_DEMAND_BC
TOU5_DEMAND_BC_TIME
POWER_OUTAGE_COUNT
SOURCE
SOURCE_DETAIL
VALIDATION_STATUS
CHANGE_METHOD
VALIDATION_FAIL_CODES
UOM_FAIL_CODES
REGISTER_FLAGS
LOCKED
EXT_VERSION_TIME
UTILITY_ID
COMMENTS


----------8x-------

CHANNEL_ID
MEAS_TYPE_ID
READ_START_TIME
READ_TIME
READ_VALUE
ORIG_READ_VALUE
DEMAND_PEAK_TIME
DEVICE_ID
FLAGS
EST_METHOD
ESTIMATED
VAL_STATUS
VAL_FAIL_CODE
EXT_VERSION_TIME
DEVICE_DATA_SRC_ID
READ_REASON
LOCKED
AMI_RECORD_NUM
ORG_ID
LAST_UPD_BY
LAST_UPD_TIME
REC_VERSION_NUM





-------------------------------------
220201017167

ELM3.1 refactoring project
**************************

Questions:
-MemStore
-Timeseries data
-


ELM 3.0 - 3 main processes:
-Power Factor Calculation Process
-KVA Calculation/Aggregation Process.
-Derived Rating Calculation Process



IntervalKvaCalculation : Kva Calculation & Aggregation Process
-Commercial & Residential
-Data goes to (Res + Comm) HIVE ELM_INTERVAL_KVA_F  --> Hive querry to ELM_INTERVAL_SUM_DN
-SQL query to prepare input file using Sqoop. 
-The output file will be put in HDFS for the MR job to take as input.
-IntervalKvaCalculationJob	
	-IntervalKVACalcMapper
		-IntervalKvaCalculator
		-IntervalKvaValidator
			-Valid KWH validation
			-Valid PF validation
		-SummaryByDistNodeJob: The process will be aggregating KVA per dist node based on dist node hierarchy


CommercialKVACalculator:

ResidentialKVACalculator :



 Channel type		subtype		uom_cd			
-Cumulative 		Cummulative
 Consumption
 
-channel_lookup for 7x  
-RandomForestTree / RandomForestTree
-SparkML
 

Some techniques, often called ensemble methods, construct more than one decision tree:

A Random Forest classifier uses a number of decision trees, in order to improve the classification rate

RF algo works on large collection of de corelated decision trees 

https://wiki.emeter.com/display/ENG/%28Draft%29+Cloud+Analytics+2.0+Transactional+data+extraction+and+data+load-+8x
https://wiki.emeter.com/display/ENG/Analytics+Cloud+-+Revenue+Protection+HLD


Feature creation - scaling
Scoring - Spark ML

tables


mudr - sdp centric
scoring - table should be diff in HBASE







7x RP tables
********Meta data Table***********
RP_ALGORITHM_DEF;
RP_ALGORITHM_DEF_PARAM;
RP_ALGORITHM;
RP_ALGORITHM_PARAM;
**********************************
RP_FEATURE -> The algoritm puts scoring data in this table
RP_MODEL_OUTPUT; -> R script puts data in this table
RP_INVESTIGATION_REQUEST;
RP_INVESTIGATION;

(HOST=ind-db08.emeter.com)(PORT=1521))(CONNECT_DATA=(SERVICE_NAME=poltp01)
eip111
mudr111/mudr111

ind-db08.emeter.com
eip53/eip53



Eclipse sandbox-revenueProtection-2.0

D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/core/sandbox/RaviGupta/em-revenueprotection2.0/trunk/input -SpropFile=file:/D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/core/sandbox/RaviGupta/em-revenueprotection2.0/trunk/test/conf/env.properties 1


-Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Dsun.io.serialization.extendedDebugInfo=true

--universal ami input adapter



No of partitions in spark : lower bound is at least 2x times number of cores
https://www.youtube.com/watch?v=dmL0N3qfSc8



Use the copyTable command. Example :
http://stackoverflow.com/questions/18942895/copy-data-from-one-hbase-table-to-another


$ bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable --new.name=logdata hbasetest


hbase org.apache.hadoop.hbase.mapreduce.CopyTable --new.name='ravigu:sdp' 'aparsh:sdp'
hbase org.apache.hadoop.hbase.mapreduce.CopyTable --new.name='ravigu:channellookup' 'aparsh:channellookup'


hbase org.apache.hadoop.hbase.mapreduce.CopyTable --new.name='devorg1:sdp2' 'devorg1:sdp'

http://awsapp02.emeter.com:8080/em-ui/admin.html#admin/Channel/1644914000001


    <!--<dependencies>
        <dependency org="com.emeter" name="em-analytics-core" rev="${analytics.artifact.version}" />
    </dependencies>-->


1) Design document for Feature creation job (1)
2) Junit for feature creation job (2)
3) code base setup integrated with basic Spark job (2 day)
4) Implementation of one Algorithm with pattern recognition library (4)
5) Design review (.5)
6) Implementation of remaining Algorithms with pattern recognition library (10)









1341119700000
1341120600000
1341121500000
1341122400000
1341123300000



4/6
* Using JUnit test cases, updated IntKva and PF job code to fetch and store data in new model
* IntKva store data against SdpId and endDate


- Need to add for additional support of meas (as a list of name/value pair of flexible tags, use short names i.e. pf:2, pf:3…. As comments)


How do i set java heap size for Secondary Name Node
https://community.cloudera.com/t5/Cloudera-Manager-Installation/How-do-i-set-java-heap-size-for-Secondary-Name-Node/td-p/4077

Memory Overcommit Validation Threshold


http://stackoverflow.com/questions/25909132/how-to-import-export-hbase-data-via-hdfs-hadoop-commands



disable 'ravigu:sdp'
drop 'ravigu:sdp'
create 'ravigu:sdp','s','m'
scan 'ravigu:sdp'


http://stackoverflow.com/questions/25909132/how-to-import-export-hbase-data-via-hdfs-hadoop-commands



URLs

NameNode:http://ind-lnxapp122.emeter.com:50070/

Cloudera location in cloudera:
	/var/run/cloudera-scm-agent
	/etc/hadoop/conf
	/etc/hadoop/conf/hdfs-site.xml
	/etc/hbase/conf
	/etc/hive/conf
	/var/run/cloudera-scm-agent/process/unique-process-name
	/opt/cloudera/parcels/CDH/lib/ (With parcel software distribution, the path to the CDH libraries is /opt/cloudera/parcels/CDH/lib/)

Logs:

/var/log/cloudera-scm-firehose/mgmt-cmf-mgmt-SERVICEMONITOR-ind-lnxapp122.emeter.com.log.out

================================================================================================================================================

HBASE
-----

* You  need  to  run  HBase  on  HDFS  to  ensure  all  writes  are  preserved. The  HDFS  local  filesystem
  implementation will lose edits if files are not properly closed. This is very likely to happen when you
  are experimenting with new software, starting and stopping the daemons often and not always cleanly
  
* Loopback issue: Prior to HBase 0.94.x, HBase expected the loopback IP address to be 127.0.0.1
	 Example /etc/hosts File for Ubuntu ( Ubuntu and some other distributions default to 127.0.1.1)
	 127.0.0.1 localhost
 	 127.0.0.1 ubuntu.ubuntu-domain ubuntu

* For hadoop 2, download file mostly like -- hbase-0.98.3-hadoop2-bin.tar.gz.
	$ tar xzvf hbase-<?eval ${project.version}?>-hadoop2-bin.tar.gz
	$ cd hbase-<?eval ${project.version}?>-hadoop2/

*  0.98.5  and  later,  you  are  required  to  set  the  JAVA_HOME  environment  variable  before starting HBase.

* Hbase command

		- create table 
			create table 'test', 'cf'

		- List table

		- Put data:
			put 'test', 'row1', 'cf:a', 'value1'
			- Columns in HBase are comprised of a column family prefix, cf in this example, followed by a colon and then a column qualifier suffix, a in this case	

		- Scan: get all or filter data 
			scan 'test'

		- Get data: Get a single row of data.
			get 'test', 'row1'

		- Disable table:delete a table or change its settings, you need to disable the table first, using the disable command. 
						You can re-enable it using the enable command	.
			disable 'test'
			enable 'test'

* Hbase modes

	- Standalone mode: This is the default mode. Standalone mode is what is described in the quickstart section. In standalone
						mode, HBase does not use HDFS?—?it uses the local filesystem instead?—?and it runs all HBase daemons
						and a local ZooKeeper all up in the same JVM. Zookeeper binds to a well known port so clients may talk
						to HBase.

	- Distributed
		- Pseudo-distributed mode:  all  daemons  run  on  a  single  node. Means  that  HBase  still  runs  
									completely  on  a  single  host,  but  each  HBase daemon 
									(HMaster, HRegionServer, and Zookeeper) runs as a separate process.
									Pseudo-distributed mode can run against the local filesystem or it can run against an instance of the
									Hadoop Distributed File System (HDFS).
		- Distributed: 	All daemon threads are spread across cluster. Fully-distributed mode can ONLY run on HDFS
					

* The HMaster server controls the HBase cluster:To  start  a  backup  HMaster,  use  the  local-master-backup.sh.
												For each backup master you want to start, add a parameter representing the port offset
												for that master							
	start:$ ./bin/local-master-backup.sh 2 3 5
	kill: find process Id from  /tmp/hbase-USER-X-master.pid and kill it
		 $ cat /tmp/hbase-testuser-1-master.pid |xargs kill -9 

* Each node of your cluster needs to have the same configuration information.

* ZooKeeper  starts  first,  followed  by  the  master,  then  the  RegionServers,  and  finally  the  backup masters

* Conf files: All  configuration  files  are located in the conf/directory, which needs to be kept in sync for each node on your cluster.
	-backup-masters (txt file, not by default)
	-hadoop-metrics2-hbase.properties
	-hbase-env.cmd and hbase-env.sh
	-hbase-policy.xml (only used if security is enabled)
	-hbase-site.xml
	-log4j.properties
	-regionservers (A plain-text file containing a list of hosts which should run a RegionServer in your HBase cluster)

* ssh
		HBase  uses  the  Secure  Shell  (ssh)  command  and  utilities  extensively  to  communicate  between
		cluster  nodes.  Each  server  in  the  cluster  must  be  running  ssh  so  that  the  Hadoop  and  HBase
		daemons  can  be  managed.

* DNS: The  hadoop-dns-checker  tool  can  be  used  to verify DNS is working correctly on the cluster

* Loopback IP

* NTP

* Limits on Number of Files and Processes (ulimit):
	Apache  HBase  is  a  database.  It  requires  the  ability  to  open  a  large  number  of  files  at  once.  Many 
	Linux distributions limit the number of files a single user is allowed to open to 1024 (or 256 on older versions of OS X).
	$ ulimit -n
	Recommendation: ulimit to at least 10,000, but more likely 10,240
	The following is a rough formula for calculating the potential number of open files on a RegionServer.
		Calculate the Potential Number of Open Files
		(StoreFiles per ColumnFamily) x (regions per RegionServer)
	
	
* Hbase files
	
	- conf/hbase-env.sh (set JAVA_HOME to executable java)
	- conf/hbase-site.xml 
		(- By default, a new  directory  is  created  under  /tmp.  Many  servers  are  configured  to  delete  the  contents  of  /tmp
	       upon reboot, so you should store the data elsewhere.
	       <configuration>
				  <property>
					<name>hbase.rootdir</name>
					<value>file:///home/testuser/hbase</value>
				  </property>
				  <property>
					<name>hbase.zookeeper.property.dataDir</name>
					<value>/home/testuser/zookeeper</value>
				  </property>
			</configuration>
			
			<property>
			  <name>hbase.zookeeper.quorum</name>
			  <value>node-a.example.com,node-b.example.com,node-c.example.com</value>
			</property>
			<property>
			  <name>hbase.zookeeper.property.dataDir</name>
			  <value>/usr/local/zookeeper</value>
			</property>
			
			<property>
			  <name>dfs.support.append</name>
			  <value>true</value>
			</property>
			
			Sync has to be explicitly enabled by setting dfs.support.append equal to true on both the client side in
			hbase-site.xml and on the serverside in hdfs-site.xml
		 
		 -	dfs.datanode.max.transfer.threads:
		)
	
	- Hadoop's conf/hdfs-site.xml: 
		-dfs.datanode.max.transfer.threads:(value as 4096) An HDFS DataNode has an upper bound on the number of files that it will serve at any one time
		
	- conf/regionservers: Edit it for distributed. replace localhost with hostnames/IP add of nodes	

* Hbase property: 
		- hbase.rootdir (hdfs / local file system) 
		- hbase.secure.configuration: ???? read more for hbase secure configuration
		- hbase-cluster.distributed  property  to  true
		- hbase.ipc.server.callqueue.read.ratio
		- hbase.ipc.server.callqueue.scan.ratio
		- hbase.regionserver.msginterval
		- hbase.regionserver.logroll.period
		
* Hbase scripts
	- bin/start-hbase.sh: to  start  HBase. You can  use  the  jps  command  to  verify  that  you  have  one  running  process  called  HMaster
	  In standalone  mode  HBase  runs  all  daemons  within  this  single  JVM,  i.e.  the  HMaster,  a  single
	  HRegionServer, and the ZooKeeper daemon.
	
	- bin/stop-hbase.sh: script stops
	- bin/hbase shell: connect to hbase. Use help 
	- local-master-backup.sh: 
		-start: The  following  command  starts  3  backup  servers  using  ports  16012/16022/16032, 16013/16023/16033, and 16015/16025/16035.
			$ ./bin/local-master-backup.sh 2 3 5
			
		-kill: find process Id from  /tmp/hbase-USER-X-master.pid and kill it
		 	$ cat /tmp/hbase-testuser-1-master.pid |xargs kill -9 
	- local-regionservers.sh start</stop> command allows you  to  run  multiple  RegionServers.  It  works  in  a  similar  way  to  the  
		local-master-backup.sh command
		(default  ports  are  16020  and  16030 OR 16200 and 16300)
		$ .bin/local-regionservers.sh start 2 3 4 5
		$ .bin/local-regionservers.sh stop 3
		
* Hbase urls
	- Hbase master: http://node-a.example.com:16610/  |  http://master.example.org:16010
	- Hbase regionServer: http://node-a.example.com:16630/

* TroubleShooting
		-Precondition: Hadoop and HDFS is set up and is available
		-jps command should work
		-JAVA_HOME set
		-Pseudo-distributed mode: 
			-Hadoop Configuration
			-Stop HBase if it is running.
			-Edit the hbase-site.xml configuration:
				-Hbase to run with one JVM instance per daemon.
					<property>
					  <name>hbase.cluster.distributed</name>
					  <value>true</value>
					</property>
				- hbase.rootdir from the local filesystem to the address of your HDFS instance
					<property>
					  <name>hbase.rootdir</name>
					  <value>hdfs://localhost:8020/hbase</value>
					</property>
					You  do  not  need  to  create  the  directory  in  HDFS.  HBase  will  do  this  for  you.  If  you  create  the
					directory, HBase will attempt to do a migration, which is not what you want
			- Start HBase: bin/start-hbase.sh  command  to  start  HBase. The  jps command should show the HMaster and HRegionServer processes running.
			-Check the HBase directory in HDFS: 
				./bin/hadoop fs -ls /hbase
			-It  is  recommended  that you  run  a  Network  Time  Protocol  (NTP)  service				
			-Limits on Number of Files and Processes (ulimit):
					Apache  HBase  is  a  database.  It  requires  the  ability  to  open  a  large  number  of  files  at  once.  Many 
					Linux distributions limit the number of files a single user is allowed to open to 1024 (or 256 on older versions of OS X).
					$ ulimit -n

					Problem:
						you may experience if the limit is too low. You may also notice errors such as the following:
						2010-04-06 03:04:37,542 INFO org.apache.hadoop.hdfs.DFSClient: Exception
						increateBlockOutputStream java.io.EOFException
						2010-04-06 03:04:37,542 INFO org.apache.hadoop.hdfs.DFSClient: Abandoning block blk_-
						6935524980745310745_1391901
			-Number of processes a user is allowed to run at once. In Linux and Unix,  
				the  number  of  processes  is  set  using  the  ulimit -u  command	
			-To see which user started HBase, and that user’s ulimit configuration, look at the first line of the HBase log
			 for that instance.
			-A useful   read   setting   config   on   you   hadoop   cluster   is   Aaron   Kimballs' Configuration Parameters 
			-ulimit settings on Ubuntu, edit /etc/security/limits.conf
			-Sync has to be explicitly enabled by setting dfs.support.append equal to true on both the client side in hbase-site.xml
			 and on the serverside in hdfs-site.xml	
			-dfs.datanode.max.transfer.threads:
				
				10/12/08 20:10:31 INFO hdfs.DFSClient: Could not obtain block
				          blk_XXXXXXXXXXXXXXXXXXXXXX_YYYYYYYY from any node: java.io.IOException: No live
				nodes
          		contain current block. Will get new block locations from namenode and retry...
          		
          		
			
================================================================================================================================================
ZOOKEEPER
---------


================================================================================================================================================
Cloudera manager

http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cm_ig_install_path_a.html?scroll=cmig_topic_6_5#cmig_topic_6_5_1_unique_4


configuration
https://www.youtube.com/watch?v=UzQ5_S5agqU

To start the Cloudera Manager Server:
$ sudo service cloudera-scm-server start

$ sudo service cloudera-scm-server stop

$ sudo service cloudera-scm-server restart


Host inspector
**************

If you log into Cloudera Manager and then click on the "Cloudera Manager" logo in the top left and then 
click the "Hosts" link in the menu at the top of the page, do you see your host listed?  If so, Click 
the Host Inspector button.

================================================================================================================================================
Spark
-----
spark shell is a REPL (read-eval-print loop)

$ spark-shell --master yarn-client
	Local 
$ spark-shell --master local[N]    -- N no of threads or * to match the no of cores avlbl on machine
	You will simply pass paths to local files, rather than paths on HDFS beginning with  hdfs://.
	 :history
	  and  :h?	

* An  RDD  is  Spark’s fundamental abstraction for representing a collection of objects that can be distributed
  across multiple machines in a cluster.

* Create RDD from 
	-Using the  SparkContext to create an RDD from an external data source, like 
			-a file in HDFS, 
			-a database table via JDBC, 
	-or from a local collection of objects that we create in the Spark shell.

* Transformation
	-filtering records
	-aggregating records by a common key, 
	-or joining multiple RDDs together

* Partitions define the unit of parallelism in Spark

================================================================================================================================================
Phoenix
-------
Direct use of the HBase API, along with coprocessors and custom filters, results in performance on the order of 
milliseconds for small queries, or seconds for tens of millions of rows

-Using console: Start Sqlline
	$ /home/eip/phoenix/bin
	$ ./sqlline.py localhost:2181:/hbase
	
-Using Java

-Can load bulk data in phoenix
	CSV
	CSV data can be bulk loaded with built in utility named psql. Typical upsert rates are 20K - 50K 
	rows per second (depends on how wide are the rows).
	
	Usage example:
	Create table using psql $ psql.py [zookeeper] ../examples/web_stat.sql
	
	Upsert CSV bulk data $ psql.py [zookeeper] ../examples/web_stat.csv
	
-Phoenix table to existing Hbase table
	Use create table/create view
	* HBase metadata is kept as-is, except for with a TABLE we turn KEEP_DELETED_CELLS on
	* Phoenix also adds an empty key value for each row so that queries behave as expected 
	  (without requiring all columns to be projected during scans).
	* The other caveat is that the way the bytes were serialized must match the way the bytes are serialized 
	  by Phoenix. For VARCHAR,CHAR, and UNSIGNED_* types, we use the HBase Bytes methods	

- Phoenix composite row keys are formed by simply concatenating the values together, with a zero byte character 
  used as a separator after a variable length type.

- Note that you don’t need the double quotes if you create your HBase table with all caps names 
  (since this is how Phoenix normalizes strings, by upper casing them). 

- Use multiple column families
  Column family contains related data in separate files.	

- Use compression On disk compression improves performance on large tables
  e.g. CREATE TABLE TEST (HOST VARCHAR NOT NULL PRIMARY KEY, DESCRIPTION VARCHAR) COMPRESSION='GZ'	
  'GZ'
  
- Create indexes (secondary indexes), covered indexes  
  Why isn’t my secondary index being used?  
  The secondary index won’t be used unless all columns used in the query are in it ( as indexed or covered columns).
  All columns making up the primary key of the data table will automatically be included in the index.
  
  Functional indexes: Enables an index to be defined as expressions as opposed to just column names and have the 
  index be used when a query contains this expression.	
  
- Optimize cluster parameters:
  http://hbase.apache.org/book.html#performance

- Optimize phoenix parameters
  https://phoenix.apache.org/tuning.html

- Full table scan of 100M rows usually completes in 20 seconds   

- By default, Phoenix let’s HBase manage the timestamps and just shows you the latest values for everything.
  Phoenix also allows arbitrary timestamps to be supplied by the user. To do that you’d specify a 
  "CurrentSCN" (or PhoenixRuntime.CURRENT_SCN_ATTRIB if you want to use our constant) at connection time
  
  e.g. 
  Properties props = new Properties();
  props.setProperty(PhoenixRuntime.CURRENT_SCN_ATTRIB, Long.toString(ts));
  Connection conn = DriverManager.connect(myUrl, props);
  
  conn.createStatement().execute("UPSERT INTO myTable VALUES ('a')");
  conn.commit();
  
  Usecase: By specifying a CurrentSCN, you’re telling Phoenix that you want everything for that connection to be 
  done at that timestamp. Note that this applies to queries done on the connection as well - for example, a query 
  over myTable above would not see the data it just upserted, since it only sees data that was created before its 
  CurrentSCN property. This provides a way of doing snapshot, flashback, or point-in-time queries.

- Scan
  * Range scan:
    DDL: CREATE TABLE TEST (pk1 char(1) not null, pk2 char(1) not null, pk3 char(1) not null, non-pk varchar CONSTRAINT PK PRIMARY KEY(pk1, pk2, pk3));
  
    RANGE SCAN means that only a subset of the rows in your table will be scanned over. This occurs if you use one or 
    more leading columns from your primary key constraint. Query that is not filtering on leading PK columns 
    ex. select * from test where pk2='x' and pk3='y'; will result in full scan 
    whereas the following query will result in range scan select * from test where pk1='x' and pk2='y';. 
    Note that you can add a secondary index on your "pk2" and "pk3" columns and that would cause a range scan to be done for the first query (over the index table).
    
  * Degenerate scan: means that a query can’t possibly return any rows. If we can determine that at compile time, 
    then we don’t bother to even run the scan.
    
  * Full scan
  
  * SKIP SCAN means that either a subset or all rows in your table will be scanned over, however it will skip large groups of rows depending on the conditions in your filter. See this blog for more detail. We don’t do a SKIP SCAN if you have no filter on the leading primary key columns, but you can force a SKIP SCAN by using the /+ SKIP_SCAN / hint. Under some conditions, namely when the cardinality of your leading primary key columns is low, it will be more efficient than a FULL SCAN.

- Connect to secure Hbase cluster:
  By default, Phoenix distribution is unable to connect to secure cluster. But, Phoenix can connect to a 
  secure HBase cluster by modifying sqlline.sh script and setting up some more java parameters

  URL: http://bigdatanoob.blogspot.in/2013/09/connect-phoenix-to-secure-hbase-cluster.html

- Aggregation Enhancements. COUNT DISTINCT, PERCENTILE, and STDDEV are now supported.
- Type Additions. The FLOAT, DOUBLE, TINYINT, and SMALLINT are now supported including JDBC Array

- Salting of row key:  
  Salting Row Key. To prevent hot spotting on writes, the row key may be "salted" by inserting a leading byte into 
  the row key which is a mod over N buckets of the hash of the entire row key. This ensures even distribution of 
  writes when the row key is a monotonically increasing value (often a timestamp representing the current time).

- TopN Queries

- Dynamic Columns

- Apache bigTop inclusions
	
Performance tips:
-----------------

- Use Salting to increase read/write performance by pre-splitting the data into multiple regions
  e.g. CREATE TABLE TEST (HOST VARCHAR NOT NULL PRIMARY KEY, DESCRIPTION VARCHAR) SALT_BUCKETS=16
  
  Note: Ideally for a 16 region server cluster with quad-core CPUs, choose salt buckets between 32-64 for optimal performance.
  
  To have control over splitting choose pre-split a table: 
  e.g. CREATE TABLE TEST (HOST VARCHAR NOT NULL PRIMARY KEY, DESCRIPTION VARCHAR) SPLIT ON ('CS','EU','NA')
  
  Salting Row Key. To prevent hot spotting on writes, the row key may be "salted" by inserting a leading byte into 
  the row key which is a mod over N buckets of the hash of the entire row key. This ensures even distribution of 
  writes when the row key is a monotonically increasing value (often a timestamp representing the current time).
  
- Use multiple column families
  If you query use selected columns then it make sense to group those columns together in a column family 
  to improve read performance.
  
- Use compression On disk  

- Create indexes (secondary indexes)
  
  * Indexes over immutable table gives better performance:
    e.g. of Immutable table: 
    create table test (mykey varchar primary key, col1 varchar, col2 varchar) IMMUTABLE_ROWS=true;
  
  Index example: 
  * Creating index on col2
    create index idx on test (col2)
  
  * Creating index on col1 and a covered index on col2
  	e.g. create index idx on test (col1) include (col2)
  	
  * Functional indexes: Enables an index to be defined as expressions as opposed to just column names and have the 
    index be used when a query contains this expression.	
    
   

- Optimize cluster parameters

- Optimize phoenix parameters  
   
- Statistics Collection



- Upsert rows in this test table and Phoenix query optimizer will choose correct index to use. You can see in 
  explain plan if Phoenix is using the index table. You can also give a hint in Phoenix query to use a specific index

-  Full table scan of 100M rows usually completes in 20 seconds (narrow table on a medium sized cluster)
   Phoenix is fast. Full table scan of 100M rows usually completes in 20 seconds (narrow table on a medium sized cluster).
   This time come down to few milliseconds if query contains filter on key columns. For 
   filters on non-key columns or non-leading key columns, you can add index on these columns which leads to 
   performance equivalent to filtering on key column by making copy of table with indexed column(s) part of key.	

- Why is Phoenix fast even when doing full scan:

  Phoenix chunks up your query using the region boundaries and runs them in parallel on the client using a configurable number of threads
  The aggregation will be done in a coprocessor on the server-side, collapsing the amount of data that gets returned back to the client rather than returning it all.

- Keep in mind that creating a new connection is not an expensive operation. The same underlying HConnection 
  is used for all connections to the same cluster, so it’s more or less like instantiating a few objects.
  Mainly used for saving object with timestamp
  
- For range scan, filter column of query should be leading column from primary key constratint or add secondary index
 

? How to make snapshot queries over prior versions in Phoenix: Arbitrary timestamp will help for it. Check above. This provides a way of doing snapshot, flashback, or point-in-time queries.
? indexing, secondary index, covered index
? salting (see above)
? Phoenix 1.2 query filter leverages [HBase Filter Essential Column Family]
? compression ('GZ' is just seen in ex . Check above)
? hint to query for using an index
? working on arbitrary timestamp
? can phoenix work on tables with arbitrary timestamp: Yes (see above)
? Range scan: RANGE SCAN means that only a subset of the rows in your table will be scanned over. Check above
? Connecting to secure HBase cluster: Yes. Check above. URL: http://bigdatanoob.blogspot.in/2013/09/connect-phoenix-to-secure-hbase-cluster.html
? How to get all version data from phoenix.
? Understand RP algo
? Hbase Vs Phoenix 
? Stored from Phoenix and fetched from hbase

================================================================================================================================================


================================================================================================================================================



ABR-PR-APBTZ4L
ETZ 7

http://ind-lnxapp122.emeter.com:7180/cmf/express-wizard/wizard#step=installStep


*****************************************************************************************************************
Device events
-------------

Design
------

Changes:
1) Load SDP will be modified to store sdpRowId against 8x sdpId in sdpLookup table (Hbase)
   Purpose: The deviceEvents from 7x has sdpRowId and Loader application need to resolve 7x rowId with 8x sdpId.

2)    

Accumulators
		:noOfRecords
		:noOfInvalidRecords
		:noOfLookupFailures
		:noOfEventsProcessed
		
Input arguements:
		:inputDir
		:OutputDir
		
Job parameters from input:
		:numPartitions
		:inputDelimiter
		:premiseTz
		:bulkInsert (T/F)  ??




Questions:
1) What is bulk insert in loader job (em-hadoop-common-apps)
2) 

Assumptions:
1)

*****************************************************************************************************************

 hbase zkcli
 
 
 here's a few parameters to netstat that are useful for this :
 
 -l or --listening shows only the sockets currently listening for incoming connection.
 -a or --all shows all sockets currently in use.
 -t or --tcp shows the tcp sockets.
 -u or --udp shows the udp sockets.
 -n or --numeric shows the hosts and ports as numbers, instead of resolving in dns and looking in /etc/services.
 You use a mix of these to get what you want. To know which port numbers are currently in use, use one of these:
 
 netstat -atn           # For tcp
 netstat -aun           # For udp
netstat -atun          # For both


down vote
You can use this command:

netstat -tulnp | grep <port no>



sudo su - root
hostname ind-phoenix.emeter.com

hbase repair
hbase org.apache.hadoop.hbase.util.hbck.OfflineMetaRepair

hbase diagnostics 
hbase hbck


 mv /home/eip/hadoop/HBase/HFiles ~/RGA/


New hbase.root file 
hbase zkcli 
rmr /hbase


*********************************************************************************************************************

CREATE TABLE RGAEventLog (
    eventId BIGINT NOT NULL,
    eventTime TIME NOT NULL,
    eventType CHAR(3)
    CONSTRAINT pk PRIMARY KEY (eventId, eventTime));
    
    
CREATE TABLE device_event_all_columns (
  svc_pt_id            INTEGER   NOT NULL primary key,
  device_event_type_id INTEGER        ,
  event_time           TIME ,
  event_record_num     INTEGER        ,
  reported_time        TIME      ,
  desc_text            VARCHAR        ,
  device_id            INTEGER        ,
  device_data_src_id   INTEGER        ,
  event_start_time     TIME      ,
  num_occur            INTEGER        ,
  threshold            DOUBLE         ,
  measured_value       DOUBLE      ,
  duration             INTEGER        ,
  variance             INTEGER        ,
  correlation_id       VARCHAR        ,
  incoming_device_id   INTEGER        ,
  incoming_device_type VARCHAR        ,
  org_id               INTEGER ,
  insert_time          TIME      
)
COMPRESSION='GZ';    


***************************************************************

CREATE TABLE device_event (
  svc_pt_id            INTEGER   NOT NULL,
  device_event_type_id INTEGER   NOT NULL,
  event_time           TIME NOT NULL,
  event_record_num     INTEGER        ,
  reported_time        TIME      ,
  desc_text            VARCHAR        ,
  device_id            INTEGER        ,
  device_data_src_id   INTEGER        ,
  event_start_time     TIME      ,
  num_occur            INTEGER        ,
  threshold            DOUBLE         ,
  measured_value       DOUBLE      ,
  duration             INTEGER        ,
  variance             INTEGER        ,
  correlation_id       VARCHAR        ,
  incoming_device_id   INTEGER        ,
  incoming_device_type VARCHAR        ,
  org_id               INTEGER ,
  insert_time          TIME,
  constraint pk_composite primary key(svc_pt_id, device_event_type_id, event_time)
)
COMPRESSION='GZ',
versions=10; 

create index org_idx on device_event(org_id);


[eip@ind-phoenix HBase]$ du -sh /home/eip/hadoop/HBase/HFiles/
21M     /home/eip/hadoop/HBase/HFiles/



0: jdbc:phoenix:localhost> !tables
+------------------------------------------+------------------------------------------+------------------------------------------+------------------------------------------+----+
|                TABLE_CAT                 |               TABLE_SCHEM                |                TABLE_NAME                |                TABLE_TYPE                |    |
+------------------------------------------+------------------------------------------+------------------------------------------+------------------------------------------+----+
|                                          | SYSTEM                                   | CATALOG                                  | SYSTEM TABLE                             |    |
|                                          | SYSTEM                                   | FUNCTION                                 | SYSTEM TABLE                             |    |
|                                          | SYSTEM                                   | SEQUENCE                                 | SYSTEM TABLE                             |    |
|                                          | SYSTEM                                   | STATS                                    | SYSTEM TABLE                             |    |
|                                          |                                          | DEVICE_EVENT                             | TABLE                                    |    |
+------------------------------------------+------------------------------------------+------------------------------------------+------------------------------------------+----+

0: jdbc:phoenix:localhost> !desc device_event
+------------------------------------------+------------------------------------------+------------------------------------------+------------------------------------------+----+
|                TABLE_CAT                 |               TABLE_SCHEM                |                TABLE_NAME                |               COLUMN_NAME                |    |
+------------------------------------------+------------------------------------------+------------------------------------------+------------------------------------------+----+
|                                          |                                          | DEVICE_EVENT                             | SVC_PT_ID                                | 4  |
|                                          |                                          | DEVICE_EVENT                             | DEVICE_EVENT_TYPE_ID                     | 4  |
|                                          |                                          | DEVICE_EVENT                             | EVENT_TIME                               | 92 |
|                                          |                                          | DEVICE_EVENT                             | EVENT_RECORD_NUM                         | 4  |
|                                          |                                          | DEVICE_EVENT                             | REPORTED_TIME                            | 92 |
|                                          |                                          | DEVICE_EVENT                             | DESC_TEXT                                | 12 |
|                                          |                                          | DEVICE_EVENT                             | DEVICE_ID                                | 4  |
|                                          |                                          | DEVICE_EVENT                             | DEVICE_DATA_SRC_ID                       | 4  |
|                                          |                                          | DEVICE_EVENT                             | EVENT_START_TIME                         | 92 |
|                                          |                                          | DEVICE_EVENT                             | NUM_OCCUR                                | 4  |
|                                          |                                          | DEVICE_EVENT                             | THRESHOLD                                | 8  |
|                                          |                                          | DEVICE_EVENT                             | MEASURED_VALUE                           | 8  |
|                                          |                                          | DEVICE_EVENT                             | DURATION                                 | 4  |
|                                          |                                          | DEVICE_EVENT                             | VARIANCE                                 | 4  |
|                                          |                                          | DEVICE_EVENT                             | CORRELATION_ID                           | 12 |
|                                          |                                          | DEVICE_EVENT                             | INCOMING_DEVICE_ID                       | 4  |
|                                          |                                          | DEVICE_EVENT                             | INCOMING_DEVICE_TYPE                     | 12 |
|                                          |                                          | DEVICE_EVENT                             | ORG_ID                                   | 4  |
|                                          |                                          | DEVICE_EVENT                             | INSERT_TIME                              | 92 |
+------------------------------------------+------------------------------------------+------------------------------------------+------------------------------------------+----+


sql execution time = 3403 millisec
Table insertion time = 36603 millisec
Svc_Pt_id = 2
Record time  = 318 millisec
recCount = 100000
Record time  = 981 millisec



Create row_key as composite key svc_pt_id, event_type_id, event_time
Measure db file size in Hdfs
Insert 10K records and time taken to insert
Measure size of 10k records
Fetch / scan a record time


1) with and without compaction ratio and version


- without compaction	

	-DDL 
		ddl_create_device_event = "CREATE TABLE device_event ("
															  + " svc_pt_id            INTEGER   NOT NULL,"
															  + " device_event_type_id INTEGER   NOT NULL,"
															  + " event_time           TIME NOT NULL,"
															  + " event_record_num     INTEGER ,"
															  + " reported_time        TIME ,"
															  + " desc_text            VARCHAR ,"
															  + " device_id            INTEGER ,"
															  + " device_data_src_id   INTEGER ,"
															  + " event_start_time     TIME  ,"
															  + " num_occur            INTEGER ,"
															  + " threshold            DOUBLE ,"
															  + " measured_value       DOUBLE ,"
															  + " duration             INTEGER ,"
															  + " variance             INTEGER ,"
															  + " correlation_id       VARCHAR ,"
															  + " incoming_device_id   INTEGER ,"
															  + " incoming_device_type VARCHAR ,"
															  + " org_id               INTEGER ,"
															  + " insert_time          TIME,"
															  + " constraint pk_composite primary key(svc_pt_id, device_event_type_id, event_time)"
													  + ")";
													  
		
		-table structure in hbase (No Compression)
		
		DESCRIPTION                                                                                                    ENABLED                                                     
		 'DEVICE_EVENT', {TABLE_ATTRIBUTES => {coprocessor$1 => '|org.apache.phoenix.coprocessor.ScanRegionObserver|80 true                                                        
		 5306366|', coprocessor$2 => '|org.apache.phoenix.coprocessor.UngroupedAggregateRegionObserver|805306366|', co                                                             
		 processor$3 => '|org.apache.phoenix.coprocessor.GroupedAggregateRegionObserver|805306366|', coprocessor$4 =>                                                              
		 '|org.apache.phoenix.coprocessor.ServerCachingEndpointImpl|805306366|', coprocessor$5 => '|org.apache.phoenix                                                             
		 .hbase.index.Indexer|805306366|org.apache.hadoop.hbase.index.codec.class=org.apache.phoenix.index.PhoenixInde                                                             
		 xCodec,index.builder=org.apache.phoenix.index.PhoenixIndexBuilder', coprocessor$6 => '|org.apache.hadoop.hbas                                                             
		 e.regionserver.LocalIndexSplitter|805306366|'}, {NAME => '0', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMOR                                                             
		 Y => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'FAST_DIFF', TTL => 'FOREVER', COMPRESSIO                                                             
 		N => 'NONE', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}	
 		
 		
 		-Phoenix data
 		
 		0: jdbc:phoenix:localhost> !desc device_event
		+------------------------------------------+------------------------------------------+------------------------------------------+------------------------------------------+----+
		|                TABLE_CAT                 |               TABLE_SCHEM                |                TABLE_NAME                |               COLUMN_NAME                |    |
		+------------------------------------------+------------------------------------------+------------------------------------------+------------------------------------------+----+
		|                                          |                                          | DEVICE_EVENT                             | SVC_PT_ID                                | 4  |
		|                                          |                                          | DEVICE_EVENT                             | DEVICE_EVENT_TYPE_ID                     | 4  |
		|                                          |                                          | DEVICE_EVENT                             | EVENT_TIME                               | 92 |
		|                                          |                                          | DEVICE_EVENT                             | EVENT_RECORD_NUM                         | 4  |
		|                                          |                                          | DEVICE_EVENT                             | REPORTED_TIME                            | 92 |
		|                                          |                                          | DEVICE_EVENT                             | DESC_TEXT                                | 12 |
		|                                          |                                          | DEVICE_EVENT                             | DEVICE_ID                                | 4  |
		|                                          |                                          | DEVICE_EVENT                             | DEVICE_DATA_SRC_ID                       | 4  |
		|                                          |                                          | DEVICE_EVENT                             | EVENT_START_TIME                         | 92 |
		|                                          |                                          | DEVICE_EVENT                             | NUM_OCCUR                                | 4  |
		|                                          |                                          | DEVICE_EVENT                             | THRESHOLD                                | 8  |
		|                                          |                                          | DEVICE_EVENT                             | MEASURED_VALUE                           | 8  |
		|                                          |                                          | DEVICE_EVENT                             | DURATION                                 | 4  |
		|                                          |                                          | DEVICE_EVENT                             | VARIANCE                                 | 4  |
		|                                          |                                          | DEVICE_EVENT                             | CORRELATION_ID                           | 12 |
		|                                          |                                          | DEVICE_EVENT                             | INCOMING_DEVICE_ID                       | 4  |
		|                                          |                                          | DEVICE_EVENT                             | INCOMING_DEVICE_TYPE                     | 12 |
		|                                          |                                          | DEVICE_EVENT                             | ORG_ID                                   | 4  |
		|                                          |                                          | DEVICE_EVENT                             | INSERT_TIME                              | 92 |
		+------------------------------------------+------------------------------------------+------------------------------------------+------------------------------------------+----+
 		

 		
	
	-Sample data:
	
		int recordCount = 1000000;
		int commitCount = 10000;
	
	
		Hbase:
		
		ROW                                         COLUMN+CELL                                                                                                                    
		 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:CORRELATION_ID, timestamp=1435127014768, value=corrId_1                                                               
		 01N$<[\xD1                                                                                                                                                                
		 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:DESC_TEXT, timestamp=1435127014768, value=Desc_1                                                                      
		 01N$<[\xD1                                                                                                                                                                
		 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:DEVICE_DATA_SRC_ID, timestamp=1435127014768, value=\x80\x00\x00\x01                                                   
		 01N$<[\xD1                                                                                                                                                                
		 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:DEVICE_ID, timestamp=1435127014768, value=\x80\x00\x00\x01                                                            
		 01N$<[\xD1                                                                                                                                                                
		 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:DURATION, timestamp=1435127014768, value=\x80\x00\x00\x01                                                             
		 01N$<[\xD1                                                                                                                                                                
		 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:EVENT_RECORD_NUM, timestamp=1435127014768, value=\x80\x00\x00\x01                                                     
		 01N$<[\xD1                                                                                                                                                                
		 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:EVENT_START_TIME, timestamp=1435127014768, value=\x80\x00\x01N$<[\xD1                                                 
		 01N$<[\xD1                                                                                                                                                                
		 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:INCOMING_DEVICE_ID, timestamp=1435127014768, value=\x80\x00\x00\x01                                                   
		 01N$<[\xD1                                                                                                                                                                
		 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:INCOMING_DEVICE_TYPE, timestamp=1435127014768, value=deviceType_1                                                     
		 01N$<[\xD1                                                                                                                                                                
		 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:INSERT_TIME, timestamp=1435127014768, value=\x80\x00\x01N$<[\xD1                                                      
		 01N$<[\xD1                                                                                                                                                                
		 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:MEASURED_VALUE, timestamp=1435127014768, value=\xBF\xF0\x00\x00\x00\x00\x00\x01                                       
		 01N$<[\xD1                                                                                                                                                                
		 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:NUM_OCCUR, timestamp=1435127014768, value=\x80\x00\x00\x01                                                            
		 01N$<[\xD1                                                                                                                                                                
		 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:ORG_ID, timestamp=1435127014768, value=\x80\x00\x00\x01                                                               
		 01N$<[\xD1                                                                                                                                                                
		 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:REPORTED_TIME, timestamp=1435127014768, value=\x80\x00\x01N$<[\xD1                                                    
		 01N$<[\xD1                                                                                                                                                                
		 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:THRESHOLD, timestamp=1435127014768, value=\xBF\xF0\x00\x00\x00\x00\x00\x01                                            
		 01N$<[\xD1                                                                                                                                                                
		 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:VARIANCE, timestamp=1435127014768, value=\x80\x00\x00\x01                                                             
		 01N$<[\xD1                                                                                                                                                                
		 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:_0, timestamp=1435127014768, value=                                                                                   
 		 01N$<[\xD1
		 
		 
		 By default converts all fields to bytes while storing but leaves string field as it is
		 
		sqlline.py (phoenix):
		
	
	
	-Data file size
		[eip@192 ~]$ du -sh /home/eip/hadoop/HBase/HFiles/
		2.6M    /home/eip/hadoop/HBase/HFiles/

		After:
		[eip@192 ~]$ du -sh /home/eip/hadoop/HBase/HFiles/
		1.8G    /home/eip/hadoop/HBase/HFiles/
		
				
		
		
	-Execution time taken to 
		
				Run 1
				
				From Java (include n/w latency)

				(drop table)sql execution time = 3637 millisec
				(create table)sql execution time = 835 millisec
				Record insertion time = 322872 millisec (~5 min 36 sec)
				Record found for sdpId = 2
				Record time  = 544 millisec
				Total Records = 1000000
				sql-count * time  = 7428 millisec

				From sqlline.py 

				Fetch data for sdpId=2 :: 0.81 sec
				0: jdbc:phoenix:localhost> select * from device_event where svc_pt_id = 2;
				+------------------------------------------+------------------------------------------+-------------------------+------------------------------------------+-------------+
				|                SVC_PT_ID                 |           DEVICE_EVENT_TYPE_ID           |       EVENT_TIME        |             EVENT_RECORD_NUM             |      REPORT |
				+------------------------------------------+------------------------------------------+-------------------------+------------------------------------------+-------------+
				| 2                                        | 2                                        | 2015-06-24 06:23:32.305 | 2                                        | 2015-06-24  |
				+------------------------------------------+------------------------------------------+-------------------------+------------------------------------------+-------------+
				1 row selected (0.81 seconds)


				select count(*):: 3.771

				0: jdbc:phoenix:localhost> select count(*) as count1 from device_event;
				+------------------------------------------+
				|                  COUNT1                  |
				+------------------------------------------+
				| 1000000                                  |
				+------------------------------------------+
				1 row selected (3.771 seconds)
				
				
				Run 2
				
				From Java
				
				create sql execution time = 3591 millisec
				Record insertion time = 328454 millisec (~5 min 46 sec)
				Record found for sdpId = 2
				Record fetch time  = 941 millisec
				Total Records = 1000000
				sql-count* execution time  = 7711 millisec
				
				
				From sqlline.py
				
				Fetch data for sdpId=2 :: 0.692
				
				0: jdbc:phoenix:localhost> select * from device_event where svc_pt_id =2;
				+------------------------------------------+------------------------------------------+-------------------------+------------------------------------------+-------------+
				|                SVC_PT_ID                 |           DEVICE_EVENT_TYPE_ID           |       EVENT_TIME        |             EVENT_RECORD_NUM             |      REPORT |
				+------------------------------------------+------------------------------------------+-------------------------+------------------------------------------+-------------+
				| 2                                        | 2                                        | 2015-06-24 07:40:02.258 | 2                                        | 2015-06-24  |
				+------------------------------------------+------------------------------------------+-------------------------+------------------------------------------+-------------+
				1 row selected (0.692 seconds)

				
				select count(*):: 3.6 sec
				
				0: jdbc:phoenix:localhost> select count(*) from device_event;
				+------------------------------------------+
				|                 COUNT(1)                 |
				+------------------------------------------+
				| 1000000                                  |
				+------------------------------------------+
				1 row selected (3.6 seconds)
		
	
	- WITH COMPRESSION='GZ', versions=5";
		-DDL
			-ddl_create_device_event = 
								ddl_create_device_event = "CREATE TABLE device_event ("
													  + " svc_pt_id            INTEGER   NOT NULL,"
													  + " device_event_type_id INTEGER   NOT NULL,"
													  + " event_time           TIME NOT NULL,"
													  + " event_record_num     INTEGER ,"
													  + " reported_time        TIME ,"
													  + " desc_text            VARCHAR ,"
													  + " device_id            INTEGER ,"
													  + " device_data_src_id   INTEGER ,"
													  + " event_start_time     TIME  ,"
													  + " num_occur            INTEGER ,"
													  + " threshold            DOUBLE ,"
													  + " measured_value       DOUBLE ,"
													  + " duration             INTEGER ,"
													  + " variance             INTEGER ,"
													  + " correlation_id       VARCHAR ,"
													  + " incoming_device_id   INTEGER ,"
													  + " incoming_device_type VARCHAR ,"
													  + " org_id               INTEGER ,"
													  + " insert_time          TIME,"
													  + " constraint pk_composite primary key(svc_pt_id, device_event_type_id, event_time)"
													  + ") COMPRESSION='GZ', versions=5";								  
					
			-table structure in hbase (GZ)
				DESCRIPTION                                                                                                    ENABLED                                                     
				 'DEVICE_EVENT', {TABLE_ATTRIBUTES => {coprocessor$1 => '|org.apache.phoenix.coprocessor.ScanRegionObserver|80 true                                                        
				 5306366|', coprocessor$2 => '|org.apache.phoenix.coprocessor.UngroupedAggregateRegionObserver|805306366|', co                                                             
				 processor$3 => '|org.apache.phoenix.coprocessor.GroupedAggregateRegionObserver|805306366|', coprocessor$4 =>                                                              
				 '|org.apache.phoenix.coprocessor.ServerCachingEndpointImpl|805306366|', coprocessor$5 => '|org.apache.phoenix                                                             
				 .hbase.index.Indexer|805306366|org.apache.hadoop.hbase.index.codec.class=org.apache.phoenix.index.PhoenixInde                                                             
				 xCodec,index.builder=org.apache.phoenix.index.PhoenixIndexBuilder', coprocessor$6 => '|org.apache.hadoop.hbas                                                             
				 e.regionserver.LocalIndexSplitter|805306366|'}, {NAME => '0', BLOOMFILTER => 'ROW', VERSIONS => '5', IN_MEMOR                                                             
				 Y => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'FAST_DIFF', TTL => 'FOREVER', COMPRESSIO                                                             
 				N => 'GZ', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}  
 		
 			
 			-Phoenix describe
 		
 		0: jdbc:phoenix:localhost> !desc device_event
		+------------------------------------------+------------------------------------------+------------------------------------------+----------------------------------------+
		|                TABLE_CAT                 |               TABLE_SCHEM                |                TABLE_NAME                |               COLUMN_NAME              |
		+------------------------------------------+------------------------------------------+------------------------------------------+----------------------------------------+
		|                                          |                                          | DEVICE_EVENT                             | SVC_PT_ID                              |
		|                                          |                                          | DEVICE_EVENT                             | DEVICE_EVENT_TYPE_ID                   |
		|                                          |                                          | DEVICE_EVENT                             | EVENT_TIME                             |
		|                                          |                                          | DEVICE_EVENT                             | EVENT_RECORD_NUM                       |
		|                                          |                                          | DEVICE_EVENT                             | REPORTED_TIME                          |
		|                                          |                                          | DEVICE_EVENT                             | DESC_TEXT                              |
		|                                          |                                          | DEVICE_EVENT                             | DEVICE_ID                              |
		|                                          |                                          | DEVICE_EVENT                             | DEVICE_DATA_SRC_ID                     |
		|                                          |                                          | DEVICE_EVENT                             | EVENT_START_TIME                       |
		|                                          |                                          | DEVICE_EVENT                             | NUM_OCCUR                              |
		|                                          |                                          | DEVICE_EVENT                             | THRESHOLD                              |
		|                                          |                                          | DEVICE_EVENT                             | MEASURED_VALUE                         |
		|                                          |                                          | DEVICE_EVENT                             | DURATION                               |
		|                                          |                                          | DEVICE_EVENT                             | VARIANCE                               |
		|                                          |                                          | DEVICE_EVENT                             | CORRELATION_ID                         |
		|                                          |                                          | DEVICE_EVENT                             | INCOMING_DEVICE_ID                     |
		|                                          |                                          | DEVICE_EVENT                             | INCOMING_DEVICE_TYPE                   |
		|                                          |                                          | DEVICE_EVENT                             | ORG_ID                                 |
		|                                          |                                          | DEVICE_EVENT                             | INSERT_TIME                            |
		+------------------------------------------+------------------------------------------+------------------------------------------+----------------------------------------+
 
		
		-sample data
			int recordCount = 1000000;
			int commitCount = 10000;
			
			By default converts all fields to binary while storing but leaves string field as it is
		
			ROW                                         COLUMN+CELL                                                                                                                    
			 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:CORRELATION_ID, timestamp=1435140805616, value=corrId_1                                                               
			 01N%\x0E\xC9\x8A                                                                                                                                                          
			 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:DESC_TEXT, timestamp=1435140805616, value=Desc_1                                                                      
			 01N%\x0E\xC9\x8A                                                                                                                                                          
			 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:DEVICE_DATA_SRC_ID, timestamp=1435140805616, value=\x80\x00\x00\x01                                                   
			 01N%\x0E\xC9\x8A                                                                                                                                                          
			 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:DEVICE_ID, timestamp=1435140805616, value=\x80\x00\x00\x01                                                            
			 01N%\x0E\xC9\x8A                                                                                                                                                          
			 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:DURATION, timestamp=1435140805616, value=\x80\x00\x00\x01                                                             
			 01N%\x0E\xC9\x8A                                                                                                                                                          
			 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:EVENT_RECORD_NUM, timestamp=1435140805616, value=\x80\x00\x00\x01                                                     
			 01N%\x0E\xC9\x8A                                                                                                                                                          
			 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:EVENT_START_TIME, timestamp=1435140805616, value=\x80\x00\x01N%\x0E\xC9\x8A                                           
			 01N%\x0E\xC9\x8A                                                                                                                                                          
			 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:INCOMING_DEVICE_ID, timestamp=1435140805616, value=\x80\x00\x00\x01                                                   
			 01N%\x0E\xC9\x8A                                                                                                                                                          
			 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:INCOMING_DEVICE_TYPE, timestamp=1435140805616, value=deviceType_1                                                     
			 01N%\x0E\xC9\x8A                                                                                                                                                          
			 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:INSERT_TIME, timestamp=1435140805616, value=\x80\x00\x01N%\x0E\xC9\x8A                                                
			 01N%\x0E\xC9\x8A                                                                                                                                                          
			 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:MEASURED_VALUE, timestamp=1435140805616, value=\xBF\xF0\x00\x00\x00\x00\x00\x01                                       
			 01N%\x0E\xC9\x8A                                                                                                                                                          
			 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:NUM_OCCUR, timestamp=1435140805616, value=\x80\x00\x00\x01                                                            
			 01N%\x0E\xC9\x8A                                                                                                                                                          
			 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:ORG_ID, timestamp=1435140805616, value=\x80\x00\x00\x01                                                               
			 01N%\x0E\xC9\x8A                                                                                                                                                          
			 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:REPORTED_TIME, timestamp=1435140805616, value=\x80\x00\x01N%\x0E\xC9\x8A                                              
			 01N%\x0E\xC9\x8A                                                                                                                                                          
			 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:THRESHOLD, timestamp=1435140805616, value=\xBF\xF0\x00\x00\x00\x00\x00\x01                                            
			 01N%\x0E\xC9\x8A                                                                                                                                                          
			 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:VARIANCE, timestamp=1435140805616, value=\x80\x00\x00\x01                                                             
			 01N%\x0E\xC9\x8A                                                                                                                                                          
			 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x column=0:_0, timestamp=1435140805616, value=                                                                                   
 			01N%\x0E\xC9\x8A                                                                         
			
			
		-Data File size
			[eip@ind-phoenix ~]$ du -sh /home/eip/hadoop/HBase/HFiles/
			2.6M    /home/eip/hadoop/HBase/HFiles/
			
			[eip@ind-phoenix ~]$ du -sh /home/eip/hadoop/HBase/HFiles/
			694M    /home/eip/hadoop/HBase/HFiles/
			
		

		-Execution Time
		
			Run1
			
			create sql execution time = 3430 millisec
			Record insertion time = 394983 millisec (6 min 5 sec)
			Record found for sdpId = 2
			Record fetch time  = 389 millisec
			Total Records = 1000000
			sql-count* execution time  = 8872 millisec
			
			
			Run 2
			
			create sql execution time = 2374 millisec
			Record insertion time = 362880 millisec 
			Record found for sdpId = 2
			Record fetch time  = 233 millisec
			Total Records = 1000000
			sql-count* execution time  = 8623 millisec
			
		
	- With Snappy
		-DDL
			-ddl_create_device_event = 
																		  
							
			-table structure in hbase (No Compression)
				
		-sample data
			int recordCount = 1000000;
			int commitCount = 10000;
				
				
				
		-File size
		
		-Execution Time



table delete size 
before: 1.8 GB
after: 1.6 GB


HBASE table

ddl
create 'DEVICE_EVENT_HBASE','de'



hbase structure 

hbase(main):005:0* describe 'DEVICE_EVENT_HBASE'
DESCRIPTION                                                                                                        ENABLED                                                       
 'DEVICE_EVENT_HBASE', {NAME => 'de', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CE true                                                          
 LLS => 'false', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOC                                                               
 KCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}                                                                                                               
1 row(s) in 0.0710 seconds


sample data 

ROW                                           COLUMN+CELL                                                                                                                        
 \x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x01 column=de:correlation_id, timestamp=1435230597635, value=corrId_1                                                                  
 N*h\xEB\xA2                                                                                                                                                                     
 \x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x01 column=de:desc_text, timestamp=1435230597635, value=Desc_1                                                                         
 N*h\xEB\xA2                                                                                                                                                                     
 \x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x01 column=de:device_data_src_id, timestamp=1435230597635, value=\x00\x00\x00\x01                                                      
 N*h\xEB\xA2                                                                                                                                                                     
 \x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x01 column=de:device_id, timestamp=1435230597635, value=\x00\x00\x00\x01                                                               
 N*h\xEB\xA2                                                                                                                                                                     
 \x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x01 column=de:duration, timestamp=1435230597635, value=\x00\x00\x00\x01                                                                
 N*h\xEB\xA2                                                                                                                                                                     
 \x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x01 column=de:event_record_num, timestamp=1435230597635, value=\x00\x00\x00\x01                                                        
 N*h\xEB\xA2                                                                                                                                                                     
 \x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x01 column=de:event_start_time, timestamp=1435230597635, value=\x00\x00\x01N*h\xEB\xA2                                                 
 N*h\xEB\xA2                                                                                                                                                                     
 \x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x01 column=de:incoming_device_id, timestamp=1435230597635, value=\x00\x00\x00\x01                                                      
 N*h\xEB\xA2                                                                                                                                                                     
 \x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x01 column=de:incoming_device_type, timestamp=1435230597635, value=deviceType_1                                                        
 N*h\xEB\xA2                                                                                                                                                                     
 \x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x01 column=de:insert_time, timestamp=1435230597635, value=\x00\x00\x01N*h\xEB\xA2                                                      
 N*h\xEB\xA2                                                                                                                                                                     
 \x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x01 column=de:measured_value, timestamp=1435230597635, value=\x00\x00\x00\x01                                                          
 N*h\xEB\xA2                                                                                                                                                                     
 \x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x01 column=de:num_occur, timestamp=1435230597635, value=\x00\x00\x00\x01                                                               
 N*h\xEB\xA2                                                                                                                                                                     
 \x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x01 column=de:org_id, timestamp=1435230597635, value=\x00\x00\x00\x01                                                                  
 N*h\xEB\xA2                                                                                                                                                                     
 \x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x01 column=de:reported_time, timestamp=1435230597635, value=\x00\x00\x01N*h\xEB\xA2                                                    
 N*h\xEB\xA2                                                                                                                                                                     
 \x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x01 column=de:threshold, timestamp=1435230597635, value=\x00\x00\x00\x01                                                               
 N*h\xEB\xA2                                                                                                                                                                     
 \x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x01 column=de:variance, timestamp=1435230597635, value=\x00\x00\x00\x01                                                                
 N*h\xEB\xA2 


data file size:

before:
[eip@ind-phoenix ~]$ du -sh /home/eip/hadoop/HBase/HFiles/
2.4M    /home/eip/hadoop/HBase/HFiles/

after:

[eip@ind-phoenix ~]$ du -sh /home/eip/hadoop/HBase/HFiles/
1.8G    /home/eip/hadoop/HBase/HFiles/


exec time

Hbase sql execution time = 198329 millisec
fetchSingleHbaseData sql execution time = 5 millisec
null
Total cout::1000000
fetchCountAll sql execution time = 46810 millisec


Time::1435231261353
Hbase sql execution time = 215117 millisec (~3 min 58 sec)
Data found for composite key
fetchSingleHbaseData sql execution time = 233 millisec
Desc_1
Total count::1000000
fetchCountAll sql execution time = 82322 millisec


Phoenix Hbase table

ddl

create 'DEVICE_EVENT_HBASE','de'

CREATE TABLE DEVICE_EVENT_HBASE (
 svc_pt_id            INTEGER   NOT NULL,
 device_event_type_id INTEGER   NOT NULL,
 event_time           TIME NOT NULL,
 de.event_record_num     INTEGER ,
 de.reported_time        TIME ,
 de.desc_text            VARCHAR ,
 de.device_id            INTEGER ,
 de.device_data_src_id   INTEGER ,
 de.event_start_time     TIME  ,
 de.num_occur            INTEGER ,
 de.threshold            DOUBLE ,
 de.measured_value       DOUBLE ,
 de.duration             INTEGER ,
 de.variance             INTEGER ,
 de.correlation_id       VARCHAR ,
 de.incoming_device_id   INTEGER ,
 de.incoming_device_type VARCHAR ,
 de.org_id               INTEGER ,
 de.insert_time          TIME,
 constraint pk_composite primary key(svc_pt_id, device_event_type_id, event_time)
 );



UPSERT INTO DEVICE_EVENT_HBASE values (
1,
1,
now(),
1,
now(),
'a',
2,
3,
now(),
4,
5,
6,
7,
8,
'e',
9,
'g',
10,
now()
 );


hbase structure 

Before:
DESCRIPTION                                                                                                        ENABLED                                                       
 'DEVICE_EVENT_HBASE', {NAME => 'de', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CE true                                                          
 LLS => 'false', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOC                                                               
 KCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'} 


After: 

DESCRIPTION                                                                                                        ENABLED                                                       
 'DEVICE_EVENT_HBASE', {TABLE_ATTRIBUTES => {coprocessor$1 => '|org.apache.phoenix.coprocessor.ScanRegionObserver| true                                                          
 805306366|', coprocessor$2 => '|org.apache.phoenix.coprocessor.UngroupedAggregateRegionObserver|805306366|', copr                                                               
 ocessor$3 => '|org.apache.phoenix.coprocessor.GroupedAggregateRegionObserver|805306366|', coprocessor$4 => '|org.                                                               
 apache.phoenix.coprocessor.ServerCachingEndpointImpl|805306366|', coprocessor$5 => '|org.apache.phoenix.hbase.ind                                                               
 ex.Indexer|805306366|org.apache.hadoop.hbase.index.codec.class=org.apache.phoenix.index.PhoenixIndexCodec,index.b                                                               
 uilder=org.apache.phoenix.index.PhoenixIndexBuilder', coprocessor$6 => '|org.apache.hadoop.hbase.regionserver.Loc                                                               
 alIndexSplitter|805306366|'}, {NAME => 'DE', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DE                                                               
 LETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'FAST_DIFF', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS                                                               
  => '0', BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}, {NAME => 'de', BLOOMFILTER => 'RO                                                               
 W', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'false', DATA_BLOCK_ENCODING => 'NONE', TTL => '                                                               
 FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCO                                                               
 PE => '0'}    

sample data 

ROW                                           COLUMN+CELL                                                                                                                        
 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x01 column=DE:CORRELATION_ID, timestamp=1435233662874, value=corrId_1                                                                  
 N*\x97\xAD\xBC                                                                                                                                                                  
 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x01 column=DE:DESC_TEXT, timestamp=1435233662874, value=Desc_1                                                                         
 N*\x97\xAD\xBC                                                                                                                                                                  
 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x01 column=DE:DEVICE_DATA_SRC_ID, timestamp=1435233662874, value=\x80\x00\x00\x01                                                      
 N*\x97\xAD\xBC                                                                                                                                                                  
 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x01 column=DE:DEVICE_ID, timestamp=1435233662874, value=\x80\x00\x00\x01                                                               
 N*\x97\xAD\xBC                                                                                                                                                                  
 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x01 column=DE:DURATION, timestamp=1435233662874, value=\x80\x00\x00\x01                                                                
 N*\x97\xAD\xBC                                                                                                                                                                  
 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x01 column=DE:EVENT_RECORD_NUM, timestamp=1435233662874, value=\x80\x00\x00\x01                                                        
 N*\x97\xAD\xBC                                                                                                                                                                  
 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x01 column=DE:EVENT_START_TIME, timestamp=1435233662874, value=\x80\x00\x01N*\x97\xAD\xBC                                              
 N*\x97\xAD\xBC                                                                                                                                                                  
 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x01 column=DE:INCOMING_DEVICE_ID, timestamp=1435233662874, value=\x80\x00\x00\x01                                                      
 N*\x97\xAD\xBC                                                                                                                                                                  
 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x01 column=DE:INCOMING_DEVICE_TYPE, timestamp=1435233662874, value=deviceType_1                                                        
 N*\x97\xAD\xBC                                                                                                                                                                  
 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x01 column=DE:INSERT_TIME, timestamp=1435233662874, value=\x80\x00\x01N*\x97\xAD\xBC                                                   
 N*\x97\xAD\xBC                                                                                                                                                                  
 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x01 column=DE:MEASURED_VALUE, timestamp=1435233662874, value=\xBF\xF0\x00\x00\x00\x00\x00\x01                                          
 N*\x97\xAD\xBC                                                                                                                                                                  
 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x01 column=DE:NUM_OCCUR, timestamp=1435233662874, value=\x80\x00\x00\x01                                                               
 N*\x97\xAD\xBC                                                                                                                                                                  
 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x01 column=DE:ORG_ID, timestamp=1435233662874, value=\x80\x00\x00\x01                                                                  
 N*\x97\xAD\xBC                                                                                                                                                                  
 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x01 column=DE:REPORTED_TIME, timestamp=1435233662874, value=\x80\x00\x01N*\x97\xAD\xBC                                                 
 N*\x97\xAD\xBC                                                                                                                                                                  
 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x01 column=DE:THRESHOLD, timestamp=1435233662874, value=\xBF\xF0\x00\x00\x00\x00\x00\x01                                               
 N*\x97\xAD\xBC                                                                                                                                                                  
 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x01 column=DE:VARIANCE, timestamp=1435233662874, value=\x80\x00\x00\x01                                                                
 N*\x97\xAD\xBC                                                                                                                                                                  
 \x80\x00\x00\x01\x80\x00\x00\x01\x80\x00\x01 column=DE:_0, timestamp=1435233662874, value=                                                                                      
 N*\x97\xAD\xBC                                                                          

data file size

before
[eip@ind-phoenix ~]$ du -sh /home/eip/hadoop/HBase/HFiles/
2.6M    /home/eip/hadoop/HBase/HFiles/

after:
[eip@ind-phoenix ~]$ du -sh /home/eip/hadoop/HBase/HFiles/
2.2G    /home/eip/hadoop/HBase/HFiles/

exec time

create sql execution time = 8029 millisec
Record insertion time = 330959 millisec
Record found for sdpId = 2
Record fetch time  = 887 millisec
Total Records = 1000000
sql-count* execution time  = 9719 millisec


Run 2
Time::1435234524378
Record insertion time = 285024 millisec
Record found for sdpId = 2
Record fetch time  = 372 millisec
Total Records = 1000000
sql-count* execution time  = 9494 millisec
---------------------------------------

compressions

Hbase support for compression
•  none
•  Snappy
•  LZO
•  LZ4
•  GZ (built into Java)

for others we need to install them

GZIP compression uses more CPU resources than Snappy or LZO, but provides a higher compression ratio. 
GZip is often a good choice for cold data, which is accessed infrequently. 
Snappy or LZO are a better choice for hot data, which is accessed frequently.

Use  Snappy  or  LZO  for  hot  data,  which  is  accessed  frequently.  Snappy  and  LZO  use  fewer  CPU
resources than GZIP, but do not provide as high of a compression ratio.

Before  Snappy  became  available  by  Google  in  2011,  LZO  was  the  default.  Snappy  has  similar
qualities as LZO but has been shown to perform better.

In  most  cases,  enabling  Snappy  or  LZO  by  default  is  a  good  choice,  because  they  have  a  low
performance overhead and provide space savings.



LZO compreses data faster but compressed size is less 






****************************************************************
drop table test_device_event;


1) 
upsert into test_device_event values (1,1,now(),1,null);
upsert into test_device_event(svc_pt_id,device_event_type_id, desc_text ) values (1,1,'desc2');
select * from test_device_event ;



SLF4J: Found binding in [jar:file:/home/eip/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
0    [master:ind-phoenix:60000] FATAL org.apache.hadoop.hbase.master.HMaster  - Master server abort: loaded coprocessors are: []
2    [master:ind-phoenix:60000] FATAL org.apache.hadoop.hbase.master.HMaster  - Unhandled exception. Starting shutdown.
org.apache.hadoop.hbase.TableExistsException: hbase:namespace
        at org.apache.hadoop.hbase.master.handler.CreateTableHandler.prepare(CreateTableHandler.java:133)
        at org.apache.hadoop.hbase.master.TableNamespaceManager.createNamespaceTable(TableNamespaceManager.java:232)
        at org.apache.hadoop.hbase.master.TableNamespaceManager.start(TableNamespaceManager.java:86)
        at org.apache.hadoop.hbase.master.HMaster.initNamespace(HMaster.java:1046)
        at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:925)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:605)
        at java.lang.Thread.run(Thread.java:745)
181  [main] ERROR org.apache.hadoop.hbase.master.HMasterCommandLine  - Master exiting
java.lang.RuntimeException: HMaster Aborted
        at org.apache.hadoop.hbase.master.HMasterCommandLine.startMaster(HMasterCommandLine.java:194)
        at org.apache.hadoop.hbase.master.HMasterCommandLine.run(
        
        
        
        
        
        
        
        
2181
60000
60010
60020
60030
9000
50070
8088
        
        
 mv /home/eip/hadoop/HBase/HFiles ~/RGA/


New hbase.root file 
hbase zkcli 
rmr /hbase

*************************
Structure Difference Between HBase & Phoenix Created Tables
HBase Command> create 'test_hbase','s','m' 
=> Hbase::Table - test_hbase 
hbase(main):003:0> desc 'test_hbase' 
DESCRIPTION                                                                                           
'test_hbase', {NAME => 'm', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETE true  D_CELLS => 'false', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERS  IONS => '0', BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}, {NAME => 's', BL  OOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'false', DATA_BLOCK  _ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOCKCACHE => 'tr  ue', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'} 
1 row(s) in 0.1730 seconds
  
Phoenix> create table test_phoenix (sdp_id Integer primary key, s varbinary, m.lpi varbinary, m.rr varbinary, m.de varbinary); 
No rows affected (0.475 seconds) 
HBase> desc 'TEST_PHOENIX' 
DESCRIPTION                                                                                           
'TEST_PHOENIX', {TABLE_ATTRIBUTES => {coprocessor$1 => '|org.apache.phoenix.coprocessor.ScanRegionOb true  server|805306366|', coprocessor$2 => '|org.apache.phoenix.coprocessor.UngroupedAggregateRegionObserver|805306366|', coprocessor$3 => '|org.apache.phoenix.coprocessor.GroupedAggregateRegionObserver|805  306366|', coprocessor$4 => '|org.apache.phoenix.coprocessor.ServerCachingEndpointImpl|805306366|', coprocessor$5 => '|org.apache.phoenix.hbase.index.Indexer|805306366|org.apache.hadoop.hbase.index.codec.class=org.apache.phoenix.index.PhoenixIndexCodec,index.builder=org.apache.phoenix.index.PhoenixIn  dexBuilder', coprocessor$6 => '|org.apache.hadoop.hbase.regionserver.LocalIndexSplitter|805306366|'}  , {NAME => '0', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => '  FALSE', DATA_BLOCK_ENCODING => 'FAST_DIFF', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS =>   '0', BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}, {NAME => 'M', BLOOMFILT  ER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODI  NG => 'FAST_DIFF', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOCKCACHE => 'true  ', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'} 
1 row(s) in 0.1830 seconds

Statistical Analysis
Execution time taken in inserting 10,000 records having SDP, Interval Data and Register Reads data = 24.0 seconds.
Execution time for fetching the count of above inserted data = 346.0 milli seconds.
*************************



SparkML
--------
Regression refers to predicting a numeric quantity like size or income or temperature, while  
Classification refers to predicting a label or category,like spam or picture of a cat




URLs

R 
http://www.toptal.com/machine-learning/machine-learning-theory-an-introductory-primer
https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm

decision tree
https://www.youtube.com/watch?v=eKD5gxPPeY0

Quick-R good for quick ref
http://www.statmethods.net/index.html

training Tutorial
https://www.kaggle.com/c/bike-sharing-demand/forums/t/11525/tutorial-0-433-score-with-randomforest-in-r/80074



*************************

netezza all tables with data
 select tablename, reltuples from _v_table;
 
 
**************************
Ports:
4040
spark: http://ind-phoenix.emeter.com:4040/jobs/


D:/work/codebase/P4-WS/Perforce-WS/EnergyIP/core/sandbox/RaviGupta/em-revenueprotection2.0/trunk/com/emeter/test/sparkML/data/rp_model_input_v.csv
D:\work\codebase\P4-WS\Perforce-WS\EnergyIP\core\sandbox\RaviGupta\em-revenueprotection2.0\trunk\output\com\emeter\test\sparkML\data

********************************************************************************************************
----Working Model query
select distinct
(case when ivst.DISPOSITION_GROUP = 'Clean' then 'Clean' else 'Theft' end) as Target,
rp.*
from  rp_model_input_v rp, 
sdp_d sd
,  
(select *, rank() over (partition by ESI_ID order by ESI_ID, DISPOSITION_GROUP desc) 
from  x_cnp_investigated_sdps 
where DISPOSITION_DESCR not in ('Unable to complete', 'Distributed Generation') ) ivst    
where
rank = 1 
and 
DISPOSITION_DESCR not in ('Unable to complete', 'Distributed Generation')
and
--ESI_ID = udc_id
sd.wid = ivst.sdp_wid -- used instead above line
and
rp.sdp_wid = sd.wid 
and cast(CREATION_DATE as date) Between ANALYSIS_START_DATE and ANALYSIS_END_DATE + interval '15 days' 
and rp.batch_id in (1953, 1954, 1955 , 1956, 1957 , 1958 , 1959 , 2002 , 2003 , 2004 , 2005 )
order by SDP_WID, ANALYSIS_END_DATE;


-is the data masked ??
as ESI_ID = udc_id


----Working Model query

--0 - clean
--1 - thefty

we have seen example of training sample with 0 or 1


select distinct
(case when ivst.DISPOSITION_GROUP = 'Clean' then '0' else '1' end) as Target,
rp.*
from  rp_model_input_v rp, 
sdp_d sd
,  
(select *, rank() over (partition by ESI_ID order by ESI_ID, DISPOSITION_GROUP desc) 
from  x_cnp_investigated_sdps 
where DISPOSITION_DESCR not in ('Unable to complete', 'Distributed Generation') ) ivst    
where
rank = 1 
and 
DISPOSITION_DESCR not in ('Unable to complete', 'Distributed Generation')
and
--ESI_ID = udc_id
sd.wid = ivst.sdp_wid
and
rp.sdp_wid = sd.wid 
and cast(CREATION_DATE as date) Between ANALYSIS_START_DATE and ANALYSIS_END_DATE + interval '15 days' 
and rp.batch_id in (1953, 1954, 1955 , 1956, 1957 , 1958 , 1959 , 2002 , 2003 , 2004 , 2005 )
order by SDP_WID, ANALYSIS_END_DATE;



Final working query:
-------------------
select distinct
(case when DISPOSITION_GROUP = 'Clean' then '0' else '1' end) as Target,
rp.*    
from 
rp_model_input_v rp, 
sdp_d sd, 
(select rank() over (partition by ESI_ID order by ESI_ID, DISPOSITION_GROUP desc) , *
from  x_cnp_investigated_sdps  
where DISPOSITION_DESCR not in ('Unable to complete', 'Distributed Generation') ) ivst 
where
rank = 1 
and DISPOSITION_DESCR not in ('Unable to complete', 'Distributed Generation')
and ESI_ID = udc_id
and rp.sdp_wid = wid 
and cast(CREATION_DATE as date) Between ANALYSIS_START_DATE and ANALYSIS_END_DATE + interval '15 days' 
and rp.batch_id in (1953, 1954, 1955 , 1956, 1957 , 1958 , 1959 , 2002 , 2003 , 2004 , 2005 )
order by SDP_WID, ANALYSIS_END_DATE;


libSvm format Query:
-------------------

select 
	(case when DISPOSITION_GROUP = 'Clean' then '0' else '1' end) as Target,
	'1:' ||NVL(ei_actual_value,0.0) ei_actual_value,
	'2:' ||NVL(uwnaa_actual_value,0.0) uwnaa_actual_value,
	'3:' ||NVL(rr_actual_value,0.0) rr_actual_value,
	'4:' ||NVL(ntb_actual_value,0.0) ntb_actual_value,
	'5:' ||NVL(oupctn_actual_value,0.0) oupctn_actual_value,
	'6:' ||NVL(twudz_actual_value,0.0) twudz_actual_value,
	'7:' ||NVL(rdlsvzu_actual_value,0.0) rdlsvzu_actual_value,
	'8:' ||NVL(iwnaa_actual_value,0.0) iwnaa_actual_value,--
	'9:' ||NVL(eu_actual_value,0.0) eu_actual_value,
	'10:'||NVL(sdplv_actual_value,0.0) sdplv_actual_value,
	'11:'||NVL(nufxd_actual_value,0.0) nufxd_actual_value
	from 
	rp_model_input_v rp, 
	sdp_d sd, 
	(select rank() over (partition by ESI_ID order by ESI_ID, DISPOSITION_GROUP desc) , *
	from  x_cnp_investigated_sdps  
	where DISPOSITION_DESCR not in ('Unable to complete', 'Distributed Generation') ) ivst 
	where
	rank = 1 
	and DISPOSITION_DESCR not in ('Unable to complete', 'Distributed Generation')
	and ESI_ID = udc_id
	and rp.sdp_wid = wid 
	and cast(CREATION_DATE as date) Between ANALYSIS_START_DATE and ANALYSIS_END_DATE + interval '15 days' 
	and rp.batch_id in (1953, 1954, 1955 , 1956, 1957 , 1958 , 1959 , 2002 , 2003 , 2004 , 2005 )
	order by Target;


Total records: 332740
Theft: 168626
Clean: 164114


[(1.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2880.0])), 
(0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0])), 


(0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,6.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,6.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,6.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,7.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,7.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,7.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,7.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,7.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,7.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,15.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,15.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,15.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,15.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,15.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,15.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,15.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,15.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,7.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,7.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,7.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,7.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,7.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,7.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,7.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,7.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,8.0,0.0,0.0,0.0,0.0])), (1.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0])), (1.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0])), (1.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0])), (1.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0])), (1.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0])), (1.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0])), (1.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0])), (1.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0])), (1.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0])), (1.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0])), (1.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0])), (1.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0])), (1.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0])), (1.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0])), (1.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0])), (1.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0])), (1.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0])), (1.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0])), (1.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0])), (1.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2880.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2880.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2880.0])), (1.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,0.0,3.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,4.287,0.0,0.0,0.0,0.0,0.0,1170.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,4.287,0.0,0.0,0.0,0.0,0.0,1170.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,4.287,0.0,0.0,0.0,0.0,0.0,1170.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,2.627,0.0,0.0,0.0,0.0,0.0,720.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,2.021,0.0,0.0,0.0,0.0,0.0,390.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,4.287,0.0,0.0,0.0,0.0,0.0,1170.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,4.287,0.0,0.0,0.0,0.0,0.0,1170.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,2.266,0.0,0.0,0.0,0.0,0.0,780.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,4.287,0.0,0.0,0.0,0.0,0.0,1170.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,4.287,0.0,0.0,0.0,0.0,0.0,1170.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0])), (1.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,5.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,5.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,5.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,5.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,5.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])), (0.0,(11,[0,1,2,3,4,5,6,7,8,9,10],[0.0,0.0,5.0,0.0,0.0,...


********************************************************************************************************



TARGET,
EI_ACTUAL_VALUE,
UWNAA_ACTUAL_VALUE,
RR_ACTUAL_VALUE,
NTB_ACTUAL_VALUE,
OUPCTN_ACTUAL_VALUE,
TWUDZ_ACTUAL_VALUE,
RDLSVZU_ACTUAL_VALUE,
IWNAA_ACTUAL_VALUE,
EU_ACTUAL_VALUE,
SDPLV_ACTUAL_VALUE,
NUFXD_ACTUAL_VALUE



***********************************************************************************************************
libsvmFormat_NoOrderBy.csv

(gini,10,100)
totalTrainingDataCount::265870
totalTestDataCount::66870
wrongPrdCount::21615
correctPrdCount::45255
testErr0.3232391206819201

(gini,20,300)
totalTrainingDataCount::265972
totalTestDataCount::66768
wrongPrdCount::20244
correctPrdCount::46524
testErr0.30319913731128684


libsvmFormat_NoOrderBy2.csv (removing feature with 0 value)

(gini,20,300)
totalTrainingDataCount::266660
totalTestDataCount::66080
wrongPrdCount::20032
correctPrdCount::46048
testErr0.30314769975786926


1 & 2 
-------------
1)

(gini,15,300)
totalTrainingDataCount::265756
totalTestDataCount::66984
wrongPrdCount::20961
correctPrdCount::46023
testErr0.31292547474023646

(gini,15,300)
totalTrainingDataCount2::265775
totalTestDataCount2::66965
wrongPrdCount2::20924
correctPrdCount2::46041
testErr20.31246173374150676

2) 

(1= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:15 maxBins:300 seed:12345)
totalTrainingDataCount::266417
totalTestDataCount::66323
wrongPrdCount::0
correctPrdCount::0
testErr0.3128175745970478

(2= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:15 maxBins:300 seed:12345)
totalTrainingDataCount2::266185
totalTestDataCount2::66555
wrongPrdCount2::0
correctPrdCount2::0
testErr20.3098189467357824


3)

(1= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:15 maxBins:300 seed:12345)
totalTrainingDataCount::266318
totalTestDataCount::66422
wrongPrdCount::20445
correctPrdCount::45977
testErr0.30780464303995664

(2= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:15 maxBins:300 seed:12345)
totalTrainingDataCount2::266661
totalTestDataCount2::66079
wrongPrdCount2::20593
correctPrdCount2::45486
testErr20.3116421253348265

4) 
(1= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:4 maxBins:100 seed:12345)
totalTrainingDataCount::265997
totalTestDataCount::66743
wrongPrdCount::23012
correctPrdCount::43731
testErr0.3447852209220443

(2= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:4 maxBins:100 seed:12345)
totalTrainingDataCount2::266294
totalTestDataCount2::66446
wrongPrdCount2::23041
correctPrdCount2::43405
testErr0.34676278481774675





# Test Metrics
tp = sum((testing_data[,1] == prediction) & (testing_data[,1] == 'Theft'))
fp = sum((testing_data[,1] != prediction) & (prediction == 'Theft'))
tn = sum((testing_data[,1] == prediction) & (testing_data[,1] != 'Theft'))
fn = sum((testing_data[,1] != prediction) & (prediction != 'Theft'))

precision = tp / (tp + fp);
recall = tp / (tp + fn);
f1 = 2 * (precision * recall) / (precision + recall);



1	numClasses	2	
Number of classes for classification.
Specifies number of target values e.g. 0 & 1, hence 2
2	categoricalFeaturesInfo	Empty Map	
Map storing  categorical features. E.g., an entry (n -> k) indicates that feature n is categorical with k categories indexed from 0: {0, 1, ..., k-1}.
Empty categoricalFeaturesInfo indicates all features are continuous.
3	numTrees	50	Number of trees in the random forest
4	featureSubsetStrategy	auto	
Number of features to consider for splits at each node. Supported:
auto 
all
sqrt
log2
onethird
A strategy for choosing which features to evaluate at each level of the tree, which is here set to "auto". The random decision forest 
implementation will actually not even consider every feature as the basis of a decision rule, but only a subset of all features. This 
parameter controls how it picks the subset. Checking only a few features is of course faster, and speed is helpful now that so many 
more trees are being constructed.
5	impurity	gini	
Criterion used for information gain calculation. Supported values
gini (recommended) 
entropy
Good rules divide the training data’s target values into relatively homogeneous, or "pure", subsets. Picking a best rule means minimizes the impurity of the two subsets it induces
6	maxDepth	5	Maximum depth of the tree. E.g., depth 0 means 1 leaf node. It simply limits the number of levels in the decision tree
7	maxBins	100	
Maximum number of bins used for splitting features (suggested value: 100)
The decision tree algorithm is responsible for coming up with potential decision rules to try at each level. The set of decision rules to try is really a set of values to plug into the decision rule. These are referred to as bins in the Spark MLlib.
8	seed	12345	Random seed for bootstrapping and choosing feature subsets

-----------------------------------------------------------------------------------------------------

1)

impurity:gini
maxDepth = 5
maxBins = 100

libsvm 1
	
	(1= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:5 maxBins:100 seed:12345)
	totalTrainingDataCount::266074
	totalTestDataCount::66666
	wrongPrdCount::0
	correctPrdCount::0
	testErr0.34120841208412084
	precision::0.7584336277939171
	recall::0.6367120424059926
	f1::0.692262943571844
	-----------
	(1= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:entropy maxDepth:5 maxBins:100 seed:12345)
	totalTrainingDataCount::266074
	totalTestDataCount::66666
	wrongPrdCount::0
	correctPrdCount::0
	testErr0.34119341193411934
	precision::0.7584336277939171
	recall::0.6367278881091035
	f1::0.6922723091076357
	
	
libsvm 2

	(2= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:5 maxBins:100 seed:12345)
	totalTrainingDataCount::266522
	totalTestDataCount::66218
	wrongPrdCount::0
	correctPrdCount::0
	testErr0.3406928629677731
	precision::0.734982332155477
	recall::0.6448016255503165
	f1::0.6869449378330372
	-----------
	(2= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:entropy maxDepth:5 maxBins:100 seed:12345)
	totalTrainingDataCount::266522
	totalTestDataCount::66218
	wrongPrdCount::0
	correctPrdCount::0
	testErr0.3416291642755746
	precision::0.7326068236481872
	recall::0.6443626106714723
	f1::0.6856571158602675
	
	
	
2)

(1= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:5 maxBins:100 seed:12345)
totalTrainingDataCount::266160
totalTestDataCount::66580
wrongPrdCount::22836
correctPrdCount::43744
testErr::0.34298588164613997
precision::0.7546975319549979
recall::0.6385178909598326
f1::0.691763626056205

-----------
(1= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:5 maxBins:200 seed:12345)
totalTrainingDataCount::266160
totalTestDataCount::66580
wrongPrdCount::45668
correctPrdCount::87492
testErr::0.342925803544608
precision::0.7546975319549979
recall::0.6385497134313481
f1::0.6917823011716431

-----------
(1= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:5 maxBins:300 seed:12345)
totalTrainingDataCount::266160
totalTestDataCount::66580
wrongPrdCount::68196
correctPrdCount::131544
testErr::0.3383598678281766
precision::0.7474131668335592
recall::0.6419308600337268
f1::0.6906677794812711

-----------
(1= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:10 maxBins:100 seed:12345)
totalTrainingDataCount::266160
totalTestDataCount::66580
wrongPrdCount::89693
correctPrdCount::176627
testErr::0.3228747371583058
precision::0.7400821699946987
recall::0.6488731948846728
f1::0.6914829580046986

-----------
(1= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:10 maxBins:200 seed:12345)
totalTrainingDataCount::266160
totalTestDataCount::66580
wrongPrdCount::111232
correctPrdCount::221668
testErr::0.3235055572243917
precision::0.7336219591211639
recall::0.6535978924830496
f1::0.6913017656233522

-----------
(1= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:10 maxBins:300 seed:12345)
totalTrainingDataCount::266160
totalTestDataCount::66580
wrongPrdCount::132762
correctPrdCount::266718
testErr::0.3233703814959447
precision::0.7279603777659971
recall::0.6572432681569198
f1::0.6907967058560489

-----------
(1= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:15 maxBins:100 seed:12345)
totalTrainingDataCount::266160
totalTestDataCount::66580
wrongPrdCount::153532
correctPrdCount::312528
testErr::0.3119555422048663
precision::0.7241940777017646
recall::0.6617545289576477
f1::0.6915677947374553

-----------
(1= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:15 maxBins:200 seed:12345)
totalTrainingDataCount::266160
totalTestDataCount::66580
wrongPrdCount::174393
correctPrdCount::358247
testErr::0.31332231901471913
precision::0.7205005301289981
recall::0.6652695770999684
f1::0.6917844171681556

-----------
(1= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:15 maxBins:300 seed:12345)
totalTrainingDataCount::266160
totalTestDataCount::66580
wrongPrdCount::195173
correctPrdCount::404047
testErr::0.3121057374586963
precision::0.717781573763196
recall::0.6681715025512147
f1::0.6920886440402549

-----------
(1= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:20 maxBins:100 seed:12345)
totalTrainingDataCount::266160
totalTestDataCount::66580
wrongPrdCount::216018
correctPrdCount::449782
testErr::0.31308200660859115
precision::0.7159303763915886
recall::0.6703037622845042
f1::0.6923661904436393

-----------
(1= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:20 maxBins:200 seed:12345)
totalTrainingDataCount::266160
totalTestDataCount::66580
wrongPrdCount::236492
correctPrdCount::495888
testErr::0.30750976269149893
precision::0.7148441474294098
recall::0.6725561618024173
f1::0.6930556853461254

-----------
(1= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:20 maxBins:300 seed:12345)
totalTrainingDataCount::266160
totalTestDataCount::66580
wrongPrdCount::256800
correctPrdCount::542160
testErr::0.3050165214779213
precision::0.7130333195892482
recall::0.6750092936802974
f1::0.6935004917359712

-----------
(1= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:entropy maxDepth:5 maxBins:100 seed:12345)
totalTrainingDataCount::266160
totalTestDataCount::66580
wrongPrdCount::22765
correctPrdCount::43815
testErr::0.3419194953439471
precision::0.7570831124462508
recall::0.6390870894761703
f1::0.6930989390242258

-----------
(1= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:entropy maxDepth:5 maxBins:200 seed:12345)
totalTrainingDataCount::266160
totalTestDataCount::66580
wrongPrdCount::45604
correctPrdCount::87556
testErr::0.34303094022228897
precision::0.7558903222006244
recall::0.6387789641354936
f1::0.6924176817341805

-----------
(1= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:entropy maxDepth:5 maxBins:300 seed:12345)
totalTrainingDataCount::266160
totalTestDataCount::66580
wrongPrdCount::68369
correctPrdCount::131371
testErr::0.3419194953439471
precision::0.7562879189491665
recall::0.6388817475389987
f1::0.6926448573342384

-----------
(1= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:entropy maxDepth:10 maxBins:100 seed:12345)
totalTrainingDataCount::266160
totalTestDataCount::66580
wrongPrdCount::89875
correctPrdCount::176445
testErr::0.3230099128867528
precision::0.7443673793956529
recall::0.6470083774391532
f1::0.6922816183846968

-----------
(1= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:entropy maxDepth:10 maxBins:200 seed:12345)
totalTrainingDataCount::266160
totalTestDataCount::66580
wrongPrdCount::111444
correctPrdCount::221456
testErr::0.32395614298588166
precision::0.7366613653766861
recall::0.6520490093847758
f1::0.6917775257903034

-----------
(1= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:entropy maxDepth:10 maxBins:300 seed:12345)
totalTrainingDataCount::266160
totalTestDataCount::66580
wrongPrdCount::132984
correctPrdCount::266496
testErr::0.3235205767497747
precision::0.7309644420883156
recall::0.6557532255933771
f1::0.6913192236092607

-----------
(1= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:entropy maxDepth:15 maxBins:100 seed:12345)
totalTrainingDataCount::266160
totalTestDataCount::66580
wrongPrdCount::153883
correctPrdCount::312177
testErr::0.31389306097927305
precision::0.7264828886140072
recall::0.6601935437002711
f1::0.6917537523461553

-----------
(1= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:entropy maxDepth:15 maxBins:200 seed:12345)
totalTrainingDataCount::266160
totalTestDataCount::66580
wrongPrdCount::174781
correctPrdCount::357859
testErr::0.31387804145389003
precision::0.7223559816221947
recall::0.6638326809414742
f1::0.6918589378555775

-----------
(1= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:entropy maxDepth:15 maxBins:300 seed:12345)
totalTrainingDataCount::266160
totalTestDataCount::66580
wrongPrdCount::195599
correctPrdCount::403621
testErr::0.3126764794232502
precision::0.7193425091463614
recall::0.6668213738605512
f1::0.6920869402321324

-----------
(1= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:entropy maxDepth:20 maxBins:100 seed:12345)
totalTrainingDataCount::266160
totalTestDataCount::66580
wrongPrdCount::216216
correctPrdCount::449584
testErr::0.30965755482126767
precision::0.7169641279377982
recall::0.6696098494850805
f1::0.6924783669846905

-----------
(1= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:entropy maxDepth:20 maxBins:200 seed:12345)
totalTrainingDataCount::266160
totalTestDataCount::66580
wrongPrdCount::236658
correctPrdCount::495722
testErr::0.307029137879243
precision::0.7157169860827751
recall::0.6719926796649539
f1::0.6931659946323692

-----------
(1= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:entropy maxDepth:20 maxBins:300 seed:12345)
totalTrainingDataCount::266160
totalTestDataCount::66580
wrongPrdCount::256968
correctPrdCount::541992
testErr::0.3050465605286873
precision::0.7146212522824998
recall::0.6742223210192144
f1::0.693834220967223

-----------
(2= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:5 maxBins:100 seed:12345)
totalTrainingDataCount::266325
totalTestDataCount::66415
wrongPrdCount::22431
correctPrdCount::43984
testErr::0.3377399683806369
precision::0.7375905474409215
recall::0.6463823919660744
f1::0.6889810180113976
-----------
(2= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:5 maxBins:200 seed:12345)
totalTrainingDataCount::266325
totalTestDataCount::66415
wrongPrdCount::45379
correctPrdCount::87451
testErr::0.34552435443800344
precision::0.7438546490915569
recall::0.6405317313222982
f1::0.6883374655741984
-----------
(2= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:5 maxBins:300 seed:12345)
totalTrainingDataCount::266325
totalTestDataCount::66415
wrongPrdCount::68265
correctPrdCount::130980
testErr::0.34459083038470223
precision::0.7466057079523414
recall::0.6388044536641124
f1::0.6885109761495184
-----------
(2= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:10 maxBins:100 seed:12345)
totalTrainingDataCount::266325
totalTestDataCount::66415
wrongPrdCount::89696
correctPrdCount::175964
testErr::0.32268312881126254
precision::0.7373753117206983
recall::0.6465554268459346
f1::0.6889853605087415
-----------
(2= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:10 maxBins:200 seed:12345)
totalTrainingDataCount::266325
totalTestDataCount::66415
wrongPrdCount::111018
correctPrdCount::221057
testErr::0.3210419332982007
precision::0.7329889561809761
recall::0.6514580628819302
f1::0.6898228085762661
-----------
(2= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:10 maxBins:300 seed:12345)
totalTrainingDataCount::266325
totalTestDataCount::66415
wrongPrdCount::132515
correctPrdCount::265975
testErr::0.32367688022284125
precision::0.7314649883228437
recall::0.653906889306646
f1::0.690514948187557
-----------
(2= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:15 maxBins:100 seed:12345)
totalTrainingDataCount::266325
totalTestDataCount::66415
wrongPrdCount::153262
correctPrdCount::311643
testErr::0.3123842505458104
precision::0.7264873530459566
recall::0.6586623498469631
f1::0.6909142976993321
-----------
(2= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:15 maxBins:200 seed:12345)
totalTrainingDataCount::266325
totalTestDataCount::66415
wrongPrdCount::174337
correctPrdCount::356983
testErr::0.31732289392456525
precision::0.7235111625697661
recall::0.661356798067823
f1::0.6910392029333527
-----------
(2= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:15 maxBins:300 seed:12345)
totalTrainingDataCount::266325
totalTestDataCount::66415
wrongPrdCount::195165
correctPrdCount::402570
testErr::0.31360385455092976
precision::0.7207444352082756
recall::0.6641175666023312
f1::0.6912732674113082
-----------
(2= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:20 maxBins:100 seed:12345)
totalTrainingDataCount::266325
totalTestDataCount::66415
wrongPrdCount::215869
correctPrdCount::448281
testErr::0.31173680644432733
precision::0.7188635553972212
recall::0.666483170159064
f1::0.691683103550224
-----------
(2= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:20 maxBins:200 seed:12345)
totalTrainingDataCount::266325
totalTestDataCount::66415
wrongPrdCount::236284
correctPrdCount::494281
testErr::0.3073853798087781
precision::0.7177321846897907
recall::0.6687984427890975
f1::0.6924018235831692
-----------
(2= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:gini maxDepth:20 maxBins:300 seed:12345)
totalTrainingDataCount::266325
totalTestDataCount::66415
wrongPrdCount::256626
correctPrdCount::540354
testErr::0.3062862305202138
precision::0.7166260934964177
recall::0.6709114070642733
f1::0.6930156778211345
-----------
(2= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:entropy maxDepth:5 maxBins:100 seed:12345)
totalTrainingDataCount::266325
totalTestDataCount::66415
wrongPrdCount::22742
correctPrdCount::43673
testErr::0.34242264548671236
precision::0.7589656810355064
recall::0.6361351647257888
f1::0.6921431665583713
-----------
(2= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:entropy maxDepth:5 maxBins:200 seed:12345)
totalTrainingDataCount::266325
totalTestDataCount::66415
wrongPrdCount::45636
correctPrdCount::87194
testErr::0.34471128510125726
precision::0.7554922218263864
recall::0.6357232075943042
f1::0.6904522885747619
-----------
(2= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:entropy maxDepth:5 maxBins:300 seed:12345)
totalTrainingDataCount::266325
totalTestDataCount::66415
wrongPrdCount::68378
correctPrdCount::130867
testErr::0.34242264548671236
precision::0.7566995210386731
recall::0.6358495900480633
f1::0.6910306809452803
-----------
(2= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:entropy maxDepth:10 maxBins:100 seed:12345)
totalTrainingDataCount::266325
totalTestDataCount::66415
wrongPrdCount::90025
correctPrdCount::175635
testErr::0.3259354061582474
precision::0.7471351383446146
recall::0.6427362869602417
f1::0.6910148031452823
-----------
(2= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:entropy maxDepth:10 maxBins:200 seed:12345)
totalTrainingDataCount::266325
totalTestDataCount::66415
wrongPrdCount::111567
correctPrdCount::220508
testErr::0.3243544380034631
precision::0.7398824367652298
recall::0.6477706907038037
f1::0.6907693970714185
-----------
(2= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:entropy maxDepth:10 maxBins:300 seed:12345)
totalTrainingDataCount::266325
totalTestDataCount::66415
wrongPrdCount::133127
correctPrdCount::265363
testErr::0.32462546111571183
precision::0.7361803823773898
recall::0.6508729488654508
f1::0.6909033496866707
-----------
(2= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:entropy maxDepth:15 maxBins:100 seed:12345)
totalTrainingDataCount::266325
totalTestDataCount::66415
wrongPrdCount::154179
correctPrdCount::310726
testErr::0.31697658661446965
precision::0.7308175140380342
recall::0.6551342637828056
f1::0.6909094554093201
-----------
(2= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:entropy maxDepth:15 maxBins:200 seed:12345)
totalTrainingDataCount::266325
totalTestDataCount::66415
wrongPrdCount::175086
correctPrdCount::356234
testErr::0.31479334487691035
precision::0.726858449115307
recall::0.6587119556078695
f1::0.6911093782527848
-----------
(2= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:entropy maxDepth:15 maxBins:300 seed:12345)
totalTrainingDataCount::266325
totalTestDataCount::66415
wrongPrdCount::196039
correctPrdCount::401696
testErr::0.31548595949710156
precision::0.7241948039953028
recall::0.6613348756367273
f1::0.6913388975748157
-----------
(2= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:entropy maxDepth:20 maxBins:100 seed:12345)
totalTrainingDataCount::266325
totalTestDataCount::66415
wrongPrdCount::216601
correctPrdCount::447549
testErr::0.30959873522547615
precision::0.7216156038475241
recall::0.6643062703095663
f1::0.6917760363378154
-----------
(2= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:entropy maxDepth:20 maxBins:200 seed:12345)
totalTrainingDataCount::266325
totalTestDataCount::66415
wrongPrdCount::237037
correctPrdCount::493528
testErr::0.307701573439735
precision::0.719750947307057
recall::0.6669075704643608
f1::0.6923223698642404
-----------
(2= numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:entropy maxDepth:20 maxBins:300 seed:12345)
totalTrainingDataCount::266325
totalTestDataCount::66415
wrongPrdCount::257470
correctPrdCount::539510
testErr::0.30765640292102686
precision::0.7187017575109844
recall::0.6689471114877312
f1::0.6929324491937793	


// Save and load model
model.save(sc.sc(), "myModelPath");
RandomForestModel sameModel = RandomForestModel.load(sc.sc(), "myModelPath");



--------------------------------

(1 =  numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:variance maxDepth:20 maxBins:300 seed:12345)
totalTrainingDataCount::266054
totalTestDataCount::66686
wrongPrdCount::66686
correctPrdCount::0
testMeanSquareErr::0.19621579196682482
precision::0.0
recall::0.0
f1::NaN


[13079.953890533008, (a+b) 0.09859021156727306]
[0.3139907826151479,0.0]
[13080.052480744576, (a+b) 0.09859021156727306]
[0.3139907826151479,0.0]
[13080.151070956144, (a+b) 0.09859021156727306]
[0.4582026832168264,0.0]
[13080.249661167712, (a+b) 0.20994969890709936]
[0.4582026832168264,0.0]
[13080.45961086662, (a+b) 0.20994969890709936]
[0.33841731285992127,0.0]
[13080.669560565526, (a+b) 0.11452627764332983]
[0.5190717455975009,1.0]
[13080.784086843169, (a+b) 0.2312919858826349]
[0.5190717455975009,1.0]
[13081.015378829052, (a+b) 0.2312919858826349]
[0.5190717455975009,1.0]
[13081.246670814935, (a+b) 0.2312919858826349]
[0.8597264111574096,1.0]
[13081.477962800818, (a+b) 0.019676679726780114]
[0.8597264111574096,1.0]
[13081.497639480545, (a+b) 0.019676679726780114]
[0.8597264111574096,1.0]
[13081.517316160272, (a+b) 0.019676679726780114]
[0.8597264111574096,1.0]
[13081.53699284, (a+b) 0.019676679726780114]
[0.8597264111574096,1.0]
[13081.556669519727, (a+b) 0.019676679726780114]
[0.30195167651145327,1.0]
[13081.576346199454, (a+b) 0.4872714619251708]
[0.30195167651145327,1.0]
[13082.063617661379, (a+b) 0.4872714619251708]
[0.30195167651145327,1.0]
[13082.550889123304, (a+b) 0.4872714619251708]
[0.3364562178408073,1.0]
[13083.038160585229, (a+b) 0.44029035084212625]
[0.3364562178408073,1.0]
[13083.47845093607, (a+b) 0.44029035084212625]
[0.3364562178408073,1.0]
[13083.918741286912, (a+b) 0.44029035084212625]
[0.30195167651145327,1.0]
[13084.359031637754, (a+b) 0.4872714619251708]


--------------------------------------------------------------------------------------------------
[0.7618608463138437,1.0]
[0.7618608463138437,1.0]
[0.7618608463138437,1.0]
[0.7618608463138437,1.0]
[0.7618608463138437,1.0]
[0.9543863433731758,1.0]
[0.9543863433731758,1.0]
[0.9543863433731758,1.0]
[0.9543863433731758,1.0]
[0.9543863433731758,1.0]
[0.9543863433731758,1.0]
[0.9543863433731758,1.0]
[0.9543863433731758,1.0]
[0.9543863433731758,1.0]
[0.9348173602268648,1.0]
[0.9389915772443903,1.0]
[0.9518233998154568,1.0]
[0.8432217988938703,1.0]
[0.8432217988938703,1.0]
[0.7618608463138437,1.0]
[0.7618608463138437,1.0]
[0.7618608463138437,1.0]
[0.7618608463138437,1.0]
[0.7279861196817808,1.0]
[0.7279861196817808,1.0]
[0.7279861196817808,1.0]
[0.7279861196817808,1.0]
[0.7279861196817808,1.0]
[0.7279861196817808,1.0]
[0.7279861196817808,1.0]
[0.7279861196817808,1.0]
[0.7279861196817808,1.0]
[0.7279861196817808,1.0]
[0.7279861196817808,1.0]
[0.7279861196817808,1.0]
[0.7279861196817808,1.0]
[0.9543863433731758,1.0]
[0.9543863433731758,1.0]
[0.9543863433731758,1.0]
[0.9543863433731758,1.0]
[0.9543863433731758,1.0]
[0.9543863433731758,1.0]
[0.9543863433731758,1.0]
[0.9543863433731758,1.0]
[0.6509410508942544,1.0]
[0.5874246266512233,1.0]
[0.5874246266512233,1.0]
[0.6314608158724232,1.0]
[0.6314608158724232,1.0]
[0.7425658373682148,1.0]
[0.7632689893965486,1.0]
[0.7229235960269945,1.0]
[0.7279861196817808,1.0]
[0.7279861196817808,1.0]
[0.7279861196817808,1.0]
[0.7279861196817808,1.0]
[0.7279861196817808,1.0]
[0.7279861196817808,1.0]
[0.7279861196817808,1.0]
[0.6672584149573322,0.0]
[0.6672584149573322,0.0]
[0.6672584149573322,0.0]
[0.33354701781590956,0.0]
[0.33354701781590956,0.0]
[0.33354701781590956,0.0]
[0.33354701781590956,0.0]
[0.33354701781590956,0.0]
[0.33354701781590956,0.0]
[0.33354701781590956,0.0]
[0.33354701781590956,0.0]
[0.33354701781590956,0.0]
[0.33354701781590956,0.0]
[0.33354701781590956,0.0]
[0.33354701781590956,0.0]
[0.33354701781590956,0.0]
[0.33354701781590956,0.0]
[0.33354701781590956,0.0]
[0.33354701781590956,0.0]
[0.33354701781590956,0.0]
[0.33354701781590956,0.0]
[0.33354701781590956,0.0]
[0.33354701781590956,0.0]
[0.33354701781590956,0.0]
[0.33354701781590956,0.0]
[0.33354701781590956,0.0]
[0.33354701781590956,0.0]
[0.33354701781590956,0.0]
[0.33354701781590956,0.0]
[0.33354701781590956,0.0]
[0.5271798632389771,0.0]
[0.5271798632389771,0.0]
[0.686085688598056,0.0]
[0.686085688598056,0.0]
[0.7549947794152289,0.0]
[0.686085688598056,0.0]
[0.686085688598056,0.0]
[0.686085688598056,0.0]
[0.686085688598056,0.0]
[0.686085688598056,0.0]
[0.686085688598056,0.0]
[0.686085688598056,0.0]
[0.5892839241225493,0.0]
[0.686085688598056,0.0]
[0.686085688598056,0.0]
[0.686085688598056,0.0]
[0.686085688598056,0.0]
[0.686085688598056,0.0]
[0.6455695903569088,0.0]
[0.6261654962882883,0.0]
[0.6619629382982278,0.0]
[0.653867098258982,0.0]
[0.6579975317898159,1.0]
[0.650715754303875,1.0]
[0.672480981771881,1.0]
[0.5614630156483525,1.0]
[0.5576935918141959,1.0]
[0.5331385164116931,1.0]
[0.43873178442694405,1.0]
[0.4448333144358593,1.0]
[0.45462239395922244,1.0]
[0.4570139368289062,1.0]
[0.6471877459996604,1.0]
[0.55028150741122,1.0]
[0.55028150741122,1.0]
[0.4960691237331956,1.0]
[0.4768528463909834,1.0]
[0.45651012581874206,1.0]
[0.4511158460496145,1.0]
[0.43728735951865644,1.0]
[0.4129035542610243,1.0]
[0.43308882127947934,1.0]
[0.43308882127947934,1.0]
[0.43308882127947934,1.0]
[0.43308882127947934,1.0]
[0.3715436113421656,0.0]
[0.6672584149573322,0.0]
[0.6672584149573322,0.0]
[0.6672584149573322,0.0]
[0.6672584149573322,0.0]
[0.6672584149573322,0.0]
[0.6672584149573322,0.0]
[0.6286359634330277,1.0]
[0.7130781694404714,1.0]
[0.6983918127766537,1.0]
[0.6014976091978602,1.0]
[0.6014976091978602,1.0]
[0.6364720954051858,1.0]
[0.2980110048766723,0.0]
[0.2980110048766723,0.0]
[0.2980110048766723,0.0]
[0.2980110048766723,0.0]
[0.2980110048766723,0.0]
[0.2980110048766723,0.0]
[0.2980110048766723,0.0]
[0.2980110048766723,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.6764682792777674,0.0]
[0.6764682792777674,0.0]
[0.6537420925614867,0.0]
[0.5961023985000268,0.0]
[0.6869898839398257,0.0]
[0.5277272276754387,0.0]
[0.6764682792777674,0.0]
[0.6764682792777674,0.0]
[0.6764682792777674,0.0]
[0.6764682792777674,0.0]
[0.6764682792777674,0.0]
[0.8049817362494417,0.0]
[0.6005325700504147,0.0]
[0.7602395666552606,0.0]
[0.7742581290285981,0.0]
[0.6470402920197739,0.0]
[0.5721440281097303,0.0]
[0.6746666333744663,0.0]
[0.62652499181379,0.0]
[0.6957973590485149,0.0]
[0.7767463210060681,0.0]
[0.7767463210060681,0.0]
[0.7767463210060681,0.0]
[0.7767463210060681,0.0]
[0.7767463210060681,0.0]
[0.7767463210060681,0.0]
[0.7767463210060681,0.0]
[0.7767463210060681,0.0]
[0.7767463210060681,0.0]
[0.7767463210060681,0.0]
[0.33354701781590956,1.0]
[0.33354701781590956,1.0]
[0.33354701781590956,1.0]
[0.3047303668922508,1.0]
[0.3047303668922508,1.0]
[0.7117634536234422,0.0]
[0.6847885695425916,0.0]
[0.6847885695425916,0.0]
[0.6735750996454154,0.0]
[0.6656520998500284,0.0]
[0.6306123281511434,0.0]
[0.6335797356278681,0.0]
[0.6170389976878667,0.0]
[0.6742276880340999,0.0]
[0.7150169884837255,0.0]
[0.7135534727251716,0.0]
[0.5271798632389771,1.0]
[0.5271798632389771,1.0]
[0.5271798632389771,1.0]
[0.5271798632389771,1.0]
[0.5271798632389771,1.0]
[0.5271798632389771,1.0]
[0.5271798632389771,1.0]
[0.5271798632389771,1.0]
[0.5271798632389771,1.0]
[0.5271798632389771,1.0]
[0.5271798632389771,0.0]
[0.20254549105468736,0.0]
[0.2682791022273338,0.0]
[0.2404953799434611,0.0]
[0.2663887737307716,0.0]
[0.438752951302255,0.0]
[0.5707557360151099,0.0]
[0.48597260101441025,0.0]
[0.2669173029560833,0.0]
[0.26669666803544834,0.0]
[0.16075529482466958,0.0]
[0.19683813203985864,0.0]
[0.22930392102920574,0.0]
[0.36932853133454685,0.0]
[0.3803827867666991,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.43308882127947934,0.0]
[0.32143857624077915,0.0]
[0.32143857624077915,0.0]
[0.32143857624077915,0.0]
[0.32143857624077915,0.0]
[0.32143857624077915,0.0]
[0.32143857624077915,0.0]
[0.32143857624077915,0.0]
[0.3096374956146848,0.0]
[0.3096374956146848,0.0]
[0.32143857624077915,0.0]
[0.32143857624077915,0.0]
[0.32143857624077915,0.0]
[0.32143857624077915,0.0]
[0.32143857624077915,0.0]
[0.32143857624077915,0.0]
[0.32143857624077915,0.0]
[0.32143857624077915,0.0]
[0.33354701781590956,0.0]
[0.32143857624077915,0.0]
[0.32143857624077915,0.0]
[0.32143857624077915,0.0]
[0.32143857624077915,0.0]
[0.3096374956146848,1.0]
[0.3096374956146848,1.0]
[0.3096374956146848,1.0]
[0.3096374956146848,1.0]
[0.39915081669176883,0.0]
[0.39915081669176883,0.0]
[0.39915081669176883,0.0]
[0.39915081669176883,0.0]
[0.39915081669176883,0.0]
[0.3665524037888044,0.0]
[0.39915081669176883,0.0]
[0.39915081669176883,0.0]
[0.39915081669176883,0.0]
[0.39915081669176883,0.0]
[0.39915081669176883,0.0]
[0.39915081669176883,0.0]
[0.33354701781590956,0.0]
[0.33354701781590956,0.0]
[0.33354701781590956,0.0]
[0.3096374956146848,0.0]
[0.3096374956146848,0.0]
[0.2980110048766723,0.0]
[0.2980110048766723,0.0]
[0.5243234483636713,1.0]
[0.5243234483636713,1.0]
[0.7374674005398689,1.0]
[0.7374674005398689,1.0]
[0.7374674005398689,1.0]
[0.7374674005398689,1.0]
[0.3047303668922508,1.0]
[0.3047303668922508,1.0]
[0.3047303668922508,1.0]
[0.3047303668922508,1.0]
[0.3047303668922508,1.0]
[0.3047303668922508,1.0]
[0.3047303668922508,1.0]
[0.3047303668922508,1.0]
[0.3324253031200465,1.0]
[0.3324253031200465,1.0]
[0.3324253031200465,1.0]
[0.3324253031200465,1.0]
[0.3047303668922508,1.0]
[0.3047303668922508,1.0]
[0.3047303668922508,1.0]
[0.3047303668922508,1.0]
[0.3047303668922508,1.0]
[0.3047303668922508,1.0]

(1 =  numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:variance maxDepth:20 maxBins:300 seed:12345)
totalTrainingDataCount::266192
totalTestDataCount::66548
wrongPrdCount::19442
correctPrdCount::47106
testMeanSquareErr::0.1956305210187858
precision::0.0
recall::0.0
f1::NaN


----------------------------------------------------------
[0.9630355158583018,1.0]-correctPred
[0.9630355158583018,1.0]-correctPred
[0.9630355158583018,1.0]-correctPred
[0.9630355158583018,1.0]-correctPred
[0.9630355158583018,1.0]-correctPred
[0.9630355158583018,1.0]-correctPred
[0.9630355158583018,1.0]-correctPred
[0.9630355158583018,1.0]-correctPred
[0.5265999337551293,1.0]-correctPred
[0.6218503127836511,1.0]-correctPred
[0.7687796281454893,1.0]-correctPred
[0.7687796281454893,1.0]-correctPred
[0.7687796281454893,1.0]-correctPred
[0.7564617755551499,1.0]-correctPred
[0.7564617755551499,1.0]-correctPred
[0.7564617755551499,1.0]-correctPred
[0.7564617755551499,1.0]-correctPred
[0.7564617755551499,1.0]-correctPred
[0.7564617755551499,1.0]-correctPred
[0.6597794340976649,0.0]-wrongPred
[0.3282702489864207,0.0]-correctPred
[0.3282702489864207,0.0]-correctPred
[0.3282702489864207,0.0]-correctPred
[0.3282702489864207,0.0]-correctPred
[0.3282702489864207,0.0]-correctPred
[0.3282702489864207,0.0]-correctPred
[0.3282702489864207,0.0]-correctPred
[0.3282702489864207,0.0]-correctPred
[0.3282702489864207,0.0]-correctPred
[0.3282702489864207,0.0]-correctPred
[0.3282702489864207,0.0]-correctPred
[0.3282702489864207,0.0]-correctPred
[0.3282702489864207,0.0]-correctPred
[0.3282702489864207,0.0]-correctPred
[0.3282702489864207,0.0]-correctPred
[0.3282702489864207,0.0]-correctPred
[0.3282702489864207,0.0]-correctPred
[0.3282702489864207,0.0]-correctPred
[0.3282702489864207,0.0]-correctPred
[0.3282702489864207,0.0]-correctPred
[0.3282702489864207,0.0]-correctPred
[0.3282702489864207,0.0]-correctPred
[0.3282702489864207,0.0]-correctPred
[0.3282702489864207,0.0]-correctPred
[0.5344488735709783,0.0]-wrongPred
[0.5344488735709783,0.0]-wrongPred
[0.5344488735709783,0.0]-wrongPred
[0.5344488735709783,0.0]-wrongPred
[0.5344488735709783,0.0]-wrongPred
[0.5344488735709783,0.0]-wrongPred
[0.6550051364994125,0.0]-wrongPred
[0.5792127148415059,0.0]-wrongPred
[0.6550051364994125,0.0]-wrongPred
[0.6550051364994125,0.0]-wrongPred
[0.6550051364994125,0.0]-wrongPred
[0.6550051364994125,0.0]-wrongPred
[0.6612621899095693,0.0]-wrongPred
[0.6407767659078716,0.0]-wrongPred
[0.6824504484788683,0.0]-wrongPred
[0.6759888137894421,1.0]-correctPred
[0.5909667692972395,1.0]-correctPred
[0.6495662677247793,1.0]-correctPred
[0.6657287668662518,1.0]-correctPred
[0.6028645780002327,1.0]-correctPred
[0.608032135991931,1.0]-correctPred
[0.6295259713168128,1.0]-correctPred
[0.5279325832965722,1.0]-correctPred
[0.5639811261772976,1.0]-correctPred
[0.5194088577045841,1.0]-correctPred
[0.4164549207944928,1.0]-wrongPred
[0.4281366346446866,1.0]-wrongPred
[0.4692815579754936,1.0]-wrongPred
[0.47270742385939635,1.0]-wrongPred
[0.47270742385939635,1.0]-wrongPred
[0.5921420493587134,1.0]-correctPred
[0.3995656861188763,1.0]-wrongPred
[0.6041592744658041,1.0]-correctPred
[0.6171631816262154,1.0]-correctPred
[0.6233236291033082,1.0]-correctPred
[0.5712045731635652,1.0]-correctPred
[0.4927860749118437,1.0]-wrongPred
[0.4420829024570112,1.0]-wrongPred
[0.3949907776047943,1.0]-wrongPred
[0.42053249114756364,1.0]-wrongPred
[0.40353940143741407,0.0]-correctPred
[0.6597794340976649,0.0]-wrongPred
[0.6597794340976649,0.0]-wrongPred
[0.6597794340976649,0.0]-wrongPred
[0.6597794340976649,0.0]-wrongPred
[0.6597794340976649,0.0]-wrongPred
[0.5794203902050848,1.0]-correctPred
[0.617179274785686,1.0]-correctPred
[0.8021687630539519,1.0]-correctPred
[0.7907901880726999,1.0]-correctPred
[0.7895231324783751,1.0]-correctPred
[0.6802485429077868,1.0]-correctPred
[0.7379375552842569,1.0]-correctPred
[0.570815460671703,1.0]-correctPred
[0.5934160829228174,1.0]-correctPred
[0.30011573225916655,0.0]-correctPred
[0.30011573225916655,0.0]-correctPred
[0.30011573225916655,0.0]-correctPred
[0.30011573225916655,0.0]-correctPred
[0.30011573225916655,0.0]-correctPred
[0.30011573225916655,0.0]-correctPred
[0.30011573225916655,0.0]-correctPred
[0.30011573225916655,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.683869271778185,0.0]-wrongPred
[0.683869271778185,0.0]-wrongPred
[0.683869271778185,0.0]-wrongPred
[0.683869271778185,0.0]-wrongPred
[0.6710054804853272,0.0]-wrongPred
[0.6348996182915054,0.0]-wrongPred
[0.6344941917129536,0.0]-wrongPred
[0.4594086823336651,0.0]-correctPred
[0.6775873693923025,0.0]-wrongPred
[0.6672108086101108,0.0]-wrongPred
[0.683869271778185,0.0]-wrongPred
[0.683869271778185,0.0]-wrongPred
[0.683869271778185,0.0]-wrongPred
[0.683869271778185,0.0]-wrongPred
[0.683869271778185,0.0]-wrongPred
[0.683869271778185,0.0]-wrongPred
[0.7039446322639071,0.0]-wrongPred
[0.6258062829979969,0.0]-wrongPred
[0.6249060927113911,0.0]-wrongPred
[0.6249060927113911,0.0]-wrongPred
[0.613382557654947,0.0]-wrongPred
[0.7093463874089713,0.0]-wrongPred
[0.8830246610305701,0.0]-wrongPred
[0.8827246745792617,0.0]-wrongPred
[0.5653043302144629,0.0]-wrongPred
[0.5653043302144629,0.0]-wrongPred
[0.5653043302144629,0.0]-wrongPred
[0.5653043302144629,0.0]-wrongPred
[0.5653043302144629,0.0]-wrongPred
[0.5653043302144629,0.0]-wrongPred
[0.5653043302144629,0.0]-wrongPred
[0.5653043302144629,0.0]-wrongPred
[0.5653043302144629,0.0]-wrongPred
[0.5653043302144629,0.0]-wrongPred
[0.5653043302144629,0.0]-wrongPred
[0.8611350235637711,0.0]-wrongPred
[0.3282702489864207,1.0]-wrongPred
[0.30011573225916655,1.0]-wrongPred
[0.31042710668357626,1.0]-wrongPred
[0.6491556327186179,0.0]-wrongPred
[0.6491556327186179,0.0]-wrongPred
[0.6261139012562758,0.0]-wrongPred
[0.6261139012562758,0.0]-wrongPred
[0.6258062829979969,0.0]-wrongPred
[0.6249060927113911,0.0]-wrongPred
[0.6806561014994985,0.0]-wrongPred
[0.5344488735709783,1.0]-correctPred
[0.5344488735709783,1.0]-correctPred
[0.5344488735709783,1.0]-correctPred
[0.5344488735709783,1.0]-correctPred
[0.5344488735709783,1.0]-correctPred
[0.5344488735709783,1.0]-correctPred
[0.5344488735709783,1.0]-correctPred
[0.5344488735709783,1.0]-correctPred
[0.5344488735709783,1.0]-correctPred
[0.5344488735709783,1.0]-correctPred
[0.5344488735709783,0.0]-wrongPred
[0.5344488735709783,0.0]-wrongPred
[0.5343035926783057,0.0]-wrongPred
[0.14168357076408852,0.0]-correctPred
[0.20210718136174843,0.0]-correctPred
[0.18507754222613693,0.0]-correctPred
[0.5018537107343225,0.0]-wrongPred
[0.716896954208822,0.0]-wrongPred
[0.3388941327471244,0.0]-correctPred
[0.3715943464866426,0.0]-correctPred
[0.15734349157268898,0.0]-correctPred
[0.13368519421984032,0.0]-correctPred
[0.12837184303416552,0.0]-correctPred
[0.685967599784364,0.0]-wrongPred
[0.6815005209724628,0.0]-wrongPred
[0.5668629072826962,0.0]-wrongPred
[0.5183264278000557,0.0]-wrongPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.42053249114756364,0.0]-correctPred
[0.32264071103605174,0.0]-correctPred
[0.32264071103605174,0.0]-correctPred
[0.32264071103605174,0.0]-correctPred
[0.32264071103605174,0.0]-correctPred
[0.32264071103605174,0.0]-correctPred
[0.32264071103605174,0.0]-correctPred
[0.32264071103605174,0.0]-correctPred
[0.32264071103605174,0.0]-correctPred
[0.32264071103605174,0.0]-correctPred
[0.32264071103605174,0.0]-correctPred
[0.32264071103605174,0.0]-correctPred
[0.32264071103605174,0.0]-correctPred
[0.32264071103605174,0.0]-correctPred
[0.31042710668357626,1.0]-wrongPred
[0.31042710668357626,1.0]-wrongPred
[0.31042710668357626,1.0]-wrongPred
[0.31042710668357626,1.0]-wrongPred
[0.3039616654843647,1.0]-wrongPred
[0.5183756788773084,0.0]-wrongPred
[0.5183756788773084,0.0]-wrongPred
[0.3542869833458189,0.0]-correctPred
[0.4865499231556161,0.0]-correctPred
[0.4865499231556161,0.0]-correctPred
[0.4865499231556161,0.0]-correctPred
[0.4865499231556161,0.0]--correctPred
[0.4865499231556161,0.0]-correctPred
[0.3282702489864207,0.0]-correctPred
[0.3282702489864207,0.0]-correctPred
[0.31042710668357626,0.0]-correctPred
[0.5241590816977466,1.0]-correctPred
[0.5241590816977466,1.0]-correctPred
[0.7523550418239773,1.0]-correctPred
[0.7523550418239773,1.0]-correctPred
[0.7523550418239773,1.0]-correctPred
[0.7523550418239773,1.0]-correctPred
[0.7523550418239773,1.0]-correctPred
[0.7523550418239773,1.0]-correctPred
[0.7523550418239773,1.0]-correctPred
[0.3039616654843647,1.0]-wrongPred
[0.3039616654843647,1.0]-wrongPred
[0.3039616654843647,1.0]-wrongPred
[0.3039616654843647,1.0]-wrongPred
[0.3039616654843647,1.0]-wrongPred
[0.31042710668357626,1.0]-wrongPred
[0.3039616654843647,1.0]-wrongPred
[0.3039616654843647,1.0]-wrongPred
[0.3039616654843647,1.0]-wrongPred
[0.3039616654843647,1.0]-wrongPred

(1 =  numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:variance maxDepth:20 maxBins:300 seed:12345)
totalTrainingDataCount::266387
totalTestDataCount::66353
wrongPrdCount::19547
correctPrdCount::46806
testMeanSquareErr::0.19618501678890266
precision::0.7026970645209358
recall::0.5037602016835449
f1::0.5868269434178272
--------------------------------------------------------------------------------------------------------------------------------

(1 =  numClasses:2 numTrees:50 featureSubsetStrategy:auto impurity:variance maxDepth:20 maxBins:300 seed:12345)
totalTrainingDataCount::266202
totalTestDataCount::66538
wrongPrdCount::19849
correctPrdCount::46689
testMeanSquareErr::0.19740093030849715
precision::0.7042149225437635
recall::0.7067899399488674
f1::0.7055000816035847

---------------------------------------------------------------------------------------------------------------------------------
impurity:variance
maxDepth = 5
maxBins = 100

(1 =  numTrees:50 featureSubsetStrategy:auto impurity:variance maxDepth:5 maxBins:100 seed:12345)
totalTrainingDataCount::266337
totalTestDataCount::66403
wrongPrdCount::22419
correctPrdCount::43984
testMeanSquareErr::0.2156675223005972
precision::0.7300354198291514
recall::0.6475604604498891
f1::0.6863291032977488

-----------
(1 =  numTrees:50 featureSubsetStrategy:auto impurity:variance maxDepth:5 maxBins:200 seed:12345)
totalTrainingDataCount::266337
totalTestDataCount::66403
wrongPrdCount::22532
correctPrdCount::43871
testMeanSquareErr::0.21564289633173078
precision::0.736613388100128
recall::0.6439593036871276
f1::0.6871772088632199

-----------
(1 =  numTrees:50 featureSubsetStrategy:auto impurity:variance maxDepth:5 maxBins:300 seed:12345)
totalTrainingDataCount::266337
totalTestDataCount::66403
wrongPrdCount::22355
correctPrdCount::44048
testMeanSquareErr::0.21540483737291302
precision::0.7313450605708843
recall::0.648311345646438
f1::0.6873295383023065

-----------
(1 =  numTrees:50 featureSubsetStrategy:auto impurity:variance maxDepth:10 maxBins:100 seed:12345)
totalTrainingDataCount::266337
totalTestDataCount::66403
wrongPrdCount::21234
correctPrdCount::45169
testMeanSquareErr::0.20660709774245506
precision::0.7068785903503289
recall::0.6759356766756796
f1::0.6910609323168247

-----------
(1 =  numTrees:50 featureSubsetStrategy:auto impurity:variance maxDepth:10 maxBins:200 seed:12345)
totalTrainingDataCount::266337
totalTestDataCount::66403
wrongPrdCount::21141
correctPrdCount::45262
testMeanSquareErr::0.2065132861195744
precision::0.7088430514629283
recall::0.6770626030590777
f1::0.6925884457111282

-----------
(1 =  numTrees:50 featureSubsetStrategy:auto impurity:variance maxDepth:10 maxBins:300 seed:12345)
totalTrainingDataCount::266337
totalTestDataCount::66403
wrongPrdCount::21180
correctPrdCount::45223
testMeanSquareErr::0.20618513983347136
precision::0.7084858767151829
recall::0.6764329762141579
f1::0.6920885063821126

-----------
(1 =  numTrees:50 featureSubsetStrategy:auto impurity:variance maxDepth:15 maxBins:100 seed:12345)
totalTrainingDataCount::266337
totalTestDataCount::66403
wrongPrdCount::20585
correctPrdCount::45818
testMeanSquareErr::0.20238525062145024
precision::0.7030984909366907
recall::0.6900560878709979
f1::0.6965162393666426

-----------
(1 =  numTrees:50 featureSubsetStrategy:auto impurity:variance maxDepth:15 maxBins:200 seed:12345)
totalTrainingDataCount::266337
totalTestDataCount::66403
wrongPrdCount::20338
correctPrdCount::46065
testMeanSquareErr::0.20060863303223406
precision::0.7054498913593475
recall::0.6941686436458425
f1::0.6997638027753174

-----------
(1 =  numTrees:50 featureSubsetStrategy:auto impurity:variance maxDepth:15 maxBins:300 seed:12345)
totalTrainingDataCount::266337
totalTestDataCount::66403
wrongPrdCount::20004
correctPrdCount::46399
testMeanSquareErr::0.2000189809039446
precision::0.7075036461588833
recall::0.7002091495566618
f1::0.7038374985194836

-----------
(1 =  numTrees:50 featureSubsetStrategy:auto impurity:variance maxDepth:20 maxBins:100 seed:12345)
totalTrainingDataCount::266337
totalTestDataCount::66403
wrongPrdCount::20208
correctPrdCount::46195
testMeanSquareErr::0.2003819173115619
precision::0.7049438938000416
recall::0.6970187468730686
f1::0.7009589203267431

-----------
(1 =  numTrees:50 featureSubsetStrategy:auto impurity:variance maxDepth:20 maxBins:200 seed:12345)
totalTrainingDataCount::266337
totalTestDataCount::66403
wrongPrdCount::19809
correctPrdCount::46594
testMeanSquareErr::0.19821507541282374
precision::0.7094681072714826
recall::0.7034588596387675
f1::0.7064507046427883

-----------
(1 =  numTrees:50 featureSubsetStrategy:auto impurity:variance maxDepth:20 maxBins:300 seed:12345)
totalTrainingDataCount::266337
totalTestDataCount::66403
wrongPrdCount::19900
correctPrdCount::46503
testMeanSquareErr::0.19762679257391097
precision::0.7073250587850106
recall::0.7024326800863113
f1::0.7048703802574598

-----------

(1 =  numTrees:50 featureSubsetStrategy:auto impurity:variance maxDepth:5 maxBins:400 seed:12345)
totalTrainingDataCount::265615
totalTestDataCount::67125
wrongPrdCount::22469
correctPrdCount::44656
testMeanSquareErr::0.21502548892666581
precision::0.7351020648273427
recall::0.6499024834221818
f1::0.6898817164230605

-----------
(1 =  numTrees:50 featureSubsetStrategy:auto impurity:variance maxDepth:10 maxBins:400 seed:12345)
totalTrainingDataCount::265615
totalTestDataCount::67125
wrongPrdCount::21077
correctPrdCount::46048
testMeanSquareErr::0.20566812367937914
precision::0.7104829695864463
recall::0.6825567266664783
f1::0.696239929669823

-----------
(1 =  numTrees:50 featureSubsetStrategy:auto impurity:variance maxDepth:15 maxBins:400 seed:12345)
totalTrainingDataCount::265615
totalTestDataCount::67125
wrongPrdCount::20150
correctPrdCount::46975
testMeanSquareErr::0.1993958930810045
precision::0.7081593034884405
recall::0.7018423507462687
f1::0.7049866768176627

-----------
(1 =  numTrees:50 featureSubsetStrategy:auto impurity:variance maxDepth:20 maxBins:400 seed:12345)
totalTrainingDataCount::265615
totalTestDataCount::67125
wrongPrdCount::19682
correctPrdCount::47443
testMeanSquareErr::0.19598794535141975
precision::0.7093064297899876
recall::0.7110632777024237
f1::0.7101837672281776



66557



yarn logs -applicationId <applicationID>
yarn logs -applicationId application_1439498058973_0126

resource manager UI .. Id -> application_1439498058973_0126
http://lnxcdh05.emeter.com:8088/cluster



******************************************************************************************************
TEST Data:


Analysis Date Range
-------------------
	Start Date - 2011-APR-20
	End Date - 2011-APR-30

SDP  
---
	id = 5351

SDP - ACCNT Rel 
---------------
	Start Date - Wed Apr 25 12:30:00 IST 2011
	End Date - Wed Apr 29 12:30:00 IST 2011
    ACCNT Type = Residential

LPI Data 
--------
	Reads are from 2011-APR-20 to 2011-APR-29
	
DeviceEvent Data (DeviceEvent is not integrated, testData is hardCoded)
----------------
	Events from 2011-APR-20 to 2011-APR-27
	'TAMPER' events are on 20th, 26th and 27th
	
Algorithms Implemented
----------------------
	1. Night Time ByPass
	2. Usage With No Active Account



Algo result
-----------

5351,Wed Apr 20 00:00:00 PDT 2011,{1=0.0, 2=0.0, 3=0.0, 4=0.0, 5=0.0, 6=0.0, 7=0.0, 8=0.0, 9=0.0, 10=0.0, 11=0.0}
5351,Thu Apr 21 00:00:00 PDT 2011,{1=0.0, 2=0.0, 3=0.0, 4=0.0, 5=0.0, 6=0.0, 7=0.0, 8=0.0, 9=0.0, 10=0.0, 11=0.0}
5351,Fri Apr 22 00:00:00 PDT 2011,{1=0.0, 2=19.178, 3=0.0, 4=0.0, 5=0.0, 6=0.0, 7=0.0, 8=0.0, 9=0.0, 10=0.0, 11=0.0}
5351,Sat Apr 23 00:00:00 PDT 2011,{1=0.0, 2=25.481, 3=0.0, 4=0.0, 5=0.0, 6=0.0, 7=0.0, 8=0.0, 9=0.0, 10=0.0, 11=0.0}
5351,Sun Apr 24 00:00:00 PDT 2011,{1=0.0, 2=64.578, 3=0.0, 4=0.0, 5=0.0, 6=0.0, 7=0.0, 8=0.0, 9=0.0, 10=0.0, 11=0.0}
5351,Mon Apr 25 00:00:00 PDT 2011,{1=0.0, 2=98.222, 3=0.0, 4=0.0, 5=0.0, 6=0.0, 7=0.0, 8=0.0, 9=0.0, 10=0.0, 11=0.0}
5351,Tue Apr 26 00:00:00 PDT 2011,{1=0.0, 2=100.383, 3=0.0, 4=0.0, 5=0.0, 6=0.0, 7=0.0, 8=0.0, 9=0.0, 10=0.0, 11=0.0}
5351,Wed Apr 27 00:00:00 PDT 2011,{1=0.0, 2=100.383, 3=0.0, 4=0.0, 5=0.0, 6=0.0, 7=0.0, 8=0.0, 9=0.0, 10=0.0, 11=0.0}
5351,Thu Apr 28 00:00:00 PDT 2011,{1=0.0, 2=100.383, 3=0.0, 4=1.0, 5=0.0, 6=0.0, 7=0.0, 8=0.0, 9=0.0, 10=0.0, 11=0.0}
5351,Fri Apr 29 00:00:00 PDT 2011,{1=0.0, 2=100.383, 3=0.0, 4=1.0, 5=0.0, 6=0.0, 7=0.0, 8=0.0, 9=0.0, 10=0.0, 11=0.0}


Scoring result
--------------

5351,Wed Apr 20 00:00:00 PDT 2011,MLScore,56.0,{1=0.0, 2=0.0, 3=0.0, 4=0.0, 5=0.0, 6=0.0, 7=0.0, 8=0.0, 9=0.0, 10=0.0, 11=0.0}
5351,Thu Apr 21 00:00:00 PDT 2011,MLScore,56.0,{1=0.0, 2=0.0, 3=0.0, 4=0.0, 5=0.0, 6=0.0, 7=0.0, 8=0.0, 9=0.0, 10=0.0, 11=0.0}
5351,Fri Apr 22 00:00:00 PDT 2011,MLScore,73.0,{1=0.0, 2=19.178, 3=0.0, 4=0.0, 5=0.0, 6=0.0, 7=0.0, 8=0.0, 9=0.0, 10=0.0, 11=0.0}
5351,Sat Apr 23 00:00:00 PDT 2011,MLScore,73.0,{1=0.0, 2=25.481, 3=0.0, 4=0.0, 5=0.0, 6=0.0, 7=0.0, 8=0.0, 9=0.0, 10=0.0, 11=0.0}
5351,Sun Apr 24 00:00:00 PDT 2011,MLScore,70.0,{1=0.0, 2=64.578, 3=0.0, 4=0.0, 5=0.0, 6=0.0, 7=0.0, 8=0.0, 9=0.0, 10=0.0, 11=0.0}
5351,Mon Apr 25 00:00:00 PDT 2011,MLScore,59.0,{1=0.0, 2=98.222, 3=0.0, 4=0.0, 5=0.0, 6=0.0, 7=0.0, 8=0.0, 9=0.0, 10=0.0, 11=0.0}
5351,Tue Apr 26 00:00:00 PDT 2011,MLScore,59.0,{1=0.0, 2=100.383, 3=0.0, 4=0.0, 5=0.0, 6=0.0, 7=0.0, 8=0.0, 9=0.0, 10=0.0, 11=0.0}
5351,Wed Apr 27 00:00:00 PDT 2011,MLScore,59.0,{1=0.0, 2=100.383, 3=0.0, 4=0.0, 5=0.0, 6=0.0, 7=0.0, 8=0.0, 9=0.0, 10=0.0, 11=0.0}
5351,Thu Apr 28 00:00:00 PDT 2011,MLScore,62.0,{1=0.0, 2=100.383, 3=0.0, 4=1.0, 5=0.0, 6=0.0, 7=0.0, 8=0.0, 9=0.0, 10=0.0, 11=0.0}
5351,Fri Apr 29 00:00:00 PDT 2011,MLScore,62.0,{1=0.0, 2=100.383, 3=0.0, 4=1.0, 5=0.0, 6=0.0, 7=0.0, 8=0.0, 9=0.0, 10=0.0, 11=0.0}

Result group by score 
---------------------

[56.0:: [5351,Wed Apr 20 00:00:00 PDT 2011,56.0,5351,Thu Apr 21 00:00:00 PDT 2011,56.0,]
59.0:: [5351,Mon Apr 25 00:00:00 PDT 2011,59.0,5351,Tue Apr 26 00:00:00 PDT 2011,59.0,5351,Wed Apr 27 00:00:00 PDT 2011,59.0,]
62.0:: [5351,Thu Apr 28 00:00:00 PDT 2011,62.0,5351,Fri Apr 29 00:00:00 PDT 2011,62.0,]
70.0:: [5351,Sun Apr 24 00:00:00 PDT 2011,70.0,]
73.0:: [5351,Fri Apr 22 00:00:00 PDT 2011,73.0,5351,Sat Apr 23 00:00:00 PDT 2011,73.0,]
]


HDFS file write 
---------------
header:
batchId,sdpId,analysisDateLongValue,scorerName,score

-bash-4.1$ hdfs dfs -cat /user/rporg/data/1440401866813/1440401866813-part-00003
1440401866813,5351,1303282800000,MLScore,56
1440401866813,5351,1303369200000,MLScore,56
1440401866813,5351,1303455600000,MLScore,73
1440401866813,5351,1303542000000,MLScore,73
1440401866813,5351,1303628400000,MLScore,70
1440401866813,5351,1303714800000,MLScore,59
1440401866813,5351,1303801200000,MLScore,59
1440401866813,5351,1303887600000,MLScore,59
1440401866813,5351,1303974000000,MLScore,62
1440401866813,5351,1304060400000,MLScore,62
******************************************************************************************************


	
/*to be deleted*/
if(null != intDailyMap){
	System.err.println("++++++++++++++++++++++++++ " + getEipCode() + "  9 :: intDailyMap.size from dataStore:" + intDailyMap.size());
	System.err.println("++++++++++++++++++++++++++ " + getEipCode() + "  9 :: intDailyMap.keySet from dataStore :" + intDailyMap.keySet());
}
/*to be deleted*/	





/*List<String> list = new ArrayList<String>();
				list.add("5351#SampleRPAnalysisDataService");*/
				
				//JavaRDD<String> sdpIdRdd = jsc.parallelize(list);
				
				/*EipSpringContext eipSpringContext = EipSpringContext.getInstance(eipSparkContext.getJobProperties());
				FeatureCreationExecutor executor = (FeatureCreationExecutor) eipSpringContext
						.getBeanById("a301-featuteCreationExecutor");
				//RPFeature result = executor.execute(rpContext, eipSpringContext, eipSparkContext.getSc(), list.get(0));
				String[] split = list.get(0).split(RPConstants.COL_DELIMETER);
				Long svcPtId = Long.parseLong(split[0]);
				String dataSvcName = split[1];
				RPAnalysisRequest request = new RPAnalysisRequest();
				request.setSvcPtId(svcPtId);
				request.setDataSvcName(dataSvcName);
				RPFeature result = executor.execute(eipSparkContext, eipSpringContext, request);
				List<RPFeature> rddList = new ArrayList<RPFeature>();
				rddList.add(result);
				resultRDD = jsc.parallelize(rddList);*/
				
				
				
*********************************************************************************************************
Demo review comment:

-diagram arrows
-job properties not relevant ->  /em-ac-revenueprotection/resources/org/common/refData/eip/revenueprotection/job/JobDef_RevenueProtection.xml

	-move preFetch related to dataSvc RPAnalysis DataServiceVersion.xml
	-segment Query in oozie.xml TODO
	-start date ... not required instead last 3 months kind of info is req.:JobDef_RevenueProtection.xml 

-DataSvc name - RP analysis: DataServiceVersion.xml
	-type - should have prefix - rp.RPfeature : EntityDefReferenceData_RPFeatures.xml

-Define algoDef attributes in entity_flex_attr
-Move RPContext out of driver.....
-remove line propogation from driver  
-Executor
		-Abstract EIDG .. not to be embeded so deep ...
		-move dataFetch sperate from executor and embed it to EIDG...
		-Instantiate new RPContext and execute fetch logic :---RPContext is per SDP
		-Rename algoDef to DataSvcConfiguration
		-Criteria creation should be encapsulated in some diff function ....
		-Dao is not serializable so encapsulated out and not put in context


-DataStore
	-Move ... data from dataFetch function as overTime they can change
	
	-seperate package -> data ... which will contain all dataStructures


/em-ac-revenueprotection/resources/org/common/refData/eip/revenueprotection/job/JobDef_RevenueProtection.xml

--- two 

Why Prefecth Request

*********************************************************************************************************

D:\work\codebase\P4-WS\Perforce-WS\EnergyIP\opt8\hadoop-common\trunk\em-bt-schema>.\compiler\protoc.exe --proto_path=src --java_out=src src\schema\proto\
/*
 * TODOs:
 * 
 * -From config:
 * 		-noOfPartitions
 * 		-SegmentQ
 * 
 * -Get orgId from sessionHolder
 *
 * -TimeZone from envServer time zone
 * 
 * -Algo
 * 		-Get channelLength from MeasType
 * 
 * 	
 * 
 * */




/* TO be deleted - start*************************/
			/*JavaRDD<RPFeature> resultRDD = null;
			try{
				
				String line = sdpIdRdd.take(1).get(0).getString(1);
				logger.info("++++++++++++++++++line::" + line);
				//String line = sdpIdRdd.take(1)
				EipSpringContext eipSpringContext = EipSpringContext.getInstance(eipSparkContext.getJobProperties());
				FeatureCreationExecutor executor = (FeatureCreationExecutor) eipSpringContext
						.getBeanById("a301-featuteCreationExecutor");
				String[] split = line.split(RPConstants.COL_DELIMETER);
				Long svcPtId = Long.parseLong(split[0]);
				String dataSvcName = split[1];
				RPAnalysisRequest request = new RPAnalysisRequest();
				request.setSvcPtId(svcPtId);
				request.setDataSvcName(dataSvcName);
				RPFeature result = executor.execute(eipSparkContext, eipSpringContext, request);
				List<RPFeature> rddList = new ArrayList<RPFeature>();
				rddList.add(result);
				resultRDD = jsc.parallelize(rddList);
				
			}catch(Exception e){
				e.printStackTrace();
				throw new RuntimeException(e);
			}*/
			/*TO be deleted - end***************************/
			
			
			
			uwnaa
			/*For test cases*/
				/*public double execute(Long batchId, MeteredSvcPt sdp,
						NavigableMap<Long, LPIntervalsDaily> intDailyMap,
						Date startDate) throws Exception {
						
					return getAlgoValueForEndDate(batchId, sdp, intDailyMap, startDate);
	}*/
	
	NTB
		// TODO: remove below method. This is added for JUnit tests for now.
		public double executeAlgo(Long batchId, MeteredSvcPt sdp, NavigableMap<Long, LPIntervalsDaily> intDailyMap,
				NavigableMap<Long, DeviceEventsDaily> dvcEvntDailyMap, Date startDate) {
	
			try {
				return execute(batchId, sdp, intDailyMap, dvcEvntDailyMap, startDate);
			} catch (Exception e) {
				// TODO Auto-generated catch block
				e.printStackTrace();
			}
			return 0;
		}


/*
 JavaPairRDD<Double, RPFeature> scoreData = null;
			PairFunction<RPFeature, Double, RPFeature> keyPairData = new PairFunction<RPFeature, Double, RPFeature>() {
				private static final long serialVersionUID = 4849964120630321946L;

				public Tuple2<Double, RPFeature> call(RPFeature rpFeature) {
					return new Tuple2(rpFeature.getScore(), rpFeature);
				}
			};
			scoreData = resultRDD.mapToPair(keyPairData);

			if (logger.isDebugEnabled()) {
				logger.debug(" key/ Value op done" + scoreData.count());
			}
			// sort by key
			JavaPairRDD<Double, Iterable<RPFeature>> rpFeatureGroupByScore = scoreData.groupByKey().sortByKey();
			if (logger.isDebugEnabled()) {
				logger.debug("Scoring done." + resultRDD.count());
			}
			
 */ 
 
 
 <!--  Parquet -->
         <!--
         <dependency org="com.twitter" name="parquet-column" rev="1.5.0-${cdh.version}" transitive="false" />
         <dependency org="com.twitter" name="parquet-hadoop" rev="1.5.0-${cdh.version}" transitive="false" />
         <dependency org="com.twitter" name="parquet-avro" rev="1.5.0-${cdh.version}" transitive="false" />
         <dependency org="com.twitter" name="parquet-cascading" rev="1.5.0-${cdh.version}" transitive="false" />
         <dependency org="com.twitter" name="parquet-common" rev="1.5.0-${cdh.version}" transitive="false" />
         <dependency org="com.twitter" name="parquet-encoding" rev="1.5.0-${cdh.version}" transitive="false" />
         <dependency org="com.twitter" name="parquet-format" rev="2.1.0-${cdh.version}-javadoc" transitive="false" />
         <dependency org="com.twitter" name="parquet-format" rev="2.1.0-${cdh.version}-sources" transitive="false" />
         <dependency org="com.twitter" name="parquet-format" rev="2.1.0-${cdh.version}" transitive="false" />
         <dependency org="com.twitter" name="parquet-generator" rev="1.5.0-${cdh.version}" transitive="false" />
         <dependency org="com.twitter" name="parquet-hadoop-bundle" rev="1.5.0-${cdh.version}" transitive="false" />
         <dependency org="com.twitter" name="parquet-jackson" rev="1.5.0-${cdh.version}" transitive="false" />
         <dependency org="com.twitter" name="parquet-pig" rev="1.5.0-${cdh.version}" transitive="false" />
         <dependency org="com.twitter" name="parquet-pig-bundle" rev="1.5.0-${cdh.version}" transitive="false" />
         <dependency org="com.twitter" name="parquet-protobuf" rev="1.5.0-${cdh.version}" transitive="false" />
         <dependency org="com.twitter" name="parquet-scala_2.10" rev="1.5.0-${cdh.version}" transitive="false" />
         <dependency org="com.twitter" name="parquet-scrooge_2.10" rev="1.5.0-${cdh.version}" transitive="false" />
         <dependency org="com.twitter" name="parquet-test-hadoop2" rev="1.5.0-${cdh.version}" transitive="false" />
         <dependency org="com.twitter" name="parquet-thrift" rev="1.5.0-${cdh.version}" transitive="false" />
         <dependency org="com.twitter" name="parquet-tools" rev="1.5.0-${cdh.version}" transitive="false" />
        -->
        
        
parquet-column-1.5.0-cdh5.4.1.jar
parquet-hadoop-1.5.0-cdh5.4.1.jar
parquet-avro-1.5.0-cdh5.4.1.jar
parquet-cascading-1.5.0-cdh5.4.1.jar
parquet-common-1.5.0-cdh5.4.1.jar
parquet-encoding-1.5.0-cdh5.4.1.jar
parquet-format-2.1.0-cdh5.4.1-javadoc.jar
parquet-format-2.1.0-cdh5.4.1-sources.jar
parquet-format-2.1.0-cdh5.4.1.jar
parquet-generator-1.5.0-cdh5.4.1.jar
parquet-hadoop-bundle-1.5.0-cdh5.4.1.jar
parquet-jackson-1.5.0-cdh5.4.1.jar
parquet-pig-1.5.0-cdh5.4.1.jar
parquet-pig-bundle-1.5.0-cdh5.4.1.jar
parquet-protobuf-1.5.0-cdh5.4.1.jar
parquet-scala_2.10-1.5.0-cdh5.4.1.jar
parquet-scrooge_2.10-1.5.0-cdh5.4.1.jar
parquet-test-hadoop2-1.5.0-cdh5.4.1.jar
parquet-thrift-1.5.0-cdh5.4.1.jar
parquet-tools-1.5.0-cdh5.4.1.jar
		
		
		hadoop.version=2.6.0
		hbase.version=1.0.0
		cdh.version=cdh5.4.1
		hive.version=1.1.0
		cdh.protobuf.version=2.5.0
		cdh.jets3t.version=0.9.0
		cdh.avro.version=1.7.6-cdh5.4.1
		cdh.hadoop.version=2.6.0-cdh5.4.1
		cdh.slf4j.version=1.7.5
		servlet-version=3.0.20100224
		zookeeper.version=3.4.5
		spark.version=1.3.0
		oozie.version=4.1.0
		
		<dependencies>
		    <dependency>
		      <groupId>com.twitter</groupId>
		      <artifactId>parquet-column</artifactId>
		      <version>${project.version}</version>
		    </dependency>
		    <dependency>
		      <groupId>com.twitter</groupId>
		      <artifactId>parquet-hadoop</artifactId>
		      <version>${project.version}</version>
		    </dependency>
		    <dependency>
		      <groupId>com.twitter</groupId>
		      <artifactId>parquet-format</artifactId>
		      <version>${parquet.format.version}</version>
		    </dependency>
		    <dependency>
		      <groupId>org.apache.avro</groupId>
		      <artifactId>avro</artifactId>
		      <version>${avro.version}</version>
		    </dependency>
		    <dependency>
		      <groupId>org.apache.hadoop</groupId>
		      <artifactId>hadoop-client</artifactId>
		      <version>${hadoop.version}</version>
		      <scope>provided</scope>
		    </dependency>
		    <dependency>
		      <groupId>com.google.guava</groupId>
		      <artifactId>guava</artifactId>
		      <version>11.0</version>
		      <scope>test</scope>
		    </dependency>
		    <dependency>
		      <groupId>com.twitter</groupId>
		      <artifactId>parquet-column</artifactId>
		      <version>${project.version}</version>
		      <type>test-jar</type>
		      <scope>test</scope>
		    </dependency>
		    <dependency>
		      <groupId>com.twitter</groupId>
		      <artifactId>parquet-hadoop</artifactId>
		      <version>${project.version}</version>
		      <type>test-jar</type>
		      <scope>test</scope>
		    </dependency>
  </dependencies>
  
  
  

parquet jar issue


* MTM deployment steps
* org user should be able to login & org 

workflow.xml -> prop file .. 
oozie jobExecutor: see hadoop utils prop file, .. all common env scope prop.

-sPropfile ...

-spark.enable property = true .. builds classPath for spark from sparkLib.lst  (add to job def)- chk from em-ui .. 
-all jars should only be /user/rporg/jars
-hdfs folders are not proper.. owner should be orgUdcId
-org creation in EIP - org udcid is org user udcId 

devCluster: ? cdhmg02 -- cdhmg03 

cloudera version is 5.4.4 

-error in hadoop distributed cache looking for file
-jar file not part of sparkLib.lst
-


https://wiki.emeter.com/display/ENG/MTM+-+Manual+Install+Steps+for+deploying+MTM+Jobs
http://lnxcdhmg03.emeter.com:7180/cmf/login  
	admin/admin
	
CDH21-DEV is cluster name  
  
  
Yarn mempry checker:
https://www.mapr.com/blog/best-practices-yarn-resource-management#.VhTpM_mqpBc



***************************************************************************************************************************

2015-10-07 01:07:22,965 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 13949 for container-id container_1443234915897_0592_01_000002: 18.3 MB of 1.5 GB physical memory used; 2.7 GB of 3.1 GB virtual memory used
2015-10-07 01:07:26,071 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 13949 for container-id container_1443234915897_0592_01_000002: 125.1 MB of 1.5 GB physical memory used; 2.8 GB of 3.1 GB virtual memory used
2015-10-07 01:07:29,154 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 13949 for container-id container_1443234915897_0592_01_000002: 159.2 MB of 1.5 GB physical memory used; 2.9 GB of 3.1 GB virtual memory used
2015-10-07 01:07:32,193 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 13949 for container-id container_1443234915897_0592_01_000002: 285.1 MB of 1.5 GB physical memory used; 2.9 GB of 3.1 GB virtual memory used
2015-10-07 01:07:35,274 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 13949 for container-id container_1443234915897_0592_01_000002: 272.6 MB of 1.5 GB physical memory used; 2.9 GB of 3.1 GB virtual memory used
2015-10-07 01:07:38,355 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 13949 for container-id container_1443234915897_0592_01_000002: 272.6 MB of 1.5 GB physical memory used; 2.9 GB of 3.1 GB virtual memory used
2015-10-07 01:07:41,436 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 13949 for container-id container_1443234915897_0592_01_000002: 272.6 MB of 1.5 GB physical memory used; 2.9 GB of 3.1 GB virtual memory used
2015-10-07 01:07:44,489 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 13949 for container-id container_1443234915897_0592_01_000002: 273.1 MB of 1.5 GB physical memory used; 2.9 GB of 3.1 GB virtual memory used
2015-10-07 01:07:47,524 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 13949 for container-id container_1443234915897_0592_01_000002: 273.1 MB of 1.5 GB physical memory used; 2.9 GB of 3.1 GB virtual memory used
2015-10-07 01:07:50,560 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 13949 for container-id container_1443234915897_0592_01_000002: 273.2 MB of 1.5 GB physical memory used; 2.9 GB of 3.1 GB virtual memory used
2015-10-07 01:07:53,599 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 13949 for container-id container_1443234915897_0592_01_000002: 281.6 MB of 1.5 GB physical memory used; 2.9 GB of 3.1 GB virtual memory used
2015-10-07 01:07:56,672 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 13949 for container-id container_1443234915897_0592_01_000002: 429.7 MB of 1.5 GB physical memory used; 3.0 GB of 3.1 GB virtual memory used
2015-10-07 01:07:59,707 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Memory usage of ProcessTree 13949 for container-id container_1443234915897_0592_01_000002: 693.6 MB of 1.5 GB physical memory used; 3.1 GB of 3.1 GB virtual memory used
2015-10-07 01:08:01,618 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exit code from container container_1443234915897_0592_01_000002 is : 1
2015-10-07 01:08:01,619 WARN org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Exception from container-launch with container ID: container_1443234915897_0592_01_000002 and exit code: 1
ExitCodeException exitCode=1: 
        at org.apache.hadoop.util.Shell.runCommand(Shell.java:538)
        at org.apache.hadoop.util.Shell.run(Shell.java:455)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)
        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:211)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
2015-10-07 01:08:01,619 INFO org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor: Exception from container-launch.
2015-10-07 01:08:01,619 INFO org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor: Container id: container_1443234915897_0592_01_000002



https://www.mapr.com/blog/best-practices-yarn-resource-management#.VhTpM_mqpBc

Please refer content under heading 'Virtual/physical memory checker'


***************************************************************************************************************************


use yarn logs -applicationId application_1443234915897_0602 > application_1443234915897_0602.log
the vi application_1443234915897_0602.log to search the errors
however there is a catch
you can do it as the user who ran the job
so first you have to sudo su - rporg
then you can do it


Intervals with no active account - IWNAA
No usage for x days -			   NUFXD

ze551ml - 221 () - 13949 + 221
***************************************************************************************************************************
DeviceEvents

JobDef_loadDeviceEvents.xml
//EnergyIP/opt8/hadoop-common/trunk/em-hdoop-common-apps/resources/org/Sample/refData/eip/hadoop/common

testenv_pq.properties
//EnergyIP/opt8/hadoop-common/trunk/em-hdoop-common-apps/src


DeviceEventDao & MeterData
//EnergyIP/opt8/hadoop-common/trunk/em-hdoop-common-apps/src/com/emeter/hadoop/apps/common/dao

DeviceEventEntity & DevieEventResult
//EnergyIP/opt8/hadoop-common/trunk/em-hdoop-common-apps/src/com/emeter/hadoop/apps/common/data


DeviceEventDriverTest.java
//EnergyIP/opt8/hadoop-common/trunk/em-hdoop-common-apps/test/com/emeter/loader/deviceevent


//EnergyIP/opt8/hadoop-common/trunk/em-hdoop-common-apps/src/com/emeter/loader/deviceevent

//EnergyIP/opt8/hadoop-common/trunk/em-hdoop-common-apps/resources/org/common/oozie/wf/LoadDeviceEvents



(select svc_pt_id, (case when UPPER(DISPOSITION_GROUP) = 'CLEAN' then '0' else '1' end) as outcome, TO_CHAR(insert_time, 'YYYY-MM-DD HH24:MI:SS') as insert_time, TO_CHAR(analysis_date, 'YYYY-MM-DD HH24:MI:SS') as analysis_date from investigated_sdps_outcome where eligible = 'Y' and segment_name = '<SegmentName>')


----------------------------------------------------------------------------------------------------------------------------------

drop table tmp_srcid_to_udcid;

create table tmp_srcid_to_udcid(src_id VARCHAR2(15 BYTE), udc_id VARCHAR2(50), type VARCHAR2(50), sub_type VARCHAR2(50));

insert into tmp_srcid_to_udcid (src_id, udc_id, type,  sub_type) select row_id as src_id,  x_udc_asset_id as udc_id, type_cd as type, cfg_type_cd as sub_type 
																	 from s_asset@Mudr2sebl
																	 where type_cd in ('Service Point'
																					 ,'Distribution Node'
																					 ,'Meter'
																					 ,'CT-PT'
																					 ,'Communication Module'
																					 ,'Disconnect Collar')
           												 order by type;
commit;

select count(*) from tmp_srcid_to_udcid;
------------------------------------------------------------------------------------------------


select  /*+ PARALLEL(de, 4) */ 
        TO_CHAR (FROM_TZ
                              (TO_TIMESTAMP (TO_CHAR (de.utc_event_time, 'MM-DD-YYYY HH24:MI:SS' ),
                                             'MM-DD-YYYY HH24:MI:SS' ),
                               '00:00'
                              ) AT TIME ZONE '+08:00',--'$TARGET_DB_UTC_OFFSET', 
        'yyyy-mm-dd"T"hh24:mi:ssTZH:TZM')||'~~'||
        de.event_type_id||'~~'||
        (select tmp.udc_id from tmp_srcid_to_udcid tmp where de.sdp_row_id = tmp.src_id)||'~~'|| --de.sdp_row_id
        TO_CHAR (FROM_TZ
                              (TO_TIMESTAMP (TO_CHAR (de.utc_reported_time, 'MM-DD-YYYY HH24:MI:SS' ),
                                             'MM-DD-YYYY HH24:MI:SS' ),
                               '00:00'
                              ) AT TIME ZONE '+08:00',--'$TARGET_DB_UTC_OFFSET', 
       'yyyy-mm-dd"T"hh24:mi:ssTZH:TZM')||'~~'|| 
        de.event_description||'~~'||
        (select tmp.udc_id from tmp_srcid_to_udcid tmp where de.device_id = tmp.src_id)||'~~'|| --de.device_id
        de.source||'~~'|| -- TODO: Verify how to get DEVICE_DATA_SRC_ID on the basis of SOURCE and SOURCE_DETAILS
        de.source_details||'~~'||        
        TO_CHAR (FROM_TZ
                              (TO_TIMESTAMP (TO_CHAR (de.utc_event_start_time, 'MM-DD-YYYY HH24:MI:SS' ),
                                             'MM-DD-YYYY HH24:MI:SS' ),
                               '00:00'--'$MUDR_UTC_OFFSET'
                              ) AT TIME ZONE '+08:00',--'$TARGET_DB_UTC_OFFSET', 
       'yyyy-mm-dd"T"hh24:mi:ssTZH:TZM')||'~~'||
        -1||'~~'||--event_record_num
        null||'~~'||--num_occur
        de.threshold||'~~'||
        de.measured_value||'~~'||
        de.duration||'~~'||
        null||'~~'||--variance
        null||'~~'||--correlation_id
        (select tmp.udc_id from tmp_srcid_to_udcid tmp where de.incoming_device_id = tmp.src_id)||'~~'||
        de.incoming_device_type||'~~'||
        null||'~~'||--org_id
        TO_CHAR (FROM_TZ
                              (TO_TIMESTAMP (TO_CHAR (de.insert_time, 'MM-DD-YYYY HH24:MI:SS' ),
                                             'MM-DD-YYYY HH24:MI:SS' ),
                               'America/Chicago'--'$MUDR_UTC_OFFSET'
                              ) AT TIME ZONE '+08:00',--'$TARGET_DB_UTC_OFFSET', 
       'yyyy-mm-dd"T"hh24:mi:ssTZH:TZM')||'~~'||
        ded.attrib_name ||'~~'|| 
        ded.attrib_value
from device_event de, device_event_details ded--, tmp_srcid_to_udcid tmp
where de.event_id = ded.event_id(+)
and de.utc_event_time between to_date('2012-04-02 00:00:00'/*'$INT_START_TIME'*/,'yyyy-mm-dd hh24:mi:ss')
and to_date(/*'$INT_END_TIME'*/'2012-04-05 00:00:00','yyyy-mm-dd hh24:mi:ss')
;

************************************************************************************************************************************************	

EnergyIP\opt\Analytics\trunk\bin\AdfRegisterReadExtract.0702.sh

//EnergyIP/core/sandbox/analytics/hadoop_scripts/extracts_for_hbase/RR/

EnergyIP\opt8\hadoop-common\trunk\em-bt-schema\src\schema\proto\RegisterData.proto


CREATE DATABASE LINK mudr2eip
  CONNECT TO rw_eip227 IDENTIFIED BY rw_eip227 USING 'emdb35.emeter.com:1521/poltp01';


drop table tmp_sdp_id_to_udcid;
drop table tmp_meas_type_id_to_udcid;
drop table tmp_channel_id_to_udcid;
drop table tmp_device_id_to_udcid;
drop table tmp_org_id_to_udcid;


create table tmp_sdp_id_to_udcid(id VARCHAR2(15 BYTE), udc_id VARCHAR2(50), type VARCHAR2(50), sub_type VARCHAR2(50));
create table tmp_device_id_to_udcid(id VARCHAR2(15 BYTE), udc_id VARCHAR2(50), type VARCHAR2(50), sub_type VARCHAR2(50));
create table tmp_meas_type_id_to_udcid(id VARCHAR2(15 BYTE), udc_id VARCHAR2(50));
create table tmp_channel_id_to_udcid(id VARCHAR2(15 BYTE), udc_id VARCHAR2(50), svc_pt_udc_id VARCHAR2(50));
create table tmp_org_id_to_udcid(id VARCHAR2(15 BYTE), udc_id VARCHAR2(50));


insert into tmp_sdp_id_to_udcid (id, udc_id, type, sub_type) select id, udc_id, type, sub_type from svc_pt@mudr2eip order by type;
insert into tmp_device_id_to_udcid (id, udc_id, type, sub_type) select id, udc_id, type, sub_type from device@mudr2eip order by type;
insert into tmp_meas_type_id_to_udcid (id, udc_id) select id, udc_id from meas_type@mudr2eip;
insert into tmp_channel_id_to_udcid (id, udc_id, svc_pt_udc_id) 
select channel.id, channel.udc_id, sdp.udc_id from channel@mudr2eip channel, svc_pt@mudr2eip sdp 
where channel.svc_pt_id = sdp.id;
insert into tmp_org_id_to_udcid (id, udc_id) select id, udc_id from org@mudr2eip;
commit;


select  channel.udc_id as CHANNEL_UDC_ID, --0
		channel.svc_pt_udc_id as svc_pt_udc_id, --1
        meas.udc_id as MEAS_TYPE_UDC_ID, --2
        DECODE(READ_START_TIME, NULL, '', translate(TO_CHAR(READ_START_TIME,'yyyy-mm-dd hh24:mi:ss'),' ','T')||'+00:00') as READ_START_TIME, --3
        DECODE(READ_TIME, NULL, '', translate(TO_CHAR(READ_TIME,'yyyy-mm-dd hh24:mi:ss'),' ','T')||'+00:00') as READ_TIME, --4
        READ_VALUE, --5
        ORIG_READ_VALUE, --6
        DECODE(DEMAND_PEAK_TIME, NULL, '', translate(TO_CHAR(DEMAND_PEAK_TIME,'yyyy-mm-dd hh24:mi:ss'),' ','T')||'+00:00') as DEMAND_PEAK_TIME, --7
        DECODE(device.udc_id, NULL, ) as device_udc_id, --8
        FLAGS, --9
        EST_METHOD,--10 
        ESTIMATED, --11
        VAL_STATUS, --12
        VAL_FAIL_CODE, --13
        DECODE(EXT_VERSION_TIME, NULL, '', translate(TO_CHAR(EXT_VERSION_TIME,'yyyy-mm-dd hh24:mi:ss'),' ','T')||'+00:00') as EXT_VERSION_TIME, --14
        DEVICE_DATA_SRC_ID, --15
        READ_REASON, --16
        LOCKED, --17
        AMI_RECORD_NUM, --18
        org.udc_id as ORG_UDC_ID, --19
        LAST_UPD_BY, --20
        DECODE(LAST_UPD_TIME, NULL, '', translate(TO_CHAR(LAST_UPD_TIME,'yyyy-mm-dd hh24:mi:ss'),' ','T')||'+00:00') as LAST_UPD_TIME, --21                
        REC_VERSION_NUM --22
from register_read rr, tmp_meas_type_id_to_udcid meas, tmp_channel_id_to_udcid channel, tmp_device_id_to_udcid device, tmp_org_id_to_udcid org
where rr.channel_id = channel.id
and rr.meas_type_id = meas.id
and rr.device_id = device.id
and rr.org_id = org.id
order by svc_pt_udc_id, MEAS_TYPE_UDC_ID, CHANNEL_UDC_ID, device_udc_id, READ_TIME, ORG_UDC_ID;

RegisterReads
-------------

CHANNEL_UDC_ID,SVC_PT_UDC_ID,MEAS_TYPE_UDC_ID,READ_START_TIME,READ_TIME,READ_VALUE,ORIG_READ_VALUE,DEMAND_PEAK_TIME,DEVICE_UDC_ID,FLAGS,EST_METHOD,ESTIMATED,VAL_STATUS,VAL_FAIL_CODE,EXT_VERSION_TIME,DEVICE_DATA_SRC_ID,READ_REASON,LOCKED,AMI_RECORD_NUM,ORG_UDC_ID,LAST_UPD_BY,LAST_UPD_TIME,REC_VERSION_NUM
1-4WJEI,1-4W9UT Udc,EM.ELECTRIC.KWH.11.1,,2014-06-10T22:25:00+00:00,17144,,,1-1000U,0,,,VAL,,,46,,,,rporg,53,2014-06-11T01:43:38+00:00,1402454618.000000000000000000000000000004
1-4WJEI,1-4W9UT Udc,EM.ELECTRIC.KWH.11.1,,2014-06-10T22:30:00+00:00,17144,,,1-1000U,0,,,VAL,,,46,,,,rporg,53,2014-06-11T01:43:38+00:00,1402454618.000000000000000000000000000004
1-4WJEI,1-4W9UT Udc,EM.ELECTRIC.KWH.11.1,,2014-06-10T22:35:00+00:00,17144,,,1-1000U,0,,,VAL,,,46,,,,rporg,53,2014-06-11T01:43:38+00:00,1402454618.000000000000000000000000000004
1-93PJM,1-93FQV Udc,EM.ELECTRIC.KWH.11.1,+00:00,2014-06-10T22:35:00+00:00,17144,,+00:00,1-1000U,0,,,VAL,,+00:00,46,,,,testds1,53,2014-06-11T01:43:38+00:00,1402454618.000000000000000000000000000004
1-1WF40X,1-93FQV Udc,EM.ELECTRIC.KWH.11.1,+00:00,2014-06-10T22:35:00+00:00,17144,,+00:00,1-1000U,0,,,VAL,,+00:00,46,,,,testds1,53,2014-06-11T01:43:38+00:00,1402454618.000000000000000000000000000004


-Read CSV 
-Validate record
-Filter & save invalid records in file
-Parse record 
	Get MeteredSvcPtBO using svcPtUdcId
	using MeteredSvcPtBO get Ids for key creation
-Create Map with <key,value>
	Key: svcptId,measTypeId,channelId,channelLen,usageDate,orgId
	value:record
-Iterate Map and tuple<key, value>
-Filter and save channel lookup failed records in file
-BuildAndSaveEntity
	-Merge RR of Input with Hbase 
	-Create & Saves MeterDataEntity in HBase
	-Identify duplicate record with same intervalEndTime 

-Save Dup records in file	




1) ch-error chk - done
2) jobDef - done
3) Deploy and integration test - Done

4) accumulator test with sdpId processed acc  
5)  Create test EIP data 
	Create HBase table 
	Run with generic testEnv.properties
6) RR version time - Done

	
Questions:
RRLoader:
-LoadSDP uses EBO, hence multi org data cannot be loaded - mail
-Accumulators usage. I could not find any usage for it - mail
-What is the usage of staging directory in DeviceEvent


LPInt loaders:
-----------

CHANNEL_UDC_ID,SVC_PT_UDC_ID,MEAS_TYPE_UDC_ID,INTERVAL_END_TIME,LP_VALUE,VALIDATION_STATUS,CHANGE_METHOD,FAIL_CODE,LOCKED,ESTIMATED,EXT_VERSION_TIME,FLAGS,INTERVAL_LEN,DEVICE_UDC_ID,DEVICE_DATA_SRC_ID,AMI_RECORD_NUM,ORG_UDC_ID,LAST_UPD_BY,LAST_UPD_TIME,REC_VERSION_NUM

drop table tmp_sdp_id_to_udcid;
drop table tmp_meas_type_id_to_udcid;
drop table tmp_channel_id_to_udcid;
drop table tmp_device_id_to_udcid;
drop table tmp_org_id_to_udcid;


create table tmp_sdp_id_to_udcid(id VARCHAR2(15 BYTE), udc_id VARCHAR2(50), type VARCHAR2(50), sub_type VARCHAR2(50));
create table tmp_device_id_to_udcid(id VARCHAR2(15 BYTE), udc_id VARCHAR2(50), type VARCHAR2(50), sub_type VARCHAR2(50));
create table tmp_meas_type_id_to_udcid(id VARCHAR2(15 BYTE), udc_id VARCHAR2(50));
create table tmp_channel_id_to_udcid(id VARCHAR2(15 BYTE), udc_id VARCHAR2(50), svc_pt_udc_id VARCHAR2(50));
create table tmp_org_id_to_udcid(id VARCHAR2(15 BYTE), udc_id VARCHAR2(50));


insert into tmp_sdp_id_to_udcid (id, udc_id, type, sub_type) select id, udc_id, type, sub_type from svc_pt@mudr2eip order by type;
insert into tmp_device_id_to_udcid (id, udc_id, type, sub_type) select id, udc_id, type, sub_type from device@mudr2eip order by type;
insert into tmp_meas_type_id_to_udcid (id, udc_id) select id, udc_id from meas_type@mudr2eip;
insert into tmp_channel_id_to_udcid (id, udc_id, svc_pt_udc_id) 
select channel.id, channel.udc_id, sdp.udc_id from channel@mudr2eip channel, svc_pt@mudr2eip sdp 
where channel.svc_pt_id = sdp.id;
insert into tmp_org_id_to_udcid (id, udc_id) select id, udc_id from org@mudr2eip;
commit;


select  channel.udc_id as CHANNEL_UDC_ID, --0
        channel.svc_pt_udc_id as SVC_PT_UDC_ID, --1
        meas.udc_id as MEAS_TYPE_UDC_ID, --2
        DECODE(INTERVAL_END_TIME, NULL, '', translate(TO_CHAR(INTERVAL_END_TIME,'yyyy-mm-dd hh24:mi:ss'),' ','T')||'+00:00') as INTERVAL_END_TIME, --3
        LP_VALUE, --4
        VALIDATION_STATUS, --5
        CHANGE_METHOD, --6
        FAIL_CODE, --7
        LOCKED, --8
        ESTIMATED, --9
        DECODE(EXT_VERSION_TIME, NULL, '', translate(TO_CHAR(EXT_VERSION_TIME,'yyyy-mm-dd hh24:mi:ss'),' ','T')||'+00:00') as EXT_VERSION_TIME, --10
        FLAGS, --11
        INTERVAL_LEN, --12
        device.udc_id as DEVICE_UDC_ID, --13 --TODO add decode to check null
        DEVICE_DATA_SRC_ID, --14
        AMI_RECORD_NUM, --15
        org.udc_id as ORG_UDC_ID, --16
        LAST_UPD_BY, --17
        DECODE(LAST_UPD_TIME, NULL, '', translate(TO_CHAR(LAST_UPD_TIME,'yyyy-mm-dd hh24:mi:ss'),' ','T')||'+00:00') as LAST_UPD_TIME, --18
        REC_VERSION_NUM --19
from lp_intervals lp, tmp_meas_type_id_to_udcid meas, tmp_channel_id_to_udcid channel, tmp_device_id_to_udcid device, tmp_org_id_to_udcid org
where lp.channel_id = channel.id
and lp.meas_type_id = meas.id
and lp.device_id = device.id
and lp.org_id = org.id
order by SVC_PT_UDC_ID, MEAS_TYPE_UDC_ID, CHANNEL_UDC_ID, DEVICE_UDC_ID, INTERVAL_END_TIME, ORG_UDC_ID;

LPInt Header:
CHANNEL_UDC_ID,SVC_PT_UDC_ID,MEAS_TYPE_UDC_ID,INTERVAL_END_TIME,LP_VALUE,VALIDATION_STATUS,CHANGE_METHOD,FAIL_CODE,LOCKED,ESTIMATED,EXT_VERSION_TIME,FLAGS,INTERVAL_LEN,DEVICE_UDC_ID,DEVICE_DATA_SRC_ID,AMI_RECORD_NUM,ORG_UDC_ID,LAST_UPD_BY,LAST_UPD_TIME,REC_VERSION_NUM
CHN-REVPRO-VS-6A_1-4,SDP-REVPRO-VS-6A_1,EM.ELECTRIC.KWH.1.4,2015-08-03T15:45:00+00:00,0.261,VAL,,,,,,0,900,0,5,,rporg1,53,2015-11-22T23:08:30+00:00,1448233710
CHN-REVPRO-VS-6A_1-4,SDP-REVPRO-VS-6A_1,EM.ELECTRIC.KWH.1.4,2015-08-03T16:00:00+00:00,0.36,VAL,,,,,,0,900,0,5,,rporg1,53,2015-11-22T23:08:30+00:00,1448233710
CHN-REVPRO-VS-6A_1-4,SDP-REVPRO-VS-6A_1,EM.ELECTRIC.KWH.1.4,2015-08-03T16:15:00+00:00,0.691,VAL,,,,,,0,900,0,5,,rporg1,53,2015-11-22T23:08:30+00:00,1448233710
CHN-REVPRO-VS-6A_1-4,SDP-REVPRO-VS-6A_1,EM.ELECTRIC.KWH.1.4,2015-08-03T16:30:00+00:00,0.558,VAL,,,,,,0,900,0,5,,rporg1,53,2015-11-22T23:08:30+00:00,1448233710


-Read CSV 
-Validate record
-Filter & save invalid records in file
-Parse record -- group first and then look up.. (use simple Query, )-- cache light objects.. in executor (grping is expnsv)
	Get MeteredSvcPtBO using svcPtUdcId -- BO is heavy
	using MeteredSvcPtBO get Ids for key creation
-Create Map with <key,value>
	Key (non bulk insert): svcptId,measTypeId,channelId,channelLen,usageDate,orgId
	Key (bulk insert):     svcptId,measTypeId,channelId,channelLen,year-month,orgId
	
	suggested incremental Key: svcptId,measTypeId
	suggested initial Key: svcptId,measTypeId,year-month
	
	keep one key : svcptId,measTypeId,usageDate
	
	value:record
-Iterate Map and tuple<key, value>
-Filter and save channel lookup failed records in file(missing udcId records)
-BuildAndSaveEntity
	-Merge Interval of Input with Hbase - Fill any missing intervals (Create NoDataLP i.e. value as 0) 
			-- chk last upd time .. only latest record ()
			-- always changing top of stack (discarding it OR creating new version) explicit setting versionTime in HBase
	-Create & Saves MeterDataEntity in HBase
	-Identify duplicate record with same intervalEndTime (correct it, )

-Save Dup records in file	

TODO
--discuss further with flowchart , handling of historical data
-top of stack, discared, versioning
-document assumptions

LP

CHANNEL_UDC_ID -- remove
SVC_PT_UDC_ID
MEAS_TYPE_UDC_ID
INTERVAL_END_TIME
LP_VALUE
VALIDATION_STATUS
CHANGE_METHOD
FAIL_CODE
LOCKED
ESTIMATED
EXT_VERSION_TIME
FLAGS
INTERVAL_LEN -- remove it
DEVICE_UDC_ID
DEVICE_DATA_SRC_ID -- need look up
AMI_RECORD_NUM
ORG_UDC_ID -- chk it
LAST_UPD_BY -- keep default value
LAST_UPD_TIME
REC_VERSION_NUM-- chk it (drop it)


header part of job arg

verify all fields are covered in 7x to 8x & 8x to 8x

validation: if readValue == null and readVale = 0

svcPtUdcId,measTypeUdcId,endTime,value,valStatus,changeMethod,failCode,locked,estimated,extVersionTime,flags,deviceUdcId,source,sourceDetail,amiRecNumber,origReadValue,lastUpdateBy,lastUpdTime


RP
-	java docs / start with hello world kind of / customization is extension in docs
refer  spring for sample 

remove sparkContext from scorer

https://wiki.emeter.com/display/ENG/mtm+-+Define+standard+for+historical+meter+data+load+to+HBase


https://docs.emeter.com/display/EIPDE/Structural+Data+Extraction+Overview


create extraction project and create its zip.. + classpath
************************************************************************************************************************************************


#mudrDataSource.url=jdbc:oracle:thin:@(DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=emdb35.emeter.com)(PORT=1521))(CONNECT_DATA=(SERVICE_NAME=poltp01)))
#mudrDataSource.username=rw_mudr227
#mudrDataSource.password=rw_mudr227

#amiDataSource.url=jdbc:oracle:thin:@(DESCRIPTION=(ADDRESS=(PROTOCOL=TCP)(HOST=emdb35.emeter.com)(PORT=1521))(CONNECT_DATA=(SERVICE_NAME=poltp01)))
#amiDataSource.username=rw_eip227
#amiDataSource.password=rw_eip227

jsap-2.1.jar
spektrum-emeter-0.9.1.jar

10:45 PMYadav Manas IN NOI STS
ALTER USER <username> QUOTA 100M ON data_s
GRANT UNLIMITED TABLESPACE TO <username>

10:45 PMRavi, Gupta IN NOI STSo
k ..
do i need to login sysdba 

10:46 PMYadav Manas IN NOI STS
tablespace pae privs denae hotae hai

10:46 PMRavi, Gupta IN NOI STS
k
10:46 PMYadav Manas IN NOI STS
eip_dba would do
10:46 PMRavi, Gupta IN NOI STS
k


missing 

 scan 'rporg:sdp',{COLUMNS => 'm:lpi'}
 scan 'rporg:sdp',{COLUMNS => 'm:rr'}
 
 
key:sdpId,measTypeId,channelId,channelLen,usageDateLong,deviceId,orgId
    5351, 33, 15786891,0,1402358400000,-1,402
    
    
2014-06-10T22:15:00+00:00  - 1402425000000
2014-06-10T22:30:00+00:00  - 1402425000000
2014-06-10T22:45:00+00:00  - 1402425000000
2014-06-10T22:45:00+00:00  - 1402425000000
2014-07-10T22:50:00+00:00  - 1405017000000


1405017900000
1405032600000 

---------------
EU > 50%
EI > 50% - gertEstimated issue

Device event header



select  /*+ PARALLEL(de, 4) */ 
		        TO_CHAR (FROM_TZ(TO_TIMESTAMP (TO_CHAR (de.event_time, 'MM-DD-YYYY HH24:MI:SS' ),'MM-DD-YYYY HH24:MI:SS' ),'00:00') AT TIME ZONE '00:00','yyyy-mm-dd"T"hh24:mi:ssTZH:TZM')||'~~'||
		        de.device_event_type_id||'~~'||
		        (select tmp.udc_id from tmp_sdp_id_to_udcid tmp where de.svc_pt_id = tmp.id)||'~~'||
		        TO_CHAR (FROM_TZ(TO_TIMESTAMP (TO_CHAR (de.reported_time, 'MM-DD-YYYY HH24:MI:SS' ),'MM-DD-YYYY HH24:MI:SS' ),'00:00') AT TIME ZONE '00:00','yyyy-mm-dd"T"hh24:mi:ssTZH:TZM')||'~~'||
		        || '"' || de.desc_text|| '"' ||'~~'|| 
		        (select tmp.udc_id from tmp_device_id_to_udcid tmp where de.device_id = tmp.id)||'~~'|| 
		        de.device_data_src_id||'~~'||
		        TO_CHAR (FROM_TZ(TO_TIMESTAMP (TO_CHAR (de.event_start_time, 'MM-DD-YYYY HH24:MI:SS' ),'MM-DD-YYYY HH24:MI:SS' ),'00:00') AT TIME ZONE '00:00','yyyy-mm-dd"T"hh24:mi:ssTZH:TZM')||'~~'||
		        de.event_record_num||'~~'||
		        de.num_occur||'~~'||
		        de.threshold||'~~'||
		        de.measured_value||'~~'||
		        de.duration||'~~'||
		        de.variance||'~~'||
		        || '"' || de.correlation_id || '"' ||'~~'||
		        (select tmp.udc_id from tmp_device_id_to_udcid tmp where de.incoming_device_id = tmp.id)||'~~'||
		        || '"' || de.incoming_device_type || '"' ||'~~'||
		        de.org_id||'~~'||--org_id
		        TO_CHAR (FROM_TZ(TO_TIMESTAMP (TO_CHAR (de.insert_time, 'MM-DD-YYYY HH24:MI:SS' ),'MM-DD-YYYY HH24:MI:SS' ),'00:00') AT TIME ZONE '00:00','yyyy-mm-dd"T"hh24:mi:ssTZH:TZM')||'~~'||
		        || '"' || dea.attr_name || '"' ||'~~'|| 
		        || '"' || dea.attr_value || '"'

To merge in Hadoop commons		        
		        
//EnergyIP/opt8/hadoop-common/trunk/em-hdoop-common-apps/src/com/emeter/loader/acloader/lp/LPEntityBuilder.java
//EnergyIP/opt8/hadoop-common/trunk/em-hdoop-common-apps/src/com/emeter/loader/acloader/lp/LpIntervalLoader.java
//EnergyIP/opt8/hadoop-common/trunk/em-hdoop-common-apps/src/com/emeter/loader/acloader/rr/RREntityBuilder.java
//EnergyIP/opt8/hadoop-common/trunk/em-hdoop-common-apps/src/com/emeter/loader/acloader/rr/RRRecordParser.java
//EnergyIP/opt8/hadoop-common/trunk/em-hdoop-common-apps/src/com/emeter/loader/acloader/rr/RegisterReadLoader.java
//EnergyIP/opt8/hadoop-common/trunk/em-hdoop-common-apps/test/com/emeter/loader/ac8x/LoaderJunitTestSuite.java
//EnergyIP/opt8/hadoop-common/trunk/em-hdoop-common-apps/test/com/emeter/loader/ac8x/csv/LPReads.csv
//EnergyIP/opt8/hadoop-common/trunk/em-hdoop-common-apps/test/com/emeter/loader/ac8x/csv/RegisterReads.csv
//EnergyIP/opt8/hadoop-common/trunk/em-hdoop-common-apps/test/com/emeter/loader/ac8x/csv/_SUCCESS
//EnergyIP/opt8/hadoop-common/trunk/em-hdoop-common-apps/test/com/emeter/loader/ac8x/csv/rr/RegisterReads.csv
//EnergyIP/opt8/hadoop-common/trunk/em-hdoop-common-apps/test/com/emeter/loader/ac8x/csv/rr/_SUCCESS
//EnergyIP/opt8/hadoop-common/trunk/em-hdoop-common-apps/test/com/emeter/loader/ac8x/rr/TestRRLoader.java		        

-compare default data threshold values
-Docs
-Sanjay issue


------------
-EDP: new set or req
-Epic and story updated in Jira

**********************************************************************************************************************

--expression evaluate - java exp language, scheme, velocity exp / spring exp / jet exp
-

groupByKeys	String	svcPtUdcId,measTypeUdcId,endTime	Key will group the data and grouped data will be processed on each node. InputDelimiter property to separate fields will be used to group the data before processing. svcPtUdcId and measTypeUdcId are must key and if eventTime is also passed then Year and Month as YYYYMM will be used from that to group the monthly data for processing.


************************************************************************
- LastUpdTime from CSV is stored to extVersion Time in HBase
- Loader is not populating Version time, its being take care by system


Input LPInterval - from CSV
HBase LPInterval - from HBase


Merge Cases for LPInterval data:
1) Duplicate: Input LPInterval data has all fields same as Hbase Interval data -> consider it as duplicate and don't save

If(isDuplicate(InputLpInterval, HbaseLpInterval)){

	//..exclude duplicate records from saving
}

-

2) Merge case: No data and latest record merge

if( No InputLpInterval data for endTime but HbaseLpInterval has 
		|| InputLpInterval.extVersionTimeLong <  HbaseLpInterval.extVersionTimeLong ) {
		
				
		//..merge record from HBase to input
}

- Input LPInterval does not have endTime data but Hbase Interval has -> merge record from HBase to input

- Input LPInterval data has smaller extVersionTime(lastUpdTime) does  Hbase Interval's extVersionTime(lastUpdTime)  
			-> merge record from HBase to input


3) Missing intervals : Use Channel Len to populate missing intervals with 0 value


4) LPdaily has MaxVersion time






	public Calendar usageDateForIntervalEndTs(String date, TimeZone premiseTz) throws Exception {
		
		Calendar cal = null; 
		cal = dt.iso8601StringToCalendar(date);
		long ts = cal.getTimeInMillis();
		cal = Calendar.getInstance(premiseTz); 
		cal.setTimeInMillis(ts);		
		cal.getTime();
		if (cal.get(Calendar.HOUR_OF_DAY) == 0 && cal.get(Calendar.MINUTE) == 0 && cal.get(Calendar.SECOND) == 0) {
			cal.add(Calendar.DAY_OF_MONTH, -1);
		} else {
			cal.set(Calendar.HOUR_OF_DAY, 0); 
			cal.set(Calendar.MINUTE, 0); 
			cal.set(Calendar.SECOND, 0);
			cal.set(Calendar.MILLISECOND, 0);
		}
		cal.getTime();		
		return cal;
	}
	
	
-No type default is used
-No data is provided in CSV it is not set to protoBuff(Hbase)
-Timezone is settlement timezone
-

****************************************************************************
Use cases:
LP loader 

1) Filter (filterCriteria): LP loader by default filters records starting with #
# svcPtUdcId,svcPtType,measTypeUdcId,endTime,value,valStatus,changeMethod,failCode,locked,estimated,origExtVersionTime,extVersionTime,flags,deviceUdcId,deviceType,deviceDataSrc,deviceDataSrcDetail,origAmiRecNumber,amiRecNumber,lastUpdateBy

2) Invalid record: Such records are not reusable and are mainly used for logging
		(i) err: Record which fail the validation process are logged to file err-part-00000 under invalid folder
				 Below e.g. record does not have SvcPtUdcId
				 ,ServiceDeliveryPoint,EM.ELECTRIC.KWH.1.1,2014-07-10T22:30:00+00:00,17144,VAL,,134225920,,,,2014-06-11T01:43:38+00:00,,1-2LXCRM Udc,Meter,1001,1001,0,1,testds1
		
		(ii) exception: Exception records are recorded to exception-part-00000 under invalid folder
		Below e.g. record string value abc for value. We get number for
		1-3M0301 Udc,ServiceDeliveryPoint,EM.ELECTRIC.KWH.1.1,2014-07-10T22:30:00+00:00,abc,VAL,,134225920,,,,2014-06-11T01:43:38+00:00,,1-2LXCRM Udc,Meter,1001,1001,0,1,testds1

Note: Every alternate line in file is a comment and starts with #. It tells the potential cause of failure

3) MissingFK records: missing FK referecnce records are logged in file missingFK-part-00000		
		(i)  invalid folder:  missing FK referecnce records are logged but they are non reusable records with comment.
		(ii) missingFK folder: exact records are copied to this folder and can be re-used once FK ref is resolved
	
4)Input file has duplicate record
	(i) Records with same endTime: Out of N similar records (same endTime), only one record wins which
			-latest ie record with older extVerdion time(lastUpdTime). If same extVersion then record with 
			 greator amiRecNumber.
					
	e.g. it is saved
1-3M0301 Udc,ServiceDeliveryPoint,EM.ELECTRIC.KWH.1.1,2014-06-10T22:45:00+00:00,17144,VAL,,134225920,,,,2014-06-11T01:43:38+00:00,,1-2LXCRM Udc,Meter,1001,1001,0,1,testds1
1-3M0301 Udc,ServiceDeliveryPoint,EM.ELECTRIC.KWH.1.1,2014-06-10T22:45:00+00:00,17145,VAL,,134225920,,,,2014-06-11T01:43:39+00:00,,1-2LXCRM Udc,Meter,1001,1001,0,2,testds1
1-3M0301 Udc,ServiceDeliveryPoint,EM.ELECTRIC.KWH.1.1,2014-06-10T22:45:00+00:00,17146,VAL,,134225920,,,,2014-06-11T01:43:39+00:00,,1-2LXCRM Udc,Meter,1001,1001,0,3,testds1
	
5) Input file and HBase has same endTime record: latest record is picked
	-latest ie record with older extVerdion time(lastUpdTime). If same extVersion then record with 
			 greator amiRecNumber.	

ind-lnxapp53

Export:
env
./ReferenceDataUtil.sh -Dapplication.command=export -DexportReferenceDataService.referenceDataOutputDir=/home/eip/RGA/RDU/

org[FLEXSYNCORG Sample rporg]
./ReferenceDataUtil.sh -Dapplication.command=export -Dapplication.scope=org -Dapplication.orgName=FLEXSYNCORG -DexportReferenceDataService.referenceDataOutputDir=/home/eip/RGA/RDU/FLEXSYNCORG
./ReferenceDataUtil.sh -Dapplication.command=export -Dapplication.scope=org -Dapplication.orgName=Sample -DexportReferenceDataService.referenceDataOutputDir=/home/eip/RGA/RDU/Sample
./ReferenceDataUtil.sh -Dapplication.command=export -Dapplication.scope=org -Dapplication.orgName=rporg -DexportReferenceDataService.referenceDataOutputDir=/home/eip/RGA/RDU/rporg

$ cd /home/eip/RGA/RDU/test
ensure java file sdoes not have package declaration

javac -cp rdu/commons-io-1.4.jar:rdu/log4j-1.2.15.jar rdu/*.java

java -cp rdu/.:rdu/commons-io-1.4.jar:rdu/log4j-1.2.15.jar TestSeedData

import 
/home/eip/bin/ReferenceDataUtil.sh -Dapplication.command=import -Dapplication.scope=org -Dapplication.orgName=rporg -DimportReferenceDataService.referenceDataSourcePath=/home/eip/ajit -DvalidationConfig.flexAttributeFieldNameValidationsEnabled=false -DvalidationConfig.flexAttributeFieldValueValidationsEnabled=false


device event
/home/pipe/RGA/deviceEvent


2012-03-29 00:00:00
2012-03-30 00:00:00

Interval
/home/pipe/RGA/interval


2011-12-30 00:00:00
2011-12-31 00:00:00



RR
/home/pipe/RGA/interval

2009-04-15 00:00:00
2012-10-30 00:00:00


CREATE DATABASE LINK MUDR2EIP CONNECT TO EIP94 IDENTIFIED BY VALUES EIP94 USING EMDB39.EMETER.COM:1521/OLTP15;

CLDALYT-1666 - Enumrations not getting updated for RP in table data_svc_gen_conf
CLDALYT-1667 - Savemode property in data svc verstion attr is never used


Test cases:
1)Reads row which has dataServiceId as null also selected from query
2)


D:\work\codebase\P4-WS\Perforce-WS\EnergyIP\opt8\hadoop-common\trunk\project.common.properties
${project_loc}/version.properties
${project_loc}/build.properties

ivysettings_hadoop.xml

...




Create HDFS folders for external tables done.
Setting Facl for HdfsDbFolders
Done setting Facl for HdfsDbFolders
Create Hive database ...
beeline -u jdbc:hive2://lnxcdh23.emeter.com:10000/rporg1;principal=hive/lnxcdh23.emeter.com@HADOOP.EMETER.COM
scan complete in 3ms
Connecting to jdbc:hive2://lnxcdh23.emeter.com:10000/rporg1;principal=hive/lnxcdh23.emeter.com@HADOOP.EMETER.COM
Connected to: Apache Hive (version 1.1.0-cdh5.4.4)
Driver: Hive JDBC (version 1.1.0-cdh5.4.4)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 1.1.0-cdh5.4.4 by Apache Hive
0: jdbc:hive2://lnxcdh23.emeter.com:10000/rpo> create database IF NOT EXISTS rporg1;
No rows affected (0.188 seconds)
0: jdbc:hive2://lnxcdh23.emeter.com:10000/rpo> !quit
Closing: 0: jdbc:hive2://lnxcdh23.emeter.com:10000/rporg1;principal=hive/lnxcdh23.emeter.com@HADOOP.EMETER.COM
Create Hive database done.
Create Hive tables ...
scan complete in 3ms
Connecting to jdbc:hive2://lnxcdh23.emeter.com:10000/rporg1;principal=hive/lnxcdh23.emeter.com@HADOOP.EMETER.COM?orgHdfsUser=rporg1;orgHdfsHome=/user/rporg1
Connect


https://docs.emeter.com/display/PQ10/Installing+Power+Quality
https://docs.emeter.com/pages/viewpage.action?spaceKey=PQ10&title=Installing+Hive

jdbc:hive2://lnxcdh23.emeter.com:10000/rporg1;AuthMech=1;KrbRealm=HADOOP.EMETER.COM;KrbHostFQDN=lnxcdh23.emeter.com;KrbServiceName=hive

supported QE and updated RP docs as per QE review comments
HC doc update. 
-Mark it as known issue in HC

Extractor: working on asset data extraction
-initial 



Note: Hive installation includes:
Creation of required folder structure
${ORG_HDFS_HOME}/data/main/rp_analysis
Creation of hive database. Database with ${orgHdfsName} is created.
Creation of RP Hive schema
Verification of Hive installation:
Check creation of Hive folder structures:
$ hdfs dfs -ls ${ORG_HDFS_HOME}/data/main/
Above command should show directory with name rp_analysis
Check Hive schema
Start Hive shell
$ hive
Switch to database with ${orgHdfsName}
hive> use ${orgHdfsName};
e.g. hive> use rporg1;
Verify existence of Hive table rp_analysis.
hive> select * from rp_analysis;
Above command gives no error and fetches 0 rows.



Verify following staging directories in HDFS, create them if case they are missing
LoadDeviceEvents job 
hdfs dfs -ls /user/<org_UdcId>/staging/device_event_s_v1
 
Command displays in and processed directory.
If no directory found, create using below commands
 
hdfs dfs -mkdir -p /user/<org_UdcId>/staging/device_event_s_v1/in
hdfs dfs -mkdir -p /user/<org_UdcId>/staging/device_event_s_v1/processed/LoadDeviceEvents
LoadIntervalReads job
hdfs dfs -ls /user/<org_UdcId>/staging/interval_read_s_v1
 
Command displays in and processed directory.
If no directory found, create using below commands
 
hdfs dfs -mkdir -p /user/<org_UdcId>/staging/interval_read_s_v1/in
hdfs dfs -mkdir -p /user/<org_UdcId>/staging/interval_read_s_v1/processed/LoadIntervalReads
LoadRegisterReads job
hdfs dfs -ls /user/<org_UdcId>/staging/register_read_s_v1
 
Command displays in and processed directory.
If no directory found, create using below commands
 
hdfs dfs -mkdir -p /user/<org_UdcId>/staging/register_read_s_v1/in
hdfs dfs -mkdir -p /user/<org_UdcId>/staging/register_read_s_v1/processed/LoadRegisterReads


-----------------RP docs
-https://docs.emeter.com/display/RP20/Revenue+Protection+Analytics+2.0
	-remove expression based scorer as child page add to architecture or some other page
	-Sample Configurations For Revenue Protection should be part of Installing and Configuring Revenue Protection

-https://docs.emeter.com/display/RP20/Revenue+Protection+Architecture
	-discuss MLLib scorer content
	

21/3
CLDALYT-1791	Incremental asset extract - create triggers to capture updated assets and SDPs
CLDALYT-1792	Incremental asset extract - create queries to extract updated assets and SDPs

under epic 

CLDALYT-1472 
EIP 8.3 - Structural data extraction and load using FlexSync


	
Extractor opt7: em-ac-dataextractor-3.0-232837.zip
Extractor opt8: em-dataextractor-xxxx.zip
Loader opt8: em-ac-adf-xxx

Extractor applicationName: CloudAlytStructuralExtraction  | StructuralDataExtraction
instanceId: 1 

Loader applicationName: ACDataLoader | StructuralDataLoader
insatnceId: 1



Extractor opt8: em-dataextractor-3.0-xxxxxx.zip | StructuralDataExtraction 1
Loader opt8: em-ac-dataloader-3.1-xxxxxx.zip | StructuralDataLoader 1


----

planning to have walkthru of design code for extractor / loader 
and later execute extraction and load flow

KCBPU data for extraction: currently 7.6 but 

eip301_ftCollins@emdb36.emeter.com
eip301
emdb36.emeter.com
oltp07

oltp06

EIP

GRANT READ ON SVC_PT TO MUDR227;
GRANT READ ON CHANNEL TO MUDR227;
GRANT READ ON DEVICE TO MUDR227;
GRANT READ ON MEAS_TYPE TO MUDR227;
GRANT READ ON EIP_USER TO MUDR227;
GRANT READ ON ORG TO MUDR227;

MUDR

CREATE SYNONYM SVC_PT FOR EIP227.SVC_PT;
CREATE SYNONYM CHANNEL FOR EIP227.CHANNEL;
CREATE SYNONYM DEVICE FOR EIP227.DEVICE;
CREATE SYNONYM MEAS_TYPE FOR EIP227.MEAS_TYPE;
CREATE SYNONYM EIP_USER FOR EIP227.EIP_USER;
CREATE SYNONYM ORG FOR EIP227.ORG;


http://goasiantv.com/ip-man-3-yip-man-3.html


loader
1) ApplicationExtension_FlexsyncFileService.xml -> update  
<con:libraries>
		<con:libraryName>em-hadoop-common-apps.jar</con:libraryName>
</con:libraries> 

2) Library_hadoopCommon.xml add-> 
<con:libraries>
		<con:libraryName>em-hadoop-common-apps.jar</con:libraryName>
	</con:libraries> 
	
3) JobDef_StructDataLoader -> rporg1

4) update job_group set VSEP_NAME = 'sharedJobExecutor' where id = 51;

5) applicationExtension to application structural Loader 

6) 

reverse 
update job_group set VSEP_NAME = 'OozieJobExecutor' where id = 51;
OozieJobExecutor


2016-04-19 05:31:26,203 [main] INFO  - APPLICATION SUCCESSFULLY STARTED
2016-04-19 05:31:26,219 [main] INFO  - AcdataLoader process start
2016-04-19 05:31:26,402 [main] INFO  - Unzip  start
2016-04-19 05:31:27,432 [main] INFO  - Unzip  completed
2016-04-19 05:31:27,459 [main] INFO  - Reference data loading start
2016-04-19 05:31:27,469 [main] INFO  - Org: rporg1
2016-04-19 05:31:27,485 [main] INFO  - building command.size: 7
2016-04-19 05:31:27,513 [main] INFO  - building command: /home/eip/bin/ReferenceDataUtil.sh
2016-04-19 05:31:27,530 [main] INFO  - building command: -Dapplication.scope=org
2016-04-19 05:31:27,549 [main] INFO  - building command: -Dapplication.orgName=rporg1
2016-04-19 05:31:27,564 [main] INFO  - building command: -Dapplication.command=import
2016-04-19 05:31:27,576 [main] INFO  - building command: -DvalidationConfig.flexAttributeFieldNameValidationsEnabled=false
2016-04-19 05:31:27,588 [main] INFO  - building command: -DvalidationConfig.flexAttributeFieldValueValidationsEnabled=false
2016-04-19 05:31:27,601 [main] INFO  - building command: -DimportReferenceDataService.referenceDataSourcePath=/home/eip/data/structuraldataloader/in/ADF_StructuralData_20160418_182021_temp
2016-04-19 05:31:27,616 [main] INFO  - Command: [/home/eip/bin/ReferenceDataUtil.sh, -Dapplication.scope=org, -Dapplication.orgName=rporg1, -Dapplication.command=import, -DvalidationConfig.flexAttributeFieldNameValidationsEnabled=false, -DvalidationConfig.flexAttributeFieldValueValidationsEnabled=false, -DimportReferenceDataService.referenceDataSourcePath=/home/eip/data/structuraldataloader/in/ADF_StructuralData_20160418_182021_temp]
2016-04-19 05:31:27,631 [main] INFO  - Executing ReferenceDataUtil tool
2016-04-19 05:31:35,710 [Thread-11] ERROR - SLF4J: Class path contains multiple SLF4J bindings.
2016-04-19 05:31:35,731 [Thread-11] ERROR - SLF4J: Found binding in [jar:file:/home/eip/core-lib/activemq-all-5.8.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2016-04-19 05:31:35,750 [Thread-11] ERROR - SLF4J: Found binding in [jar:file:/home/eip/core-lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2016-04-19 05:31:35,770 [Thread-11] ERROR - SLF4J: Found binding in [jar:file:/home/eip/core-lib/slf4j-simple-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
2016-04-19 05:31:35,794 [Thread-11] ERROR - SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
2016-04-19 05:31:35,824 [Thread-11] ERROR - SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
2016-04-19 05:33:59,674 [main] INFO  - ReferenceDataUtil tool Completed with exit value: 0
2016-04-19 05:33:59,693 [main] INFO  - Reference data loading completed
2016-04-19 05:33:59,733 [main] INFO  - AssetAndSdpStackDataLoader OrgList::[rporg1]
2016-04-19 05:33:59,768 [main] INFO  - AssetAndSdpStackDataLoader data loading of input XML to flexSync Incoming dir start for Org:rporg1
2016-04-19 06:12:27,727 [main] INFO  - AssetAndSdpStackDataLoader data loading of input XML to flexSync Incoming dir done for Org:rporg1
2016-04-19 06:12:27,744 [main] INFO  - AssetAndSdpStackDataLoader data loading of input XML to flexSync Incoming dir completed
2016-04-19 06:12:27,784 [main] INFO  - Clean  completed for file:ADF_StructuralData_20160418_182021.zip
2016-04-19 06:12:27,799 [main] INFO  - LoaderStatus  start
2016-04-19 06:45:00,206 [main] INFO  - ADF_StructuralData_20160418_182021.zip :: No of messages failed at FlexSync :8
2016-04-19 06:45:00,217 [main] INFO  - LoaderStatus Done for ADF_StructuralData_20160418_182021.zip
2016-04-19 06:45:00,224 [main] INFO  - AcdataLoader process complete
2016-04-19 06:45:00,234 [main] INFO  - Startables stopping order = [[]]
2016-04-19 06:45:00,340 [main] INFO  - APPLICATION SUCCESSFULLY STOPPED
2016-04-19 06:45:00,351 [main] INFO  - SHUTDOWN
2016-04-19 06:45:00,363 [main] INFO  - BONotificationManager : Stopping TransactionReceiver thread...
2016-04-19 06:45:00,372 [main] INFO  - BONotificationManager : TransactionReceiver shutdown invoked
2016-04-19 06:45:00,380 [Thread-8] INFO  - BONotificationManager : TransactionReceiver exiting...
2016-04-19 06:45:00,394 [main] INFO  - Shutting down JMX Registration client
2016-04-19 06:45:00,402 [main] INFO  - DeRegister port on HostAgent Started ..... 
2016-04-19 06:45:00,430 [main] INFO  - DeRegister port on HostAgent End ..... 
2016-04-19 06:45:00,457 [main] INFO  - Before releasing session
2016-04-19 06:45:00,479 [main] INFO  - Closing com.emeter.hydrofw.application.HydroApplicationContextImpl@4cc6fa2a: startup date [Tue Apr 19 05:31:15 PDT 2016]; root of context hierarchy
2016-04-19 06:45:00,512 [main] INFO  - Shutting down EhCache CacheManager
2016-04-19 06:45:00,529 [main] INFO  - Shutting down EhCache CacheManager



AssetDataDevice Total Records:44015
AssetDataRoute Total Records:0
AssetDataAccount Total Records:0
AssetDataPremise Total Records:3842
AssetDataDistNode Total Records:0

ServiceDeliveryPoint,12194
TotalChannel,57179,Channel,57179,VirtualChannel,0
servicePointServiceAgreementAssociation,13429
servicePointDataServiceAssociation,7655
servicePointDeviceAssociation,13373
deviceChannelAssociation,58885
deviceFunctionAssociation,13188
servicePointServicePointGroupAssociation,0
accountServicePointAssociation,0
servicePointServicePointAssociation SDP-DN,0
servicePointServicePointAssociation DN-DN,0

validationConfig.effectiveDatedRelationshipValidationsEnabled=true
validationConfig.effectiveDatedParameterValidationsEnabled=true
validationConfig.staticFieldValueValidationsEnabled=false
validationConfig.flexAttributeFieldNameValidationsEnabled=false
validationConfig.flexAttributeFieldValueValidationsEnabled=false


AssetDataDevice Total Records:8250
AssetDataRoute Total Records:10
AssetDataAccount Total Records:0
AssetDataPremise Total Records:4110
AssetDataDistNode Total Records:10


ServiceDeliveryPoint,4110
TotalChannel,24625,Channel,24615,VirtualChannel,10
servicePointServiceAgreementAssociation,30
servicePointDataServiceAssociation,4139
servicePointDeviceAssociation,4130
deviceChannelAssociation,24625
deviceFunctionAssociation,4120
servicePointServicePointGroupAssociation,10
accountServicePointAssociation,0
servicePointServicePointAssociation SDP-DN,10
servicePointServicePointAssociation DN-DN,0



RP overview
Prerequisites - loader 
Revenue Protection Architecture
	-Configure Revenue Protection - dataSvc gen 
		Feature + scorers + dataStores + Filter

2 RP scorer  
	- Expression based scorer
	- MLLIb scorer ()
	
	
Segment Q - SVC_PT_SEGMENT_QUERY table.
OutCome table:  RP_INVTG_SDP_OUTCOME

2015-10-04T00:00:00

hbase shell
scan 'rporg1:rpresult'

hive
use rporg1;
select * from rp_analysis;

^#(?:[0-9a-fA-F]{3}){1,2}$
^#(?:[0-9a-fA-F]{1,2}){3}$
^(?:[0-9a-fA-F]{3}){1,2}$


IN-1901-1409410
IN-1901-1409412


-*-----------------------
KCBPU Structural extraction:

jdbc:oracle:thin:@(DESCRIPTION=(ADDRESS_LIST=(ADDRESS=(PROTOCOL=TCP)(HOST=emdb44.emeter.com)(PORT=1521)))(CONNECT_DATA=(SERVER=DEDICATED)(SERVICE_NAME=kcbpu_s)))
./ApplicationLauncher.sh start CloudAlytStructuralExtraction 1 -Da108-structuralExtractorDao.listOfOrgs=007169584


No support 
	SVCAgree
		-Pricing Plan
	DataSvc
		-Web Presentment
		-CO2 Plan
		-Virtual Channel Persistence Service
		 Virtual Channel Persistence Service	Service	Virtual Channel Persistence	Virtual Channel Persistence Service	VirtualChannelPersistence
	-Estimation becomes VEE	
		
		
		
		
		
Seed data
----------
DataSvc
	-Data Collection (27)
		Data Collection, Daily CC, Interval
		Data Collection, Daily DCSI, Interval
		Data Collection, Daily Itron FN, Interval
		Data Collection, Daily Itron OpenWay, Interval
		Data Collection, Daily MV90, Interval
		Data Collection, Daily, AI
		Data Collection, Daily, CC
		Data Collection, Daily, Cellnet
		Data Collection, Daily, Cellnet DRR
		Data Collection, Daily, DCSI
		Data Collection, Daily, EnergyAxis
		Data Collection, Daily, FlexNet
		Data Collection, Daily, Generic
		Data Collection, Daily, Hunt
		Data Collection, Daily, Itron FN
		Data Collection, Daily, Itron OpenWay
		Data Collection, Daily, L+G CommandCenter
		Data Collection, Daily, LGCommandCenter4.1
		Data Collection, Daily, MAS
		Data Collection, Daily, MV90
		Data Collection, Daily, TMS
		Data Collection, Daily, TUNet
		Data Collection, Daily, TrilliantMesh
		Data Collection, Monthly, MV90
		Data Collection, Monthly, MVRS
		Data Collection, Monthly, Manual
		Data Collection, Monthly, Manual Itron PP
	-Data Delivery (15)
		Sample DDS Service
		Data Delivery, Cycle Billing
		Data Delivery, Med and Large Comm Demand
		Data Delivery, Daily, LR
		Data Delivery, Register Read Electric Billing with Col Demand
		Data Delivery, Register Read Electric Billing
		Data Delivery, Water Billing
		Data Delivery, TOU Electric Billing
		Data Delivery 3, Cycle Billing
		Data Delivery, Monthly, CAIR
		Data Delivery, Register Read Electric Billing with Demand
		Data Delivery, TOU Med and Large Comm Demand
		Data Delivery, Cycle Billing - Commercial and Industrial
		Data Delivery,Dummy
		Data Delivery, Register Read Electric Billing Test Only
	-Data Transfer	(4)
		Data Transfer, Daily
		Data Transfer, Monthly
		Data Transfer, Real-time
		Data Transfer, Weekly
	-VEE (31)
		Test - Elec VEE
		VEE Service, Residential
		Sample VEE California Rules Service with Daily Extrapolation
		VEE Service - Residential
		VEE Service - Commercial
		Small Comm - Water VEE
		Small Comm - Elec VEE
		VEE Service, Transformer Type
		VEE Service, Seasonal
		Sample VEE Australia Rules Service
		VEE Service - Large Commercial and Industrial
		VEE, Standard
		VEE Service, No Estimation
		VEE Service - No Estimation
		Sample VEE California Rules Service with Post-Processing enabled
		Sample VEE California Rules Service
		Sample VEE California Rules Service - Estimation Disabled
		Wholesale and Industrial - Water VEE
		VEE Service - Residential 3
		Res - Water VEE
		VEE Service, No Validation, No Estimation
		Generic VEE Service
		VEE Service, No Validation
		VEE Service, Residential - Electric Heat
		VEE Service - Small Commercial
		Med and Large Comm - Elec VEE
		Sample Validation Disabled VEE Service
		Sample VEE Australia Rules Service - No CLP Estimation
		VEE Service - FireLine
		Res - Elec VEE
		VEE Service, Small General Service
	-Framing (1)
		Generic Framing Service
	-Deployment Planning (0)
	-Virtual Channel Persistence (1)
		Virtual Channel Persistence Service
	-Estimation: becomes VEE (9)
		Estimation, Customer Class RES
		Estimation, No Account
		Register Read Estimation
		RRE - Elec
		RRE - Water
		Estimation, Customer Class KEY/IND
		Estimation, Customer Class LIFESUPP
		Estimation, Customer Class CBS
		Estimation, Irrigation
	-Outage(10)
		Aged Restore Event Validation
		Aged Outage Event Duration
		Aged Restore Event Duration
		Power Outage Delay Validation
		Generic Residential Outage
		Open SR Check Indicator
		Power Outage Delay Duration
		Aged Outage Event Validation
		Global Generic Outages Service
		Generic Commercial Outage
		
SvcAgree
	-Energy Purchase(13)
		Energy Purchase Service - TOU Advanced
		Energy Purchase Service - Test
		Energy Purchase Service, Generic
		Energy Purchase Service - TOU Daily
		Energy Purchase Service - TOU Standard
		Energy Purchase Service, TOU/CPP (EST)
		Demand Forgiveness Energy Purchase Service
		Energy Purchase Service, Periodic
		Energy Purchase Service, TOU/CPP (EDT)
		Generic Energy Purchase Service - Water
		Generic Energy Purchase Service
		Energy Purchase Service, TOU/CPP (CST)
		Energy Purchase Service, Hourly
	-Generation Balance Provider(0)
	-Consumption Balance Provider(0)
	-Billing(1)
		Billing Service
	-AMI Operator (1)
		AMI Operator Service
	-Energy Supplier (1)
		Energy Supplier Service
	-Energy Services Agent

Measurements
	Usage (12)
		Usage, Daily, MWH Received
		Usage, Daily, KVAH
		Usage, Daily, KVARH Received
		Collected Usage, Daily, KW - TO BE DELETED
		Usage, Daily, MWH
		Usage, Daily, GAL
		Usage, Daily, CCF
		Usage, Daily, KWH
		Usage, Daily, KWH Backup
		Usage, Daily, KWH Received
		Virtual Usage, Daily, KWH
		Usage, Daily, KVARH
	Interval Data (34)
		15 Minute Interval Data, Voltage 120V Phase A
		60 Minute Interval Data, CCF
		15 Minute Interval Data, KVAH
		15 Minute Interval Data, Voltage 240V
		5 Minute Interval Data, Voltage
		15 Minute Interval Data, MWH
		30 Minute Interval Data, KWH
		15 Minute Interval Data, Lagging KVARH
		15 Minute Interval Data, KVARH
		15 Minute Interval Data, Voltage
		30 Minute Interval Data, KVAH
		Virtual, 15 Minute Interval Data, KWH
		15 Minute Interval Data, KWH Received
		15 Minute Interval Data, KVARH Received
		15 Minute Interval Data, KWH
		30 Minute Interval Data, KVARH
		60 Minute Interval Data, GAL
		60 Minute Interval Data, KWH Received
		15 Minute Interval Data, -KWH
		5 Minute Interval Data, KVAH
		Virtual, 60 Minute Interval Data, KWH
		Virtual, 30 Minute Interval Data, KWH
		Virtual Persisted, 30 Minute Interval Data, KWH
		15 Minute Interval Data, MWH Received
		60 Minute Interval Data, KVARH
		60 Minute Interval Data, KWH
		15 Minute Interval Data, KVAR
		60 Minute Interval Data, Voltage
		Virtual Persisted, 60 Minute Interval Data, KWH
		15 Minute Interval Data, Voltage 120V Phase C
		5 Minute Interval Data, KWH
		30 Minute Interval Data, Voltage
		60 Minute Interval Data, KVAH
		Virtual Persisted, 15 Minute Interval Data, KWH
	Cumulative Consumption	(8) - split into 18
		Register, KVARH
		Register, CCF
		Register, KWH
		Register, KWH Backup
		Register, Received, KWH
		Register, GAL
		Register, KWH Received
		Register, KVARH Backup


Run1 
----
2016-04-27 02:55:56,749 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - APPLICATION SUCCESSFULLY STARTED
2016-04-27 02:55:56,749 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - StructuralExtractApplication Mode:: initial
2016-04-27 02:56:00,606 com.emeter.extractor.runner.seeddata.SeedDataCycleCdExtractRunner [pool-2-thread-2] INFO  - Total Records for cycleCd extraction:29
2016-04-27 02:56:00,636 com.emeter.extractor.runner.seeddata.SeedDataExtractRunner [pool-2-thread-1] INFO  - Total Records for Seed data extraction:482
2016-04-27 02:59:13,878 com.emeter.extractor.runner.assetdata.device.AssetDataDevice [main] INFO  - AssetDataDevice Total Records:414310
2016-04-27 02:59:13,879 com.emeter.extractor.runner.assetdata.route.AssetDataRoute [main] INFO  - AssetDataRoute Total Records:486
2016-04-27 02:59:13,879 com.emeter.extractor.runner.assetdata.account.AssetDataAccount [main] INFO  - AssetDataAccount Total Records:101352
2016-04-27 02:59:13,879 com.emeter.extractor.runner.assetdata.premise.AssetDataPremise [main] INFO  - AssetDataPremise Total Records:73998
2016-04-27 02:59:13,879 com.emeter.extractor.runner.assetdata.distnode.AssetDataDistNode [main] INFO  - AssetDataDistNode Total Records:0
2016-04-27 03:04:18,573 com.emeter.extractor.runner.sdpstackdata.SdpStackData [main] INFO  - SdpStackData stat : 
ServiceDeliveryPoint,241744
TotalChannel,498239,Channel,498239,VirtualChannel,0
servicePointServiceAgreementAssociation,0
servicePointDataServiceAssociation,208849
servicePointDeviceAssociation,240892
deviceChannelAssociation,663057
deviceFunctionAssociation,193302
servicePointServicePointGroupAssociation,294162
accountServicePointAssociation,383132
servicePointServicePointAssociation SDP-DN,0
servicePointServicePointAssociation DN-DN,0
2016-04-27 03:04:18,605 com.emeter.extractor.impl.StructuralExtractManager [main] INFO  - creating zip file... 
2016-04-27 03:05:16,392 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - Done Structural execution in 559.642767 sec	


RUN 2 - with KCBPU mapping file
-------------------------------

$ ./ApplicationLauncher.sh start CloudAlytStructuralExtraction 1 -Da108-structuralExtractorDao.listOfOrgs=007169584 -Da108-structuralExtractorDao.dnHierarchyMode=both


2016-04-28 23:16:13,860 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - APPLICATION SUCCESSFULLY STARTED
2016-04-28 23:16:13,860 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - StructuralExtractApplication Mode:: initial
2016-04-28 23:16:17,827 com.emeter.extractor.runner.seeddata.SeedDataCycleCdExtractRunner [pool-2-thread-2] INFO  - Total Records for cycleCd extraction:29
2016-04-28 23:16:18,013 com.emeter.extractor.runner.seeddata.SeedDataExtractRunner [pool-2-thread-1] INFO  - Total Records for Seed data extraction:545
2016-04-28 23:19:08,088 com.emeter.extractor.runner.assetdata.device.AssetDataDevice [main] INFO  - AssetDataDevice Total Records:414310
2016-04-28 23:19:08,089 com.emeter.extractor.runner.assetdata.route.AssetDataRoute [main] INFO  - AssetDataRoute Total Records:486
2016-04-28 23:19:08,089 com.emeter.extractor.runner.assetdata.account.AssetDataAccount [main] INFO  - AssetDataAccount Total Records:101352
2016-04-28 23:19:08,089 com.emeter.extractor.runner.assetdata.premise.AssetDataPremise [main] INFO  - AssetDataPremise Total Records:73998
2016-04-28 23:19:08,089 com.emeter.extractor.runner.assetdata.distnode.AssetDataDistNode [main] INFO  - AssetDataDistNode Total Records:0
2016-04-28 23:30:21,566 com.emeter.extractor.runner.sdpstackdata.SdpStackData [main] INFO  - SdpStackData stat : 
ServiceDeliveryPoint,241744
TotalChannel,2563053,Channel,2563053,VirtualChannel,0
servicePointServiceAgreementAssociation,182367
servicePointDataServiceAssociation,1602845
servicePointDeviceAssociation,240892
deviceChannelAssociation,3593631
deviceFunctionAssociation,193302
servicePointServicePointGroupAssociation,294162
accountServicePointAssociation,383132
servicePointServicePointAssociation SDP-DN,0
servicePointServicePointAssociation DN-DN,0

Run 3 
-------

2016-05-02 06:21:32,536 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - APPLICATION SUCCESSFULLY STARTED
2016-05-02 06:21:32,536 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - StructuralExtractApplication Mode:: initial
2016-05-02 06:21:36,659 com.emeter.extractor.runner.seeddata.SeedDataCycleCdExtractRunner [pool-2-thread-2] INFO  - Total Records for cycleCd extraction:29
2016-05-02 06:21:37,096 com.emeter.extractor.runner.seeddata.SeedDataExtractRunner [pool-2-thread-1] INFO  - Total Records for Seed data extraction:676
2016-05-02 06:24:19,687 com.emeter.extractor.runner.assetdata.device.AssetDataDevice [main] INFO  - AssetDataDevice Total Records:414310 (414344)
2016-05-02 06:24:19,687 com.emeter.extractor.runner.assetdata.route.AssetDataRoute [main] INFO  - AssetDataRoute Total Records:486
2016-05-02 06:24:19,687 com.emeter.extractor.runner.assetdata.account.AssetDataAccount [main] INFO  - AssetDataAccount Total Records:101352
2016-05-02 06:24:19,687 com.emeter.extractor.runner.assetdata.premise.AssetDataPremise [main] INFO  - AssetDataPremise Total Records:73998
2016-05-02 06:24:19,687 com.emeter.extractor.runner.assetdata.distnode.AssetDataDistNode [main] INFO  - AssetDataDistNode Total Records:0
2016-05-02 06:35:25,938 com.emeter.extractor.runner.sdpstackdata.SdpStackData [main] INFO  - SdpStackData stat : 
ServiceDeliveryPoint,241744
TotalChannel,2506108,Channel,2506108,VirtualChannel,0
servicePointServiceAgreementAssociation,182367
servicePointDataServiceAssociation,1578077
servicePointDeviceAssociation,240892
deviceChannelAssociation,3510551
deviceFunctionAssociation,193302
servicePointServicePointGroupAssociation,294162
accountServicePointAssociation,383132
servicePointServicePointAssociation SDP-DN,0
servicePointServicePointAssociation DN-DN,0


Run4 - with no dataSvc, SvcAgree & updated param query

2016-05-20 00:14:47,277 [main] INFO  - StructuralExtractApplication Mode:: initial
2016-05-20 00:14:54,724 [pool-2-thread-2] INFO  - Total Records for cycleCd extraction:29
2016-05-20 00:14:57,085 [pool-2-thread-1] INFO  - Total Records for Seed data extraction:718
2016-05-20 00:25:50,137 [main] INFO  - AssetDataDevice Total Records:414320
2016-05-20 00:25:50,137 [main] INFO  - AssetDataRoute Total Records:486
2016-05-20 00:25:50,137 [main] INFO  - AssetDataAccount Total Records:101352
2016-05-20 00:25:50,137 [main] INFO  - AssetDataPremise Total Records:73998
2016-05-20 00:25:50,137 [main] INFO  - AssetDataDistNode Total Records:0
2016-05-20 00:37:22,735 [main] INFO  - SdpStackData stat : 
ServiceDeliveryPoint,241751
TotalChannel,2506201,Channel,2506201,VirtualChannel,0
servicePointServiceAgreementAssociation,0
servicePointDataServiceAssociation,0
servicePointDeviceAssociation,240893
deviceChannelAssociation,3510644
deviceFunctionAssociation,193273
servicePointServicePointGroupAssociation,294167
accountServicePointAssociation,383139
servicePointServicePointAssociation SDP-DN,0
servicePointServicePointAssociation DN-DN,0
2016-05-20 00:37:22,761 [main] INFO  - creating zip file... 
2016-05-20 00:40:29,755 [main] INFO  - Done Structural execution in 1542.478484 sec



--1185335047

--------------Transactional extraction--------------
- create insert statements for device_data_src & device_data_src_details
- create insert statements for pipe_type_event_list


----------
select * from (
        select rr.channel_id, mapping.udc_id_8x, CASE WHEN register_index_7x = 'R1' THEN CUM_READ
WHEN register_index_7x = 'R2' THEN DEMAND_READ_BC
WHEN register_index_7x = 'R3' THEN DEMAND_READ_DAILY
WHEN register_index_7x = 'R4' THEN TOU1_CUM
WHEN register_index_7x = 'R5' THEN TOU1_DEMAND_BC
WHEN register_index_7x = 'R6' THEN TOU1_DEMAND_DAILY
WHEN register_index_7x = 'R7' THEN TOU2_CUM
WHEN register_index_7x = 'R8' THEN TOU2_DEMAND_BC
WHEN register_index_7x = 'R9' THEN TOU2_DEMAND_DAILY
WHEN register_index_7x = 'R10' THEN TOU3_CUM
WHEN register_index_7x = 'R11' THEN TOU3_DEMAND_BC
WHEN register_index_7x = 'R12' THEN TOU3_DEMAND_DAILY
WHEN register_index_7x = 'R13' THEN TOU4_CUM
WHEN register_index_7x = 'R14' THEN TOU4_DEMAND_BC
WHEN register_index_7x = 'R15' THEN TOU4_DEMAND_DAILY
WHEN register_index_7x = 'R16' THEN TOU5_CUM
WHEN register_index_7x = 'R17' THEN TOU5_DEMAND_BC
WHEN register_index_7x = 'R18' THEN TOU5_DEMAND_DAILY
END read_value,
register_index_7x,
utc_read_time
        from register_reads rr, tmp_sdp_id_to_udcid tmp, cx_product_name_mapping mapping
        where rr.channel_id = tmp.channel_id
          and tmp.prod_name = mapping.product_name_7x
          and rr.channel_id = '1-51GE') where read_value is NOT NULL;
          --and rr.utc_read_time = TO_DATE('02-JAN-2011','DD-MON-YYYY');

***************************KCBPU SIEBEL*************************************
SELECT DISTINCT         
					 p.row_id AS src_id,
					 FIRST_VALUE(pxm.row_id) OVER(PARTITION BY pxm.attrib_01, p.type, p.name ORDER BY pxm.attrib_27 NULLS FIRST, pxm.attrib_26 desc) AS param_src_id, 
					 NULL  AS data_src, 
					 FIRST_VALUE(par_row_id) OVER(PARTITION BY pxm.attrib_01, p.type, p.name ORDER BY pxm.attrib_27 NULLS FIRST, pxm.attrib_26 desc) AS par_src_id, 
					 mapping.product_name_8x AS name, 
					 FIRST_VALUE(attrib_01) OVER(PARTITION BY pxm.attrib_01, p.type, p.name ORDER BY pxm.attrib_27 NULLS FIRST, pxm.attrib_26 desc) AS param_name, 
					 FIRST_VALUE(attrib_02) OVER(PARTITION BY pxm.attrib_01, p.type, p.name ORDER BY pxm.attrib_27 NULLS FIRST, pxm.attrib_26 desc) AS param_value, 
					 FIRST_VALUE(attrib_03) OVER(PARTITION BY pxm.attrib_01, p.type, p.name ORDER BY pxm.attrib_27 NULLS FIRST, pxm.attrib_26 desc) AS param_status_cd, 
					 to_char(FIRST_VALUE(attrib_26) OVER(PARTITION BY pxm.attrib_01, p.type, p.name ORDER BY pxm.attrib_27 NULLS FIRST, pxm.attrib_26 desc), 'YYYY-MM-DD"T"HH24:MI:SS') AS param_eff_start_time,      
					 to_char(FIRST_VALUE(attrib_27) OVER(PARTITION BY pxm.attrib_01, p.type, p.name ORDER BY pxm.attrib_27 NULLS FIRST, pxm.attrib_26 desc), 'YYYY-MM-DD"T"HH24:MI:SS') AS param_eff_end_time,       
					 mapping.type_8x AS type, 
					 mapping.sub_type_8x AS sub_type, 
					 DECODE(p.type, 'Service', p.sub_type, p.type) AS handler_type, 
					 FIRST_VALUE(pxm.created) OVER(PARTITION BY pxm.attrib_01, p.type, p.name ORDER BY pxm.attrib_27 NULLS FIRST, pxm.attrib_26 desc) AS source_created, 
					 FIRST_VALUE(pxm.last_upd) OVER(PARTITION BY pxm.attrib_01, p.type, p.name ORDER BY pxm.attrib_27 NULLS FIRST, pxm.attrib_26 desc) AS source_last_upd, 
					 NULL AS insert_time, 
					 NULL AS last_upd_time, 
					 p.desc_text AS desc_text, 
					 p.status_cd AS status_cd, 
					 p.make_cd AS make, 
					 p.model AS model, 
					 p.uom_cd AS uom, 
					 x_group_name AS group_name, 
					 NULL AS udc_id_8x,
					 NULL AS register_index_7x,
					 p.type AS type_7x	 
				FROM  
					s_prod_int_xm pxm, 
					s_prod_int p, 
					cx_product_name_mapping mapping 
				WHERE  
					 pxm.par_row_id  (+) = p.row_id 
					 AND p.name     = mapping.product_name_7x 
					 AND p.type     = mapping.type_7x 
					 AND p.type in ('Service') ; 
           
           
select DISTINCT p.name, DECODE(p.type, 'Service', p.sub_type, p.type) AS handler_type      
FROM 	s_prod_int p
WHERE p.type in ('Service') and  p.sub_type in ('Energy Services Agent')
order by handler_type;

--779
select DISTINCT p.name, DECODE(p.type, 'Service', p.sub_type, p.type) AS handler_type      
FROM 	s_prod_int p
WHERE p.type in ('Measurement') 
order by handler_type;

--109

select type_cd, count(1) from s_asset GROUP BY type_cd;

select cfg_type_cd, count(1) from s_asset GROUP BY cfg_type_cd ;

SELECT distinct ch.row_id, p.name as productName, DECODE(p.type, 'Service', p.sub_type, p.type) AS handler_type, ch.type_cd AS type, ch.cfg_type_cd AS sub_type
FROM 	s_prod_int p, s_asset ch
WHERE ch.type_cd  = 'Channel'
AND ch.cfg_type_cd = 'Cumulative Consumption'
AND ch.prod_id = p.row_id

and p.row_id in ('1-1AD3','1-1AZV','1-6DK','1-6EB','1-6EZ','1-6FN','1-6G7','1-6GV','1-6HJ','1-6I7','1-6IV','1-6JJ','1-6K7','1-6KV','1-6M4','1-6MS','1-6NG','1-6O4','1-6O4FV','1-6O4W6','1-6O4WE','1-6O4XI','1-6O4XQ','1-7ADV','1-7AE8','1-7AJ8','1-7AJQ','1-7ATW','1-7AU6','1-7AYP','1-7B26','1-7B2G','1-7B2Q','1-7B30','1-7B3A','1-7B3K','1-7B63','1-7B6D','1-7B6N','1-7B6X','1-7B77','1-7B7H','1-7B7R','1-7B81','1-7DVK','1-7DVS','1-7DW0','1-7DW8','1-7DWO','1-7EDD','1-7EKZ','1-7F1','1-AG5','1-AGP','1-AH9','1-AHT','1-AID','1-D20','1-D45','1-D4P','1-IAJ','1-IK0','1-J5T');
--AND NVL(ch.x_virtual_asset,'N') = 'N';

select count(1), p.name
FROM 	s_prod_int p, s_asset ch
WHERE ch.type_cd  = 'Channel'
AND ch.cfg_type_cd = 'Cumulative Consumption'
AND ch.prod_id = p.row_id 
group by p.name;


select DISTINCT p.row_id, p.name, DECODE(p.type, 'Service', p.sub_type, p.type) AS handler_type , p.sub_type as sub_type     
FROM 	s_prod_int p
WHERE p.type in ('Channel') 
and p.sub_type in ('Cumulative Consumption')
order by handler_type;

select DISTINCT p.sub_type as sub_type     
FROM 	s_prod_int p
WHERE p.type in ('Channel') ;
--order by handler_type;



select * from cx_product_name_mapping;        
select * from s_bu;

GRANT SELECT ON s_prod_int_xm TO emapp;
GRANT SELECT ON s_prod_int TO emapp;
GRANT SELECT ON s_asset TO emapp;
GRANT SELECT ON s_bu TO emapp;
GRANT SELECT ON cx_product_name_mapping TO emapp;




SELECT   DISTINCT
					 p.row_id AS src_id,
					 FIRST_VALUE(pxm.row_id) OVER(PARTITION BY pxm.attrib_01, p.type, p.name ORDER BY pxm.attrib_27 NULLS FIRST, pxm.attrib_26 desc) AS param_src_id, 
					 NULL AS data_src, 
					 FIRST_VALUE(par_row_id) OVER(PARTITION BY pxm.attrib_01, p.type, p.name ORDER BY pxm.attrib_27 NULLS FIRST, pxm.attrib_26 desc) AS par_src_id, 
					 mapping.product_name_8x AS name, 
					 FIRST_VALUE(attrib_01) OVER(PARTITION BY pxm.attrib_01, p.type, p.name ORDER BY pxm.attrib_27 NULLS FIRST, pxm.attrib_26 desc) AS param_name, 
					 FIRST_VALUE(attrib_02) OVER(PARTITION BY pxm.attrib_01, p.type, p.name ORDER BY pxm.attrib_27 NULLS FIRST, pxm.attrib_26 desc) AS param_value, 
					 FIRST_VALUE(attrib_03) OVER(PARTITION BY pxm.attrib_01, p.type, p.name ORDER BY pxm.attrib_27 NULLS FIRST, pxm.attrib_26 desc) AS param_status_cd, 
					 to_char(FIRST_VALUE(attrib_26) OVER(PARTITION BY pxm.attrib_01, p.type, p.name ORDER BY pxm.attrib_27 NULLS FIRST, pxm.attrib_26 desc) , 'YYYY-MM-DD"T"HH24:MI:SS')AS param_eff_start_time,      
					 to_char(FIRST_VALUE(attrib_27) OVER(PARTITION BY pxm.attrib_01, p.type, p.name ORDER BY pxm.attrib_27 NULLS FIRST, pxm.attrib_26 desc) , 'YYYY-MM-DD"T"HH24:MI:SS')AS param_eff_end_time,       
					 mapping.type_8x AS type, 
					 mapping.sub_type_8x AS sub_type, 
					 DECODE(p.type, 'Service', p.sub_type, p.type) AS handler_type, 
					 FIRST_VALUE(pxm.created) OVER(PARTITION BY pxm.attrib_01, p.type, p.name ORDER BY pxm.attrib_27 NULLS FIRST, pxm.attrib_26 desc) AS source_created, 
					 FIRST_VALUE(pxm.last_upd) OVER(PARTITION BY pxm.attrib_01, p.type, p.name ORDER BY pxm.attrib_27 NULLS FIRST, pxm.attrib_26 desc) AS source_last_upd, 
					 NULL AS insert_time, 
					 NULL AS last_upd_time, 
					 p.desc_text AS desc_text, 
					 p.status_cd AS status_cd, 
					 p.make_cd AS make, 
					 p.model AS model, 
					 p.uom_cd AS uom, 
					 x_group_name AS group_name, 
					 mapping.udc_id_8x AS udc_id_8x,
					 mapping.register_index_7x AS register_index_7x,
			 		 p.type AS type_7x
				FROM  
					s_prod_int_xm pxm, 
					s_prod_int p, 
					cx_product_name_mapping mapping 
				WHERE  
					 pxm.par_row_id  (+) = p.row_id 
					 AND p.name     = mapping.product_name_8x 
					 AND p.type     = mapping.type_8x 
					 AND p.type in ('Measurement')
				order by handler_type, src_id, register_index_7x;
        
        
        select p.name, p.uom_cd , max(CASE WHEN param.attrib_01= 'Physical Channel Number' then attrib_02 END) as channel_num
        , max(CASE WHEN param.attrib_01= 'TOU Bin Number' then attrib_02 END) as tou_bin
        from s_prod_int p, s_prod_int_xm param
        where p.type in ('Measurement')
        and p.sub_type IN ('Cumulative', 'Cumulative Consumption') 
        and param.par_row_id = p.row_id
        and p.name like '%KWH%'
        and param.attrib_01 in ('Physical Channel Number','TOU Bin Number') 
        GROUP BY p.name, p.uom_cd
        order by p.name;
        
       select distinct sub_type from s_prod_int where type = 'Measurement';
        
       select distinct p.name, p.sub_type
       from s_prod_int p
       where type = 'Measurement' 
       and sub_type in ('Interval','Interval Data')
       --and p.name like '%KVAH %'
       order by p.sub_type;
       
       select distinct p.name, p.sub_type
       from s_prod_int p
       where type = 'Measurement' 
       and sub_type in ('Usage')
       --and p.name like '% %'
       order by p.sub_type;
     
     
     select data_src from s_asset;        
        
        
***************************KCBPU MUDR******************************************************        
drop table tmp_sdp_id_to_udcid;
drop table tmp_device_id_to_udcid;
--drop table tmp_meas_type_id_to_udcid;
drop table tmp_update_by_to_user_name;
drop table tmp_org_id_to_udc_id;
drop table cx_product_name_mapping;

create table tmp_sdp_id_to_udcid(id VARCHAR2(50), udc_id VARCHAR2(50), type VARCHAR2(50), sub_type VARCHAR2(50), channel_id VARCHAR2(15 BYTE), prod_name VARCHAR2(250 BYTE));
create table tmp_device_id_to_udcid(id VARCHAR2(50), udc_id VARCHAR2(50), type VARCHAR2(50), sub_type VARCHAR2(50));
create table tmp_update_by_to_user_name(name VARCHAR2(50));
create table tmp_org_id_to_udc_id(id VARCHAR2(50), org_udc_id VARCHAR2(50));
create table cx_product_name_mapping as select * from SIEBEL.cx_product_name_mapping@MUDR2SEBL;
--create table tmp_meas_type_id_to_udcid(id VARCHAR2(50), udc_id VARCHAR2(50));
--create table tmp_update_by_to_user_name(id VARCHAR2(50), name VARCHAR2(50), org_id NUMBER, org_udc_id VARCHAR2(50));
--create table tmp_org_id_to_udc_id(id VARCHAR2(50), org_udc_id VARCHAR2(50));
  
select * from SIEBEL.cx_product_name_mapping@MUDR2SEBL;

--380155


insert into tmp_sdp_id_to_udcid (id, udc_id, type, sub_type, channel_id, prod_name)
select sdp.row_id, sdp.x_udc_asset_id, sdp.type_cd, sdp.cfg_type_cd, ch.row_id, prod.name
from siebel.s_asset@mudr2sebl sdp, siebel.s_asset@mudr2sebl ch, siebel.s_prod_int@mudr2sebl prod
where ch.service_point_id = sdp.row_id
AND ch.type_cd = 'Channel'
AND sdp.type_cd = 'Service Point'
and ch.prod_id = prod.row_id
AND EXISTS (SELECT 1 FROM siebel.s_asset_rel@mudr2sebl rel
		   		           WHERE x_asset_relation_type_cd = 'METER-CHANNEL'
		   		           AND rel.asset_id = ch.row_id
		   			         AND (rel.meter_loc_start_dt <> rel.meter_loc_end_dt
		   		           OR
		   		           rel.meter_loc_end_dt IS NULL)
		   			         );

insert into tmp_device_id_to_udcid (id, udc_id, type, sub_type)
select s.row_id, s.x_udc_asset_id, s.type_cd, s.cfg_type_cd 
from siebel.s_asset@mudr2sebl s
where s.type_cd IN ('Meter','Communication Module','CT-PT','Disconnect Collar'/*, 'FRU'*/)
order by s.type_cd;

insert into tmp_update_by_to_user_name (name) 
	select 'PIPE_PROXY_'|| '007169584' --bu.name 
	from dual;
--where bu.name = '${UTILITY_ID}';

insert into tmp_org_id_to_udc_id (id, org_udc_id) 
select bu.row_id, bu.name 
from siebel.s_bu@mudr2sebl bu
where bu.name = '007169584';
--where bu.name = '${UTILITY_ID}';

commit;

select distinct s.type_cd from siebel.s_asset@mudr2sebl s;
select * from tmp_sdp_id_to_udcid;
select * from tmp_device_id_to_udcid;
select * from tmp_meas_type_id_to_udcid;
select * from tmp_update_by_to_user_name;
select * from tmp_org_id_to_udc_id;
select * from cx_product_name_mapping;        


select ch.service_point_id from siebel.s_asset@mudr2sebl s
where s.type_cd IN ('Channel');

select sdp.id, sdp.udc_id, sdp.type, sdp.sub_type, channel.id from channel channel, svc_pt sdp 
where channel.svc_pt_id = sdp.id;



select * from SIEBEL.cx_product_name_mapping@MUDR2SEBL;        
select * from SIEBEL.s_bu@MUDR2SEBL;
select distinct owner from all_tables@MUDR2SEBL where owner NOT IN ( 'SYS','SYSTEM');

select * from siebel.s_contact@mudr2sebl;


select estimation_method from register_reads where estimation_method is not null;
select * from lp_intervals;
select * from device_event;
select * from lp_batch;
select * from usage;
select * from register_reads rr
          where rr.channel_id = '1-51GE';

select * from (
        select rr.channel_id, mapping.udc_id_8x, CASE WHEN register_index_7x = 'R1' THEN CUM_READ
        WHEN register_index_7x = 'R2' THEN DEMAND_READ_BC
        WHEN register_index_7x = 'R3' THEN DEMAND_READ_DAILY
        WHEN register_index_7x = 'R4' THEN TOU1_CUM
        WHEN register_index_7x = 'R5' THEN TOU1_DEMAND_BC
        WHEN register_index_7x = 'R6' THEN TOU1_DEMAND_DAILY
        WHEN register_index_7x = 'R7' THEN TOU2_CUM
        WHEN register_index_7x = 'R8' THEN TOU2_DEMAND_BC
        WHEN register_index_7x = 'R9' THEN TOU2_DEMAND_DAILY
        WHEN register_index_7x = 'R10' THEN TOU3_CUM
        WHEN register_index_7x = 'R11' THEN TOU3_DEMAND_BC
        WHEN register_index_7x = 'R12' THEN TOU3_DEMAND_DAILY
        WHEN register_index_7x = 'R13' THEN TOU4_CUM
        WHEN register_index_7x = 'R14' THEN TOU4_DEMAND_BC
        WHEN register_index_7x = 'R15' THEN TOU4_DEMAND_DAILY
        WHEN register_index_7x = 'R16' THEN TOU5_CUM
        WHEN register_index_7x = 'R17' THEN TOU5_DEMAND_BC
        WHEN register_index_7x = 'R18' THEN TOU5_DEMAND_DAILY
        END read_value,
  CASE WHEN register_index_7x = 'R2' THEN DEMAND_READ_BC_TIME
						WHEN register_index_7x = 'R3' THEN DEMAND_READ_DAILY_TIME
						WHEN register_index_7x = 'R5' THEN TOU1_DEMAND_BC_TIME
						WHEN register_index_7x = 'R6' THEN TOU1_DEMAND_DAILY_TIME
						WHEN register_index_7x = 'R8' THEN TOU2_DEMAND_BC_TIME
						WHEN register_index_7x = 'R9' THEN TOU2_DEMAND_DAILY_TIME
						WHEN register_index_7x = 'R11' THEN TOU3_DEMAND_BC_TIME
						WHEN register_index_7x = 'R12' THEN TOU3_DEMAND_DAILY_TIME
						WHEN register_index_7x = 'R14' THEN TOU4_DEMAND_BC_TIME
						WHEN register_index_7x = 'R15' THEN TOU4_DEMAND_DAILY_TIME
						WHEN register_index_7x = 'R17' THEN TOU5_DEMAND_BC_TIME
						WHEN register_index_7x = 'R18' THEN TOU5_DEMAND_DAILY_TIME
	END DEMAND_PEAK_TIME,'yyyy-mm-dd"T"hh24:mi:ss') || '+08:00',
register_index_7x,
utc_read_time
        from register_reads rr, tmp_sdp_id_to_udcid tmp, cx_product_name_mapping mapping
        where rr.channel_id = tmp.channel_id
          and tmp.prod_name = mapping.product_name_7x
          and rr.channel_id = '1-51GE') where read_value is NOT NULL;
          --and rr.utc_read_time = TO_DATE('02-JAN-2011','DD-MON-YYYY');



select prod_name from tmp_sdp_id_to_udcid where channel_id = '1-51GE';


select count(*) from ;
select * from cx_product_name_mapping;

select * from register_reads;
select distinct estimation_method from register_reads where estimation_method is not null;

select * from siebel.s_asset_rel@mudr2sebl;



    select  sdp_udc_id||'~~'||
                'ServiceDeliveryPoint'||'~~'||
                udc_id_8x ||'~~'|| --measType_udc_id
                NULL ||'~~'|| --read_start_time
                read_time ||'~~'|| 
                read_value ||'~~'|| 
                NULL ||'~~'|| --ORIG_READ_VALUE
                demand_peak_time ||'~~'||
                device_udc_id ||'~~'||
                'Meter' ||'~~'|| --Device_type
                flags ||'~~'||
                estimation_method ||'~~'||
                NULL||'~~'||  --estimated
				val_status ||'~~'||                
                NULL||'~~'|| -- VAL_FAIL_CODE
                NULL ||'~~'|| --EXT_VERSION_TIME
                last_upd_time ||'~~'||
                source||'~~'||
                source_detail||'~~'||
                last_upd_by||'~~'||
                NULL ||'~~'|| --READ_REASON
                NULL ||'~~'|| --LOCKED
                NULL  ||'~~'|| --AMI_REC_NUM
                1 -- rec_version_num
                FROM 
               (select  /*+ PARALLEL(de, 4) */ 
		        (select sdp.udc_id from tmp_sdp_id_to_udcid sdp where rr.channel_id = sdp.channel_id) AS sdp_udc_id,
                mapping.udc_id_8x ,
                CASE 
                    WHEN rr.utc_read_time IS NULL 
                THEN NULL
                    ELSE TO_CHAR (CAST(
                                       FROM_TZ(
                                               CAST(utc_read_time AS TIMESTAMP)
                                                    ,   'UTC')
                                                         AT TIME ZONE('+08:00')
                                       AS TIMESTAMP(0)),'yyyy-mm-dd"T"hh24:mi:ss') || '+08:00'
                END AS read_time,
                CASE WHEN register_index_7x = 'R1' THEN CUM_READ
					WHEN register_index_7x = 'R2' THEN DEMAND_READ_BC
					WHEN register_index_7x = 'R3' THEN DEMAND_READ_DAILY
					WHEN register_index_7x = 'R4' THEN TOU1_CUM
					WHEN register_index_7x = 'R5' THEN TOU1_DEMAND_BC
					WHEN register_index_7x = 'R6' THEN TOU1_DEMAND_DAILY
					WHEN register_index_7x = 'R7' THEN TOU2_CUM
					WHEN register_index_7x = 'R8' THEN TOU2_DEMAND_BC
					WHEN register_index_7x = 'R9' THEN TOU2_DEMAND_DAILY
					WHEN register_index_7x = 'R10' THEN TOU3_CUM
					WHEN register_index_7x = 'R11' THEN TOU3_DEMAND_BC
					WHEN register_index_7x = 'R12' THEN TOU3_DEMAND_DAILY
					WHEN register_index_7x = 'R13' THEN TOU4_CUM
					WHEN register_index_7x = 'R14' THEN TOU4_DEMAND_BC
					WHEN register_index_7x = 'R15' THEN TOU4_DEMAND_DAILY
					WHEN register_index_7x = 'R16' THEN TOU5_CUM
					WHEN register_index_7x = 'R17' THEN TOU5_DEMAND_BC
					WHEN register_index_7x = 'R18' THEN TOU5_DEMAND_DAILY
					END AS read_value,
                CASE 
                    WHEN (CASE WHEN register_index_7x = 'R2' THEN DEMAND_READ_BC_TIME
				              WHEN register_index_7x = 'R3' THEN DEMAND_READ_DAILY_TIME
				              WHEN register_index_7x = 'R5' THEN TOU1_DEMAND_BC_TIME
				              WHEN register_index_7x = 'R6' THEN TOU1_DEMAND_DAILY_TIME
				              WHEN register_index_7x = 'R8' THEN TOU2_DEMAND_BC_TIME
				              WHEN register_index_7x = 'R9' THEN TOU2_DEMAND_DAILY_TIME
				              WHEN register_index_7x = 'R11' THEN TOU3_DEMAND_BC_TIME
				              WHEN register_index_7x = 'R12' THEN TOU3_DEMAND_DAILY_TIME
				              WHEN register_index_7x = 'R14' THEN TOU4_DEMAND_BC_TIME
				              WHEN register_index_7x = 'R15' THEN TOU4_DEMAND_DAILY_TIME
				              WHEN register_index_7x = 'R17' THEN TOU5_DEMAND_BC_TIME
				              WHEN register_index_7x = 'R18' THEN TOU5_DEMAND_DAILY_TIME
					      END) IS NULL
                    THEN NULL
                    ELSE TO_CHAR (CASE WHEN register_index_7x = 'R2' THEN DEMAND_READ_BC_TIME
						               WHEN register_index_7x = 'R3' THEN DEMAND_READ_DAILY_TIME
						               WHEN register_index_7x = 'R5' THEN TOU1_DEMAND_BC_TIME
						               WHEN register_index_7x = 'R6' THEN TOU1_DEMAND_DAILY_TIME
						               WHEN register_index_7x = 'R8' THEN TOU2_DEMAND_BC_TIME
						               WHEN register_index_7x = 'R9' THEN TOU2_DEMAND_DAILY_TIME
						               WHEN register_index_7x = 'R11' THEN TOU3_DEMAND_BC_TIME
						               WHEN register_index_7x = 'R12' THEN TOU3_DEMAND_DAILY_TIME
						               WHEN register_index_7x = 'R14' THEN TOU4_DEMAND_BC_TIME
						               WHEN register_index_7x = 'R15' THEN TOU4_DEMAND_DAILY_TIME
						               WHEN register_index_7x = 'R17' THEN TOU5_DEMAND_BC_TIME
						               WHEN register_index_7x = 'R18' THEN TOU5_DEMAND_DAILY_TIME
					               END 
					             ,'yyyy-mm-dd"T"hh24:mi:ss') || '+08:00'
                END AS demand_peak_time,
                (SELECT tmp.udc_id 
                   FROM siebel.s_asset_rel@mudr2sebl rel
                      , tmp_device_id_to_udcid tmp 
                  WHERE rel.par_asset_id = tmp.id 
                    AND rel.x_asset_relation_type_cd = 'METER-CHANNEL' 
                    AND rel.asset_id = rr.channel_id
                    AND rr.local_read_time between rel.meter_loc_start_dt AND rel.meter_loc_end_dt) AS device_udc_id,
                (DECODE (demand_reset_flag,'Y', POWER (2, 5) , 'N', 0, NULL, 0)
                + DECODE (power_off,        'Y', POWER (2, 7) , 'N', 0, NULL, 0)
                + DECODE (reverse_rotation, 'Y', POWER (2, 9) , 'N', 0, NULL, 0)
                + DECODE (battery_low,      'Y', POWER (2, 10), 'N', 0, NULL, 0)
                + DECODE (cover_off,        'Y', POWER (2, 11), 'N', 0, NULL, 0)
                + DECODE (magnet,           'Y', POWER (2, 12), 'N', 0, NULL, 0)
                + DECODE (dc_verified,      'Y', POWER (2, 16), 'N', 0, NULL, 0)) AS flags,
                DECODE (rr.estimation_method,'E','RSC','I','RSA',rr.estimation_method) AS estimation_method,
                CASE WHEN source = 'eMeter' AND source_detail IN ('EXT','INT') THEN
                     'EST'
                ELSE
                	 'VAL'
                END AS val_status,
                CASE -- last_upd_time
                    WHEN rr.insert_time IS NULL 
                THEN NULL
                    ELSE TO_CHAR (rr.insert_time,'yyyy-mm-dd"T"hh24:mi:ss') || '+08:00' 
                END AS last_upd_time,
                source,
                source_detail,
                (select usr.name from tmp_update_by_to_user_name usr) last_upd_by
        from register_reads rr, tmp_sdp_id_to_udcid tmp, cx_product_name_mapping mapping
        where rr.channel_id = tmp.channel_id
        and tmp.prod_name = mapping.product_name_7x)
		WHERE read_value is not null;
    
    
    
    
    
    
    
    
   select  sdp_udc_id||'~~'||
                'ServiceDeliveryPoint'||'~~'||
                udc_id_8x ||'~~'|| --measType_udc_id
                lp_end_time ||'~~'|| 
                lp_value ||'~~'|| 
                val_status ||'~~'||                
                change_method ||'~~'||
                fail_code ||'~~'|| -- FAIL_CODE
                locked ||'~~'|| --LOCKED
                estimated||'~~'||  --estimated
                ext_version_time ||'~~'|| --EXT_VERSION_TIME
                last_upd_time ||'~~'||
                flags ||'~~'||
                device_udc_id ||'~~'||
                'Meter' ||'~~'|| --Device_type
                'lp_data_source' ||'~~'||
                'lp_data_source_detail' ||'~~'||
                NULL ||'~~'|| --AMI_REC_NUM
                1 ||'~~'|| -- rec_version_num
                last_upd_by
                FROM 
               (select  /*+ PARALLEL(de, 4) */ 
		        (select sdp.udc_id from tmp_sdp_id_to_udcid sdp where lpi.channel_id = sdp.channel_id) AS sdp_udc_id,
                mapping.udc_id_8x,
                CASE 
                    WHEN lpi.UTC_INTERVAL_TIME IS NULL 
                THEN NULL
                    ELSE TO_CHAR (CAST(
                                       FROM_TZ(
                                               CAST(UTC_INTERVAL_TIME AS TIMESTAMP)
                                                    ,   'UTC')
                                                         AT TIME ZONE('+08:00')
                                       AS TIMESTAMP(0)),'yyyy-mm-dd"T"hh24:mi:ss') || '+08:00'
                END AS lp_end_time,
                lpi.lp_value lp_value,
                lpi.validation_status val_status,
                DECODE (lpi.change_method, 'ESA', 'IEA',
										 'ESB', 'IEB',
										 'ESE', 'IEE',
										 'ESD', 'IEF',
										 'RSB', 'IEG',
										 'ESI', 'IEJ',
										 'ESG', 'IEK',
										 '55a', 'IEL',
										 'EPE', 'IEL',
										 'ESH', 'IEL',
										 '51' ,'IEN' ,
										 '52' ,'IEO' ,
										 '54' ,'IER' ,
										 '55d','IES' ,
										 '56' ,'IET' ,
										 '55f','IEU' ,
										 '58' ,'IEV' ,
										 '57' ,'IEW' ,
										 '55e','IEX' ,
						 lpi.change_method) change_method,
                lpi.fail_code fail_code,
                lpi.locked locked,
                lpi.estimated estimated,
                TO_CHAR (CAST(
				              FROM_TZ(
				              		  CAST(lpi.utc_ext_version_time AS TIMESTAMP)
				                            ,   'UTC')
				                      AT TIME ZONE('+08:00')
                                      AS TIMESTAMP(0)),'yyyy-mm-dd"T"hh24:mi:ss') || '+08:00'
                AS ext_version_time,
                CASE -- last_upd_time
				                    WHEN lpi.insert_time IS NULL 
				                THEN NULL
				                    ELSE TO_CHAR (lpi.insert_time,'yyyy-mm-dd"T"hh24:mi:ss') || '+08:00' 
                END last_upd_time,
                (DECODE (NVL (lpi.no_data,            'N'),  'Y', POWER (2, 0) ,  'N', 0)
				                     + DECODE (NVL (lpi.short_interval,     'N'),  'Y', POWER (2, 1) ,  'N', 0)
				                     + DECODE (NVL (lpi.long_interval,      'N'),  'Y', POWER (2, 2) ,  'N', 0)
				                     + DECODE (NVL (lpi.partial_interval,   'N'),  'Y', POWER (2, 3) ,  'N', 0)
				                     + DECODE (NVL (lpi.meter_reset,        'N'),  'Y', POWER (2, 4) ,  'N', 0)
				                     + DECODE (NVL (lpi.pulse_overflow,     'N'),  'Y', POWER (2, 6) ,  'N', 0)
				                     + DECODE (NVL (lpi.power_off,          'N'),  'Y', POWER (2, 7) ,  'N', 0)
				                     + DECODE (NVL (lpi.power_on,           'N'),  'Y', POWER (2, 8) ,  'N', 0)
				                     + DECODE (NVL (lpi.reverse_rotation,   'N'),  'Y', POWER (2, 9) ,  'N', 0)
				                     + DECODE (NVL (lpi.time_change,        'N'),  'Y', POWER (2, 13),  'N', 0)
				                     + DECODE (NVL (lpi.test_mode,          'N'),  'Y', POWER (2, 14),  'N', 0)
				                     + DECODE (NVL (lpi.dc_data_estimation, 'N'),  'Y', POWER (2, 15),  'N', 0)
				                     + DECODE (NVL (lpi.ami_error,          'N'),  'Y', POWER (2, 17),  'N', 0)
				                     + DECODE (NVL (lpi.crc_error,          'N'),  'Y', POWER (2, 18),  'N', 0)
				                     + DECODE (NVL (lpi.phase_failure,      'N'),  'Y', POWER (2, 19),  'N', 0)
                 ) flags,
                (SELECT tmp.udc_id 
				                   FROM siebel.s_asset_rel@mudr2sebl rel
				                      , tmp_device_id_to_udcid tmp 
				                  WHERE rel.par_asset_id = tmp.id 
				                    AND rel.x_asset_relation_type_cd = 'METER-CHANNEL' 
				                    AND rel.asset_id = lpi.channel_id
                	AND lpi.local_interval_time between rel.meter_loc_start_dt AND rel.meter_loc_end_dt) AS device_udc_id,
                (select usr.name from tmp_update_by_to_user_name usr) last_upd_by    
        from lp_intervals lpi, tmp_sdp_id_to_udcid tmp, cx_product_name_mapping mapping
        where lpi.channel_id = tmp.channel_id
        and tmp.prod_name = mapping.product_name_7x)
    WHERE lp_value is not null;
    
    select * from lp_intervals;
    
    
    select * from register_reads partition();
    
    select partition_name, num_rows from user_tab_partitions where table_name = 'REGISTER_READS' and num_rows > 0 order by TO_DATE(REPLACE(REPLACE(partition_name,'RR','01'),'_','-'),'DD-MON-YYYY');
    select partition_name, num_rows from user_tab_partitions where table_name = 'LP_INTERVALS' and num_rows > 0 order by TO_DATE(REPLACE(REPLACE(partition_name,'LP','01'),'_','-'),'DD-MON-YYYY');
    
    
    select count(*) from tmp_sdp_id_to_udcid;
    
    
    select count(1), par_asset_id, asset_id from siebel.s_asset_rel@mudr2sebl where x_asset_relation_type_cd = 'METER-CHANNEL'
    and (meter_loc_start_dt <> meter_loc_end_dt or meter_loc_end_dt IS NULL)
    group by par_asset_id, asset_id
    having count(1) > 1;
    
    select * from siebel.s_asset_rel@mudr2sebl where par_asset_id = '1-5CZHM' and asset_id = '1-10LAL'
    and (meter_loc_start_dt <> meter_loc_end_dt or meter_loc_end_dt IS NULL);
    
    
    (SELECT tmp.udc_id 
                   FROM siebel.s_asset_rel@mudr2sebl rel
                      , tmp_device_id_to_udcid tmp 
                  WHERE rel.par_asset_id = tmp.id 
                    AND rel.x_asset_relation_type_cd = 'METER-CHANNEL' 
                    AND rel.asset_id = rr.channel_id
                    AND rr.local_read_time >= rel.meter_loc_start_dt AND rr.local_read_time < rel.meter_loc_end_dt
		                	AND (meter_loc_start_dt <> meter_loc_end_dt or meter_loc_end_dt IS NULL)) AS device_udc_id,
                    ) AS device_udc_id,
                    
                    
                    
-20065543
select count(*) from device_event;
select * from device_event;



select distinct TO_CHAR(UTC_EVENT_TIME, 'YYYY-MM') as eventTime
--TO_DATE(REPLACE(REPLACE(partition_name,'DE','01'),'_','-'),'DD-MON-YYYY') as eventTime
from device_event
order by eventTime;


select count(*), TO_CHAR(UTC_EVENT_TIME, 'YYYY-MM') eventTime
from device_event
where TO_CHAR(UTC_EVENT_TIME, 'YYYY-MM') in (select distinct TO_CHAR(UTC_EVENT_TIME, 'YYYY-MM') as eventTime
                                             from device_event) group by TO_CHAR(UTC_EVENT_TIME, 'YYYY-MM')
order by eventTime;
                    
                    
set serveroutput on
declare
v_start_dt date := to_date('2016-03-01 00:00:00','yyyy-mm-dd hh24:mi:ss');
v_end_dt date := to_date('2016-03-01 23:59:59','yyyy-mm-dd hh24:mi:ss');
v_counter NUMBER := 1;
v_count NUMBER:= 0;
v_count2 NUMBER:= 0;
BEGIN

LOOP
v_count := 0;
select  count(1)   
INTO v_count
		        from lp_intervals lpi, tmp_sdp_id_to_udcid tmp, cx_product_name_mapping mapping
		        where lpi.channel_id = tmp.channel_id
		        and tmp.prod_name = mapping.product_name_7x
				and lpi.UTC_INTERVAL_TIME between v_start_dt and v_end_dt
		and lp_value is not null;

DBMS_OUTPUT.PUT_LINE(v_counter || ': ' || v_count);
v_count2 := v_count2 + v_count;
v_counter := v_counter + 1;
select v_start_dt+1 into v_start_dt from dual;
select v_end_dt+1 into v_end_dt from dual;
exit when v_counter = 32;
END LOOP;
DBMS_OUTPUT.PUT_LINE ('v_count2 : ' || v_count2);
END;
/


select * from tmp_sdp_id_to_udcid where channel_seq = 380128;
--7844032
/*
anonymous block completed
1: 7844032
2: 7842497
3: 7840327
4: 7837407
5: 7833892
6: 7830005
7: 7826705
8: 7700483
9: 2097214
*/
select * from tmp_sdp_id_to_udcid where udc_id = '1-ZZY4';




drop table tmp_sdp_id_to_udcid;
	drop table tmp_device_id_to_udcid;
	drop table tmp_event_sdp_id_to_udcid;
	drop table tmp_update_by_to_user_name;
	drop table tmp_org_id_to_udc_id;
	drop table cx_product_name_mapping;
	drop sequence channel_seq;
  
  
  CREATE SEQUENCE channel_seq START WITH 1 INCREMENT BY 1 NOMINVALUE NOCACHE NOCYCLE;
	create table tmp_sdp_id_to_udcid(channel_seq number,id VARCHAR2(50), udc_id VARCHAR2(50), type VARCHAR2(50), sub_type VARCHAR2(50), channel_id VARCHAR2(15 BYTE), prod_name VARCHAR2(250 BYTE), channel_type VARCHAR2(50));
	create table tmp_event_sdp_id_to_udcid(id VARCHAR2(50), udc_id VARCHAR2(50), type VARCHAR2(50), sub_type VARCHAR2(50), channel_id VARCHAR2(15 BYTE), prod_name VARCHAR2(250 BYTE));
	create table tmp_device_id_to_udcid(id VARCHAR2(50), udc_id VARCHAR2(50), type VARCHAR2(50), sub_type VARCHAR2(50));
	create table tmp_update_by_to_user_name(name VARCHAR2(50));
	create table tmp_org_id_to_udc_id(id VARCHAR2(50), org_udc_id VARCHAR2(50));
	create table cx_product_name_mapping as select * from SIEBEL.cx_product_name_mapping@MUDR2SEBL;
  
  
  insert into tmp_sdp_id_to_udcid (channel_seq, id, udc_id, type, sub_type, channel_id, prod_name, channel_type)
	select channel_seq.nextval, sdp.row_id, sdp.x_udc_asset_id, sdp.type_cd, sdp.cfg_type_cd, ch.row_id, prod.name, ch.cfg_type_cd
	from siebel.s_asset@mudr2sebl sdp, siebel.s_asset@mudr2sebl ch, siebel.s_prod_int@mudr2sebl prod
	where ch.service_point_id = sdp.row_id
	AND ch.type_cd = 'Channel'
	AND sdp.type_cd = 'Service Point'
	and ch.prod_id = prod.row_id
	AND sdp.data_src = '007169584'
	AND EXISTS (SELECT 1 FROM siebel.s_asset_rel@mudr2sebl rel
			   		           WHERE x_asset_relation_type_cd = 'METER-CHANNEL'
			   		           AND rel.asset_id = ch.row_id
			   			         AND (rel.meter_loc_start_dt <> rel.meter_loc_end_dt
			   		           OR
			   		           rel.meter_loc_end_dt IS NULL)
			   			         );
	commit;
  		   			         
	insert into tmp_event_sdp_id_to_udcid (id, udc_id, type, sub_type)
	select sdp.row_id, sdp.x_udc_asset_id, sdp.type_cd, sdp.cfg_type_cd
	from siebel.s_asset@mudr2sebl sdp, siebel.s_prod_int@mudr2sebl prod
	where sdp.type_cd = 'Service Point'
	and sdp.prod_id = prod.row_id
	AND sdp.data_src = '007169584';
	commit;
  
	insert into tmp_device_id_to_udcid (id, udc_id, type, sub_type)
	select s.row_id, s.x_udc_asset_id, 
		CASE WHEN s.type_cd = 'Meter' THEN 'Meter'
		WHEN s.type_cd = 'Communication Module' THEN 'CommModule'
		WHEN s.type_cd = 'CT-PT' THEN 'CTPT'
		WHEN s.type_cd = 'Disconnect Collar' THEN 'DisconnectCollar'
		END,
		s.cfg_type_cd 
	from siebel.s_asset@mudr2sebl s
	where s.type_cd IN ('Meter','Communication Module','CT-PT','Disconnect Collar'/*, 'FRU'*/)
	AND s.data_src = '007169584'
	order by s.type_cd;
	commit;
  
	insert into tmp_update_by_to_user_name (name) 
	select 'PIPE_PROXY_'|| '007169584' --bu.name 
	from dual;
	commit;
	
  insert into tmp_org_id_to_udc_id (id, org_udc_id) 
	select bu.row_id, bu.name 
	from siebel.s_bu@mudr2sebl bu
	where bu.name = '007169584';
	commit;
*******************************************************************************************        
KCBPU transactional extraction:
^z
bg
disown

Integration test
-Load all structural data 
-Load small files of 
	-RR
	-LP
	-DE
-performance enhancement





RR 

Test:
/home/pipe/RGA/transaction/registerRead/output/
2016-02-29 00:00:00
2016-03-01 00:00:00

Partition data:
select partition_name, num_rows from user_tab_partitions where table_name = 'REGISTER_READS' and num_rows > 0 order by TO_DATE(REPLACE(REPLACE(partition_name,'RR','01'),'_','-'),'DD-MON-YYYY');

RR_JUN_2011	179
RR_OCT_2011	1
RR_NOV_2011	27
RR_DEC_2011	2
RR_MAR_2012	3
RR_MAY_2012	10
RR_JUN_2012	14
RR_JUL_2012		   13
RR_AUG_2012	       32
RR_SEP_2012	     2615 		| sept-4205
RR_OCT_2012	  2449811   	| oct-13161486
RR_NOV_2012	  2404154		| nov-12494788
RR_DEC_2012	  2993228		| dec-15600124	
RR_JAN_2013	 10197846	    | jan-23362737 | nohup ./AC_AdfMasterExtract.sh rr /home/pipe/RGA/transaction/registerRead/output/2013/jan '2013-01-01' '2013-01-31' & > /dev/null &
--------------------------------------------------------------------------------
RR_FEB_2013	  8300925	
RR_MAR_2013	 10121750		| febMarApril -63919084 | nohup ./AC_AdfMasterExtract.sh rr /home/pipe/RGA/transaction/registerRead/output/2013/2013_febMarApril_noRec '2013-02-01' '2013-04-30' & > /dev/null &
RR_APR_2013	 10013387		|	
-------------------------------------------------------------------------------
RR_MAY_2013	 11042362		|
RR_JUN_2013	 10829875		|
RR_JUL_2013	 10610255		|
RR_AUG_2013	 11035618		|	nohup ./AC_AdfMasterExtract.sh rr /home/pipe/RGA/transaction/registerRead/output/2013/2013_May-Dec_noRec/ '2013-05-01' '2013-12-31' & > /dev/null &
RR_SEP_2013	 10976290		|	
RR_OCT_2013	 11335504		|	totalafterExtraction- 200476439
RR_NOV_2013	 10968369		|   total in DB - 88074552
RR_DEC_2013	 11276279		|
---------------------------------------------------------------------------
RR_JAN_2014	 11085904
RR_FEB_2014	 10097550
RR_MAR_2014	 11465057
RR_APR_2014	 11019476
RR_MAY_2014	 11758452
RR_JUN_2014	 18706145
---------------------------------------------------
RR_Jan_2016	 378646965, 20154214 | 34830081 | 2016.05.24 11:50:19 PDT	2016.05.24 11:50:25 PDT	2016.05.24 22:19:21 PDT
---------------------------------------------------
RR_FEB_2016	 18892909		| total rec - 42556649 | 2016.05.23 23:01:28 PDT	2016.05.23 23:01:36 PDT	2016.05.24 10:26:44 PDT
RR_MAR_2016	  5498257		|
---------------------------------------------------

187182757
 13293682


nohup ./AC_AdfMasterExtract.sh rr /home/pipe/RGA/transaction/registerRead/output/2016/2016_Feb-Mar_noRec '2016-02-01' '2016-03-31' & > /dev/null &




/home/pipe/RGA/transaction/registerRead/output/12/sept
2012-09-01 00:00:00
2012-09-05 23:59:59

LP
---
select partition_name, num_rows from user_tab_partitions where table_name = 'LP_INTERVALS' and num_rows > 0 order by TO_DATE(REPLACE(REPLACE(partition_name,'LP','01'),'_','-'),'DD-MON-YYYY');

LP_SEP_2013	1891937244
LP_Jan_2016	6519407072 | Jan1- 42180747 | jan2-13503752 | jan3-31457797 | incomplete
LP_FEB_2016	 227858370 | feb1- 74524080 | feb2- 63594736| feb3-89739554 =227858370
LP_MAR_2016	  64652562 | mar-64652562
			  
LP_SEP_2013	1891937244
LP_Jan_2016	6519407072 
LP_FEB_2016	 227858370
LP_MAR_2016	  64652562 


nohup ./AC_AdfMasterExtract.sh lp /home/pipe/RGA/transaction/interval/output/16/2016_jan1_noRec '2016-01-01' '2016-01-20' & > /dev/null &

nohup ./AC_AdfMasterExtract.sh lp /home/pipe/RGA/transaction/interval/output/16/tmp_feb/ '2016-02-01' '2016-02-29' & > /dev/null &

nohup ./AC_AdfMasterExtract.sh lp /home/pipe/RGA/transaction/interval/output/16/tmp/ '2015-06-01' '2015-06-30' & > /dev/null &


2 Million in 13 mins

top on DB 
top - 02:30:07 up 23 days,  5:25,  1 user,  load average: 24.16, 25.35, 23.55
Tasks: 404 total,  22 running, 382 sleeping,   0 stopped,   0 zombie
Cpu(s): 94.4%us,  4.4%sy,  0.0%ni,  0.0%id,  0.0%wa,  0.0%hi,  1.2%si,  0.0%st
Mem:  16335736k total, 16098636k used,   237100k free,    45368k buffers
Swap:  8601596k total,        0k used,  8601596k free, 14503904k cached


64652562
 
20160517005728
20160517040556

~3:10 min

1)
LP_Jan_2016	6519407072



/home/pipe/RGA/transaction/interval/output/16
'2016-03-01 00:00:00'
'2016-03-01 23:59:59'


nohup ./AC_AdfIntervalExtract.sh /home/pipe/RGA/transaction/interval/output/16/test '2016-03-01 00:00:00' '2016-03-01 23:59:59' 1 25000 & > /dev/null &




DE
---

	2012-10 308041 	
	2012-11	324275
	2012-12	412828
	2013-01	321582
	2013-02	363044
	2013-03	464546
	2013-04	415603
	2013-05	429689
	2013-06	422343
	2013-07	421059
	2013-08	429355
	2013-09	427423
	2013-10	450330
	2013-11	457119
	2013-12	519090
	2014-01	535587
	2014-02	489282
	2014-03	542196
	2014-04	466734
	2014-05	475020
	2014-06	472308
	2014-07	488981
	2014-08	489374
	2014-09	469438
	2014-10	490129
	2014-11	653095
	2014-12	556836
	2015-01	590490
	2015-02	509354
	2015-03	554249
	2015-04	498047
	2015-05	501900
	2015-06	482725
	2015-07	531180
	2015-08	537025
	2015-09	502521
	2015-10	562122
	2015-11	523160
	2015-12	593138
	2016-01	634819
	2016-02	571363
	2016-03	178143

=20065543
extracted:20066329
totalFiles:9793

nohup ./AC_AdfMasterExtract.sh de /home/pipe/RGA/transaction/deviceEvent/output/12-16 '2012-10-01' '2016-03-31' & > /dev/null &

1) 
/home/pipe/RGA/transaction/deviceEvent/output/12/oct
2012-10-01 00:00:00
2012-10-31 23:59:59



DataService
segmentName
analysisDate

https://wiki.emeter.com/pages/editpage.action?pageId=39240564


./AC_AdfMasterExtract.sh lp /home/pipe/RGA/transaction/interval/output/16/test '2016-03-01' '2016-03-02'
./AC_AdfMasterExtract.sh lp /home/pipe/RGA/transaction/interval/output/16/mar '2016-03-01' '2016-03-31'

nohup ./AC_AdfMasterExtract.sh lp /home/pipe/RGA/transaction/interval/output/16/mar '2016-03-01' '2016-03-31' & > /dev/null &

2 Million in 13 mins

top on DB 
top - 02:30:07 up 23 days,  5:25,  1 user,  load average: 24.16, 25.35, 23.55
Tasks: 404 total,  22 running, 382 sleeping,   0 stopped,   0 zombie
Cpu(s): 94.4%us,  4.4%sy,  0.0%ni,  0.0%id,  0.0%wa,  0.0%hi,  1.2%si,  0.0%st
Mem:  16335736k total, 16098636k used,   237100k free,    45368k buffers
Swap:  8601596k total,        0k used,  8601596k free, 14503904k cached


64652562
 
20160517005728
20160517040556

~3:10 min


LP_Jan_2016	6519407072
Jan1- 42180747 total


kill -9 `ps -ef | grep -v grep | grep -i adf | awk '{print $2}'`


RR-16
LP-12
DE-20

Structural extract
java -server -classpath /home/pipe/lib/*:/home/pipe/opt/em-ac-adf/lib/em-ac-dataextractor.jar:. -Xmx1024m -Xms512m -D_AppName=CLDSTRUCT_1 com.eMeter.PIPe.hydrofw.application.PropertiesBootstrap /home/pipe/conf/systemProperties/eipenv.properties start > abc2.log


1653270


Load strategy:
DE - all data (Oct_2012 to Feb_2016)  
LP - 1) LP_FEB_2016	 227858370
	 2) LP_MAR_2016	  64652562 
RR - 1) RR_FEB_2016	 18892909
	 2) RR_MAR_2016	  5498257	 
    
	 


proto:
D:\PerforceWorkspace1\EnergyIP\opt8\hadoop-common\trunk\em-bt-schema>.\compiler\protoc.exe --proto_path=src --java_out=src src\schema\proto\DeviceEventData.proto

D:\work\codebase\P4-WS\Perforce-WS\EnergyIP\opt8\hadoop-common\trunk\em-bt-schema>.\compiler\protoc.exe --proto_path=src --java_out=src src\schema\proto\RPAnalysisData2.proto



******************************Spark yarn history server UI ************************************************
 1.3.0
Jobs
Stages
Storage
Environment
Executors
com.emeter.loader.acloader.lp.Lp... application UI
Environment
Runtime Information

Name	Value
Java Home	/usr/java/jdk1.8.0_51/jre
Java Version	1.8.0_51 (Oracle Corporation)
Scala Version	version 2.10.4
Spark Properties

Name	Value
spark.driver.host	lnxcdh21.emeter.com
spark.eventLog.enabled	true
spark.ui.port	0
spark.driver.port	11548
spark.yarn.historyServer.address	http://lnxcdh21.emeter.com:18088
spark.yarn.app.id	application_1463008687526_0059
spark.app.name	com.emeter.loader.acloader.lp.LpIntervalLoader
spark.scheduler.mode	FIFO
spark.executor.instances	25
spark.yarn.secondary.jars	a2f.jar,eidg.jar,em-lookupdata.jar,hbase-common.jar,spring-beans-4.1.2.RELEASE.jar,spring-security-core-3.1.0.jar,xpp3.jar,hbase-protocol.jar,protobuf-java-2.5.0.jar,spring-context-4.1.2.RELEASE.jar,spring-security-web-3.1.0.jar,xstream-1.4.3.jar,em-bo.jar,hbase-server.jar,spring-context-support-4.1.2.RELEASE.jar,spring-tx-4.1.2.RELEASE.jar,commons-configuration.jar,em-bt-schema.jar,htrace-core-3.0.4.jar,kryo-2.20.jar,spring-core-4.1.2.RELEASE.jar,spring-web-4.1.2.RELEASE.jar,commons-dbcp2-2.0.1.jar,em-bt-schema-utils.jar,hadoop-auth.jar,htrace-core-3.1.0-incubating.jar,spark-assembly.jar,spring-expression-4.1.2.RELEASE.jar,em-core.jar,hadoop-common.jar,spring-jdbc-4.1.2.RELEASE.jar,voldemort.jar,commons-pool2-2.2.jar,em-hadoop-utils.jar,hadoop-hdfs.jar,jdom-1.1.jar,spring-security-cas-3.1.0.jar,xbean-2.5.jar,ehcache-2.9.0.jar,hbase-client.jar,em-hadoop-common-apps.jar,jetty-util-9.3.3.jar,ojdbc7.jar,spring-aop-4.1.2.RELEASE.jar,spring-security-config-3.1.0.jar,xmlpull-1.1.3.1.jar,em-job.jar
spark.executor.id	<driver>
spark.yarn.app.container.log.dir	/var/log/hadoop-yarn/container/application_1463008687526_0059/container_1463008687526_0059_01_000001
spark.master	yarn-cluster
spark.ui.filters	org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
spark.eventLog.dir	hdfs://lnxcdh21.emeter.com:8020/user/spark/applicationHistory
spark.fileserver.uri	http://192.168.145.181:47641
spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS	lnxcdh21.emeter.com
spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES	http://lnxcdh21.emeter.com:8088/proxy/application_1463008687526_0059
spark.tachyonStore.folderName	spark-19b555e1-039f-434b-a8f9-6cf9fe5f434c
spark.app.id	application_1463008687526_0059
System Properties

Name	Value
java.io.tmpdir	/data/yarn/nm/usercache/u007169584/appcache/application_1463008687526_0059/container_1463008687526_0059_01_000001/tmp
line.separator	
path.separator	:
sun.management.compiler	HotSpot 64-Bit Tiered Compilers
sun.cpu.endian	little
java.specification.version	1.8
java.vm.specification.name	Java Virtual Machine Specification
java.vendor	Oracle Corporation
java.vm.specification.version	1.8
user.home	/var/lib/hadoop-yarn
file.encoding.pkg	sun.io
sun.nio.ch.bugLevel	
sun.arch.data.model	64
sun.boot.library.path	/usr/java/jdk1.8.0_51/jre/lib/amd64
user.dir	/data/yarn/nm/usercache/u007169584/appcache/application_1463008687526_0059/container_1463008687526_0059_01_000001
java.library.path	:/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/native:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
sun.cpu.isalist	
os.arch	amd64
java.vm.version	25.51-b03
java.endorsed.dirs	/usr/java/jdk1.8.0_51/jre/lib/endorsed
java.runtime.version	1.8.0_51-b16
java.vm.info	mixed mode
java.ext.dirs	/usr/java/jdk1.8.0_51/jre/lib/ext:/usr/java/packages/lib/ext
java.runtime.name	Java(TM) SE Runtime Environment
file.separator	/
java.class.version	52.0
java.specification.name	Java Platform API Specification
sun.boot.class.path	/usr/java/jdk1.8.0_51/jre/lib/resources.jar:/usr/java/jdk1.8.0_51/jre/lib/rt.jar:/usr/java/jdk1.8.0_51/jre/lib/sunrsasign.jar:/usr/java/jdk1.8.0_51/jre/lib/jsse.jar:/usr/java/jdk1.8.0_51/jre/lib/jce.jar:/usr/java/jdk1.8.0_51/jre/lib/charsets.jar:/usr/java/jdk1.8.0_51/jre/lib/jfr.jar:/usr/java/jdk1.8.0_51/jre/classes
file.encoding	UTF-8
user.timezone	America/Los_Angeles
java.specification.vendor	Oracle Corporation
sun.java.launcher	SUN_STANDARD
os.version	2.6.32-504.el6.x86_64
sun.os.patch.level	unknown
java.vm.specification.vendor	Oracle Corporation
user.country	US
sun.jnu.encoding	UTF-8
user.language	en
java.vendor.url	http://java.oracle.com/
java.awt.printerjob	sun.print.PSPrinterJob
java.awt.graphicsenv	sun.awt.X11GraphicsEnvironment
awt.toolkit	sun.awt.X11.XToolkit
os.name	Linux
java.vm.vendor	Oracle Corporation
java.vendor.url.bug	http://bugreport.sun.com/bugreport/
user.name	yarn
java.vm.name	Java HotSpot(TM) 64-Bit Server VM
sun.java.command	org.apache.spark.deploy.yarn.ApplicationMaster --class com.emeter.loader.acloader.lp.LpIntervalLoader --jar hdfs://lnxcdh21.emeter.com:8020/user/u007169584/lib/em-hadoop-common-apps.jar --arg -SpropFile=hdfs://lnxcdh21.emeter.com:8020/user/u007169584/oozie/wf/LoadIntervalReads/spark.properties --executor-memory 1024m --executor-cores 1 --num-executors 25
java.home	/usr/java/jdk1.8.0_51/jre
java.version	1.8.0_51
sun.io.unicode.encoding	UnicodeLittle
Classpath Entries

Resource	Source
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/lib/paranamer-2.3.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/xz-1.0.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/jackson-mapper-asl-1.8.8.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/hadoop-yarn-registry.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/jline-2.11.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/zookeeper.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.8.8.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/hadoop-hdfs-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/hadoop-yarn-server-applicationhistoryservice.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/commons-net-3.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hamcrest-core-1.3.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/hadoop-yarn-server-web-proxy-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/stax-api-1.0-2.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/asm-3.2.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/hadoop-auth-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/jackson-xc-1.8.8.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/jasper-runtime-5.5.23.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/hadoop-nfs-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/lib/jasper-runtime-5.5.23.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-datajoin-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/hadoop-auth.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/commons-io-2.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/lib/xmlenc-0.52.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/parquet-pig.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/jsch-0.1.42.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/jersey-core-1.9.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/parquet-format.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-mapreduce-client-hs-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/lib/jackson-core-asl-1.8.8.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/commons-codec-1.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/parquet-pig-bundle.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/hadoop-yarn-server-common.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/hadoop-yarn-api.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/gson-2.2.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/jackson-core-asl-1.8.8.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/api-asn1-api-1.0.0-M20.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/commons-beanutils-1.7.0.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/logredactor-1.0.3.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/commons-beanutils-core-1.8.0.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/hadoop-yarn-registry-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/jsp-api-2.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/jersey-server-1.9.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/commons-digester-1.8.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.4.4-tests.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-archives-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/commons-httpclient-3.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/lib/javax.inject-1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/jackson-core-asl-1.8.8.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/hadoop-annotations.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-sls-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/log4j-1.2.17.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/servlet-api-2.5.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/hadoop-common-2.6.0-cdh5.4.4-tests.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/aopalliance-1.0.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-ant.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/avro.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/hadoop-aws.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-auth-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/jackson-jaxrs-1.8.8.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/hadoop-yarn-server-web-proxy.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/parquet-scala_2.10.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/hadoop-yarn-api-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/guice-servlet-3.0.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/jetty-6.1.26.cloudera.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-mapreduce-client-shuffle.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/lib/servlet-api-2.5.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-datajoin.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-azure-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-mapreduce-client-jobclient-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/commons-beanutils-core-1.8.0.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/hadoop-yarn-server-common-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/hadoop-hdfs-nfs-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/jsch-0.1.42.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/guice-3.0.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/commons-lang-2.6.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-mapreduce-client-jobclient-tests.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/avro.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/javax.inject-1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/slf4j-api-1.7.5.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/apacheds-kerberos-codec-2.0.0-M15.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-archives.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/parquet-avro.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/lib/commons-el-1.0.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/jersey-server-1.9.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-rumen.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/commons-configuration-1.6.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-mapreduce-client-core-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/commons-cli-1.2.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-auth.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/lib/guice-3.0.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/jackson-mapper-asl-1.8.8.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/lib/xz-1.0.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/xmlenc-0.52.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/lib/commons-cli-1.2.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/jasper-compiler-5.5.23.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-streaming-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/jersey-guice-1.9.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/jackson-xc-1.8.8.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/java-xmlbuilder-0.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/httpclient-4.2.5.jar	System Classpath
/data/yarn/nm/usercache/u007169584/appcache/application_1463008687526_0059/container_1463008687526_0059_01_000001	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/stax-api-1.0-2.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/api-util-1.0.0-M20.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/jasper-runtime-5.5.23.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/guava-11.0.2.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-distcp-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/commons-collections-3.2.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/parquet-generator.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/lib/asm-3.2.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/commons-codec-1.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/hadoop-hdfs-nfs.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/commons-lang-2.6.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/java-xmlbuilder-0.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.8.8.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/parquet-hadoop-bundle.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/zookeeper.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/aws-java-sdk-1.7.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/lib/commons-io-2.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/hadoop-yarn-client-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/parquet-test-hadoop2.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/jackson-mapper-asl-1.8.8.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/servlet-api-2.5.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/htrace-core-3.0.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/hadoop-yarn-server-tests-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-extras-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/commons-el-1.0.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/commons-configuration-1.6.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/lib/jetty-6.1.26.cloudera.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/lib/commons-codec-1.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/curator-client-2.7.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/snappy-java-1.0.4.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/parquet-jackson.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/parquet-protobuf.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/commons-math3-3.1.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/lib/htrace-core-3.0.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/parquet-hadoop.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/parquet-column.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/jettison-1.1.jar	System Classpath
/etc/hadoop/conf.cloudera.yarn	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/jaxb-impl-2.2.3-1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/commons-math3-3.1.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/guava-11.0.2.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/jersey-json-1.9.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/snappy-java-1.0.4.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/xmlenc-0.52.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/hadoop-yarn-common.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/httpclient-4.2.5.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/lib/netty-3.6.2.Final.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/commons-net-3.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/hadoop-yarn-common-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/jersey-core-1.9.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/metrics-core-3.0.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-mapreduce-client-nativetask.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/curator-framework-2.7.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/parquet-thrift.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-extras.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/lib/jsp-api-2.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/zookeeper.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/microsoft-windowsazure-storage-sdk-0.6.0.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/jettison-1.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/jetty-util-6.1.26.cloudera.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-mapreduce-client-app.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/jackson-core-asl-1.8.8.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/curator-recipes-2.7.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/apacheds-i18n-2.0.0-M15.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/commons-logging-1.1.3.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-azure.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/parquet-encoding.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-ant-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/junit-4.11.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/jets3t-0.9.0.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/jsp-api-2.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/servlet-api-2.5.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/jasper-compiler-5.5.23.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-gridmix-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/jaxb-api-2.2.2.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-mapreduce-client-nativetask-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/jackson-core-2.2.3.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/api-util-1.0.0-M20.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/activation-1.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/hadoop-yarn-applications-distributedshell-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/commons-cli-1.2.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/curator-client-2.7.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/lib/jersey-server-1.9.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/commons-el-1.0.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/hadoop-common.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/xz-1.0.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-mapreduce-client-common.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/lib/commons-lang-2.6.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/hadoop-yarn-applications-unmanaged-am-launcher.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/jackson-jaxrs-1.8.8.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-distcp.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/parquet-common.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-mapreduce-client-app-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-rumen-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/commons-cli-1.2.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/log4j-1.2.17.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/jetty-6.1.26.cloudera.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/parquet-scrooge_2.10.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/hadoop-yarn-client.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/lib/log4j-1.2.17.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/paranamer-2.3.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/junit-4.11.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/gson-2.2.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/jsr305-3.0.0.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/hadoop-nfs.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/commons-logging-1.1.3.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/commons-codec-1.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/jetty-util-6.1.26.cloudera.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/hue-plugins-3.7.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/hadoop-common-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/jsr305-3.0.0.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/commons-httpclient-3.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-sls.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/hadoop-yarn-server-applicationhistoryservice-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/guava-11.0.2.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/stax-api-1.0-2.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/jackson-annotations-2.2.3.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/parquet-cascading.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/commons-lang-2.6.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/hadoop-hdfs-tests.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/lib/avro.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/activation-1.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/lib/guava-11.0.2.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/jersey-client-1.9.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/lib/commons-compress-1.4.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/lib/asm-3.2.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/httpcore-4.2.5.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/hadoop-common-tests.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-mapreduce-client-hs-plugins.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/commons-compress-1.4.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/jets3t-0.9.0.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/jetty-6.1.26.cloudera.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/commons-httpclient-3.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/hadoop-yarn-server-nodemanager.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/hadoop-yarn-server-tests.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/protobuf-java-2.5.0.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/activation-1.1.jar	System Classpath
/data/yarn/nm/usercache/u007169584/appcache/application_1463008687526_0059/container_1463008687526_0059_01_000001/__spark__.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/hadoop-yarn-server-resourcemanager.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/slf4j-log4j12.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/hadoop-yarn-server-resourcemanager-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/hadoop-hdfs-2.6.0-cdh5.4.4-tests.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/commons-io-2.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/curator-framework-2.7.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/lib/jackson-core-asl-1.8.8.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/asm-3.2.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/jersey-core-1.9.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/jackson-databind-2.2.3.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/jaxb-api-2.2.2.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/htrace-core-3.0.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/mockito-all-1.8.5.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/hadoop-yarn-server-nodemanager-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-mapreduce-client-core.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/ojdbc7.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/hamcrest-core-1.3.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/commons-collections-3.2.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/jackson-xc-1.8.8.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/asm-3.2.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/jersey-server-1.9.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/log4j-1.2.17.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/commons-compress-1.4.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/hadoop-hdfs.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/lib/netty-3.6.2.Final.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/hadoop-aws-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/spark-1.3.0-cdh5.4.4-yarn-shuffle.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/lib/commons-io-2.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-gridmix.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/jetty-util-6.1.26.cloudera.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/paranamer-2.3.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-mapreduce-client-hs-plugins-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar	System Classpath
/var/run/cloudera-scm-agent/process/1606-yarn-NODEMANAGER	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/lib/junit-4.11.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/httpcore-4.2.5.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/lib/jersey-core-1.9.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/commons-digester-1.8.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/protobuf-java-2.5.0.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/jackson-jaxrs-1.8.8.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/jettison-1.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/xz-1.0.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-streaming.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/commons-collections-3.2.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-mapreduce-client-jobclient.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/netty-3.6.2.Final.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/mockito-all-1.8.5.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/commons-io-2.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/commons-compress-1.4.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/parquet-format-javadoc.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/commons-beanutils-1.7.0.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/jsr305-3.0.0.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/curator-recipes-2.7.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/lib/jersey-json-1.9.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/parquet-format-sources.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/parquet-tools.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-mapreduce-client-common-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-mapreduce-client-hs.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-hdfs/lib/jetty-util-6.1.26.cloudera.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn/hadoop-yarn-applications-distributedshell.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/lib/snappy-java-1.0.4.1.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-mapreduce/hadoop-mapreduce-client-shuffle-2.6.0-cdh5.4.4.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/lib/jersey-json-1.9.jar	System Classpath
/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop/hadoop-annotations-2.6.0-cdh5.4.4.jar	System Classpath
********************************************************************************************************************
Oozie urls

oozie job -oozie http://lnxcdh21.emeter.com:11000/oozie/ -kill 0000029-160511162000176-oozie-oozi-W
oozie job -oozie http://lnxcdh21.emeter.com:11000/oozie/ -info 0000029-160511162000176-oozie-oozi-W

oozie job -oozie http://lnxcdh23.emeter.com:11000/oozie/ -info 0001886-160620101622823-oozie-oozi-W
oozie job -oozie http://lnxcdh23.emeter.com:11000/oozie/ -kill 0001886-160620101622823-oozie-oozi-W

/user/u007169584/staging/interval_read_s_v1/wkg/LoadIntervalReads/1106/20160601095621

oozie job -oozie http://lnxcdh21.emeter.com:11000/oozie/ -kill 0000340-161005154451618-oozie-oozi-W
oozie job -oozie http://lnxcdh21.emeter.com:11000/oozie/ -kill 0000341-161005154451618-oozie-oozi-W

-----------------------------------------------------------------------
3 phase low voltage:
Roll over concept : NA
Va or Vb or Vc < 30
3 measType handle in Feature
Iterate for day and use 3 mesType values in one iteration
? Do we need to provide o/p as 1 or sum of ocurrances of (Va or Vb or Vc < 30)


http://www.livelaw.in/no-service-tax-buying-flats-preferential-location-charges-taxable-delhi-hc-read-judgment/



for LP is 3
for RR look for channel table in EIP and look for channel.meas_type_id OR
query hbase using MeterDataBrowser.java


0000002-160614150044005-oozie-oozi-W
0000141-160427161843622-oozie-oozi-W

feb3 - 191 files
89739554 records



//case 1 : No meter change || meterChange but no meterParam to indicate 12 RegisterReads
//case 2 : meter change and meterParam to indicate 12 RegisterReads for some intervals
//case 3 : meter change and meterParam to indicate 12 RegisterReads for all intervals

createDataSet(startDate, endDate, minWindow, isAllRegisterToConsider, NavigableMap<Long, RegisterReadsDaily>... maps)

Some meters will record both Voltage Phase Angle and Current Phase Angle registers for each phase (Pia, Pva, Pib, Pvb, Pic, Pvc). In this case, the Voltage and Current Phase 
Angle values for each phase will be combined to determine the final Current Phase Angle value for evaluation.  To do this, the formula is:  Current Angle Phase x – Voltage Angle Phase x.  
(Pia = Pia-Pva; Pib =  Pib - Pvb, Pic = Pic - Pvc). If the result is negative, then add 360.

-//EnergyIP/core/SP/rel_7/7.6/eMeter/dw/adftransactionmonitor/
-//EnergyIP/core/SP/rel_7/7.7/eMeter/dw/adftransactionmonitor/
-


-bitMask for validation code
- Check whether the jth object is in the subset (check whether jth bit is 1):
   use the bitwise AND operation T = A & (1 << j).
   If T = 0, then the j-th item of the set is off.
   If T != 0 (to be precise, T = (1 << j)), then the j-th item of the set is on.

   For example:    A = 42 (base 10) = 101010 (base 2)
                   j = 3, 1 << j    = 001000 <- bit ‘1’ is shifted to the left 3 times
                                     -------- AND (only true if both bits are true)
                   T = 8 (base 10)  = 001000 (base 2) -> not zero, the 3rd item is on
   
   
   
                   
                   
******************************************************
old: Pre_Fetch_Meastype_UDC_ID_List_RR
new: Pre_Fetch_Identifer_List_RR

XML:
Resources
-/resources/org/common/refData/eip/revenueprotection/EntityDefReferenceData_RPDataStores.xml
-/resources/org/Sample/refData/eip/revenueprotection/RPAnalysisDataServiceVersion.xml

Sample (sample data for configuring sample feature etc)
-trunk/sample/resources/org/common/refData/eip/revenueprotection/EntityDefReferenceData_RPDataStores.xml

Java:
RPConstants: ref is used all places


Test:
XML
Java

<refdata:dataSvcGenConfiguration>
		<refdata:seq>5</refdata:seq>
		<refdata:name>RR</refdata:name>
		<refdata:type>RevenueProtectionAnalysis.PreLoadDataStore</refdata:type>
		<refdata:subType>RRData</refdata:subType>
		<refdata:descText>Defines the Data Store For Register Reads Data</refdata:descText>
		<refdata:active>Y</refdata:active>
		<refdata:dataSvcGenConfAttributes>
			<refdata:dataSvcGenConfAttribute>
				<refdata:name>Pre_Fetch_MinuteBackwards_For_RR</refdata:name>
				<refdata:value>86400</refdata:value>
			</refdata:dataSvcGenConfAttribute>
			<refdata:dataSvcGenConfAttribute>
				<!-- Old 
				<refdata:name>Pre_Fetch_Meastype_UDC_ID_List_RR</refdata:name>
				-->
				<!-- new -->
				<refdata:name>Pre_Fetch_Identifer_List_RR</refdata:name>
				<refdata:value>KWH.SAMPLE.1</refdata:value>
			</refdata:dataSvcGenConfAttribute>
		</refdata:dataSvcGenConfAttributes>
</refdata:dataSvcGenConfiguration>




<refdata:dataSvcGenConfiguration>
		<refdata:seq>30</refdata:seq>
		<refdata:name>QoC</refdata:name>
		<refdata:type>RevenueProtectionAnalysis.PreLoadDataStore</refdata:type>
		<refdata:subType>QoCData</refdata:subType>
		<refdata:descText>Defines the Data Store For QoC Data</refdata:descText>
		<refdata:active>Y</refdata:active>
		<refdata:dataSvcGenConfAttributes>
			<refdata:dataSvcGenConfAttribute>
				<refdata:name>Pre_Fetch_MinuteBackwards</refdata:name>
				<refdata:value>86400</refdata:value>
			</refdata:dataSvcGenConfAttribute>
			<refdata:dataSvcGenConfAttribute>
				<refdata:name>Pre_Fetch_MeterParamName_ToCount_QoCRegister</refdata:name>
				<refdata:value>QoCRegisterMeterParam</refdata:value>
			</refdata:dataSvcGenConfAttribute>
			<refdata:dataSvcGenConfAttribute>
				<refdata:name>Pre_Fetch_MinuteDataSetWindow</refdata:name>
				<refdata:value>5</refdata:value>
			</refdata:dataSvcGenConfAttribute>
			<refdata:dataSvcGenConfAttribute>
				<refdata:name>Pre_Fetch_RR_ReadFlag_FilterList</refdata:name>
				<refdata:value></refdata:value>
			</refdata:dataSvcGenConfAttribute>
			<refdata:dataSvcGenConfAttribute>
				<refdata:name>Pre_Fetch_RR_ValidationStatusCode_FilterList</refdata:name>
				<refdata:value>DEL</refdata:value>
			</refdata:dataSvcGenConfAttribute>
			<refdata:dataSvcGenConfAttribute>
				<refdata:name>Pre_Fetch_Data_Identifier_va</refdata:name>
				<refdata:value>KWH.SAMPLE.VA.1</refdata:value>
			</refdata:dataSvcGenConfAttribute>
			<refdata:dataSvcGenConfAttribute>
				<refdata:name>Pre_Fetch_Data_Identifier_vb</refdata:name>
				<refdata:value>KWH.SAMPLE.VB.1</refdata:value>
			</refdata:dataSvcGenConfAttribute>
			<refdata:dataSvcGenConfAttribute>
				<refdata:name>Pre_Fetch_Data_Identifier_vc</refdata:name>
				<refdata:value>KWH.SAMPLE.VC.1</refdata:value>
			</refdata:dataSvcGenConfAttribute>
			<refdata:dataSvcGenConfAttribute>
				<refdata:name>Pre_Fetch_Data_Identifier_ia</refdata:name>
				<refdata:value>KWH.SAMPLE.IA.1</refdata:value>
			</refdata:dataSvcGenConfAttribute>
			<refdata:dataSvcGenConfAttribute>
				<refdata:name>Pre_Fetch_Data_Identifier_ib</refdata:name>
				<refdata:value>KWH.SAMPLE.IB.1</refdata:value>
			</refdata:dataSvcGenConfAttribute>
			<refdata:dataSvcGenConfAttribute>
				<refdata:name>Pre_Fetch_Data_Identifier_ic</refdata:name>
				<refdata:value>KWH.SAMPLE.IC.1</refdata:value>
			</refdata:dataSvcGenConfAttribute>
			<refdata:dataSvcGenConfAttribute>
				<refdata:name>Pre_Fetch_Data_Identifier_pia</refdata:name>
				<refdata:value>KWH.SAMPLE.PIA.1</refdata:value>
			</refdata:dataSvcGenConfAttribute>
			<refdata:dataSvcGenConfAttribute>
				<refdata:name>Pre_Fetch_Data_Identifier_pib</refdata:name>
				<refdata:value>KWH.SAMPLE.PIB.1</refdata:value>
			</refdata:dataSvcGenConfAttribute>
			<refdata:dataSvcGenConfAttribute>
				<refdata:name>Pre_Fetch_Data_Identifier_pic</refdata:name>
				<refdata:value>KWH.SAMPLE.PIC.1</refdata:value>
			</refdata:dataSvcGenConfAttribute>
			<refdata:dataSvcGenConfAttribute>
				<refdata:name>Pre_Fetch_Data_Identifier_pva</refdata:name>
				<refdata:value>KWH.SAMPLE.PvA.1</refdata:value>
			</refdata:dataSvcGenConfAttribute>
			<refdata:dataSvcGenConfAttribute>
				<refdata:name>Pre_Fetch_Data_Identifier_pvb</refdata:name>
				<refdata:value>KWH.SAMPLE.PvB.1</refdata:value>
			</refdata:dataSvcGenConfAttribute>
			<refdata:dataSvcGenConfAttribute>
				<refdata:name>Pre_Fetch_Data_Identifier_pvc</refdata:name>
				<refdata:value>KWH.SAMPLE.PvC.1</refdata:value>
			</refdata:dataSvcGenConfAttribute>
		</refdata:dataSvcGenConfAttributes>
</refdata:dataSvcGenConfiguration>
********************************************************************************************

? (spark / Hadoop)debugging on cluster
? string encoding:

TestEntityRepository
sdps

? Which HC version RP2.3 will support as newer changes are in trunk ?

lnxapp333
*************************














SSH Secure Shell 3.2.9 (Build 283)
Copyright (c) 2000-2003 SSH Communications Security Corp - http://www.ssh.com/

This copy of SSH Secure Shell is a non-commercial version.
This version does not include PKI and PKCS #11 functionality.


be eip
[ravigu@lnxapp333 ~]$ be eip
eip@lnxapp333:/home/eip->cd opt/hadoop-common/bin/
eip@lnxapp333:/home/eip/opt/hadoop-common/bin->lsl
bash: lsl: command not found
eip@lnxapp333:/home/eip/opt/hadoop-common/bin->ls
1-PrepareEnv-EIP.sh  2-HadoopOrgAdmin-ROOT.sh  3-EipOrgAdmin-EIP.sh  deploy.properties  EIPTomcatSharedLinks.sh  entity  
.sh
eip@lnxapp333:/home/eip/opt/hadoop-common/bin->cd entity/
eip@lnxapp333:/home/eip/opt/hadoop-common/bin/entity->ls
1-1466465110.json  1-1466811165.json  2-1466811087.json     4591-1466526165.json  4592-1466533183.json  4952-1466619317.json  sdp-1466464981.json
1-1466811093.json  *-1466525800.json  2-1466811128.json     4592-1466526139.json  4592-1466533231.json  4952-1466705014.json
1-1466811122.json  1.json             2351-1466465132.json  4592-1466526207.json  4592-1466704873.json  entityShell.sh
eip@lnxapp333:/home/eip/opt/hadoop-common/bin/entity->./entityShell.sh 
Enter EnergyIP Org UserName: apratim
Enter Password: *******
Launching Entity Shell for user apratim ...

     ______      __  _ __             _____ __         ____ 
    / ____/___  / /_(_) /___  __     / ___// /_  ___  / / / 
   / __/ / __ \/ __/ / __/ / / /_____\__ \/ __ \/ _ \/ / /  
  / /___/ / / / /_/ / /_/ /_/ /_____/__/ / / / /  __/ / /   
 /_____/_/ /_/\__/_/\__/\__, /     /____/_/ /_/\___/_/_/    
                       /____/                               

Base URL for the environment->  [http://lnxapp333.emeter.com:8081/em-hc-rs/entity]
Please use setBaseUrl command to correct BASE_URL if it's not pointing to correct End Point
 Base Url: http://lnxapp333.emeter.com:8081/em-hc-rs/entity
 Entity NAme: sdp
Available Commands:
get, put, delete, setBaseUrl, setEntity, ls
Entity> vi 1.json
{
  "lpi" : {
    "{\"usageDate\":20160101,\"measTypeId\":1001,\"_version\":1465251018618}" : {
      "intValues" : [ {
        "changeMethod" : "EDT",
        "valStatus" : "EST",
        "failCode" : 9,
        "estimated" : 1,
        "endTime" : 213412341,
        "value" : 100.56,
        "deviceId" : 1990,
        "deviceDataSrcId" : 9999
      }, {
        "changeMethod" : "EDT",
        "valStatus" : "EST",
        "failCode" : 9,
        "estimated" : 1,
        "endTime" : 213412342,
        "value" : 101.56,
        "deviceId" : 1991,
        "deviceDataSrcId" : 10000
      }, {
        "changeMethod" : "EDT",
        "valStatus" : "EST",
        "failCode" : 9,
        "estimated" : 1,
        "endTime" : 213412343,
        "value" : 102.56,
        "deviceId" : 1992,
        "deviceDataSrcId" : 10001
      }, {
        "changeMethod" : "EDT",
        "valStatus" : "EST",
        "failCode" : 9,
        "estimated" : 1,
        "endTime" : 213412344,
        "value" : 103.56,
        "deviceId" : 1993,
Entity> 
Entity> put
Enter Entity Id: 1
* About to connect() to lnxapp333.emeter.com port 8081 (#0)
*   Trying 192.168.170.234... connected
* Connected to lnxapp333.emeter.com (192.168.170.234) port 8081 (#0)
* Server auth using Basic with user 'apratim'
> PUT /em-hc-rs/entity/sdp/1 HTTP/1.1
> Authorization: Basic YXByYXRpbTphcHJhdGlt
> User-Agent: curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.19.1 Basic ECC zlib/1.2.3 libidn/1.18 libssh2/1.4.2
> Host: lnxapp333.emeter.com:8081
> Accept: */*
> Content-Type: application/json
> Content-Length: 2263
> Expect: 100-continue
> 
< HTTP/1.1 404 Not Found
< Server: Apache-Coyote/1.1
< Content-Length: 0
< Date: Fri, 22 Jul 2016 11:29:10 GMT
< Connection: close
< 
* Closing connection #0
Entity> ls
-rw-rw-r-- 1 eip eip    0 Jun 20 16:25 1-1466465110.json
-rw-rw-r-- 1 eip eip   17 Jun 24 16:31 1-1466811093.json
-rw-rw-r-- 1 eip eip   17 Jun 24 16:32 1-1466811122.json
-rw-rw-r-- 1 eip eip    0 Jun 24 16:32 1-1466811165.json
-rw-rw-r-- 1 eip eip    0 Jun 21 09:16 *-1466525800.json
-rw-rw-r-- 1 eip eip 2351 Jun  6 16:04 1.json
-rw-rw-r-- 1 eip eip 2391 Jun 24 16:31 2-1466811087.json
-rw-rw-r-- 1 eip eip 2391 Jun 24 16:32 2-1466811128.json
-rw-rw-r-- 1 eip eip    0 Jun 20 16:25 2351-1466465132.json
-rw-rw-r-- 1 eip eip    0 Jun 21 09:22 4591-1466526165.json
-rw-rw-r-- 1 eip eip    0 Jun 21 09:22 4592-1466526139.json
-rw-rw-r-- 1 eip eip    0 Jun 21 09:23 4592-1466526207.json
-rw-rw-r-- 1 eip eip 1072 Jun 21 11:19 4592-1466533183.json
-rw-rw-r-- 1 eip eip 1070 Jun 21 11:20 4592-1466533231.json
-rw-rw-r-- 1 eip eip    0 Jun 23 11:01 4592-1466704873.json
-rw-rw-r-- 1 eip eip 1072 Jun 22 11:15 4952-1466619317.json
-rw-rw-r-- 1 eip eip    0 Jun 23 11:03 4952-1466705014.json
-rw-rw-r-- 1 eip eip    0 Jun 20 16:23 sdp-1466464981.json
Entity> rm -rf 1.json
Entity> ls
-rw-rw-r-- 1 eip eip    0 Jun 20 16:25 1-1466465110.json
-rw-rw-r-- 1 eip eip   17 Jun 24 16:31 1-1466811093.json
-rw-rw-r-- 1 eip eip   17 Jun 24 16:32 1-1466811122.json
-rw-rw-r-- 1 eip eip    0 Jun 24 16:32 1-1466811165.json
-rw-rw-r-- 1 eip eip    0 Jun 21 09:16 *-1466525800.json
-rw-rw-r-- 1 eip eip 2391 Jun 24 16:31 2-1466811087.json
-rw-rw-r-- 1 eip eip 2391 Jun 24 16:32 2-1466811128.json
-rw-rw-r-- 1 eip eip    0 Jun 20 16:25 2351-1466465132.json
-rw-rw-r-- 1 eip eip    0 Jun 21 09:22 4591-1466526165.json
-rw-rw-r-- 1 eip eip    0 Jun 21 09:22 4592-1466526139.json
-rw-rw-r-- 1 eip eip    0 Jun 21 09:23 4592-1466526207.json
-rw-rw-r-- 1 eip eip 1072 Jun 21 11:19 4592-1466533183.json
-rw-rw-r-- 1 eip eip 1070 Jun 21 11:20 4592-1466533231.json
-rw-rw-r-- 1 eip eip    0 Jun 23 11:01 4592-1466704873.json
-rw-rw-r-- 1 eip eip 1072 Jun 22 11:15 4952-1466619317.json
-rw-rw-r-- 1 eip eip    0 Jun 23 11:03 4952-1466705014.json
-rw-rw-r-- 1 eip eip    0 Jun 20 16:23 sdp-1466464981.json
Entity> get
Enter Entity Id: 1
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
Entity> ls
-rw-rw-r-- 1 eip eip    0 Jun 20 16:25 1-1466465110.json
-rw-rw-r-- 1 eip eip   17 Jun 24 16:31 1-1466811093.json
-rw-rw-r-- 1 eip eip   17 Jun 24 16:32 1-1466811122.json
-rw-rw-r-- 1 eip eip    0 Jun 24 16:32 1-1466811165.json
-rw-rw-r-- 1 eip eip    0 Jul 22 04:29 1-1469186979.json
-rw-rw-r-- 1 eip eip    0 Jun 21 09:16 *-1466525800.json
-rw-rw-r-- 1 eip eip 2391 Jun 24 16:31 2-1466811087.json
-rw-rw-r-- 1 eip eip 2391 Jun 24 16:32 2-1466811128.json
-rw-rw-r-- 1 eip eip    0 Jun 20 16:25 2351-1466465132.json
-rw-rw-r-- 1 eip eip    0 Jun 21 09:22 4591-1466526165.json
-rw-rw-r-- 1 eip eip    0 Jun 21 09:22 4592-1466526139.json
-rw-rw-r-- 1 eip eip    0 Jun 21 09:23 4592-1466526207.json
-rw-rw-r-- 1 eip eip 1072 Jun 21 11:19 4592-1466533183.json
-rw-rw-r-- 1 eip eip 1070 Jun 21 11:20 4592-1466533231.json
-rw-rw-r-- 1 eip eip    0 Jun 23 11:01 4592-1466704873.json
-rw-rw-r-- 1 eip eip 1072 Jun 22 11:15 4952-1466619317.json
-rw-rw-r-- 1 eip eip    0 Jun 23 11:03 4952-1466705014.json
-rw-rw-r-- 1 eip eip    0 Jun 20 16:23 sdp-1466464981.json
Entity> vi 1-1466811093.json
{
  "sdpId" : 1
}
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
Entity> 
Entity> 
Entity> 
Entity> ls
-rw-rw-r-- 1 eip eip    0 Jun 20 16:25 1-1466465110.json
-rw-rw-r-- 1 eip eip   17 Jun 24 16:31 1-1466811093.json
-rw-rw-r-- 1 eip eip   17 Jun 24 16:32 1-1466811122.json
-rw-rw-r-- 1 eip eip    0 Jun 24 16:32 1-1466811165.json
-rw-rw-r-- 1 eip eip    0 Jul 22 04:29 1-1469186979.json
-rw-rw-r-- 1 eip eip    0 Jun 21 09:16 *-1466525800.json
-rw-rw-r-- 1 eip eip 2391 Jun 24 16:31 2-1466811087.json
-rw-rw-r-- 1 eip eip 2391 Jun 24 16:32 2-1466811128.json
-rw-rw-r-- 1 eip eip    0 Jun 20 16:25 2351-1466465132.json
-rw-rw-r-- 1 eip eip    0 Jun 21 09:22 4591-1466526165.json
-rw-rw-r-- 1 eip eip    0 Jun 21 09:22 4592-1466526139.json
-rw-rw-r-- 1 eip eip    0 Jun 21 09:23 4592-1466526207.json
-rw-rw-r-- 1 eip eip 1072 Jun 21 11:19 4592-1466533183.json
-rw-rw-r-- 1 eip eip 1070 Jun 21 11:20 4592-1466533231.json
-rw-rw-r-- 1 eip eip    0 Jun 23 11:01 4592-1466704873.json
-rw-rw-r-- 1 eip eip 1072 Jun 22 11:15 4952-1466619317.json
-rw-rw-r-- 1 eip eip    0 Jun 23 11:03 4952-1466705014.json
-rw-rw-r-- 1 eip eip    0 Jun 20 16:23 sdp-1466464981.json
Entity> 


*************************

RP refactor:
delete:
	/em-revenueprotection-trunk/src/com/emeter/revenueprotection/prefetch/MudrDataLookupHelper.java
	

changes for Rp2.3

-IDataStore will not have getData()
-Hive table column 
	-analysisdate string, datasvclabel string, segmentlabel string


-IDataStore : 
	-only Load() 
	-no getData(): getxxx() should move to respective helper classes
			
-seperation of concern should be clearly defined (preFetch)
	-Configurations: load dataSvcConfigurations from eidg cache
	-Manager: returns the initializes beans to provide a dataStore service. Like DataStoreManager
	-Services: stateless beans, can be reused and provide services like LPDataStore, RRDataStore etc
				Load is called with RPContext, dataStoreConfiguration



-seperation of concern should be clearly defined (featureExecution)
	-Configurations: load dataSvcConfigurations from eidg cache
	-Manager: returns the initializes beans to provide a feature service. Like FeatureManager
	-Services: stateless beans, can be reused and provide services like NTB	
				executeFeature is called with RPContext, featureConfiguration
				
-StreamLined the Orchestration logic in FeatureCreationExecutor				

-TODO: update configuration Parameter name


FeatureExecutor
//IDataStore<LPIntervalsDaily> lDataStore = dsm.getDataStore(ctx,preFetchDataSetName);		
//LPDataStore lDataStore = (LPDataStore) dsm.getDataStore(ctx,preFetchDataSetName);
NavigableMap<Long, LPIntervalsDaily> intervalDataMap = LPDataHelper.getData(ctx, measTypeId, preFetchDataSetName);


propUpdate:
DataStore
Pre_Fetch_MinuteBackwards_For_RR | Pre_Fetch_MinuteBackwards_For_LP | Pre_Fetch_MinuteBackwards_For_DE | minuteBackwards
PreFetchEventTypeList | preFetchEventTypeList

Pre_Fetch_Meastype_UDC_ID_List_LP | Pre_Fetch_Meastype_UDC_ID_List_RR | meastypeUDCIdList

meastypeUDCIdVA



@Deprecated
	public static final String LP_MinuteBackwards = "Pre_Fetch_MinuteBackwards_For_LP";
	@Deprecated
	public static final String RR_MinuteBackwards = "Pre_Fetch_MinuteBackwards_For_RR";
	@Deprecated
	public static final String DE_MinuteBackwards = "Pre_Fetch_MinuteBackwards_For_DE";
	@Deprecated
	public static final String PRE_FETCH_DATASET_NAME="PreFetchDataSetName";
	@Deprecated
	public static final String RP_HBASE_RESULT_TABLE ="rpresult";
	@Deprecated
	public static final String MEAS_TYPE_UDC_ID_LIST_RR="Pre_Fetch_Meastype_UDC_ID_List_RR";
	@Deprecated
	public static final String MEAS_TYPE_UDC_ID_LIST_LP="Pre_Fetch_Meastype_UDC_ID_List_LP";
	//New prop
	public static final String MEAS_TYPE_UDC_ID_LIST ="meastypeUdcIdList";
	public static final String MINUTE_BACKWARDS = "minuteBackwards";
	public static final String MEAS_TYPE_UDC_ID = "measTypeUDCId";
	public static final String DEVICE_EVENT_TYPE_LIST = "deviceEventTypeList";
	public static final String MinuteBackwards_Analysis_Start_NEW = "analysisStartMinsBackward";
	public static final String MinuteBackwards_Analysis_End_NEW = "analysisEndMinsBackward";
	public static final String BEAN_ID_DSM = "a301-dataStoreManager";
	public static final String BEAN_ID_AMI_DATA_DAO = "a301-amiDataDao";
	public static final String PREFETCH_DATA_SET_NAME_LP = "preFetchDataSetNameLP";
	public static final String PREFETCH_DATA_SET_NAME_RR = "preFetchDataSetNameRR";
	public static final String PREFETCH_DATA_SET_NAME_DE = "preFetchDataSetNameDE";
	
	


features:
measTypeUDCId
minuteBackwards

ThresholdUsageAfterRD | thresholdUsageAfterRD
ThresholdMinuteForZeroUsage | thresholdMinuteForZeroUsage
ThresholdLSVInMinutes | thresholdLSVInMinutes

ThresholdSumUsagePriorToTamper |  thresholdSumUsagePriorToTamper
ThresholdSumUsageAfterTamper | thresholdSumUsageAfterTamper
ThresholdMinutesForIntervalReadsPriorToTamper | thresholdMinutesForIntervalReadsPriorToTamper
ThresholdMinutesForIntervalReadsAfterTamper | thresholdMinutesForIntervalReadsAfterTamper
ThresholdUsage | thresholdUsage
ThresholdScore | thresholdScore
TopNValue | topNValue
TicketAlreadyExist | ticketAlreadyExist



-------
TestRPAlgosWithoutHbase -> testAmiDataDao
junit.framework.AssertionFailedError: expected:<2> but was:<0>

No test data to insert channel hence query fails and then assertion

TestAbstractFeatureCreation -> testFetchDataForProcessingAP
Assert.assertEquals(14, result.size()); 15 to 14


Sun Jul 15 12:30:00 IST 2012
Sun Jul 15 12:00:00 IST 2012

Sun Jul 01 12:30:00 IST 2012
Mon Jul 02 12:00:00 IST 2012


QoCRRData
AllQoCRRData
ThreePhCurrentQoCRRData
ThreePhCurrentPhAngleQoCRRData

thresholdVoltagePhaseA

171	111	meterParamNumQoCRegister numQoCRegisters 2	Meter param to identify 9 Vs 12 QoC registers
172	111	sdpParamMaxPhaseAngle	maxPhaseAngle	2	sdp param can have value of either 360 or 180 to represent the maximum value recorded by the meter
173	111	minuteDataSetWindow	5 	2	QoC data set window in minutes

<sch:entityFlexAttrDef>
				<sch:dataSrc></sch:dataSrc>
				<sch:attrName>meterParamNumQoCRegister</sch:attrName>
				<sch:dataType>string</sch:dataType>
				<sch:descText>Meter param to identify 9 Vs 12 QoC registers </sch:descText>
				<sch:active>Y</sch:active>
				<sch:defaultValue>numQoCRegisters</sch:defaultValue>
			</sch:entityFlexAttrDef>
			<sch:entityFlexAttrDef>
				<sch:dataSrc></sch:dataSrc>
				<sch:attrName>sdpParamMaxPhaseAngle</sch:attrName>
				<sch:dataType>string</sch:dataType>
				<sch:descText>sdp param can have value of either 360 or 180 to represent the maximum value recorded by the meter</sch:descText>
				<sch:active>Y</sch:active>
				<sch:defaultValue>maxPhaseAngle</sch:defaultValue>
			</sch:entityFlexAttrDef>
			<sch:entityFlexAttrDef>
				<sch:dataSrc></sch:dataSrc>
				<sch:attrName>minuteDataSetWindow</sch:attrName>
				<sch:dataType>string</sch:dataType>
				<sch:descText>QoC data set window in minutes</sch:descText>
				<sch:active>Y</sch:active>
				<sch:defaultValue>5</sch:defaultValue>
			</sch:entityFlexAttrDef>

QoCDataStore
bean://			
a301-qocRRDataStore			
a301-allQoCRRDataStore
a301-currentQoCRRDataStore
a301-currentPhAngleQoCRRDataStore

QoCFeature
a301-tplv



There will be a meter parameter that determines the 9 Vs 12 QoC registers.
The QoC data store will create a QoC dataset and cache it in RPContext as other dataStores like LP and RR.

Assumptions
All mandatory 9-resister QoC reads are required to create the QoC dataSets. Discard processing if any 1 is not available i.e va, vab, vc, ia, ib, ic, pia, pib, pic should be present.
The dataStore will consider 9 as default value for meter param numQoCRegisters.
The dataStore will consider 5 as default minuteDataSetWindow .
The dataStore wil consider 360 as default sdpParamMaxPhaseAngle.
A single read value can be part of two overlapping QoCDataSets.
There will be only one configurable minuteDataSetWindow and wil be considered for all:
	Meter change.
	Meter param change(numQoCRegisters).


put example of each component .. how .. on docs
eg one of Implementation 
dataSetWindowInMinute


CLDALYT-2234

--minBckward
-

minuteBackwards
durationsBackward
analysisStartDurationsBackward
analysisEndDurationsBackward

//String lpmin = RPUtil.getAttributeValue(dataStoreConfig, RPConstants.MINUTE_BACKWARDS, RPConstants.LP_MinuteBackwards, null);;
//UsageDateRange udr = RPUtil.getUsageDateRange(ctx, dataStoreConfig, lpmin);



 <con:property>
                        <con:name>jobs.maxAllowedWaitTimeInQueue</con:name>
                        <con:description>Job framework property used by job status monitor the value of which specifies maximum allowed duration to keep a job execution request in JMS queue. </con:description>
                        <con:introducedInVersion>8.1.1</con:introducedInVersion>
                        <con:defaultValue>
                                <con:value>1H</con:value>
                        </con:defaultValue>
                        <con:dataTypeName>Duration</con:dataTypeName>
                        <con:visibility>S</con:visibility>
                </con:property>

<sch:entityFlexAttrDef>
				<sch:dataSrc></sch:dataSrc>
				<!-- TODO add new property for using duration compatible java object 
				1D 2M... 3 more properties-->
				<sch:attrName>minuteBackwards</sch:attrName>
				<sch:dataType>string</sch:dataType>
				<sch:descText>Pre Fetch MinuteBackwards For LP</sch:descText>
				<sch:active>Y</sch:active>
				<sch:defaultValue>14400</sch:defaultValue>
			</sch:entityFlexAttrDef>                
string of the format [number]D [number]H [number]M [number]S        

<!-- TODO add new property for using duration compatible java object 
				1D 2M... 3 more properties-->
				
else{
			logger.error(dataSvcConfiguration.getName() + " Error, no dataSvcGenConf attribute set for: " + RPConstants.DURATION_BACKWARDS_ANALYSIS_END);
			throw new RevenueProtectionException(dataSvcConfiguration.getName() + " Error, no dataSvcGenConf attribute set for: " + RPConstants.DURATION_BACKWARDS_ANALYSIS_END);
		}
		
		
		Instant ins = Instant.ofEpochMilli(epocMil);
		ins = ins.plus(Duration.ofDays(days));
		end = ins.toEpochMilli();
		
		1303151400000
		1304015400000
		
		
		
		
		
String lpmin = RPUtil.getAttributeValue(dataSvcConfiguration, RPConstants.MINUTE_BACKWARDS, RPConstants.LP_MinuteBackwards, null);		

			<sch:entityFlexAttrDef>
				<sch:dataSrc></sch:dataSrc>
				<sch:attrName>durationsBackward</sch:attrName>
				<sch:dataType>Duration</sch:dataType><!--com.emeter.hydrofw.util.Duration-->
				<sch:descText>Pre Fetch durations backward For LP. String of the format [number]D [number]H [number]M [number]S</sch:descText>
				<sch:active>Y</sch:active>
				<sch:defaultValue>10D</sch:defaultValue>
			</sch:entityFlexAttrDef>
			<sch:entityFlexAttrDef>
				<sch:dataSrc></sch:dataSrc>
				<sch:attrName>analysisStartDurationsBackward</sch:attrName>
				<sch:dataType>Duration</sch:dataType><!--com.emeter.hydrofw.util.Duration-->
				<sch:descText>Duration backwards to fetch start date. String of the format [number]D [number]H [number]M [number]S</sch:descText>
				<sch:active>Y</sch:active>
				<sch:defaultValue>15D</sch:defaultValue>
			</sch:entityFlexAttrDef>
			<sch:entityFlexAttrDef>
				<sch:dataSrc></sch:dataSrc>
				<sch:attrName>analysisEndDurationsBackward</sch:attrName>
				<sch:dataType>Duration</sch:dataType><!--com.emeter.hydrofw.util.Duration-->
				<sch:descText>Duration to go back to fetch end date. String of the format [number]D [number]H [number]M [number]S</sch:descText>
				<sch:active>Y</sch:active>
				<sch:defaultValue>0D</sch:defaultValue>
			</sch:entityFlexAttrDef>
			
			
			durationsBackward
			
CLDALYT-2234 			

As part of HBase key changes in RP2.3 we need to update em-bt-schema for adding an entiity, .proto and generated java files.

Target: Delivery - Design and deliver market leading products

Met all major project milestones as set from time to time from release management and Met release criteria for all major milestones as specified separately for different project releases.

What 

Have successfully released 
•RevenueProtection 2.0 and 2.1
•Analytics Data extractor 1.0
•Meter data loader Jobs: developed Interval, DeviceEvent and RegisterRead loader jobs to Hadoop Common project
•Transactional data extractor (8x)
•Transactional data extractor (7x) : Done for KCBPU data load.
•Currently working on RP2.3 design 

How 
•Enhanced Big data technologies skills and implemented them in the project (Hadoop, Spark, HBase, Hive, Oozie etc). 
•Did POC on Spark Machine learning (MLLib ).
•Ported RP1.0 R machine learning scorer into Spark's MLLib scorer.
•Part of the Revenue Protection design team. 
•The RP2.1 design was appreciated by the Architects 
•Helped development and the QE team in successful completion of above releases
•As and when required I have given extra efforts on weekends and late night 
•Met all major milestones and never missed any.
•Have improved and enhanced RP as an engine which can now help other applications to build e.g. Water leak analysis.
•Shared knowledge of Hadoop and Spark in the team.
•Now creating design for RP2.3

Target: Customers - Support existing end customers, with good customer satisfaction and make key new end customers / projects successful (within your control)

What

We have new clients for Analytics. 

How 

We have developed Revenue Protection and Extractor-Loader applications which has enhanced Analytics portfolio and in turn helped in winning more clients for Analytics.

2.0 & 2.1 
BigData learned and delivered... MLLib


Appreciate Team member as .. to make them feel motivated..

Build complex algorithms ... 
machine Learning ... 

Leadership ..Team members
Add shambhu's exit and handled team.

make how common for all whats or targets


Personal perspective
Interpersonal develpment .. looking for opprtunity to work with US team and Architects..
To understand end to end stack of Hadoop and big Data...need to work with US team ...

Need to add about .. Looking for more responsibility and complete ownership of Analytics applications.

Expecting more trainings/workshops in Bigdata to handle complex business usecases.


-------------
Target: Delivery - Design and deliver market leading products

Met all major project milestones as set from time to time from release management and Met release criteria for all major milestones as specified separately for different project releases.

What

Have successfully released
•RevenueProtection 2.0(January, 2016) and 2.1(March, 2016) with zero open issues and on time.
•Analytics Data extractor 1.0 (April, 2016) with zero open issues and on time.
•Meter data loader Jobs: developed Interval, DeviceEvent and RegisterRead loader jobs to Hadoop Common project(June, 2016)
•Transactional data extractor (8x): released on April, 2016 with zero open issues and on time.
•Transactional data extractor (7x): Internal project and have done it for KCBPU data load(July, 2016). 



Target: Customers - Support existing end customers, with good customer satisfaction and make key new end customers / projects successful (within your control)

What
We have new clients for Analytics.




How
•Enhanced Big data technologies skills and implemented them in the project (Hadoop, Spark, HBase, Hive, Oozie etc).
•Did POC on Spark Machine learning (MLLib ).
•Ported RP1.0 R machine learning scorer into Spark's MLLib scorer.
•Part of the Revenue Protection design team.
•The RP2.1 design was appreciated by the Architects 
•Helped development and the QE team in successful completion of above releases
•As and when required I have given extra efforts on weekends and late night
•Met all major milestones and never missed any.
•Have improved and enhanced RP as an engine which can now help other applications to build e.g. Water leak analysis.
•Shared knowledge of Hadoop and Spark in the team.
•Now creating design for RP2.3

Behaviors:

I have handled the team when a senior member left the team. No impact on upcoming releases.

collaboration: 
Have provided training to Omnetric team on Extarction and load tools
 



We have developed Revenue Protection and Extractor-Loader applications which has enhanced Analytics portfolio and in turn helped in winning more clients for Analytics.

 
Personal perspective:
Ready for new roles and responsibility and looking forward for it positively.
Need more trainings/workshops in Bigdata to understand end-end stack which would help me to handle complex business usecases.




-------------


Behaviors 
 Respect 
 §Treat everyone with dignity and respect irrespective of hierarchy and role
§Promote work-life balance
§Listen more, talk less!
§Respect other’s time
§Deliver what is promised  
Focus 
 §Focus on the Forest not on Tree
§First things first!
§Meeting customer needs to foster company success
§Continuous improvement is our journey
§Focus on business goals and result orientation
§Drive innovation based on Siemens strategy 
Initiative & execution 
 §Be proactive and self-driven
§Act today with future orientation
§Always walk the extra mile
§See solutions, not only problems
§Start finishing and stop starting 

People

orientation 
 Empowerment

& trust 
 §Empower people to make decisions and take responsibility for the results
§Stop the line is everybody’s right and duty
§Promote development and continuous learning
§Mentor and coach
§Trust in capabilities and the competencies; avoid micromanagement. 
Honesty, openness

& collaboration 
 §Promote an open culture where views are expressed without fear
§Ensure transparency in every transaction
§Give and receive constructive feedback
§Create an environment where mistakes are accepted and openly discussed
§Result orientation through collaboration (internal/external)  

chandresh pamar
***************************************************************************************

Hi Chandresh,

Please find my inline comments below.

Also I would like you refer RP features descriptions on following WIKI page, this will help you in getting better understanding. Only refer to feature description as rest of Tech Spec is obsolete for RP2.x 
•	https://wiki.emeter.com/display/ENG/Revenue+Protection+Tech+Spec
Note: There feature definitions are for internal usage only and they are not available on Docs. So use them accordingly.

The RP2.x ported the existing features from RP1.0, hence there detailed description is only present in RP 1.0 Wiki page. 

Sample extracted files: Please find attached structural and transactional data files:

•	StructuralDataExtract: <fileName> | supporting Docs page

•	Transactional data extract: 
o	DeviceEvent: <fileName> | supporting Docs page
o	IntervalRead: <fileName> | supporting Docs page
o	RegisterRead: <fileName> | supporting Docs page

Loader documentation: https://docs.emeter.com/display/HC10/Data+Loaders

Hope this will help you. Let me know for any more information


Regards
Ravi Gupta

From: Parmar, Chandresh (EM DG SWS SP EN-GA AMS) 
Sent: Friday, August 26, 2016 1:43 AM
To: Gupta, Ravi (CT DD DS AA EM DG SWS PH-AP DAT); Verma, Jitendra Bahadur (CT DD DS AA EM DG SWS PH-AP DAT)
Cc: Parmar, Chandresh (EM DG SWS SP EN-GA AMS); Harsh, Robert (EM DG SWS SP EN-GA AMS)
Subject: FW: Revenue Protection Algorithm Clarification

Hi Ravi,

Were you able to get the sample extract data files (Structure and Transactional) from QA?

Would like to see what exactly is extracted in file, also if you can email the process/application which does the extract will help a lot.

In addition I have some queries from Rob, who is a SA on the project, if you can add information where you can will be greatly appreciated.

I will be online around 10.00 PM PST today my time which is around 10:30 Am Friday morning for you, we can talk if need be.

Thanks,
Chandresh Parmar.
Solutions Engineer/Sr. Data Analyst - Delivery
Regional Headquarters Americas
Siemens | Smart Grid | Applications Solutions 
 Please consider the environment before printing this email 
This communication (including any attachments) may contain privileged or confidential information intended for a specific individual and purpose, and is protected by law.  If you are not the intended recipient, you should delete this communication and/or shred the materials and any attachments.   Any disclosure, copying, or distribution of this message, or the taking of any action based on it, without the express permission of the originator, is strictly prohibited.

From: Harsh, Robert (EM DG SWS SP EN-GA AMS) 
Sent: Thursday, August 25, 2016 11:51 AM
To: Parmar, Chandresh (EM DG SWS SP EN-GA AMS)
Subject: Revenue Protection Algorithm Clarification

Chandresh –

These are the current descriptions I have for the core interval data algorithms for Revenue Protection:

Other core interval data algorithms are:
1.	Tamper with usage drop to zero – usage is zero after a reported meter tamper event.  By default, 12 consecutive intervals that have zero consumption is considered zero usage.  The number is configurable.
[Dev] TWUDZ has following configurations: 
•	THRESHOLD_USAGE: Threshold value of usage where values less than or equal are considered 0.
•	THRESHOLD_SUM_INTERVAL_MINUTES_PRIOR:  number of minutes to go back from a tamper event to investigate INTERVALS to determine if there was usage prior to the tamper event
•	THRESHOLD_SUM_USAGE_PRIOR:  sum of usage during the window prior to a tamper event that must be greater than or equal to be considered
•	THRESHOLD_SUM_INTERVAL_MINUTES_AFTER:  number of minutes after a tamper event to investigate INTERVALs to determine if there as a usage drop
•	THRESHOLD_SUM_USAGE_AFTER:  sum of usage during the window after a tamper event that must be less than or equal to be considered
It’s not 12 consecutive but feature evaluates THRESHOLD_SUM_INTERVAL_MINUTES_PRIOR of tamperEvent time and THRESHOLD_SUM_INTERVAL_MINUTES_AFTER
Of tamperEvent time. Then compares usage against respective configurable THRESHOLD_SUM_USAGE_PRIOR and THRESHOLD_SUM_USAGE_AFTER.

2.	Reverse rotation correlated with usage and time of day. Reverse Rotation is a meter event that typically results from distributed energy being put back into the grid, resulting in the meter rotating backward. The algorithm looks for reverse rotations during time periods where production is not expected.  For example, solar production is not expected during the night time.  The times for expected production are configurable.   For SDPs which do not have production, or where production is not logically time dependent, this algorithm would not be used.  The time periods for day and night are configurable, but are not seasonal.
[Dev] Yes, where production is not logically time dependent, this algorithm should not be used as it uses only configurable day and night time periods.
There are no seasonal configurations only day and night time configurations.
e.g.
•	DAY_HOUR_START: First hour of the day.
•	DAY_HOUR_END: Last hour of the day.
•	….
So day time is between DAY_HOUR_START and DAY_HOUR_END and night time is before DAY_HOUR_START and after DAY_HOUR_END.
3.	Night time bypass. Experience has shown that many cases of consumption anomalies are characterized by bypasses that occur at night in order to mask abnormally high consumption at night.  This algorithm looks for SDPs with total usage for a day during the nighttime period is <= an absolute threshold and with total usage for a day during the daytime period is >= an absolute threshold.  If the maximum usage during the daytime period divided by the average usage during the daytime period is > 1.5, then continue.  For these SDPs, check for a tamper event within the prior 7 days.  If found, then the algorithm fails.  Definitions for the time period of day and night and the absolute thresholds are configurable.
[Dev]
4.	Estimated intervals greater than threshold.  A high number of estimated intervals is also a signature of consumption anomaly. The default threshold is set to 50% intervals estimated in the past 30 days. Both the percentage and number of days are configurable.
5.	Load side voltage and zero usage after disconnect – consumption after a disconnect meter event is zero, but Load Side Voltage meter events are present during this time.
6.	Intervals with no active account – this algorithm looks at individual intervals showing consumption where there is no active account associated with the SDP.
7.	Estimated usage greater than threshold.  Whereas #4 focuses on the number of intervals, this algorithm focuses on the actual usage itself.  Since there could be intervals with zero consumption, this algorithm seeks to catch an amount of kWh that is estimated.  The default configuration is set to 50% of the kWh estimated in the course of a 30 day period.  Both the percentage and number of days are configurable.
8.	SDP Low Voltage – Voltages are expected to be in the nominal range for every SDP.  When a meter is bypassed, especially using a jumper, it will result in the meter showing voltage that is lower than nominal and a usage that is either zero or extremely low.  The algorithm evaluates voltage readings and usage, both from interval data, to flag incidents of consumption anomalies.  This algorithm considers only single phase voltage readings, not three phase voltage readings.
9.	Usage patterns unlike similar SDPs.  Similar SDPs will be grouped by the postal code or industrial classification.  Only one SDP grouping method for similar SDPs, by postal code or industrial classification, can be supplied per Revenue Protection Service.
10.	Usage with no active account – There is no active account associated with the service point, but there is consumption (the aggregated interval usage) at the service point. This is similar to the algorithm in number 6. The difference between the two algorithms is that one looks at intervals and this one looks at usage.  Both pieces of data are used for tracking zero usage to detect cases where the number of intervals may be fewer but the usage that happens during that time is quite high and also where usage may be low but happening over many intervals.  The accuracy of the overall model increases when looking at both signatures.  
11.	No usage with active account – There is an active account associated with the service point, but there is no consumption at the service point for a configurable number of days.

EDP is looking for further detailed clarification on the logic to better understand the exact conditions the algorithm is looking for.

Thanks.

- Rob 

Rob Harsh
Senior Solutions Architect
rob.harsh@siemens.com
+1.913.553.8544 (mobile)
eMeter, A Siemens Business
4000 E 3rd Avenue, Foster City, CA  94404

 

 

//QA/testcases/rel_8.2/TestCases/uaa-meterreads/UAA_ImportAdapter_Test_Cases.xlsx

29/4/2011
AnalysisDate | endDate
1304060400000

StartDate | 19/4
1303196400000

AnchorMapVA
1303200000000::1304146800000

ActiveMeterRel
1303196400000







 

 









 

 




Feature analysis data will be stored as strings instead of binary
Not implemented as this would require HC2.0 and EIP8.4



Attribute changes
Impact testing for QA

PreFetch flow RP2.1
 
Feature execution flow RP2.1
 
DataStore changes from RP2.1 & RP2.2 to RP2.3
The Pre-fetch data store and features have tight coupling as each feature knows what data it needs for execution and pre-fetch data store provides that data to features.
The pre-fetch dataStore loads the configured data and cache it. The feature request the dataStore, using an identifier, for the data.
The pre-fetch dataStore has mainly 2 types of APIs
Load(): Used to load configured data from HBase
UtilityAPIs: Utility APIs are used to provide sub-range of loaded data. Some transformation on loaded data etc.
S.No.
DataStoreName
RP2.1
Attribute name
RP2.3
Attribute name
Comment
1	LPI	Pre_Fetch_Meastype_UDC_ID_List_LP	dataIdentifierList	
Only the name of attribute is changed. Each dataStore will continue using the updated attribute name as earlier.
2	RR	Pre_Fetch_Meastype_UDC_ID_List_RR	dataIdentifierList	Only the name of attribute is changed. Each dataStore will continue using the updated attribute name as earlier.
3	DE	PreFetchEventTypeList	dataIdentifierList	Only the name of attribute is changed. Each dataStore will continue using the updated attribute name as earlier.


RP2.2	

1	LPI_AP	Pre_Fetch_Meastype_UDC_ID_List_LP	dataIdentifierList	Only the name of attribute is changed. Each dataStore will continue using the updated attribute name as earlier.

Impact:
Java
RPConstants: reference is used in all places.
XML
Resources
/resources/org/common/refData/eip/revenueprotection/EntityDefReferenceData_RPDataStores.xml
/resources/org/Sample/refData/eip/revenueprotection/RPAnalysisDataServiceVersion.xml
Sample (sample data for configuring sample feature etc)
trunk/sample/resources/org/common/refData/eip/revenueprotection/EntityDefReferenceData_RPDataStores.xml
Test cases
XML and Java
Feature changes from RP2.1 to RP2.3
The Features will now have an attribute as PreFetchDataSetName. Each feature will pass this to dataStore to fetch its respective dataSet.
S.No.
RP2.1 Feature name
RP2.1 Feature attribute name
RP2.3 Feature attribute name
Comments
1	All 2.1 features	measTypeUDCId	PreFetchDataSetName	Only the name of attribute is changed. Each feature will continue using the updated attribute name as earlier.
Impact:
Java
RPConstants: reference is used in all places.
XML
Resources
/resources/org/common/refData/eip/revenueprotection/EntityDefReferenceData_RPFeatures.xml
/resources/org/Sample/refData/eip/revenueprotection/RPAnalysisDataServiceVersion.xml
Sample (sample data for configuring sample feature etc)
trunk/sample/resources/org/common/refData/eip/revenueprotection/EntityDefReferenceData_RPFeatures.xml
Test cases
XML and Java
Note: RP2.2 already uses a dataSetName to fetch data from dataStores. So no changes are required.
Hive key changes
Hive data key changes are already done as part of RP2.2 (water analysis).
As discussed with team, changing key names will require changes in reports as well. Hence we'll keep the name from RP2.2

S.No.
KeyName in RP2.2
KeyName in RP2.3
Proposed KeyName in RP2.3
1	analysisdate	analysisdate	analysisdate (no change)
2	datasvcname	datasvcname	datasvclabel
3	segment	segment	segmentlabel
Component Impact:
em-revenueprotection
HBaseHelper and RPConstants for reference update.
HBase key changes
Feature analysis data is saved in HBase. A new schema of rpresult2 is updated with changes , however for backward compatibility we still keep the old rpresult schema.
Feature analysis data will be stored as strings instead of binary
Not implemented as this would require HC2.0 and EIP8.4
Feature analysis data is now saved in HBase with updated keys. Following keys are added:
analysisDate
dataSvcLabel
segmentLabel
rpresult2 schema

<com.emeter.bt.entitydef.schema.EntitySchema>
  <entityName>rpresult2</entityName>
  <tableName>rpresult2</tableName>
  <rowKey>
    <string>svcPtId</string>
  </rowKey>
  <columns>
    <com.emeter.bt.entitydef.schema.ColumnType>
      <columnFamily>r</columnFamily>
      <identifier>1</identifier>
      <columnTypeCode>svcPtId</columnTypeCode>
      <cellDataType>int64</cellDataType>
    </com.emeter.bt.entitydef.schema.ColumnType>
    <com.emeter.bt.entitydef.schema.ColumnType>
      <columnFamily>r</columnFamily>
      <identifier>2</identifier>
      <columnTypeCode>rpf</columnTypeCode>
      <multidimensional>true</multidimensional>
      <cellDataType>com.emeter.bt.rp.RPAnalysisData$RPFeatureAnalysis2</cellDataType>
      <columnName>
        <com.emeter.bt.entitydef.schema.ColumnNameElement>
          <identifier>1</identifier>
          <type>String</type>
          <name>segmentLabel</name>
        </com.emeter.bt.entitydef.schema.ColumnNameElement>
        <com.emeter.bt.entitydef.schema.ColumnNameElement>
          <identifier>2</identifier>
          <type>String</type>
          <name>dataSvcLabel</name>
        </com.emeter.bt.entitydef.schema.ColumnNameElement>
        <com.emeter.bt.entitydef.schema.ColumnNameElement>
          <identifier>3</identifier>
          <type>int64</type>
          <name>analysisDate</name>
        </com.emeter.bt.entitydef.schema.ColumnNameElement>
      </columnName>
    </com.emeter.bt.entitydef.schema.ColumnType>
    <com.emeter.bt.entitydef.schema.ColumnType>
      <columnFamily>r</columnFamily>
      <identifier>3</identifier>
      <columnTypeCode>rps</columnTypeCode>
      <multidimensional>true</multidimensional>
      <cellDataType>com.emeter.bt.rp.RPAnalysisData$RPScore2</cellDataType>
      <columnName>
        <com.emeter.bt.entitydef.schema.ColumnNameElement>
          <identifier>1</identifier>
          <type>String</type>
          <name>segmentLabel</name>
        </com.emeter.bt.entitydef.schema.ColumnNameElement>
        <com.emeter.bt.entitydef.schema.ColumnNameElement>
          <identifier>2</identifier>
          <type>String</type>
          <name>dataSvcLabel</name>
        </com.emeter.bt.entitydef.schema.ColumnNameElement>
        <com.emeter.bt.entitydef.schema.ColumnNameElement>
          <identifier>3</identifier>
          <type>int64</type>
          <name>analysisDate</name>
        </com.emeter.bt.entitydef.schema.ColumnNameElement>
      </columnName>
    </com.emeter.bt.entitydef.schema.ColumnType>
   </columns>
</com.emeter.bt.entitydef.schema.EntitySchema>

Component Impact:
em-revenueprotection
HBaseHelper and RPConstants for reference update as HBase table name in now rpresult2 (older table is reatined for backward compatibility).
AnalysisDate is save in SR’s DataRefEndTime
DataSVcName & SegmentName is now SR attribute name as:
dataSvcLabel
segmentLabel
em-revenueprotection-UI: Now fetch HBase records using AnalysisDate, DataSvcName and SegmentName. These fileds will now be part of SR.
em-bt-schema: entity schema change for rpresult2.xml and its respective .proto file
What QE need to focus

--------------------------
generate javadocs

<!-- Creates the documentation files - - - - - - - - - - - - - - - - - -  -->
	<target name="cleanDoc">
	      <delete dir="${doc.dir}" includeEmptyDirs="true" />
	</target>
    <target name="doc" depends="init, cleanDoc">
      <javadoc sourcepath="${src.dir}"
	       destdir="${doc.dir}"
	       windowtitle="JavaDoc for ${project.name}"
	       doctitle="JavaDoc for ${project.name}"
		   packagenames="com.emeter.revenueprotection.common.exception.uiexception,com.emeter.revenueprotection.common,com.emeter.revenueprotection.common.exception,com.emeter.revenueprotection.data.context,com.emeter.revenueprotection.investigationfilters,com.emeter.revenueprotection.prefetch.datastore.helper,com.emeter.revenueprotection.dao,com.emeter.revenueprotection.common.caching,com.emeter.revenueprotection.scorer,com.emeter.revenueprotection.prefetch.managers,com.emeter.revenueprotection.data,com.emeter.revenueprotection.rs.interfaces"
      	   classpath="../lib/jars/cxf-rt-rs-service-description-3.0.3.jar;../lib/jars/cxf-rt-databinding-aegis-3.0.3.jar;../lib/jars/cxf-rt-transports-http-3.1.3.jar;../lib/jars/cxf-rt-bindings-soap-3.0.3.jar;../lib/jars/cxf-tools-wsdlto-frontend-jaxws-3.0.3.jar;../lib/jars/cxf-rt-rs-client-3.0.3.jar;../lib/jars/cxf-core-3.0.3.jar;../lib/jars/cxf-tools-validator-3.0.3.jar;../lib/jars/cxf-rt-transports-http-3.0.3.jar;"
      >
      <fileset dir="${src.dir}/" includes="**/*.java"/>
      </javadoc>
   </target>
	<!--
	classpathref="compile.classpath"
	packagenames="com.emeter.revenueprotection.common.exception.uiexception,com.emeter.revenueprotection.common,com.emeter.revenueprotection.common.exception,com.emeter.revenueprotection.data.context,com.emeter.revenueprotection.investigationfilters,com.emeter.revenueprotection.prefetch.datastore.helper,com.emeter.revenueprotection.dao,com.emeter.revenueprotection.common.caching,com.emeter.revenueprotection.scorer,com.emeter.revenueprotection.prefetch.managers,com.emeter.revenueprotection.data,com.emeter.revenueprotection.rs.interfaces"
	-->

---------

<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<project default="javadoc">
    <target name="javadoc">
        <javadoc access="protected" author="true" classpath="../lib/jars/em-commonsvc.jar;../lib/jars/neethi-2.0.4.jar;../lib/jars/geronimo-jms_1.1_spec-1.1.1.jar;../lib/jars/em-bt-schema-utils.jar;../lib/jars/xbean-2.5.jar;../lib/jars/spring-security-web-3.1.0.jar;../lib/jars/cxf-rt-rs-service-description-3.0.3.jar;../lib/jars/transactions-api-3.8.0.jar;../lib/jars/apache-jsp-9.3.3.jar;../lib/jars/aspectjweaver-1.8.1.jar;../lib/jars/spark-assembly_2.10-1.3.0-cdh5.4.4.jar;../lib/jars/commons-io-1.4.jar;../lib/jars/hadoop-hdfs-2.6.0-cdh5.4.4.jar;../lib/jars/commons-httpclient-3.1.jar;../lib/jars/metrics-core-2.2.0.jar;../lib/jars/cxf-rt-databinding-aegis-3.0.3.jar;../lib/jars/dom4j-1.6.1.jar;../lib/jars/hbase-hadoop-compat-1.0.0-cdh5.4.4.jar;../test/lib/cxf-rt-transports-http-3.1.3.jar;../lib/jars/wsdl4j-qname-1.6.2.jar;../lib/jars/velocity-dvsl-1.0.jar;../lib/jars/dbunit-2.4.8.jar;../lib/jars/commons-dbcp2-2.0.1.jar;../lib/jars/wsdl4j-1.6.2.jar;../lib/jars/hadoop-yarn-server-web-proxy-2.6.0-cdh5.4.4.jar;../lib/jars/voldemort.jar;../lib/jars/javassist-3.18.1-GA.jar;../lib/jars/snappy-java-1.1.2-RC2.jar;../lib/jars/jaxen-core-1.0.jar;../lib/jars/cxf-rt-bindings-soap-3.0.3.jar;../lib/jars/jackson-all-1.8.5.jar;../lib/jars/em-job.jar;../lib/jars/libthrift-0.9.0.jar;../lib/jars/wss4j-policy-2.0.2.jar;../lib/jars/joda-time-2.1.jar;../lib/jars/cxf-tools-wsdlto-frontend-jaxws-3.0.3.jar;../lib/jars/htrace-core-3.1.0-incubating.jar;../lib/jars/derbyclient.jar;../lib/jars/transactions-jta-3.8.0.jar;../lib/jars/jetty-util-6.1.26.cloudera.4.jar;../lib/jars/hadoop-mapreduce-client-app-2.6.0-cdh5.4.4.jar;../lib/jars/easymock-3.1.jar;../lib/jars/TCLIServiceClient.jar;../lib/jars/jxta-2.0.jar;../lib/jars/stax2-api-3.1.4.jar;../lib/jars/jsch-0.1.51.jar;../lib/jars/velocity-tools-generic-1.4.jar;../lib/jars/ant-contrib-1.0b3.jar;../lib/jars/velocity-tools-view-1.4.jar;../lib/jars/netty-3.6.6.Final.jar;../lib/jars/cxf-rt-rs-client-3.0.3.jar;../lib/jars/cxf-core-3.0.3.jar;../lib/jars/jgroups-3.3.1.Final.jar;../lib/jars/hbase-common-1.0.0-cdh5.4.4.jar;../lib/jars/cxf-tools-validator-3.0.3.jar;../lib/jars/eip-commons.jar;../lib/jars/xmlunit-1.3.jar;../lib/jars/spring-security-core-3.1.0.jar;../lib/jars/derby.jar;../lib/jars/em-unit-testingutils.jar;../lib/jars/spark-core_2.10-1.3.0-cdh5.4.4.jar;../lib/jars/velocity-tools-1.4.jar;../lib/jars/commons-collections-3.1.jar;../lib/jars/slf4j-log4j12-1.7.5.jar;../lib/jars/commons-lang-2.4.jar;../lib/jars/hadoop-client-2.6.0-cdh5.4.4.jar;../lib/jars/slf4j-api-1.7.5.jar;../lib/jars/cxf-rt-transports-http-3.0.3.jar;../lib/jars/javax.servlet-api-3.1.0.jar;../lib/jars/enttoolkit-7.1.jar;../lib/jars/jalopy-1.5.jar;../lib/jars/wss4j-ws-security-common-2.0.2.jar;../lib/jars/spring-beans-4.1.2.RELEASE.jar;../lib/jars/parquet-hadoop-bundle-1.5.0-cdh5.4.4.jar;../lib/jars/jettison-1.3.jar;../lib/jars/javax.ws.rs-api-2.0.1.jar;../lib/jars/asm-5.0.1.jar;../lib/jars/xml-resolver-1.2.jar;../lib/jars/org.eclipse.jdt.core-3.8.2.jar;../lib/jars/jsr173-1.0.jar;../lib/jars/validation-api-1.0.0.jar;../lib/jars/xmlschema-core-2.1.0.jar;../lib/jars/jetty-all-9.3.3.jar;../lib/jars/commons-logging-1.1.1.jar;../lib/jars/hadoop-yarn-client-2.6.0-cdh5.4.4.jar;../lib/jars/xbean-spring-3.6.jar;../lib/jars/junit-addons-1.4.jar;../lib/jars/jetty-6.1.26.cloudera.4.jar;../lib/jars/cxf-tools-wsdlto-databinding-jaxb-3.0.3.jar;../lib/jars/wss4j-ws-security-stax-2.0.2.jar;../lib/jars/cglib-nodep-2.2.3.jar;../lib/jars/disruptor-3.3.0.jar;../lib/ajit/hamcrest-library-1.3.jar;../lib/jars/protobuf-java-2.5.0.jar;../lib/jars/objenesis-1.2.jar;../lib/jars/em-lookupdata.jar;../lib/jars/spring-aop-4.1.2.RELEASE.jar;../lib/jars/cxf-rt-frontend-simple-3.0.3.jar;../lib/jars/transactions-3.8.0.jar;../lib/jars/spring-security-cas-3.1.0.jar;../lib/jars/xpp3-1.1.3.3.jar;../lib/jars/cxf-rt-ws-security-3.0.3.jar;../lib/jars/velocity-1.4.jar;../lib/jars/aspectjrt-1.8.1.jar;../lib/jars/commons-configuration-1.6.jar;../lib/jars/hbase-protocol-1.0.0-cdh5.4.4.jar;../lib/jars/taglibs-standard-impl-1.2.1.jar;../lib/jars/cxf-tools-common-3.0.3.jar;../lib/jars/cxf-rt-rs-extension-providers-3.0.3.jar;../lib/jars/em-visadmin.jar;../lib/jars/quartz-2.2.1.jar;../lib/jars/reflectasm-1.07.jar;../lib/jars/jaxb-impl-2.1.7.jar;../lib/jars/asm-commons-5.0.1.jar;../lib/jars/hadoop-yarn-common-2.6.0-cdh5.4.4.jar;../lib/jars/derbynet.jar;../lib/ajit/hamcrest-core-1.3.jar;../lib/jars/activemq-all-5.8.0.jar;../lib/jars/minlog-1.2.jar;../test/lib/cxf-rt-transports-http-jetty-3.1.3.jar;../lib/jars/jaxb-xjc-2.1.7.jar;../lib/jars/jetty-util-9.3.3.jar;../lib/jars/ql.jar;../lib/httpmime-4.5.1.jar;../lib/jars/xmlsec-2.0.2.jar;../lib/jars/wss4j-bindings-2.0.2.jar;../lib/jars/htrace-core-3.0.4.jar;../lib/jars/hive_metastore.jar;../lib/jars/spring-jdbc-4.1.2.RELEASE.jar;../lib/jars/commons-vfs2-2.0.jar;../lib/jars/spring-core-4.1.2.RELEASE.jar;../lib/jars/guava-15.0.jar;../lib/jars/ehcache-2.9.0.jar;../lib/jars/commons-pool2-2.2.jar;../lib/jars/hbase-client-1.0.0-cdh5.4.4.jar;../lib/jars/spring-orm-4.1.2.RELEASE.jar;../lib/jars/snakeyaml-1.7.jar;../lib/jars/jcifs-0.8.3.jar;../lib/jars/ojdbc7.jar;../lib/jars/cxf-rt-frontend-jaxrs-3.0.3.jar;../lib/jars/hadoop-yarn-api-2.6.0-cdh5.4.4.jar;../lib/jars/hbase-server-1.0.0-cdh5.4.4.jar;../lib/jars/commons-codec-1.3.jar;../lib/jars/cxf-tools-wsdlto-core-3.0.3.jar;../lib/jars/spring-web-4.1.2.RELEASE.jar;../lib/jars/xstream-1.4.3.jar;../lib/jars/wss4j-ws-security-policy-stax-2.0.2.jar;../lib/jars/hive_service.jar;../lib/jars/em-core.jar;../lib/jars/aopalliance-1.0.jar;../lib/jars/mockito-all-1.9.5.jar;../lib/jars/hibernate-3.6.3.jar;../lib/jars/cxf-rt-wsdl-3.0.3.jar;../lib/jars/woodstox-core-asl-4.4.1.jar;../lib/jars/hibernate-jpa-1.0.0.jar;../lib/jars/HiveJDBC41.jar;../lib/jars/asm-4.0.jar;../lib/jars/transactions-jdbc-3.8.0.jar;../lib/jars/javax.annotation-api-1.2.jar;../lib/jars/em-hadoop-utils.jar;../lib/jars/a2f.jar;../lib/jars/slf4j-simple-1.6.1.jar;../lib/jars/spring-test-4.1.2.RELEASE.jar;../lib/jars/commons-net-3.3.jar;../lib/jars/taglibs-standard-spec-1.2.1.jar;../lib/jars/commons-beanutils-1.6.jar;../lib/jars/jdom-1.1.jar;../lib/jars/derbytools.jar;../lib/jars/apache-el-8.0.23.M1.jar;../lib/jars/em-bt-schema.jar;../lib/jars/spring-security-config-3.1.0.jar;../lib/jars/geronimo-j2ee-management_1.1_spec-1.0.1.jar;../lib/jars/cxf-rt-databinding-jaxb-3.0.3.jar;../lib/jars/jackson-core-2.2.3.jar;../lib/jars/hbase-hadoop2-compat-1.0.0-cdh5.4.4.jar;../lib/jars/hadoop-common-2.6.0-cdh5.4.4.jar;../lib/jars/log4j-1.2.15.jar;../lib/jars/junit-4.9.jar;../lib/jars/cas-client-core-3.1.10.jar;../lib/jars/xmlpull-1.1.3.1.jar;../lib/jars/em-jettylauncher.jar;../lib/jars/wss4j-ws-security-dom-2.0.2.jar;../lib/jars/apache-jsp-8.0.23.M1.jar;../lib/jars/hadoop-auth-2.6.0-cdh5.4.4.jar;../lib/jars/hibernate-validator-4.2.0.jar;../lib/jars/spring-tx-4.1.2.RELEASE.jar;../lib/jars/em-bo.jar;../lib/jars/spring-jms-4.1.2.RELEASE.jar;../lib/jars/scala-reflect-2.10.4.jar;../lib/jars/kryo-2.20.jar;../lib/jars/libfb303-0.9.0.jar;../lib/jars/high-scale-lib-1.1.1.jar;../lib/jars/jta-1.1.jar;../lib/jars/commons-codec-1.4.jar;../lib/jars/spring-context-4.1.2.RELEASE.jar;../lib/jars/json-simple-1.1.jar;../lib/jars/cxf-rt-databinding-xmlbeans-3.0.3.jar;../lib/jars/jackson-annotations-2.2.3.jar;../lib/jars/zookeeper-3.4.5-cdh5.4.4.jar;../lib/jars/jaxen-dom-1.0.jar;../lib/jars/eidg.jar;../lib/jars/oozie-client-4.1.0-cdh5.4.4.jar;../lib/jars/spring-expression-4.1.2.RELEASE.jar;../lib/jars/hadoop-mapreduce-client-core-2.6.0-cdh5.4.4.jar;../lib/jars/imq-4.4.jar;../lib/jars/ant-1.8.1.jar;../lib/jars/cxf-rt-frontend-jaxws-3.0.3.jar;../lib/jars/spring-context-support-4.1.2.RELEASE.jar;../lib/jars/jackson-databind-2.2.3.jar;../lib/jars/atomikos-util-3.8.0.jar;../lib/jars/saxpath-1.0.jar" destdir="../javadoc" nodeprecated="false" nodeprecatedlist="false" noindex="false" nonavbar="false" notree="false" packagenames="com.emeter.revenueprotection.common.exception.uiexception,com.emeter.revenueprotection.common,com.emeter.revenueprotection.common.exception,com.emeter.revenueprotection.data.context,com.emeter.revenueprotection.investigationfilters,com.emeter.revenueprotection.prefetch.datastore.helper,com.emeter.revenueprotection.dao,com.emeter.revenueprotection.common.caching,com.emeter.revenueprotection.scorer,com.emeter.revenueprotection.prefetch.managers,com.emeter.revenueprotection.data,com.emeter.revenueprotection.rs.interfaces" source="1.8" sourcefiles="../src/com/emeter/revenueprotection/featurecreation/RegisterFailCodes.java,../src/com/emeter/revenueprotection/featurecreation/IRPFeature.java,../src/com/emeter/revenueprotection/prefetch/StructuralDataLookupHelper.java,../src/com/emeter/revenueprotection/prefetch/datastore/DeviceEventsStore.java,../src/com/emeter/revenueprotection/prefetch/datastore/AllQoCRRDataStore.java,../src/com/emeter/revenueprotection/featurecreation/IntervalFailCodes.java,../src/com/emeter/revenueprotection/featurecreation/AbstractFeature.java,../src/com/emeter/revenueprotection/prefetch/datastore/RRDataStore.java,../src/com/emeter/revenueprotection/prefetch/datastore/IDataStore.java,../src/com/emeter/revenueprotection/featurecreation/FeatureManager.java,../src/com/emeter/revenueprotection/featurecreation/ReadsFlags.java,../src/com/emeter/revenueprotection/prefetch/datastore/CurrentQoCRRDataStore.java,../src/com/emeter/revenueprotection/prefetch/datastore/LPDataStoreDecorator.java,../src/com/emeter/revenueprotection/featurecreation/RPUtil.java,../src/com/emeter/revenueprotection/prefetch/datastore/QoCRRDataStore.java,../src/com/emeter/revenueprotection/prefetch/datastore/AbstractDataStore.java,../src/com/emeter/revenueprotection/featurecreation/AbstractFeatureCreation.java,../src/com/emeter/revenueprotection/prefetch/datastore/LPDataStore.java,../src/com/emeter/revenueprotection/prefetch/datastore/CurrentPhAngleQoCRRDataStore.java,../src/com/emeter/revenueprotection/featurecreation/FeatureCreationExecutor.java" sourcepath="../src;../test;../sample" splitindex="true" use="true" version="true"/>
    </target>
</project>


---------------ADFTransactionMonitor---------------------	
Old-Usage
	-Jira: EIP-21470
	-wiki
	-changelist #192635 
		
****************************************		
	//CLDALYT-2393 
	/**Interval read handler for creating 8x interval reads.
	 * <br/>
	 * It creates interval reads CSV compatible with HC1.1 interval loader*/
	private InEventHandler intervalReads8xHandler;
	/**to enable/disable creation of 8x reads*/
	private boolean intervalRead8xHandlerRequired;

		//CLDALYT-2393
		if(intervalRead8xHandlerRequired){
			validate.isNotNull("intervalReads8xHandler", intervalReads8xHandler);
		}


		//CLDALYT-2393
		if(intervalRead8xHandlerRequired){
			intervalReads8xHandler.onEvent(message);
		}
	
*******************************************************
select  sdp_udc_id,
                'ServiceDeliveryPoint',
                udc_id_8x,--measType_udc_id
                NULL,--device_udc_id 
                'Meter',--Device_type
                'lp_data_source' ||'~~'||
                'lp_data_source_detail' ||'~~'||
                /*NULL ||'~~'|| --AMI_REC_NUM
                1 ||'~~'|| -- rec_version_num*/
                last_upd_by
                FROM 
		               (select  /*+ PARALLEL(de, 4) */ 
				        (select sdp.udc_id from tmp_sdp_id_to_udcid sdp where sdp.channel_id = ?) AS sdp_udc_id, --lpi.channel_id
		                mapping.udc_id_8x,
		                (select usr.name from tmp_update_by_to_user_name usr) last_upd_by  
		        from tmp_sdp_id_to_udcid tmp, cx_product_name_mapping mapping
		        where 
		        /*tmp.channel_seq >= $CHANNEL_ID_START and tmp.channel_seq <= $CHANNEL_ID_END and*/ 
		        tmp.channel_type = 'Interval Data'
		        and tmp.channel_id = ?
		        and tmp.prod_name = mapping.product_name_7x
				/*and lpi.UTC_INTERVAL_TIME between to_date('$INT_START_TIME','yyyy-mm-dd hh24:mi:ss')
				and to_date('$INT_END_TIME','yyyy-mm-dd hh24:mi:ss')*/
				)
		WHERE lp_value is not null;

select   (select sdp.udc_id from tmp_event_sdp_id_to_udcid sdp where sdp.id = ?),--sdpRowId
		 1, --event_record_num , 1 expected unique value in 7x but can be duplicate in 8x
		 (select tmp.udc_id from tmp_device_id_to_udcid tmp where tmp.id = ?), --?=deviceRowId (device_udc_id)
		 '"' || (select tmp.type from tmp_device_id_to_udcid tmp where tmp.id = ?) || '"',--deviceType
from 
dual;
		
		
https://jira.emeter.com/browse/ENV-1036		

2014-06-10T22:30:00+00:00

https://docs.emeter.com/display/EIPDE/Installation+of+Data+Extractor

KCBPU:
Masked MUDR database:
SID: kcbpu_m
Host: emdb44
Username/password: emapp/emapp

Masked Siebel database:
SID: kcbpu_s
Host: emdb44
Username/password: siebel/siebel


-------------------------------------------------
77DEV19LNX-IND is ready with latest 7.7 SP5 GA. Please find the details below:

Env name: 77DEV19LNX-IND

 	User Id	Password
Application Servers	 	 
EIP – IND-LNXAPP231	pipe	pipepass
AMI UI – ind-lnxapp232	pipeadmin	Pipeadmin725
	 	 
Database	 	 
Mudr: imudr_75@ind-db13	pipe	Pipe725
Sebl: isebl_75@ind-db13	pipe	Pipe725
		

AMI UI : http://ind-lnxapp232.emeter.com/eip
System Console: http://ind-lnxapp231.emeter.com:7475/EnergyIPSystemConsole
EM-UI: http://ind-lnxapp231.emeter.com:8081/em-ui

---------------------------------------------------------------------------------------
Structural extraction ADE1.1(7.7):
ADE1.1(7) Documentation
	Structural initial/incremental extraction
	Docs installation page : https://docs.emeter.com/display/EIPDE/Installation+of+Structural+Data+Extractor+1.1
	Docs degign page: https://docs.emeter.com/display/EIPDE/Structural+Data+Extraction+Overview
	Wiki page: https://wiki.emeter.com/display/ENG/Analytics+Cloud+Solution


	Structural Loader
	Docs installation page : https://docs.emeter.com/display/HC10/Load+Structural+Data
	Wiki page:

	Transaction initial extractor
	designPage:
	installationPage:
	
	Transaction Loader
	designPage:
	installationPage:
	
	

on target Machine 
	-EIP
		-add premise
		-./orgAdmin.sh -o 007169584 -u 007169584 -f 007169584 -l 007169584 -p 007169584 -c create
		-./orgAdmin.sh -c importRefData -o 007169584
		
		eip@lnxapp26:/home/eip/opt/hadoop-common/resources/org->cp -r Sample 007169584
		eip@lnxapp26:/home/eip/opt/hadoop-common/resources/org->cdb
		
		./ReferenceDataUtil.sh -DimportReferenceDataService.referenceDataSourcePath=/home/eip/opt/hadoop-common -Dapplication.scope=org -Dapplication.orgName=007169584
		
		
	-MUDR
		-source & sourceDetails
			-
			
	<sch:MeasType>
		   <sch:dataSrc>1-1AH7</sch:dataSrc>
		   <sch:name>Received KWH Interval 60 Minutes</sch:name>
		   <sch:uomCategory>Active Energy</sch:uomCategory> 
		   <sch:uom>KWH</sch:uom>
		   <sch:metricMultiplier/>
		   <sch:direction/>
		   <sch:timeAttr/>
		   <sch:dataQualifier/>
		   <sch:accumBehavior/>
		   <sch:uomTier/>
		   <sch:phase/>
		   <sch:descText/>
		   <sch:etouBin/> 
		   <sch:etype>Interval Data</sch:etype> 
		   <sch:eintervalLen/> 
		   <sch:echannelType/> 
		   <sch:echannelNum/>
		   <sch:eapplyCtptType/>
		   <sch:eapplyMultiplierType/>   
		   <sch:udcId>EM.ELECTRIC.I.60.KWH.1.5</sch:udcId>
		   <sch:macroPeriod/>
		   <sch:commodity>Electric</sch:commodity>
		   <sch:consumptionTier/>
		   <sch:cimId/>
		   <sch:cpp>N</sch:cpp>
		</sch:MeasType>		
			
----------------
ons for the application CloudAlytStructuralExtraction
2016-10-04 10:34:07,150 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - APPLICATION SUCCESSFULLY STARTED
2016-10-04 10:34:07,150 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - StructuralExtractApplication Mode:: initial
2016-10-04 10:35:18,393 com.emeter.extractor.runner.seeddata.SeedDataCycleCdExtractRunner [pool-2-thread-2] INFO  - Total Records for cycleCd extraction:29
2016-10-04 10:35:37,801 com.emeter.extractor.runner.seeddata.SeedDataExtractRunner [pool-2-thread-1] INFO  - Total Records for Seed data extraction:718
2016-10-04 12:07:55,363 com.emeter.extractor.runner.assetdata.device.AssetDataDevice [main] INFO  - AssetDataDevice Total Records:414320
2016-10-04 12:07:55,363 com.emeter.extractor.runner.assetdata.route.AssetDataRoute [main] INFO  - AssetDataRoute Total Records:486
2016-10-04 12:07:55,364 com.emeter.extractor.runner.assetdata.account.AssetDataAccount [main] INFO  - AssetDataAccount Total Records:101352
2016-10-04 12:07:55,364 com.emeter.extractor.runner.assetdata.premise.AssetDataPremise [main] INFO  - AssetDataPremise Total Records:73998
2016-10-04 12:07:55,364 com.emeter.extractor.runner.assetdata.distnode.AssetDataDistNode [main] INFO  - AssetDataDistNode Total Records:0
2016-10-04 13:57:18,436 com.emeter.extractor.runner.sdpstackdata.SdpStackData [main] INFO  - SdpStackData stat : 
ServiceDeliveryPoint,241751
TotalChannel,2506201,Channel,2506201,VirtualChannel,0
servicePointServiceAgreementAssociation,182367
servicePointDataServiceAssociation,1578091
servicePointDeviceAssociation,240893
deviceChannelAssociation,3510644
deviceFunctionAssociation,193273
servicePointServicePointGroupAssociation,294167
accountServicePointAssociation,383139
servicePointServicePointAssociation SDP-DN,0
servicePointServicePointAssociation DN-DN,0
2016-10-04 13:57:18,448 com.emeter.extractor.impl.StructuralExtractManager [main] INFO  - creating zip file... 
2016-10-04 14:01:30,089 com.emeter.extractor.application.StructuralExtractApplication [main] INFO  - Done Structural execution in 12442.758155597 sec
2016-10-04 14:01:30,089 com.eMeter.PIPe.hydrofw.application.HydroApplicationContextImpl [main] INFO  - SHUTDOWN

--------------

INSERT INTO state_prov(id, name, iso_code, country_id, alt_name, data_src, org_id, insert_time, insert_by, last_upd_by, last_upd_time, rec_version_num)
VALUES(2257,'Kentucky','US-KY',1310,NULL,NULL,202, SYSDATE, 203,203,SYSDATE,1 );

INSERT INTO state_prov(id, name, iso_code, country_id, alt_name, data_src, org_id, insert_time, insert_by, last_upd_by, last_upd_time, rec_version_num)
VALUES(2258,'CA','US-KY',1310,NULL,NULL,202, SYSDATE, 203,203,SYSDATE,1 );


---

Transactional
-ensure mudr2sebl exists
-create Table and Sequence previledge


Usage: $0 options

OPTIONS:
    -p Use AdfExtract.properties file 
    -h Show all options
    -f . -a \"2000-01-01 00:00:00\" -b \"2012-01-01 00:00:00\" -c \"2000-01-01 00:00:00\" -d \"2012-01-01 00:00:00\" -p
    -f . -c \"2000-01-01 00:00:00\" -d \"2012-01-01 00:00:00\" -p
    -f . -a \"2000-01-01 00:00:00\" -b \"2012-01-01 00:00:00\" -p
    
    where 
      -a defines start time criteria for ${COLUMN_NAME1} (inclusive)
      -b defines end time criteria for ${COLUMN_NAME1} (inclusive)
      -c defines start time criteria for ${COLUMN_NAME2} (inclusive)
      -d defines end time criteria for ${COLUMN_NAME2} (inclusive)
      -f directory name where to place extracted files
      -p skip interactive mode
      
EXAMPLE:$0 
        $0 -p

___________________________________________________________________
"
Old:
		#nohup ./AC_AdfRegisterReadExtract.sh "$INTERVAL_DIRECTORY" "$dayStartRange" "$dayEndRange" "$channelRangeStart" "$channelRangeEnd" > /dev/null &

		###########################################################################

		LOG_EXTENSION=`date +%Y%m%d%H%M%S%N`
		dir=`pwd`
		if [ "${UTILITY_ID}" = "TBD" ]
		then
			my_echo "Invalid UTILITY_ID TBD"
			exit 1
		fi

		LOG_FILE="${LOG_DIRECTORY}/ADF_LP_INTERVAL${UTILITY_ID}.log.$LOG_EXTENSION"
		echo > $LOG_FILE

		export NLS_LANG=AMERICAN_AMERICA.AL32UTF8

		###########################################################################
		
		###########################################################################
		LOG_EXTENSION=`date +%Y%m%d%H%M%S%N`
		dir=`pwd`
		if [ "${UTILITY_ID}" = "TBD" ]
		then
		    my_echo "Invalid UTILITY_ID TBD"
		    exit 1
		fi
		
		LOG_FILE="${LOG_DIRECTORY}/ADF_DEVICE_EVENT${UTILITY_ID}.log.$LOG_EXTENSION"
		echo > $LOG_FILE
		
		export NLS_LANG=AMERICAN_AMERICA.AL32UTF8
		
		###########################################################################

nohup ./AC_AdfMasterExtract.sh rr /home/pipe/RGA/transaction/registerRead/output/2013/jan '2013-01-01' '2013-01-31' & > /dev/null &

./AC_AdfMasterExtract.sh rr /home/pipe/RGA/transaction/registerRead/output/2013/jan '2013-01-01' '2013-01-31'


nohup ./AC_AdfMasterExtract.sh rr /home/pipe/RGA/transaction/2016/mar/rr '2016-03-01' '2016-03-02' > /dev/null &
nohup ./AC_AdfRegisterReadExtract.sh /home/pipe/RGA/transaction/2016/mar/rr "2016-03-02 00:00:00" "2016-03-02 23:59:59" "1" "25000" > /dev/null &


nohup ./AC_AdfMasterExtract.sh rr /home/pipe/RGA/transaction/2016/mar/rr '2016-03-01' '2016-03-02' &
nohup ./AC_AdfMasterExtract.sh de /home/pipe/RGA/transaction/2016/mar/de '2016-03-01' '2016-03-02' &
nohup ./AC_AdfMasterExtract.sh lp /home/pipe/RGA/transaction/2016/mar/lp '2016-03-01' '2016-03-02' &


nohup ./AC_AdfMasterExtract.sh rr /home/pipe/RGA/transaction/2016/mar/test '2016-03-01' '2016-03-02' &

CLDALYT-2576

/*Length of the interval in seconds.*/
public static final int intervalLen_5Mins = 300; //5 min interval and 288 reads in a day
public static final int intervalLen_15Mins = 900; //15 min interval and 96 reads in a day
public static final int intervalLen_60Mins = 3600; //60 min interval and 24 reads in a day


ADE - Doc
https://docs.emeter.com/display/public/WELCOME/eMeter+Documentation

Chandresh, parmar EDP env 
lnxapp06.emeter.com
mudr8306/mudr8306
mudrsid is set in the properties file
I was trying to extract data from 2015-09-01 00:00:00 to 2015-09-30 23:59


No Usage  for X Days algorithm  
Can we consider cases that have more than one account in the analysis period (the x days)?.
If so, does the algorithm make any distinction between cases of  account  switching 
(zero days without active account, but 2 or more accounts) and cases where there are 
more than one day without account 
(example: 15 days analysis, day 2 – account #1 ends, day 5 –account #2 starts).


R22 Tibco:
Subject: EMETER.>
Network:
Daemon:ind-lnxapp49.emeter.com:7500    | localhost:7500
Daemon:ind-lnxapp231.emeter.com:7500    | localhost:7500
service:

ind-lnxapp49.emeter.com:7475/EnergyIPSystemConsole

savedMsg Path: C:\Users\ic033956\Desktop\tmp\77_tibco_msg


D:/home77/pipe/conf/systemProperties/prop.properties -Dapplication.name=Archiver -Dapplication.instanceId=1 -DjmxMonitoringClient.checkDependencies=false

************************************************************************************************

insert into rp_algorithm_def_param(wid,algorithm_def_wid,name,value)
values (36,5,'CORRELATION_COEFFICIENT','-0.75');

insert into rp_algorithm_def_param(wid,algorithm_def_wid,name,value)
values (35,5,'MEASUREMENT_WID','3');

insert into rp_algorithm_def_param(wid,algorithm_def_wid,name,value)
values (38,5,'NUM_DIGITS_IN_ZIP_CODE','8');

insert into rp_algorithm_def_param(wid,algorithm_def_wid,name,value)
values (37,5,'SDP_USAGE_PERCENT_COMPARED_TO_NEIGHBORS','20');


./ConfigurationManagement.sh -Dapplication.command=IncrementalImport -DsourceDirectory=/home/pipe/tools/confXML
./ReferenceDataUtil.sh -Dapplication.command=import -DimportReferenceDataService.referenceDataSourcePath=/home/pipe/opt/revenueprotection/


	
log4j.logger.com.eMeter.adf.realtime.intervalreads.ADFIntervalHandler DEBUG
log4j.logger.com.eMeter.adf.realtime.intervalreads.ADFIntervalReads8xHandler DEBUG


oozie job -oozie http://lnxcdh21.emeter.com:11000/oozie/ -kill 0000027-161122153755425-oozie-oozi-W


***************************
./meterDataBrowser.sh mtmorg171 436 -measTypeId=1032  -startDate=2014-09-10T00:00:00-07:00 -Dir=/home/eip/.keytabs -userPrincipal=mtmorg171@HADOOP.EMETER.COM

./meterDataBrowser.sh tborg1 

org mtmorg171
measTypeId: 1032  name: UAAPV KWH Interval1-60 | udc_id: UAAPV KWH Interval1-60

1032,="UAAPV KWH Interval1-60",="",="Electric",="",="",="N",,="Profile",3600,="Interval Data",1,="",="CT-PT",="UAAPV KWH Interval1-60",="Active Energy",="KWH",="",="Delivered",="",="",="IntervalData",="",="",="PV KWH Interval1-60",="",="",="",="",="N",="",="",="SOURCE1",6,52,12.09.16 10:31:10.000,52,52,12.09.16 10:31:10.000,1
sdp-436
meterUdcId: Exit_0000mtmretro1002 | id: 596

kill old UAAFileInboundAdapter
ApplicationLauncher.sh start UAAFileInboundAdapter 1 -Da31-uaav1FilePoller.maxPollIntervalMillis=10000 -Da31-uaav1DirAuthConfig.username=mtmorg171 -Da31-uaav1DirAuthConfig.password=mtmorg171 -Da31-uaav1DirAuthConfig.encrypted=false -Dlog4j.rootLogger=DEBUG,FileLog
ApplicationLauncher.sh start DeviceReadsProcessor 1 -Da5-meterDataArchiveService.changeLogEnabled=true -Da5-meterDataArchiveService.dimensionTaggingEnabled=true -Da5-meterDataArchiveService.changeLogLagDays=0

/home/eip/data/meterreads/uaav1/in


export HISTTIMEFORMAT="%d/%m/%y %T "

https://lnxapp171.emeter.com:9553/systemconsole
RegEx parsing format for file to be polled. <original file name>_EIP_<DateTimestamp>_<utilityid>_<CommunicationTechnologyId>.
[.0-9a-zA-Z_-]*\.xml_EIP_[.0-9a-zA-Z_-]*

ADF_LP_INTERVAL.xml_EIP_20161206172543_mtmorg171.xml
SELECT * FROM ami_adapter_meas WHERE meas_type_id = 148; 
--svcpt_id: 5503
--meter: 7043  | Entry_0000fgl4002  | Entry_0000fgl4002
--channel-10021 | measTypeId:  148 |   KWH 60 Interval Read
SELECT * FROM org_preference WHERE org_id=52;

/home/eip/data/meterreads/uaav1/in


add debugging for: 
No mapping dataFound for [channelId, meterId]:
-product mapping done in structural extract is not correct Or data does not exist

source: SOURCE1
sourceDetail: AMR


//EnergyIP/opt/em-ac-adf/release/1.1/db/DatabaseBuild/full/ddl/genddl.010000.sql
//EnergyIP/opt/em-ac-adf/release/1.1/db/DatabaseBuild/full/ddl/genddl.010100.sql


ALTER TABLE cx_product_name_mapping ADD ext_meas_cd varchar2(100) null;

//EnergyIP/core/trunk_7/eMeter/dw/adftransactionmonitor/tools/confXML

History date command
export HISTTIMEFORMAT="%d/%m/%y %T "

Working InterVal meterDataBrowser
---------------------------------
./meterDataBrowser.sh mtmorg171 5503 -measTypeId=148  -startDate=2014-04-30T00:00:00-07:00 -keyTabDir=/home/eip/.keytabs -userPrincipal=mtmorg171@HADOOP.EMETER.COM

eip@lnxapp171:/home/eip/opt/hadoop-common/bin->./meterDataBrowser.sh mtmorg171 5503 -measTypeId=148  -startDate=2014-04-30T00:00:00-07:00 -keyTabDir=/home/eip/.keytabs -userPrincipal=mtmorg171@HADOOP.EMETER.COM
log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).
log4j:WARN Please initialize the log4j system properly.
Connecting to tablename: mtmorg171:sdp
Got Table instance
Got Result
                                                                                                    
[MeasTypeId: 148]    [Date: 2016-10-30T21:00-07:00] and [Version Time: 2016-12-07T22:51-08:00]      
=====================================================================================================================================================================================
 |No. |     Interval End Time |          Version Time |   Value |Channel Id |Grid Area |Bal Pro |Ene Sup |Gen Type |Consum Type |Sdp Type |Chg Method |Flags |Fail Code |Val Status|
=====================================================================================================================================================================================
 |  1 |1969-12-31T16:00-08:00 |1969-12-31T16:00-08:00 |     0.0 |     10021 |     5501 |   2523 |   2524 |       0 |          1 |       2 |           |    1 |        0 |          |
 |  2 |1969-12-31T16:00-08:00 |1969-12-31T16:00-08:00 |     0.0 |     10021 |     5501 |   2523 |   2524 |       0 |          1 |       2 |           |    1 |        0 |          |
 |  3 |1969-12-31T16:00-08:00 |1969-12-31T16:00-08:00 |     0.0 |     10021 |     5501 |   2523 |   2524 |       0 |          1 |       2 |           |    1 |        0 |          |
 |  4 |1969-12-31T16:00-08:00 |1969-12-31T16:00-08:00 |     0.0 |     10021 |     5501 |   2523 |   2524 |       0 |          1 |       2 |           |    1 |        0 |          |
 |  5 |1969-12-31T16:00-08:00 |1969-12-31T16:00-08:00 |     0.0 |     10021 |     5501 |   2523 |   2524 |       0 |          1 |       2 |           |    1 |        0 |          |
 |  6 |1969-12-31T16:00-08:00 |1969-12-31T16:00-08:00 |     0.0 |     10021 |     5501 |   2523 |   2524 |       0 |          1 |       2 |           |    1 |        0 |          |
 |  7 |1969-12-31T16:00-08:00 |1969-12-31T16:00-08:00 |     0.0 |     10021 |     5501 |   2523 |   2524 |       0 |          1 |       2 |           |    1 |        0 |          |
 |  8 |1969-12-31T16:00-08:00 |1969-12-31T16:00-08:00 |     0.0 |     10021 |     5501 |   2523 |   2524 |       0 |          1 |       2 |           |    1 |        0 |          |
 |  9 |1969-12-31T16:00-08:00 |1969-12-31T16:00-08:00 |     0.0 |     10021 |     5501 |   2523 |   2524 |       0 |          1 |       2 |           |    1 |        0 |          |
 | 10 |1969-12-31T16:00-08:00 |1969-12-31T16:00-08:00 |     0.0 |     10021 |     5501 |   2523 |   2524 |       0 |          1 |       2 |           |    1 |        0 |          |
 | 11 |1969-12-31T16:00-08:00 |1969-12-31T16:00-08:00 |     0.0 |     10021 |     5501 |   2523 |   2524 |       0 |          1 |       2 |           |    1 |        0 |          |
 | 12 |1969-12-31T16:00-08:00 |1969-12-31T16:00-08:00 |     0.0 |     10021 |     5501 |   2523 |   2524 |       0 |          1 |       2 |           |    1 |        0 |          |
 | 13 |1969-12-31T16:00-08:00 |1969-12-31T16:00-08:00 |     0.0 |     10021 |     5501 |   2523 |   2524 |       0 |          1 |       2 |           |    1 |        0 |          |
 | 14 |1969-12-31T16:00-08:00 |1969-12-31T16:00-08:00 |     0.0 |     10021 |     5501 |   2523 |   2524 |       0 |          1 |       2 |           |    1 |        0 |          |
 | 15 |1969-12-31T16:00-08:00 |1969-12-31T16:00-08:00 |     0.0 |     10021 |     5501 |   2523 |   2524 |       0 |          1 |       2 |           |    1 |        0 |          |
 | 16 |1969-12-31T16:00-08:00 |1969-12-31T16:00-08:00 |     0.0 |     10021 |     5501 |   2523 |   2524 |       0 |          1 |       2 |           |    1 |        0 |          |
 | 17 |1969-12-31T16:00-08:00 |1969-12-31T16:00-08:00 |     0.0 |     10021 |     5501 |   2523 |   2524 |       0 |          1 |       2 |           |    1 |        0 |          |
 | 18 |1969-12-31T16:00-08:00 |1969-12-31T16:00-08:00 |     0.0 |     10021 |     5501 |   2523 |   2524 |       0 |          1 |       2 |           |    1 |        0 |          |
 | 19 |1969-12-31T16:00-08:00 |1969-12-31T16:00-08:00 |     0.0 |     10021 |     5501 |   2523 |   2524 |       0 |          1 |       2 |           |    1 |        0 |          |
 | 20 |1969-12-31T16:00-08:00 |1969-12-31T16:00-08:00 |     0.0 |     10021 |     5501 |   2523 |   2524 |       0 |          1 |       2 |           |    1 |        0 |          |
 | 21 |2016-10-31T18:00-07:00 |2016-12-07T22:51-08:00 |     7.1 |     10021 |     5501 |   2523 |   2524 |       0 |          1 |       2 |           |    0 |        0 |        NV|
 | 22 |1969-12-31T16:00-08:00 |1969-12-31T16:00-08:00 |     0.0 |     10021 |     5501 |   2523 |   2524 |       0 |          1 |       2 |           |    1 |        0 |          |
 | 23 |1969-12-31T16:00-08:00 |1969-12-31T16:00-08:00 |     0.0 |     10021 |     5501 |   2523 |   2524 |       0 |          1 |       2 |           |    1 |        0 |          |
 | 24 |1969-12-31T16:00-08:00 |1969-12-31T16:00-08:00 |     0.0 |     10021 |     5501 |   2523 |   2524 |       0 |          1 |       2 |           |    1 |        0 |          |
=====================================================================================================================================================================================
Program ran Successfully

working Register
----------------
./meterDataBrowser.sh mtmorg171 5503 -measTypeId=143 -startDate=2010-04-30T00:00:00-07:00 -keyTabDir=/home/eip/.keytabs -userPrincipal=mtmorg171@HADOOP.EMETER.COM

https://jira.emeter.com/browse/EIP-38546
https://jira.emeter.com/browse/CLDALYT-2221


./meterDataBrowser.sh mtmorg171 5503 -measTypeId=143 -startDate=2010-04-30T00:00:00-07:00 -keyTabDir=/home/eip/.keytabs -userPrincipal=mtmorg171@HADOOP.EMETER.COM

./meterDataBrowser.sh devorg1 5503 -measTypeId=143 -startDate=1970-04-30T00:00:00-07:00 -keyTabDir=/home/eip/.keytabs -userPrincipal=devorg1@HADOOP.EMETER.COM

//SnapShot
String type7x = sdpData.getType7x();
String subType7x = sdpData.getSubType();
String key = "[Type:" + type7x + " | SubType:" + subType7x + "]";
AtomicInteger counter = deviceTypeMap.get(key);
if(null == counter){
counter = new AtomicInteger();
counter.incrementAndGet();
deviceTypeMap.put(key, counter);
}else{
counter.incrementAndGet();
}


SELECT par_asset_id  FROM s_asset_rel rel
WHERE rel.x_asset_relation_type_cd = 'METER-CHANNEL'
AND rel.asset_id = ?
AND (
rel.meter_loc_start_dt <> rel.meter_loc_end_dt
AND rel.meter_loc_start_dt >= to_date(?,'yyyy-mm-dd hh24:mi:ss')
	 AND 
rel.meter_loc_end_dt <= to_date(?,'yyyy-mm-dd hh24:mi:ss')
)
OR (
rel.meter_loc_start_dt <> rel.meter_loc_end_dt
AND rel.meter_loc_start_dt >= to_date(?,'yyyy-mm-dd hh24:mi:ss')
AND rel.meter_loc_end_dt IS NULL 
);
			
			
			
********************************************************************
ADE 8x STructural data extraction
---------------------------------

For incremental structural extraction, I had a discussion with Shekher (platform team) regarding the design for capturing the BO changes.
BO team has done following changes for capturing  Add/Modify/Delete CDCI operations:
	BO team has customized code only for two newly created BOs i.e. explicit code is written to capture such changes for following two BOs
	o	RTU (Remote terminal unit) a data conc 
	o	PA (protocol adapter)
	The captured data is then written to following tables:
	o	BUS_OBJ_CUR_VERSION (New table to track macro BO version)
	o	BUS_OBJ_SUB_STATUS (New table to track subscription status for BO object)
	o	New table to track change events by PA

From high level it looks like that BUS_OBJ_CUR_VERSION table has the bo_id that can solve our requirement.

Concern:
All BOs does not have the custom code to capture Add/Modify/Delete CDCI operations. 
Once we have such changes at BO level only then we can implement the incremental structural extraction.
	
Please suggest.

-EIP8.4
-identify obj
-related obj

-BO changes ith cur version
-ebo-cdci
-update svcPt id in table 
-Shekher....... 

device-channel


parent BO and dep obj then we shld review
device-deviceFn


EnergyIP Data Extraction 1.1 supports initial and incremental extraction of:
<ul><li>Structural Data</li>
<li>Transactional Meter Data</li></ul>

For more details, see <a href="https://docs.emeter.com/display/EIPDE11/EnergyIP+Data+Extraction+1.1"> EnergyIP Data Extraction 1.1</a>. 

<b>Action Required:</b> No customer action is required.



---------Review release Notes
CLDALYT-2880(7x) 
EIP-38867(8x)

HBase entity shell
http://lnxapp198.emeter.com:8081/em-hc-rs/entity/sdp/1
---------------------------------------------------------
//EnergyIP/core/trunk/apps/shared/eip-seed-core/resources/org/common/refData/eip/core/EnumTypes/EnumTypeReferenceData_BOCode.xml
//EnergyIP/core/trunk/apps/shared/eip-seed-core/resources/org/common/refData/eip/core/EnumTypes/EnumTypeReferenceData_SubscriberCode.xml

com.emeter.ebo.asset.SvcPtBO

Asset data
	Premise - Done
	SvcPtGroup with param - Done
	Account with param - Done
	Device with param 
		Meter - Done
		CT-PT - Done
		Communication Module - Done
		Disconnect Collar - Done
	DistNode - done	

SDP Stack data
	Svcpt with param (Including Distnode ) - Done
	Channel (Including Virtual Channels) with param - Done
	Relationship
		SvcPt-SvcPt (Including DistNode-DistNode) - Done
		SvcPt-SvcPtGroup - Done
		Accnt-SvcPt - Done
		SvcPt-Device(meter, ctpt, commMod, DisconnectCollar) - 
		Device-Channel - Done
		Device-DeviceFunction - Done (Need to find active sdp with Meter)
		SvcPt-DataSvc - 
		SvcPt-SvcAgree -

-------------------------------------------------------------------------
SELECT * FROM org;

SELECT * FROM enum_type WHERE name = 'bo.code' and org_id = 52;
SELECT * FROM enum_value WHERE enum_type_id IN (SELECT id FROM enum_type WHERE name = 'bo.code' and org_id = 52) and org_id = 52 ORDER BY display_seq;

SELECT * FROM BUS_OBJ_CUR_VERSION ORDER BY BO_TYPE_CD;
--SELECT * FROM BUS_OBJ_SUB_STATUS;
SELECT * FROM svc_pt;
SELECT * FROM device;
SELECT * FROM device_class WHERE org_id=52;




               --3 4 5 10
               --6 7 8 9 

SELECT * FROM device WHERE org_id=52;

SELECT * FROM enum_value WHERE enum_type_id IN (SELECT id FROM enum_type WHERE name = 'bo.code' and org_id = 52);
--delete FROM enum_value WHERE enum_type_id IN (SELECT id FROM enum_type WHERE name = 'bo.code' and org_id = 52);
--DELETE FROM enum_type WHERE name = 'bo.code' and org_id = 52;
--DELETE FROM BUS_OBJ_CUR_VERSION;







--SELECT * FROM org;
--SELECT * FROM svc_pt_class WHERE org_id = 52;
--delete
DELETE FROM activity_attr WHERE org_id=52;
DELETE FROM accnt_param WHERE org_id=52;
DELETE FROM accnt_accnt_rel WHERE org_id=52;
DELETE FROM accnt_svc_pt_rel WHERE org_id=52;
DELETE FROM svc_pt_consumer_rel WHERE org_id=52;
DELETE FROM svc_pt_data_svc_attr WHERE org_id=52;
DELETE FROM svc_pt_data_svc_rel WHERE org_id=52;
DELETE FROM svc_pt_device_rel WHERE org_id=52;
DELETE FROM svc_pt_group_param WHERE org_id=52;
DELETE FROM svc_pt_group_rel WHERE org_id=52;
DELETE FROM svc_pt_group WHERE org_id=52;
DELETE FROM svc_pt_condition WHERE org_id=52;
DELETE FROM svc_pt_status WHERE org_id=52;
DELETE FROM svc_pt_param WHERE org_id=52;
DELETE FROM svc_pt_rel WHERE org_id=52;
DELETE FROM svc_pt_svc_agree_attr WHERE org_id=52;
DELETE FROM service_request_attr WHERE org_id=52;
DELETE FROM svc_pt_svc_agree_rel WHERE org_id=52;
DELETE FROM svc_agree_param WHERE org_id=52;
DELETE FROM svc_agree WHERE org_id=52;
DELETE FROM device_meas WHERE org_id=52;
DELETE FROM device_channel_rel WHERE org_id=52;
DELETE FROM device_comm_gateway_rel WHERE org_id=52;
DELETE FROM device_function_rel WHERE org_id=52;
DELETE FROM svc_pt_device_rel WHERE org_id=52;
DELETE FROM device_group WHERE org_id=52;
DELETE FROM device_multiplier WHERE org_id=52;
DELETE FROM device_param WHERE org_id=52;
DELETE FROM channel_param WHERE org_id=52;
DELETE FROM consumer_accnt_rel WHERE org_id=52;
DELETE FROM consumer_addr_rel WHERE org_id=52;
DELETE FROM consumer_consumer_rel WHERE org_id=52;
DELETE FROM consumer_contact WHERE org_id=52;
DELETE FROM consumer_param WHERE org_id=52;
DELETE FROM consumer_addr WHERE org_id=52;
DELETE FROM vc_formula WHERE org_id=52;
DELETE FROM activity WHERE org_id=52;
DELETE FROM SR_PENDING WHERE org_id=2;
DELETE FROM service_request WHERE org_id=52;
DELETE FROM device WHERE org_id=52;
DELETE FROM vc_contributor WHERE org_id=52;
DELETE FROM channel WHERE org_id=52;
DELETE FROM consumer WHERE org_id=52;
DELETE FROM accnt WHERE org_id=52;
DELETE FROM svc_pt WHERE org_id=52;
DELETE FROM premise_param WHERE org_id=52;
DELETE FROM premise WHERE org_id=52;
DELETE FROM inv_location WHERE org_id=52;
DELETE FROM inv_movement WHERE org_id=52;
DELETE FROM generic_locking;
DELETE FROM BUS_OBJ_CUR_VERSION;
COMMIT;



VPN link
ind-vpn.emeter.com 
If that doesn't work you can use US vpn link -> vpn.emeter.com 

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
RP2.4 questions:
-Feature 1: Grid loss detection - 
	-Understanding
		-Get the % loss of balanceNode for each SDP(for configurable period say 3 mnths) from GridLossDetection.
		-Compute the avg % loss for immediate parent
		-No measType required for this feature
		-Track care the rel sdp-distnode rel:
			-also avg % loss 
		-Threshold value check value is missing
		-No UI changes
		-
		
	?? We understand what feature will do but Need understanding on Grid Loss detection application output
	?? Why immediate parent is there hierarchy of parent
	?? Is this loss % is date specific, on what specifications can we fetch the % Loss
	?? Say in last 3 months we have 5 occurrences of % loss then output = avg = (l1 + l2 + l3 + l4 + l5 / 5)	
	?? Can 
	?? How RP is for non technical - top down, if there are lossy are in grid.... top down + bottom down
	
	
-Feature 2: Theft history
	-Understanding
		-Check fraud history table for configurable period a using sdpId and segmentName and analysis_date during analysis period
		-Calculate no of occurrences if SDP is involved in theft within X days 
		
	??
	-Use theft history (interval and register) ? we only have outcome not Register or Interval based ... dont have that granular info
	-UI-> No usage graph possible only scattered graph . Since we show usage graph per feature and data is of associated meatype only. 
			Since in this case  we will not have usageData  only icons related to theft history
		will discuss later	
	
	
	create table RP_INVTG_SDP_OUTCOME (
    SVC_PT_ID               number,
    ANALYSIS_DATE           timestamp(0),
    OUTCOME                 varchar2(200) not null,
    OUTCOME_DESC            varchar2(4000),
    ELIGIBLE                char(1) default 'Y',
    SEGMENT_NAME            varchar2(4000),
    ORG_ID                  number,
    INSERT_TIME             timestamp(0),
    INSERT_BY               number,
    LAST_UPD_TIME           timestamp(0),
    LAST_UPD_BY             number,
    REC_VERSION_NUM         number,
    constraint PK_RP_INVTG_SDP_OUTCOME primary key (SVC_PT_ID, ANALYSIS_DATE) using index local
) partition by range (ANALYSIS_DATE) interval ( &INTERVAL_LEN ) (
    partition P1 values less than (to_date('&START_YEAR.0101', 'yyyymmdd')));
	
-UI 
	- New - Theft history occurrences on Left panel + Theft history as an icon on usage graph
	
	
	-Feature 3 - Compare based on industrial classification - Interval reads
		-Understanding
			-SIC - standard industrial classification
			-Fetch SDP based on SIC / zip
			-For each SDP fetch its neighbors interval reads
		
		
		?? 
		-How to get the SIC code, where it is provided
		-If using SIC code case, how to handle SDP with no SIC code.. is there any default SIC that we should consider.
		-Expression score can work on fetaure output which is correlation coefficient, hence it cannot take into account usage. Usage can be part of feature only	
			-Expression scoring: if correlation coeff < 0.7 and usage > 20% of neighborhood usage then score is 3
		
	-UI 
		- New - Show sic code cohort in “neighborhood” tab. eg Bakers
			  - Show SIC usage graph below neighbor data only when SIC code exist for the the SDP

-compatibilty matrix suggest that RP 2.4 is compatible with HC1.2 | CDH 5.8	& Hortonworks 2.4

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Incremental transactional
Reads 
	-DTSReadsMonitor -> DTSRequestProcessor -> Core Export Adaptor
	
	DTS Meter Reads Data Flow
		-X -- DTS Queued Request Workflow (no need) as A DTS Request gets processed through the Queued Request Workflow if the 
			  Data Transfer Service Product parameter Queued Request is set to Y.
		
		-DTS Real Time Request Workflow (required)
		
	Flow:
		-DeviceReadsProcessor publishes NormalizedMeterData after inserting or updating any meter reads. 
		-DTSReadsMonitor listens to NormalizedMeterData for any updated normalized meter data that needs to be exported. 
		-The DTSRequestProcessor listens for DTS Request messages received by querying MUDR for the requested meter data and then forwards 
		 it to the configured ExportManager, which takes care of exporting the data to the configured Core Export Adapter.
			-Queued flow OR AdHoc request-- X
			-RealTime flow: It is used to immediately export the actual data archived by the DeviceReadsProcessor.
				-The application gets the value of Measurement Profile attribute.
				-The application filters the channels and removes all reads if the channel meas type does not belong to the 
				 Measurement Profile obtained in the previous step
				-Once the channels are filtered out, the application creates an ExportData object 
				 consisting of interval and register reads.
				-Based on the export adapter and export protocol, the application exports the data.
			-	
Questions:
?? What is the use of DTS Queued Request Workflow
Ans: A DTS Request gets processed through the Queued Request Workflow if the Data Transfer Service Product parameter Queued Request is set to Y.
Events
	-DTSEventsMonitor
	-DTSEventsProcessor
	
-Core Export Adapter

---------------------------------------------------------------------------------------------------------------------------------
Oracle sysdba pwd
SYSTEM/SYSTEM
or lower case


AAEINNOI00056L  a2w6h9q2
AAEINNOI00056L  a2w6h9q2
---------------------------------------------------------
JV -- ...96
	HC-1.2
	------
-Oozie changes
-lnxdev1042 | orgone1042 | EIP8.4
-driver changes - em-powerquality changes
	--spark changes version  -- dataFrame - accessing number like sdpid
	-Hive changes- reserved words changes
	-deprecated dataFRame load  (outageMomentry outage) 
	-dataFrame to Hive (df.save  to df.write)
https://wiki.emeter.com/display/ENG/Hadoop+Projects+Build+and+Deployment+Design

job def change for workflow dir changes
also fat jar refernce 

-HCCommon buildFile
-JobDef


db.xml
constant.java
TestSDPStackHelper
testEnv.properties


lnxapp64
cdhmg03

UAA.MeterReadHeaderNoun	String	MeterData	UAA header noun value for meter read file
UAA.MeterEventHeaderNoun	String	EventData	UAA header noun value for device event file
UAA.headerVerb	String	create	UAA header verb value
UAA.headerRevision	String	1	UAA header revision value

132.186.232.10